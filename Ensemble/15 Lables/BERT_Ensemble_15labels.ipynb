{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mj3gUkvvEZW2"
      },
      "source": [
        "### <font color='blue'>Import all packages</font> ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVqcG_Q_FX4T",
        "outputId": "3cca23c4-d008-4adb-9d08-e296b97ff9a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "#drive.flush_and_unmount()\n",
        "drive.mount('/content/drive')\n",
        "#drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "AwA6C92yJyaP",
        "outputId": "139581b7-df56-4add-c196-38b51ac61a6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 14.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 6.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 79.7 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 87.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.5.18.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.7.0 pyyaml-6.0 tokenizers-0.12.1 transformers-4.19.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 14.1 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.96\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==2.7.0\n",
            "  Downloading https://us-python.pkg.dev/colab-wheels/public/tensorflow/tensorflow-2.7.0%2Bzzzcolab20220506150900-cp37-cp37m-linux_x86_64.whl\n",
            "\u001b[K     \\ 665.5 MB 396 kB/s\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (4.2.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (0.37.1)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (2.8.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (0.2.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (14.0.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (3.1.0)\n",
            "Collecting tensorflow-estimator<2.8,~=2.7.0rc0\n",
            "  Downloading tensorflow_estimator-2.7.0-py2.py3-none-any.whl (463 kB)\n",
            "\u001b[K     |████████████████████████████████| 463 kB 14.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (2.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (1.1.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (1.14.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (1.6.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (3.3.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (1.0.0)\n",
            "Collecting keras<2.8,>=2.7.0rc0\n",
            "  Downloading keras-2.7.0-py2.py3-none-any.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 95.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (1.46.3)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (3.17.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (0.26.0)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (1.21.6)\n",
            "Collecting gast<0.5.0,>=0.2.1\n",
            "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow==2.7.0) (1.5.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (1.8.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (57.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (3.3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (2.23.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (1.35.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (1.0.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.7.0) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow==2.7.0) (4.11.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow==2.7.0) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0) (2022.5.18.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.7.0) (3.2.0)\n",
            "Installing collected packages: tensorflow-estimator, keras, gast, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.8.0\n",
            "    Uninstalling tensorflow-estimator-2.8.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.8.0\n",
            "    Uninstalling keras-2.8.0:\n",
            "      Successfully uninstalled keras-2.8.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.3\n",
            "    Uninstalling gast-0.5.3:\n",
            "      Successfully uninstalled gast-0.5.3\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.8.2+zzzcolab20220527125636\n",
            "    Uninstalling tensorflow-2.8.2+zzzcolab20220527125636:\n",
            "      Successfully uninstalled tensorflow-2.8.2+zzzcolab20220527125636\n",
            "Successfully installed gast-0.4.0 keras-2.7.0 tensorflow-2.7.0+zzzcolab20220506150900 tensorflow-estimator-2.7.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting stanza\n",
            "  Downloading stanza-1.4.0-py3-none-any.whl (574 kB)\n",
            "\u001b[K     |████████████████████████████████| 574 kB 14.2 MB/s \n",
            "\u001b[?25hCollecting emoji\n",
            "  Downloading emoji-1.7.0.tar.gz (175 kB)\n",
            "\u001b[K     |████████████████████████████████| 175 kB 91.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from stanza) (3.17.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from stanza) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from stanza) (4.64.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (from stanza) (4.19.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from stanza) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from stanza) (1.21.6)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from stanza) (1.11.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3.0->stanza) (4.2.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->stanza) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->stanza) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->stanza) (2022.5.18.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->stanza) (3.0.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers->stanza) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers->stanza) (3.7.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers->stanza) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers->stanza) (4.11.4)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers->stanza) (0.12.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers->stanza) (2019.12.20)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers->stanza) (0.7.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers->stanza) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers->stanza) (3.8.0)\n",
            "Building wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171046 sha256=1ac99f4aac6959b2f41770be1affc2cb27743ac08db8fab7320df62abc3b68da\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/4e/b6/57b01db010d17ef6ea9b40300af725ef3e210cb1acfb7ac8b6\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji, stanza\n",
            "Successfully installed emoji-1.7.0 stanza-1.4.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.19.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.5.18.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.17.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 14.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow-addons) (3.0.9)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.17.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting textacy\n",
            "  Downloading textacy-0.11.0-py3-none-any.whl (200 kB)\n",
            "\u001b[K     |████████████████████████████████| 200 kB 15.5 MB/s \n",
            "\u001b[?25hCollecting cytoolz>=0.10.1\n",
            "  Downloading cytoolz-0.11.2.tar.gz (481 kB)\n",
            "\u001b[K     |████████████████████████████████| 481 kB 86.4 MB/s \n",
            "\u001b[?25hCollecting spacy>=3.0.0\n",
            "  Downloading spacy-3.3.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.2 MB 90.2 MB/s \n",
            "\u001b[?25hCollecting jellyfish>=0.8.0\n",
            "  Downloading jellyfish-0.9.0.tar.gz (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 95.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.19.6 in /usr/local/lib/python3.7/dist-packages (from textacy) (4.64.0)\n",
            "Requirement already satisfied: joblib>=0.13.0 in /usr/local/lib/python3.7/dist-packages (from textacy) (1.1.0)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from textacy) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.19.0 in /usr/local/lib/python3.7/dist-packages (from textacy) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from textacy) (1.21.6)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from textacy) (2.6.3)\n",
            "Collecting pyphen>=0.10.0\n",
            "  Downloading pyphen-0.12.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0 MB 70.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from textacy) (2.23.0)\n",
            "Requirement already satisfied: cachetools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from textacy) (4.2.4)\n",
            "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from cytoolz>=0.10.1->textacy) (0.11.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->textacy) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->textacy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->textacy) (2022.5.18.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->textacy) (2.10)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.0->textacy) (3.1.0)\n",
            "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
            "  Downloading spacy_loggers-1.0.2-py3-none-any.whl (7.2 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (57.4.0)\n",
            "Collecting srsly<3.0.0,>=2.4.3\n",
            "  Downloading srsly-2.4.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (457 kB)\n",
            "\u001b[K     |████████████████████████████████| 457 kB 91.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (21.3)\n",
            "Collecting catalogue<2.1.0,>=2.0.6\n",
            "  Downloading catalogue-2.0.7-py3-none-any.whl (17 kB)\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.9\n",
            "  Downloading spacy_legacy-3.0.9-py2.py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (0.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (2.11.3)\n",
            "Collecting typing-extensions<4.0.0.0,>=3.7.4\n",
            "  Downloading typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\n",
            "Collecting typer<0.5.0,>=0.3.0\n",
            "  Downloading typer-0.4.1-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (0.9.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (2.0.6)\n",
            "Collecting thinc<8.1.0,>=8.0.14\n",
            "  Downloading thinc-8.0.17-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (660 kB)\n",
            "\u001b[K     |████████████████████████████████| 660 kB 77.7 MB/s \n",
            "\u001b[?25hCollecting langcodes<4.0.0,>=3.2.0\n",
            "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 91.7 MB/s \n",
            "\u001b[?25hCollecting pathy>=0.3.5\n",
            "  Downloading pathy-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (1.0.7)\n",
            "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
            "  Downloading pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 55.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy>=3.0.0->textacy) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy>=3.0.0->textacy) (3.0.9)\n",
            "Collecting smart-open<6.0.0,>=5.0.0\n",
            "  Downloading smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 6.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy>=3.0.0->textacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy>=3.0.0->textacy) (2.0.1)\n",
            "Building wheels for collected packages: cytoolz, jellyfish\n",
            "  Building wheel for cytoolz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cytoolz: filename=cytoolz-0.11.2-cp37-cp37m-linux_x86_64.whl size=1236736 sha256=936f6d6bd6b2c1cae5b6462c72ec4a4acdba22a3e35b7354e74301500d966f92\n",
            "  Stored in directory: /root/.cache/pip/wheels/38/70/71/ca13ea3d36ccd0b3d0ec7d7a4ca67522048d695b556bba4f59\n",
            "  Building wheel for jellyfish (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jellyfish: filename=jellyfish-0.9.0-cp37-cp37m-linux_x86_64.whl size=74009 sha256=3038301969a806839b2eb15c645859b55cb906b3736e06097576ca1a6a379319\n",
            "  Stored in directory: /root/.cache/pip/wheels/fe/99/4e/646ce766df0d070b0ef04db27aa11543e2767fda3075aec31b\n",
            "Successfully built cytoolz jellyfish\n",
            "Installing collected packages: typing-extensions, catalogue, typer, srsly, smart-open, pydantic, thinc, spacy-loggers, spacy-legacy, pathy, langcodes, spacy, pyphen, jellyfish, cytoolz, textacy\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing-extensions 4.2.0\n",
            "    Uninstalling typing-extensions-4.2.0:\n",
            "      Successfully uninstalled typing-extensions-4.2.0\n",
            "  Attempting uninstall: catalogue\n",
            "    Found existing installation: catalogue 1.0.0\n",
            "    Uninstalling catalogue-1.0.0:\n",
            "      Successfully uninstalled catalogue-1.0.0\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 1.0.5\n",
            "    Uninstalling srsly-1.0.5:\n",
            "      Successfully uninstalled srsly-1.0.5\n",
            "  Attempting uninstall: smart-open\n",
            "    Found existing installation: smart-open 6.0.0\n",
            "    Uninstalling smart-open-6.0.0:\n",
            "      Successfully uninstalled smart-open-6.0.0\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "Successfully installed catalogue-2.0.7 cytoolz-0.11.2 jellyfish-0.9.0 langcodes-3.3.0 pathy-0.6.1 pydantic-1.8.2 pyphen-0.12.0 smart-open-5.2.1 spacy-3.3.0 spacy-legacy-3.0.9 spacy-loggers-1.0.2 srsly-2.4.3 textacy-0.11.0 thinc-8.0.17 typer-0.4.1 typing-extensions-3.10.0.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "typing_extensions"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "!pip install tensorflow==2.7.0\n",
        "!pip install stanza\n",
        "!pip install transformers\n",
        "!pip install tensorflow-addons\n",
        "!pip install nltk\n",
        "!pip install textacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jV9qKCQ3qpl",
        "outputId": "f57b2ab1-1128-4488-a33e-473d2b51b4e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.7.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "DC59AD4KEZW7"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from textacy.datasets.supreme_court import SupremeCourt\n",
        "import numpy as np\n",
        "import re\n",
        "import unicodedata\n",
        "import nltk\n",
        "#from transformers import pipeline\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Dense,Dropout, Input, BatchNormalization\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "from sklearn.metrics import confusion_matrix,f1_score,classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "from sklearn.utils import shuffle\n",
        "from tensorflow.keras import regularizers\n",
        "#from transformers import *\n",
        "from transformers import BertTokenizer, TFBertModel, BertConfig,TFDistilBertModel,DistilBertTokenizer,DistilBertConfig\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, TFAutoModel\n",
        "import numpy as np\n",
        "import gc\n",
        "import math\n",
        "import json\n",
        "import stanza\n",
        "from tensorflow.keras import *\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import *\n",
        "import tensorflow.keras.backend as K\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import classification_report\n",
        "from transformers import TFRobertaModel,RobertaTokenizer\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.initializers import RandomUniform\n",
        "\n",
        "from numpy.random import seed\n",
        "import random as python_random\n",
        "import os\n",
        "import sys\n",
        "\n",
        "np.random.seed(1)\n",
        "python_random.seed(1)\n",
        "tf.random.set_seed(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "OzfozpOUBkOX"
      },
      "outputs": [],
      "source": [
        "!cp \"/content/drive/My Drive/summarized_sc.txt\" \"./summarized_sc.txt\"\n",
        "!cp \"/content/drive/My Drive/labels_sc.txt\" \"./labels_sc.txt\"\n",
        "!cp \"/content/drive/My Drive/labels_sc_279.txt\" \"./labels_sc_279.txt\"\n",
        "!cp \"/content/drive/My Drive/sc_model_0.txt\" \"./sc_model_0.txt\"\n",
        "!cp \"/content/drive/My Drive/sc_model_1.txt\" \"./sc_model_1.txt\"\n",
        "!cp \"/content/drive/My Drive/sc_model_2.txt\" \"./sc_model_2.txt\"\n",
        "!cp \"/content/drive/My Drive/sc_model_3.txt\" \"./sc_model_3.txt\"\n",
        "!cp \"/content/drive/My Drive/sc_model_4.txt\" \"./sc_model_4.txt\"\n",
        "!cp \"/content/drive/My Drive/sc_model_5.txt\" \"./sc_model_5.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cVKqFgEEWBz",
        "outputId": "8afbb0bf-e0ea-4508-8e74-8fba074bcec8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8419\n",
            "8419\n",
            "Average Length 489.8335906877301\n",
            "['8', '1', '8', '2', '8', '8', '8', '9', '7', '8', '1', '2', '1', '1', '8', '4', '8', '8', '12', '3', '3', '7', '3', '12', '1', '8', '8', '8', '8', '8', '8', '8', '1', '9', '5', '9', '9', '9', '11', '8', '8', '8', '4', '8', '8', '8', '8', '1', '3', '9', '3', '7', '1', '2', '9', '9', '7', '8', '8', '1', '10', '7', '8', '8', '9', '8', '7', '9', '9', '12', '7', '2', '8', '1', '11', '11', '1', '7', '7', '12', '1', '9', '8', '10', '12', '7', '8', '2', '8', '9', '9', '1', '8', '9', '1', '7', '12', '10', '10', '10', '8', '3', '7', '9', '8', '9', '1', '8', '8', '2', '7', '2', '9', '9', '11', '8', '8', '12', '12', '7', '8', '12', '4', '9', '3', '1', '12', '1', '1', '8', '8', '3', '8', '8', '8', '8', '9', '1', '8', '8', '10', '1', '8', '2', '8', '8', '7', '3', '8', '2', '4', '4', '9', '8', '10', '12', '12', '1', '1', '9', '1', '1', '1', '8', '2', '2', '8', '1', '1', '2', '2', '1', '2', '8', '1', '9', '9', '8', '8', '4', '2', '9', '9', '8', '3', '4', '3', '1', '8', '8', '2', '1', '9', '7', '8', '8', '1', '12', '3', '8', '2', '4', '2', '9', '12', '1', '4', '1', '8', '8', '8', '2', '2', '8', '9', '8', '8', '8', '10', '9', '8', '7', '9', '1', '1', '9', '4', '2', '4', '2', '2', '1', '7', '8', '11', '11', '3', '9', '2', '4', '8', '9', '1', '8', '1', '1', '4', '9', '1', '1', '8', '8', '2', '1', '8', '4', '2', '8', '9', '8', '8', '2', '8', '8', '8', '7', '1', '1', '1', '2', '1', '1', '8', '7', '8', '8', '12', '2', '12', '12', '8', '10', '12', '8', '3', '3', '12', '10', '1', '8', '12', '1', '8', '8', '2', '8', '4', '7', '8', '7', '10', '8', '10', '9', '8', '12', '12', '1', '8', '8', '3', '8', '8', '8', '8', '8', '1', '9', '8', '11', '1', '1', '1', '9', '8', '1', '9', '2', '3', '11', '8', '9', '9', '9', '2', '1', '8', '8', '9', '7', '1', '4', '9', '4', '8', '8', '4', '8', '12', '9', '4', '8', '2', '10', '10', '10', '8', '9', '9', '8', '8', '12', '7', '1', '8', '8', '8', '4', '1', '1', '1', '1', '1', '1', '8', '1', '9', '8', '9', '9', '4', '8', '12', '9', '8', '8', '2', '8', '8', '8', '6', '9', '8', '3', '7', '8', '8', '4', '12', '8', '8', '9', '12', '12', '9', '8', '2', '9', '2', '3', '1', '12', '8', '10', '9', '9', '9', '10', '10', '3', '8', '12', '1', '4', '2', '1', '10', '8', '2', '8', '4', '8', '9', '1', '9', '9', '10', '10', '1', '4', '9', '2', '4', '9', '1', '1', '3', '10', '3', '3', '8', '7', '3', '8', '9', '9', '12', '4', '8', '12', '2', '2', '4', '1', '9', '9', '4', '1', '4', '2', '8', '12', '2', '3', '10', '10', '9', '8', '9', '9', '1', '12', '8', '8', '8', '12', '4', '1', '8', '8', '1', '9', '8', '8', '2', '1', '8', '9', '8', '3', '3', '3', '1', '8', '8', '9', '1', '10', '9', '9', '9', '9', '5', '9', '9', '8', '8', '8', '8', '8', '8', '8', '8', '11', '12', '8', '8', '1', '8', '9', '11', '2', '2', '2', '2', '3', '1', '2', '2', '8', '2', '4', '9', '1', '2', '9', '8', '2', '8', '9', '9', '3', '10', '9', '9', '2', '8', '9', '8', '12', '12', '1', '3', '8', '8', '8', '2', '7', '7', '7', '7', '3', '9', '1', '9', '8', '9', '9', '1', '1', '1', '2', '9', '9', '9', '11', '1', '8', '8', '9', '1', '9', '8', '8', '8', '1', '1', '8', '7', '1', '1', '8', '8', '9', '4', '4', '8', '2', '2', '8', '8', '8', '8', '8', '8', '11', '8', '2', '9', '4', '9', '3', '9', '9', '1', '3', '9', '3', '1', '12', '8', '9', '12', '1', '8', '4', '2', '1', '4', '8', '3', '3', '8', '2', '8', '9', '7', '8', '8', '8', '5', '8', '3', '9', '8', '8', '13', '12', '1', '1', '2', '8', '4', '1', '9', '9', '12', '8', '9', '12', '9', '1', '9', '9', '9', '9', '3', '2', '9', '9', '4', '8', '12', '2', '4', '9', '3', '1', '9', '7', '8', '9', '9', '8', '4', '8', '8', '7', '9', '10', '3', '8', '8', '8', '1', '1', '1', '1', '8', '8', '4', '1', '10', '1', '5', '7', '7', '1', '8', '9', '3', '7', '2', '7', '7', '2', '4', '8', '12', '7', '4', '2', '9', '9', '12', '6', '10', '8', '2', '4', '12', '9', '9', '3', '8', '8', '1', '2', '10', '9', '9', '8', '4', '12', '2', '1', '8', '8', '8', '12', '10', '10', '9', '3', '8', '8', '9', '2', '8', '10', '1', '1', '1', '1', '2', '1', '1', '1', '1', '9', '8', '12', '9', '4', '8', '8', '9', '1', '9', '3', '9', '8', '8', '1', '7', '7', '10', '1', '8', '8', '1', '9', '8', '10', '3', '1', '7', '1', '8', '8', '12', '8', '8', '1', '8', '7', '1', '7', '7', '8', '2', '1', '8', '8', '2', '10', '8', '8', '8', '8', '8', '10', '1', '8', '8', '12', '8', '3', '3', '2', '2', '2', '10', '8', '8', '8', '2', '9', '1', '8', '9', '3', '2', '8', '10', '8', '6', '1', '1', '8', '4', '1', '9', '10', '8', '1', '7', '1', '2', '8', '1', '1', '1', '12', '1', '9', '12', '8', '12', '12', '12', '8', '8', '12', '4', '8', '8', '8', '8', '9', '9', '1', '3', '3', '3', '3', '1', '12', '12', '9', '10', '8', '8', '1', '9', '2', '2', '13', '9', '8', '9', '2', '1', '9', '1', '8', '8', '8', '4', '8', '1', '1', '1', '12', '12', '7', '2', '2', '2', '8', '3', '8', '9', '2', '10', '7', '8', '9', '2', '1', '2', '12', '12', '8', '8', '9', '2', '2', '9', '11', '1', '8', '1', '10', '9', '2', '1', '4', '7', '7', '7', '7', '7', '12', '8', '8', '8', '1', '1', '10', '1', '12', '1', '8', '2', '1', '1', '12', '8', '7', '9', '12', '8', '9', '3', '9', '8', '8', '8', '8', '3', '11', '2', '2', '9', '8', '8', '10', '8', '2', '7', '3', '1', '4', '7', '8', '8', '1', '8', '3', '7', '12', '8', '10', '9', '9', '8', '8', '2', '8', '9', '9', '1', '2', '8', '8', '9', '8', '3', '8', '1', '8', '10', '9', '8', '9', '9', '12', '4', '4', '8', '9', '9', '8', '2', '10', '1', '2', '8', '9', '1', '9', '9', '9', '7', '12', '12', '8', '1', '1', '1', '1', '8', '3', '1', '1', '8', '1', '8', '8', '7', '8', '8', '8', '8', '3', '2', '2', '10', '10', '10', '7', '8', '1', '2', '12', '7', '9', '8', '7', '8', '12', '2', '8', '9', '2', '6', '6', '7', '9', '8', '1', '8', '9', '8', '1', '12', '1', '2', '8', '7', '7', '7', '8', '2', '2', '8', '1', '2', '2', '9', '9', '1', '8', '8', '4', '3', '3', '1', '6', '3', '3', '12', '3', '8', '9', '1', '4', '3', '1', '8', '3', '9', '2', '8', '2', '8', '8', '8', '1', '1', '1', '9', '9', '8', '1', '9', '8', '1', '1', '3', '10', '8', '1', '1', '3', '9', '1', '4', '4', '1', '8', '9', '9', '2', '0', '0', '1', '8', '3', '1', '8', '8', '9', '8', '8', '1', '1', '8', '9', '8', '8', '8', '7', '9', '8', '8', '8', '10', '9', '8', '1', '2', '6', '1', '9', '9', '8', '12', '12', '12', '8', '8', '2', '8', '1', '2', '2', '2', '1', '9', '8', '2', '12', '2', '8', '12', '8', '9', '8', '8', '9', '7', '1', '1', '1', '1', '1', '8', '8', '1', '8', '8', '1', '1', '3', '2', '8', '8', '9', '10', '10', '2', '2', '1', '9', '2', '9', '9', '4', '12', '12', '12', '10', '7', '3', '3', '4', '2', '2', '9', '2', '8', '4', '2', '4', '1', '10', '9', '7', '8', '7', '1', '1', '3', '3', '1', '1', '3', '3', '3', '1', '1', '1', '1', '8', '2', '3', '1', '1', '2', '8', '8', '12', '8', '8', '8', '8', '11', '9', '1', '8', '9', '2', '8', '8', '8', '3', '9', '1', '9', '2', '7', '2', '8', '2', '8', '10', '8', '1', '10', '1', '1', '9', '9', '8', '8', '1', '8', '8', '8', '12', '8', '8', '8', '1', '8', '8', '8', '1', '9', '1', '1', '8', '1', '8', '9', '8', '2', '12', '9', '9', '0', '1', '8', '8', '1', '8', '12', '8', '8', '10', '8', '8', '8', '7', '8', '1', '8', '7', '3', '10', '1', '8', '9', '1', '8', '8', '8', '10', '1', '10', '3', '9', '1', '8', '9', '2', '8', '3', '3', '9', '9', '7', '9', '1', '1', '9', '2', '1', '1', '1', '7', '1', '1', '8', '8', '1', '1', '8', '1', '8', '3', '12', '9', '3', '3', '8', '8', '8', '8', '3', '1', '3', '3', '1', '11', '0', '8', '8', '7', '8', '12', '1', '8', '9', '8', '9', '8', '8', '3', '8', '8', '1', '1', '1', '9', '2', '2', '2', '8', '7', '12', '8', '8', '9', '10', '10', '7', '8', '1', '9', '8', '7', '3', '1', '3', '8', '2', '2', '3', '9', '8', '4', '4', '8', '9', '2', '1', '1', '7', '8', '9', '9', '7', '8', '7', '7', '8', '2', '2', '8', '4', '9', '7', '10', '0', '9', '8', '3', '7', '8', '1', '1', '8', '9', '9', '2', '2', '10', '1', '9', '10', '10', '10', '8', '3', '2', '12', '9', '9', '10', '12', '9', '12', '12', '9', '1', '2', '4', '12', '12', '7', '8', '9', '7', '7', '7', '3', '9', '8', '9', '1', '12', '8', '9', '4', '1', '3', '12', '12', '12', '12', '8', '8', '2', '1', '1', '2', '1', '1', '1', '12', '12', '8', '12', '2', '2', '12', '3', '3', '12', '8', '2', '8', '8', '12', '2', '1', '10', '3', '2', '8', '7', '1', '8', '1', '3', '7', '8', '9', '8', '3', '1', '1', '7', '8', '8', '9', '8', '2', '9', '2', '2', '9', '8', '1', '8', '8', '1', '3', '3', '1', '1', '10', '1', '2', '8', '1', '1', '1', '1', '9', '1', '4', '1', '7', '7', '7', '7', '2', '2', '8', '8', '12', '1', '9', '1', '7', '3', '3', '1', '8', '8', '10', '8', '9', '2', '9', '1', '3', '8', '8', '3', '12', '2', '8', '12', '2', '9', '1', '3', '3', '3', '3', '2', '8', '7', '9', '8', '3', '3', '1', '7', '8', '3', '1', '1', '12', '8', '9', '1', '2', '3', '8', '1', '1', '3', '3', '9', '1', '1', '1', '12', '1', '7', '3', '3', '1', '8', '8', '8', '1', '2', '4', '8', '1', '10', '2', '5', '3', '3', '12', '10', '9', '9', '12', '9', '0', '2', '8', '8', '9', '9', '9', '8', '1', '3', '1', '1', '4', '8', '1', '10', '8', '7', '2', '8', '2', '8', '4', '7', '8', '1', '9', '1', '9', '8', '2', '8', '2', '7', '9', '2', '2', '9', '1', '8', '12', '1', '8', '1', '4', '1', '9', '9', '1', '10', '12', '4', '8', '1', '7', '3', '9', '2', '12', '7', '8', '8', '2', '1', '12', '9', '8', '1', '2', '2', '8', '10', '2', '1', '7', '7', '7', '12', '3', '3', '8', '3', '8', '8', '3', '9', '8', '9', '1', '1', '8', '7', '9', '3', '3', '8', '1', '0', '9', '9', '9', '1', '8', '9', '9', '10', '1', '8', '7', '8', '8', '8', '9', '9', '8', '9', '10', '4', '9', '3', '7', '12', '1', '9', '9', '8', '9', '1', '9', '3', '1', '8', '4', '12', '10', '9', '8', '7', '10', '8', '12', '12', '3', '10', '8', '12', '2', '1', '2', '3', '9', '8', '8', '7', '1', '1', '2', '2', '1', '2', '2', '7', '1', '3', '9', '9', '3', '8', '8', '8', '8', '8', '7', '8', '8', '10', '8', '1', '8', '2', '8', '2', '2', '2', '2', '2', '1', '7', '12', '10', '1', '2', '8', '1', '4', '7', '8', '1', '12', '8', '7', '9', '2', '2', '2', '8', '1', '8', '1', '1', '0', '2', '2', '6', '1', '8', '2', '1', '8', '1', '1', '2', '2', '3', '2', '2', '10', '10', '9', '1', '7', '7', '8', '8', '1', '12', '10', '12', '3', '8', '8', '8', '3', '8', '3', '10', '2', '2', '2', '1', '2', '2', '1', '0', '1', '8', '9', '1', '1', '1', '2', '2', '2', '2', '2', '1', '1', '1', '1', '12', '9', '9', '1', '10', '11', '12', '9', '1', '1', '8', '8', '1', '2', '12', '8', '3', '7', '2', '10', '9', '7', '8', '1', '2', '2', '7', '9', '12', '2', '2', '1', '8', '9', '2', '3', '2', '2', '2', '9', '9', '8', '9', '2', '3', '8', '8', '9', '7', '3', '10', '8', '8', '9', '1', '1', '2', '8', '1', '10', '8', '8', '9', '7', '1', '7', '12', '8', '1', '7', '2', '1', '3', '9', '2', '8', '8', '1', '2', '3', '9', '3', '8', '9', '7', '7', '1', '10', '2', '8', '8', '8', '1', '8', '2', '8', '8', '1', '2', '8', '10', '2', '1', '0', '8', '3', '8', '10', '10', '12', '3', '8', '9', '3', '10', '8', '8', '8', '1', '8', '8', '2', '2', '2', '2', '2', '2', '1', '1', '10', '1', '10', '8', '2', '1', '2', '2', '8', '3', '3', '2', '2', '1', '8', '1', '3', '1', '9', '2', '2', '1', '3', '2', '2', '3', '2', '2', '3', '2', '1', '1', '2', '1', '2', '7', '8', '2', '1', '12', '12', '3', '1', '9', '9', '7', '8', '7', '2', '7', '9', '2', '2', '2', '8', '9', '7', '9', '2', '8', '9', '9', '2', '9', '1', '1', '9', '8', '3', '3', '8', '2', '7', '8', '11', '2', '8', '9', '1', '8', '3', '1', '2', '1', '2', '2', '2', '2', '8', '3', '9', '2', '10', '9', '9', '2', '7', '7', '7', '1', '3', '8', '8', '1', '1', '8', '7', '1', '8', '8', '9', '3', '3', '3', '2', '4', '8', '12', '8', '1', '12', '9', '2', '8', '2', '12', '2', '1', '3', '8', '12', '12', '11', '8', '10', '9', '2', '9', '4', '10', '1', '12', '12', '9', '3', '8', '9', '12', '9', '9', '2', '2', '2', '3', '5', '1', '1', '1', '7', '7', '8', '2', '2', '8', '2', '1', '7', '3', '2', '9', '8', '9', '1', '1', '9', '8', '9', '3', '3', '2', '9', '4', '9', '8', '8', '1', '8', '10', '2', '0', '3', '8', '8', '8', '9', '12', '9', '8', '2', '2', '8', '10', '9', '8', '2', '2', '4', '1', '9', '9', '8', '8', '10', '3', '1', '1', '2', '8', '7', '8', '2', '1', '1', '8', '12', '2', '9', '4', '8', '3', '3', '3', '8', '2', '12', '12', '8', '9', '12', '8', '2', '12', '7', '9', '2', '2', '1', '3', '1', '3', '11', '9', '8', '1', '10', '2', '12', '3', '8', '4', '1', '2', '8', '8', '3', '9', '3', '8', '7', '1', '1', '3', '1', '8', '2', '9', '8', '12', '1', '1', '1', '9', '9', '1', '8', '8', '8', '2', '2', '12', '3', '1', '1', '1', '9', '9', '3', '2', '1', '1', '9', '9', '1', '9', '9', '1', '9', '3', '9', '1', '8', '1', '9', '8', '3', '8', '8', '9', '9', '8', '2', '8', '1', '2', '2', '10', '1', '2', '1', '1', '9', '1', '9', '3', '7', '7', '2', '10', '3', '3', '1', '1', '9', '1', '2', '1', '3', '2', '1', '1', '1', '1', '1', '1', '1', '1', '1', '2', '8', '8', '7', '1', '1', '8', '8', '2', '1', '12', '1', '8', '1', '9', '8', '8', '2', '1', '9', '9', '2', '2', '8', '1', '7', '7', '9', '8', '1', '6', '8', '8', '2', '2', '8', '3', '8', '3', '2', '3', '8', '1', '2', '2', '2', '8', '9', '9', '9', '12', '1', '9', '2', '1', '9', '2', '9', '9', '10', '8', '1', '1', '9', '2', '1', '7', '1', '3', '7', '1', '1', '1', '3', '8', '8', '8', '9', '9', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '1', '0', '1', '1', '1', '2', '2', '3', '1', '1', '2', '3', '3', '3', '7', '3', '1', '9', '3', '9', '1', '4', '1', '1', '9', '10', '9', '8', '2', '3', '10', '9', '3', '10', '12', '8', '9', '8', '2', '1', '9', '8', '1', '7', '8', '1', '9', '1', '2', '10', '7', '7', '8', '1', '3', '3', '9', '3', '3', '1', '1', '1', '9', '1', '3', '8', '8', '8', '1', '8', '1', '9', '7', '8', '8', '2', '1', '3', '8', '2', '1', '6', '1', '8', '8', '2', '8', '2', '2', '9', '8', '1', '1', '6', '8', '6', '10', '2', '1', '1', '8', '9', '3', '3', '3', '1', '3', '9', '6', '8', '1', '8', '3', '1', '2', '2', '12', '7', '1', '1', '1', '1', '12', '1', '8', '8', '3', '2', '1', '9', '2', '3', '3', '2', '2', '7', '2', '2', '2', '3', '1', '9', '8', '7', '1', '1', '3', '1', '9', '4', '3', '9', '2', '1', '1', '9', '8', '8', '7', '2', '1', '3', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '2', '10', '1', '1', '8', '2', '8', '1', '8', '1', '3', '1', '2', '1', '1', '3', '1', '2', '1', '1', '1', '1', '1', '1', '1', '2', '1', '8', '1', '9', '8', '8', '1', '10', '8', '8', '1', '3', '9', '8', '9', '8', '2', '1', '3', '4', '1', '12', '1', '9', '8', '2', '7', '2', '1', '2', '9', '2', '1', '3', '8', '1', '2', '3', '8', '2', '10', '10', '10', '9', '9', '9', '3', '8', '3', '1', '1', '1', '1', '1', '1', '2', '1', '9', '2', '7', '9', '8', '7', '9', '1', '1', '1', '1', '8', '2', '2', '3', '3', '2', '12', '8', '3', '1', '1', '12', '9', '8', '2', '2', '9', '1', '2', '2', '9', '1', '1', '2', '4', '8', '1', '8', '10', '2', '9', '9', '9', '1', '2', '1', '1', '2', '2', '2', '12', '1', '2', '8', '3', '9', '9', '3', '2', '9', '9', '7', '2', '8', '8', '9', '2', '9', '1', '1', '1', '1', '1', '2', '8', '2', '1', '2', '2', '8', '3', '1', '1', '9', '1', '3', '10', '7', '9', '8', '8', '2', '2', '9', '2', '2', '2', '2', '2', '2', '9', '1', '3', '6', '4', '2', '2', '9', '1', '9', '8', '1', '1', '1', '7', '3', '2', '9', '9', '1', '9', '7', '2', '8', '9', '9', '12', '10', '8', '10', '2', '3', '1', '2', '2', '12', '12', '3', '12', '1', '2', '1', '2', '1', '2', '8', '2', '3', '12', '12', '8', '2', '2', '9', '2', '9', '2', '3', '3', '1', '1', '1', '12', '3', '7', '1', '3', '1', '2', '9', '2', '7', '2', '1', '8', '3', '7', '8', '1', '1', '3', '8', '3', '2', '9', '3', '1', '1', '1', '1', '11', '1', '2', '6', '1', '2', '3', '8', '9', '9', '2', '9', '2', '1', '1', '3', '1', '12', '9', '1', '1', '9', '1', '9', '9', '8', '9', '8', '1', '2', '7', '1', '0', '9', '8', '2', '3', '2', '4', '1', '1', '1', '8', '1', '12', '1', '2', '3', '3', '9', '9', '9', '9', '9', '7', '9', '3', '9', '1', '7', '3', '3', '3', '3', '7', '8', '2', '2', '2', '8', '2', '3', '9', '9', '3', '1', '8', '9', '12', '8', '8', '3', '2', '1', '6', '1', '9', '1', '1', '1', '1', '1', '1', '2', '2', '1', '2', '2', '2', '2', '9', '2', '5', '3', '2', '2', '8', '2', '1', '8', '3', '3', '8', '3', '1', '3', '9', '2', '8', '9', '4', '4', '9', '8', '8', '7', '7', '3', '1', '8', '8', '3', '2', '2', '10', '3', '3', '2', '2', '2', '2', '12', '3', '1', '2', '7', '7', '12', '2', '1', '2', '1', '1', '2', '1', '3', '3', '3', '1', '3', '9', '4', '8', '8', '4', '2', '4', '2', '8', '9', '1', '1', '2', '2', '2', '8', '7', '2', '8', '2', '9', '1', '8', '1', '1', '2', '1', '1', '1', '2', '9', '9', '8', '1', '8', '1', '4', '8', '1', '2', '7', '9', '2', '1', '8', '9', '8', '9', '1', '1', '2', '3', '2', '12', '7', '1', '2', '4', '4', '4', '2', '2', '2', '9', '8', '8', '1', '12', '3', '3', '2', '2', '3', '12', '1', '5', '1', '1', '3', '2', '8', '8', '2', '2', '2', '3', '9', '8', '8', '9', '8', '8', '11', '8', '2', '9', '2', '3', '1', '7', '1', '8', '2', '8', '1', '1', '1', '8', '1', '1', '1', '7', '8', '8', '2', '3', '2', '1', '8', '8', '1', '8', '4', '8', '9', '8', '9', '8', '1', '2', '3', '2', '1', '2', '2', '3', '1', '9', '4', '8', '1', '1', '9', '1', '1', '8', '2', '2', '2', '1', '7', '3', '3', '1', '3', '3', '12', '3', '1', '1', '3', '1', '1', '4', '8', '4', '4', '3', '3', '3', '4', '8', '1', '9', '9', '9', '4', '1', '7', '4', '8', '4', '9', '8', '1', '1', '3', '8', '9', '1', '9', '7', '1', '1', '9', '1', '8', '8', '8', '1', '1', '8', '2', '2', '2', '9', '4', '4', '1', '2', '1', '2', '9', '1', '1', '8', '5', '5', '5', '9', '12', '4', '1', '2', '1', '8', '8', '1', '9', '9', '2', '12', '1', '1', '7', '8', '8', '4', '1', '2', '8', '11', '2', '3', '2', '2', '1', '11', '2', '2', '2', '2', '9', '9', '2', '2', '9', '3', '1', '9', '1', '1', '1', '7', '8', '1', '8', '9', '1', '2', '9', '8', '1', '2', '9', '8', '12', '2', '2', '10', '8', '2', '2', '8', '8', '1', '2', '6', '1', '1', '7', '7', '2', '3', '2', '1', '1', '8', '1', '10', '8', '12', '6', '4', '1', '4', '1', '2', '9', '8', '9', '4', '8', '8', '10', '8', '8', '8', '8', '8', '2', '2', '8', '4', '9', '3', '3', '3', '3', '3', '9', '2', '1', '1', '2', '3', '2', '1', '2', '3', '3', '3', '2', '2', '3', '3', '2', '1', '2', '3', '3', '3', '3', '12', '9', '3', '2', '2', '3', '1', '5', '1', '1', '3', '2', '2', '1', '2', '9', '3', '9', '8', '1', '8', '7', '7', '9', '1', '1', '7', '9', '10', '1', '7', '1', '4', '3', '8', '9', '4', '9', '9', '2', '9', '2', '8', '4', '2', '2', '9', '5', '2', '1', '10', '9', '3', '1', '1', '1', '2', '1', '2', '9', '1', '7', '9', '9', '8', '2', '2', '8', '9', '1', '9', '8', '2', '3', '2', '2', '1', '2', '2', '2', '1', '1', '8', '1', '9', '4', '12', '8', '2', '7', '2', '2', '1', '9', '3', '1', '8', '12', '1', '1', '2', '1', '10', '2', '6', '12', '12', '4', '9', '1', '9', '10', '1', '2', '8', '2', '1', '8', '6', '12', '9', '9', '2', '1', '7', '8', '9', '5', '8', '1', '8', '8', '3', '1', '8', '2', '8', '2', '2', '1', '2', '9', '2', '8', '1', '12', '8', '2', '7', '3', '3', '12', '2', '3', '3', '9', '9', '3', '3', '3', '3', '3', '2', '1', '4', '1', '4', '8', '8', '2', '1', '2', '9', '12', '1', '10', '12', '1', '2', '9', '4', '8', '10', '3', '2', '8', '7', '4', '8', '9', '2', '7', '1', '10', '2', '2', '2', '8', '4', '2', '2', '1', '1', '8', '7', '9', '2', '1', '9', '8', '9', '12', '4', '8', '2', '8', '7', '7', '1', '9', '2', '1', '1', '9', '1', '8', '2', '3', '9', '9', '1', '3', '2', '9', '2', '2', '8', '1', '1', '2', '1', '2', '1', '9', '8', '2', '2', '4', '8', '8', '9', '5', '5', '9', '8', '9', '6', '2', '1', '12', '2', '3', '8', '8', '9', '2', '2', '1', '3', '2', '10', '9', '10', '7', '2', '8', '4', '2', '8', '9', '8', '1', '3', '8', '8', '1', '8', '3', '8', '8', '1', '10', '3', '1', '5', '8', '8', '9', '2', '2', '9', '2', '1', '9', '1', '4', '1', '12', '8', '8', '2', '1', '1', '1', '1', '1', '9', '2', '9', '8', '9', '5', '9', '1', '8', '2', '4', '1', '1', '1', '9', '12', '4', '1', '1', '9', '8', '4', '1', '8', '9', '8', '8', '9', '9', '9', '9', '9', '9', '1', '3', '4', '2', '2', '8', '2', '9', '2', '3', '8', '3', '1', '7', '2', '1', '1', '4', '2', '9', '2', '1', '3', '2', '9', '2', '9', '3', '8', '1', '9', '1', '1', '2', '8', '8', '8', '2', '3', '9', '9', '8', '2', '4', '1', '5', '1', '1', '9', '9', '2', '1', '1', '1', '9', '9', '1', '3', '9', '2', '2', '9', '9', '8', '3', '9', '9', '8', '2', '2', '8', '9', '2', '2', '8', '9', '10', '10', '2', '8', '7', '9', '9', '4', '2', '4', '11', '2', '1', '7', '8', '8', '8', '9', '4', '9', '8', '8', '2', '1', '1', '9', '8', '3', '3', '8', '8', '7', '9', '1', '3', '4', '8', '10', '2', '4', '4', '8', '2', '8', '2', '8', '9', '2', '1', '2', '2', '3', '1', '9', '4', '5', '9', '9', '1', '1', '1', '1', '1', '1', '7', '1', '1', '1', '8', '1', '1', '1', '1', '1', '9', '1', '2', '1', '12', '9', '3', '9', '9', '4', '1', '9', '1', '1', '2', '11', '9', '3', '2', '2', '2', '4', '2', '3', '2', '7', '7', '8', '1', '10', '9', '5', '7', '1', '10', '8', '1', '2', '7', '1', '12', '5', '8', '4', '2', '9', '9', '8', '2', '9', '8', '8', '10', '2', '3', '2', '7', '2', '8', '10', '3', '2', '9', '9', '9', '1', '1', '1', '1', '8', '2', '10', '9', '8', '1', '2', '2', '2', '1', '2', '1', '3', '9', '12', '2', '2', '4', '8', '1', '3', '9', '4', '9', '1', '9', '9', '2', '1', '1', '9', '7', '10', '3', '10', '2', '9', '2', '9', '2', '8', '2', '1', '2', '3', '1', '1', '1', '8', '5', '8', '3', '1', '1', '4', '2', '2', '1', '3', '8', '2', '1', '1', '1', '2', '9', '4', '1', '8', '1', '2', '8', '2', '9', '2', '2', '5', '5', '2', '5', '2', '1', '8', '8', '1', '3', '12', '2', '4', '3', '2', '2', '2', '6', '2', '3', '3', '1', '8', '2', '2', '1', '1', '8', '8', '10', '1', '9', '2', '9', '12', '0', '2', '1', '3', '2', '2', '1', '2', '1', '9', '9', '2', '9', '8', '8', '1', '7', '8', '1', '2', '6', '9', '8', '10', '1', '12', '2', '2', '1', '8', '2', '1', '12', '7', '4', '2', '10', '2', '2', '1', '2', '1', '2', '1', '1', '2', '1', '9', '8', '10', '2', '10', '9', '9', '12', '5', '3', '2', '8', '2', '8', '3', '3', '1', '4', '9', '10', '2', '4', '8', '1', '2', '10', '12', '12', '3', '1', '1', '2', '9', '6', '6', '1', '9', '1', '8', '1', '8', '8', '8', '8', '2', '2', '2', '8', '8', '2', '1', '1', '1', '1', '1', '8', '8', '5', '8', '8', '1', '8', '9', '9', '1', '7', '12', '9', '9', '7', '2', '2', '7', '8', '9', '8', '2', '9', '6', '3', '4', '8', '4', '1', '2', '9', '8', '8', '8', '2', '1', '1', '8', '8', '3', '1', '9', '9', '7', '1', '2', '2', '2', '9', '4', '1', '2', '12', '1', '8', '2', '1', '9', '1', '8', '9', '9', '2', '5', '4', '2', '1', '2', '8', '12', '8', '10', '3', '8', '8', '8', '2', '2', '8', '2', '8', '2', '4', '8', '10', '2', '7', '2', '1', '8', '8', '9', '5', '8', '1', '12', '7', '10', '2', '2', '2', '1', '4', '8', '12', '1', '8', '2', '2', '9', '1', '8', '3', '1', '1', '5', '10', '2', '1', '2', '4', '8', '9', '3', '2', '8', '7', '4', '4', '9', '9', '2', '8', '1', '1', '2', '4', '7', '1', '9', '1', '1', '1', '8', '4', '1', '3', '1', '2', '2', '9', '1', '8', '8', '2', '2', '9', '9', '1', '8', '8', '1', '9', '8', '8', '2', '2', '2', '2', '1', '1', '1', '7', '4', '1', '1', '4', '2', '3', '3', '2', '3', '9', '2', '8', '2', '1', '5', '1', '2', '2', '1', '3', '5', '2', '9', '1', '1', '9', '8', '1', '4', '8', '1', '8', '2', '4', '8', '4', '7', '9', '8', '1', '2', '2', '4', '4', '11', '3', '11', '1', '3', '8', '1', '9', '6', '8', '9', '3', '9', '8', '9', '2', '3', '3', '7', '1', '7', '2', '9', '9', '1', '8', '8', '1', '5', '5', '8', '8', '1', '2', '1', '3', '9', '9', '5', '9', '8', '1', '4', '3', '8', '1', '1', '2', '1', '8', '9', '8', '2', '2', '2', '1', '2', '4', '10', '8', '1', '9', '1', '8', '1', '1', '9', '9', '8', '11', '1', '8', '2', '1', '2', '8', '2', '8', '8', '2', '9', '6', '10', '1', '8', '6', '4', '5', '11', '2', '2', '8', '1', '8', '1', '8', '1', '4', '8', '1', '1', '1', '8', '3', '7', '3', '3', '7', '1', '1', '4', '8', '1', '6', '4', '2', '2', '1', '1', '1', '1', '6', '2', '2', '8', '1', '8', '5', '5', '2', '1', '1', '2', '3', '7', '1', '1', '6', '2', '9', '3', '9', '8', '9', '2', '1', '4', '2', '9', '13', '9', '2', '8', '12', '4', '1', '1', '9', '5', '1', '1', '1', '1', '8', '2', '1', '1', '2', '9', '0', '12', '1', '1', '8', '9', '8', '3', '2', '2', '12', '8', '2', '2', '2', '1', '10', '1', '9', '8', '9', '5', '2', '2', '9', '2', '8', '4', '8', '3', '7', '9', '2', '2', '7', '2', '2', '8', '1', '1', '9', '10', '9', '8', '2', '9', '0', '7', '1', '1', '1', '9', '8', '2', '8', '12', '8', '3', '2', '8', '8', '7', '7', '8', '12', '2', '1', '3', '9', '2', '2', '8', '2', '8', '8', '12', '8', '8', '1', '8', '9', '2', '4', '4', '7', '2', '1', '1', '9', '3', '7', '1', '3', '9', '8', '2', '2', '3', '1', '3', '10', '2', '3', '7', '1', '8', '1', '1', '10', '3', '10', '8', '4', '1', '2', '4', '3', '10', '9', '3', '9', '5', '8', '7', '2', '9', '3', '3', '2', '11', '8', '1', '1', '7', '2', '7', '2', '9', '4', '8', '1', '12', '8', '7', '9', '1', '0', '2', '6', '4', '9', '3', '2', '4', '12', '9', '8', '5', '9', '2', '8', '4', '6', '8', '9', '3', '1', '8', '7', '3', '1', '1', '1', '8', '9', '1', '8', '7', '9', '8', '2', '1', '2', '2', '3', '2', '2', '1', '1', '4', '9', '7', '3', '2', '8', '8', '9', '1', '9', '7', '4', '2', '2', '8', '5', '9', '1', '5', '7', '1', '4', '6', '9', '10', '1', '8', '2', '9', '1', '9', '2', '9', '8', '2', '9', '9', '11', '2', '12', '2', '2', '9', '4', '8', '1', '9', '9', '2', '8', '9', '1', '2', '9', '3', '8', '8', '1', '7', '8', '8', '2', '3', '2', '2', '2', '2', '9', '8', '10', '2', '2', '1', '9', '11', '1', '8', '8', '2', '4', '2', '1', '2', '8', '8', '9', '2', '8', '9', '2', '3', '1', '2', '1', '3', '10', '8', '1', '2', '4', '1', '9', '8', '9', '3', '3', '9', '8', '2', '11', '3', '7', '1', '8', '7', '1', '8', '8', '8', '1', '4', '8', '8', '1', '9', '3', '1', '1', '2', '8', '8', '8', '2', '1', '8', '10', '2', '12', '8', '8', '9', '1', '8', '1', '3', '2', '2', '10', '7', '2', '2', '1', '1', '8', '2', '9', '1', '2', '9', '3', '3', '10', '1', '4', '7', '10', '12', '2', '4', '10', '8', '6', '2', '9', '1', '8', '3', '1', '2', '8', '8', '2', '12', '10', '2', '10', '9', '1', '2', '5', '8', '12', '8', '8', '7', '8', '8', '1', '8', '2', '2', '9', '1', '8', '10', '5', '5', '5', '8', '1', '9', '1', '13', '1', '2', '2', '1', '2', '4', '9', '2', '1', '9', '13', '11', '1', '9', '9', '3', '10', '8', '7', '8', '8', '4', '2', '1', '9', '4', '3', '1', '1', '10', '9', '2', '8', '6', '2', '1', '1', '3', '2', '8', '1', '1', '1', '1', '2', '4', '1', '1', '10', '1', '8', '1', '1', '9', '1', '7', '1', '1', '9', '9', '2', '12', '9', '1', '8', '1', '12', '1', '8', '3', '8', '1', '8', '1', '2', '10', '1', '1', '2', '2', '9', '3', '9', '12', '1', '9', '1', '2', '8', '7', '2', '1', '1', '3', '2', '8', '4', '4', '5', '12', '7', '8', '9', '6', '1', '8', '9', '2', '1', '9', '1', '4', '1', '8', '2', '1', '1', '8', '7', '4', '9', '1', '8', '4', '2', '7', '5', '1', '3', '9', '2', '8', '2', '1', '2', '9', '1', '1', '2', '3', '4', '3', '1', '2', '2', '10', '2', '2', '8', '1', '1', '2', '4', '9', '2', '11', '9', '8', '9', '2', '1', '10', '4', '1', '1', '10', '7', '2', '10', '8', '1', '10', '8', '8', '8', '8', '8', '8', '2', '2', '2', '8', '3', '4', '1', '1', '1', '2', '1', '8', '8', '2', '8', '3', '8', '3', '1', '2', '3', '1', '1', '10', '1', '4', '4', '3', '3', '1', '9', '1', '1', '2', '1', '1', '6', '1', '1', '1', '9', '1', '4', '1', '1', '1', '1', '1', '1', '2', '12', '8', '8', '1', '9', '12', '10', '8', '2', '12', '1', '1', '2', '1', '7', '10', '7', '1', '2', '1', '10', '8', '2', '2', '8', '2', '2', '1', '9', '8', '8', '8', '8', '3', '1', '4', '2', '8', '3', '10', '10', '1', '9', '1', '9', '1', '9', '1', '8', '3', '1', '1', '8', '8', '8', '6', '3', '4', '1', '1', '1', '9', '9', '5', '2', '10', '2', '6', '2', '2', '7', '1', '8', '2', '11', '1', '1', '1', '2', '1', '9', '4', '4', '2', '8', '4', '6', '8', '8', '10', '10', '2', '1', '2', '9', '1', '8', '8', '3', '4', '8', '8', '2', '8', '8', '1', '2', '1', '2', '9', '4', '1', '3', '3', '3', '8', '8', '8', '2', '6', '4', '3', '3', '12', '3', '4', '9', '6', '8', '7', '7', '8', '6', '4', '1', '10', '6', '3', '3', '8', '1', '1', '8', '1', '11', '8', '1', '1', '2', '3', '11', '7', '2', '1', '1', '1', '9', '1', '2', '1', '1', '9', '8', '1', '1', '8', '0', '4', '1', '1', '2', '1', '1', '2', '2', '9', '1', '10', '1', '3', '8', '9', '3', '3', '1', '10', '1', '9', '1', '1', '7', '4', '1', '8', '10', '1', '2', '10', '1', '1', '2', '3', '3', '9', '1', '8', '10', '1', '7', '9', '1', '9', '6', '3', '9', '2', '8', '4', '12', '12', '3', '1', '1', '9', '1', '9', '2', '1', '1', '1', '1', '1', '9', '2', '1', '2', '10', '9', '8', '8', '10', '8', '2', '3', '2', '1', '12', '8', '12', '2', '9', '1', '3', '2', '5', '2', '2', '8', '2', '2', '4', '10', '9', '10', '9', '4', '2', '4', '12', '8', '2', '1', '8', '3', '9', '2', '9', '4', '1', '1', '1', '1', '1', '9', '6', '2', '9', '2', '1', '3', '2', '2', '5', '8', '9', '1', '6', '3', '1', '2', '2', '2', '6', '1', '8', '10', '8', '3', '3', '13', '2', '9', '9', '10', '6', '12', '8', '2', '9', '2', '3', '8', '8', '1', '2', '2', '3', '2', '10', '1', '8', '9', '1', '8', '9', '8', '8', '2', '10', '2', '1', '1', '1', '9', '9', '12', '1', '1', '4', '8', '3', '2', '2', '4', '4', '2', '2', '1', '1', '1', '9', '2', '2', '4', '1', '1', '8', '8', '8', '9', '2', '13', '9', '9', '4', '1', '9', '7', '10', '10', '8', '1', '9', '1', '1', '1', '8', '12', '4', '1', '2', '1', '1', '1', '7', '8', '3', '3', '1', '2', '2', '7', '2', '2', '10', '1', '8', '4', '0', '1', '9', '4', '1', '1', '2', '10', '10', '7', '9', '3', '12', '11', '2', '8', '10', '8', '9', '4', '3', '4', '10', '1', '1', '9', '3', '10', '1', '5', '3', '3', '2', '2', '1', '1', '6', '1', '7', '1', '1', '12', '1', '1', '1', '10', '1', '8', '8', '1', '3', '8', '3', '1', '2', '8', '3', '4', '2', '8', '8', '6', '1', '1', '4', '1', '2', '9', '5', '1', '7', '8', '9', '2', '4', '9', '8', '5', '9', '9', '9', '1', '2', '1', '3', '8', '9', '2', '9', '8', '9', '1', '8', '13', '9', '3', '8', '1', '4', '1', '3', '1', '6', '4', '10', '2', '2', '7', '8', '8', '12', '8', '9', '9', '9', '10', '3', '12', '12', '2', '9', '10', '9', '7', '2', '3', '4', '10', '10', '2', '7', '7', '1', '7', '3', '9', '1', '9', '8', '2', '9', '1', '8', '1', '10', '8', '8', '2', '7', '8', '1', '8', '9', '8', '1', '4', '1', '8', '8', '8', '1', '1', '10', '3', '2', '2', '6', '8', '9', '8', '2', '1', '1', '3', '8', '8', '1', '4', '4', '3', '9', '10', '4', '2', '4', '2', '9', '2', '2', '9', '1', '1', '10', '1', '1', '2', '1', '1', '1', '9', '1', '8', '8', '8', '2', '3', '8', '1', '6', '7', '3', '1', '9', '1', '2', '1', '6', '1', '2', '10', '1', '4', '2', '3', '2', '9', '9', '4', '9', '9', '1', '2', '8', '4', '7', '8', '7', '13', '9', '1', '2', '1', '9', '3', '3', '6', '8', '9', '10', '9', '2', '2', '3', '12', '1', '1', '1', '13', '2', '1', '7', '10', '8', '9', '1', '2', '9', '1', '1', '1', '2', '1', '12', '5', '6', '9', '10', '3', '1', '10', '1', '3', '2', '8', '9', '2', '10', '8', '8', '9', '2', '8', '9', '2', '2', '2', '8', '8', '2', '3', '1', '4', '8', '1', '9', '9', '2', '8', '10', '9', '1', '2', '12', '8', '8', '0', '2', '1', '1', '9', '10', '1', '6', '2', '2', '10', '2', '2', '4', '2', '2', '12', '1', '6', '7', '8', '9', '1', '3', '5', '7', '3', '1', '3', '1', '1', '3', '2', '6', '3', '2', '1', '8', '3', '5', '2', '1', '1', '8', '1', '1', '0', '2', '3', '5', '3', '1', '1', '9', '8', '8', '8', '8', '4', '7', '2', '9', '12', '5', '9', '3', '12', '3', '9', '1', '9', '1', '7', '3', '9', '3', '10', '1', '2', '1', '4', '9', '8', '2', '9', '11', '2', '8', '9', '4', '1', '1', '1', '1', '1', '1', '1', '9', '1', '9', '1', '1', '1', '12', '3', '10', '3', '6', '1', '7', '10', '8', '3', '7', '1', '1', '2', '6', '1', '3', '9', '2', '8', '1', '8', '8', '1', '8', '10', '12', '1', '10', '12', '10', '1', '8', '8', '1', '4', '7', '9', '2', '1', '9', '3', '8', '12', '10', '6', '1', '6', '8', '3', '1', '3', '1', '10', '10', '9', '8', '1', '2', '9', '8', '8', '1', '9', '8', '8', '3', '1', '3', '9', '12', '1', '8', '1', '5', '8', '11', '5', '5', '1', '2', '1', '3', '1', '1', '1', '9', '8', '8', '1', '9', '10', '8', '4', '1', '1', '8', '10', '8', '9', '1', '10', '1', '7', '2', '1', '1', '9', '6', '2', '9', '8', '1', '1', '1', '9', '8', '8', '1', '9', '9', '1', '1', '2', '7', '9', '2', '9', '9', '8', '7', '6', '8', '7', '8', '2', '9', '2', '1', '8', '8', '8', '2', '6', '3', '1', '8', '12', '12', '8', '7', '1', '2', '2', '7', '1', '9', '8', '1', '4', '1', '1', '8', '5', '8', '1', '8', '8', '8', '8', '1', '2', '11', '1', '1', '1', '2', '7', '1', '9', '2', '2', '2', '9', '8', '9', '2', '9', '1', '8', '1', '7', '11', '13', '1', '9', '8', '2', '2', '1', '2', '3', '10', '3', '10', '4', '3', '9', '1', '8', '1', '1', '13', '1', '1', '3', '8', '1', '2', '2', '2', '9', '4', '1', '8', '1', '9', '3', '6', '8', '5', '2', '8', '2', '8', '1', '9', '2', '2', '8', '1', '1', '4', '8', '9', '2', '2', '7', '1', '8', '12', '2', '12', '8', '2', '9', '9', '3', '8', '1', '1', '9', '9', '9', '2', '8', '1', '9', '1', '8', '9', '8', '2', '2', '4', '1', '9', '10', '10', '8', '8', '1', '1', '2', '1', '4', '1', '2', '1', '1', '3', '12', '8', '8', '8', '8', '10', '2', '8', '12', '1', '9', '9', '8', '9', '1', '2', '1', '8', '8', '8', '2', '8', '8', '3', '10', '8', '9', '1', '1', '1', '3', '4', '8', '10', '6', '3', '1', '3', '2', '8', '9', '3', '5', '4', '1', '5', '2', '9', '9', '9', '1', '1', '1', '9', '9', '6', '10', '9', '8', '12', '2', '9', '1', '2', '1', '1', '1', '8', '1', '12', '1', '12', '1', '9', '1', '8', '1', '2', '1', '2', '8', '8', '10', '1', '9', '1', '2', '2', '8', '8', '3', '12', '1', '11', '2', '10', '12', '11', '2', '1', '10', '1', '1', '9', '3', '10', '2', '7', '1', '8', '1', '9', '9', '2', '1', '3', '12', '5', '9', '9', '8', '1', '8', '8', '8', '1', '1', '3', '9', '8', '9', '8', '3', '10', '3', '7', '8', '9', '2', '3', '1', '9', '10', '9', '2', '8', '2', '6', '4', '1', '1', '3', '8', '2', '3', '9', '1', '2', '1', '8', '1', '2', '2', '2', '2', '9', '9', '4', '8', '9', '1', '1', '2', '9', '1', '5', '2', '9', '8', '10', '1', '2', '10', '8', '5', '6', '4', '8', '1', '1', '1', '8', '9', '2', '8', '8', '8', '12', '2', '2', '1', '8', '1', '2', '1', '9', '8', '8', '9', '1', '1', '8', '7', '1', '8', '0', '2', '3', '8', '1', '1', '8', '1', '1', '12', '3', '2', '10', '8', '10', '6', '1', '8', '8', '10', '8', '8', '1', '4', '8', '1', '2', '9', '8', '1', '1', '3', '3', '5', '1', '1', '2', '1', '2', '1', '9', '8', '3', '9', '8', '2', '10', '9', '1', '8', '1', '10', '9', '10', '1', '2', '3', '8', '1', '3', '1', '9', '9', '8', '1', '9', '8', '8', '9', '1', '9', '2', '8', '8', '2', '13', '8', '10', '8', '9', '3', '2', '1', '3', '1', '12', '10', '10', '1', '11', '1', '2', '4', '2', '10', '1', '8', '11', '1', '1', '1', '2', '2', '8', '2', '9', '8', '9', '9', '12', '8', '1', '9', '2', '4', '1', '8', '3', '9', '1', '6', '5', '8', '9', '3', '3', '2', '1', '1', '1', '8', '11', '1', '8', '7', '1', '11', '9', '1', '7', '9', '9', '8', '8', '8', '12', '8', '1', '2', '9', '8', '8', '2', '10', '9', '8', '8', '4', '10', '9', '8', '2', '10', '10', '2', '1', '2', '1', '2', '1', '4', '1', '7', '1', '1', '3', '12', '9', '8', '5', '9', '10', '1', '9', '9', '2', '7', '4', '1', '1', '8', '12', '8', '2', '2', '9', '4', '1', '1', '5', '9', '1', '8', '12', '8', '1', '1', '4', '9', '10', '2', '3', '1', '3', '3', '3', '8', '1', '2', '1', '2', '1', '2', '9', '1', '2', '8', '9', '4', '8', '8', '10', '2', '12', '5', '5', '1', '8', '1', '7', '8', '1', '1', '8', '9', '12', '4', '9', '3', '1', '2', '5', '9', '2', '1', '2', '2', '1', '2', '8', '1', '8', '8', '2', '2', '1', '8', '4', '1', '2', '10', '9', '10', '8', '1', '10', '4', '8', '8', '10', '2', '8', '1', '1', '3', '10', '1', '4', '2', '8', '3', '3', '2', '9', '8', '5', '5', '9', '3', '10', '8', '1', '2', '1', '2', '9', '1', '1', '2', '9', '9', '8', '8', '11', '9', '4', '1', '8', '2', '7', '1', '2', '8', '2', '9', '9', '8', '2', '9', '9', '8', '8', '1', '2', '8', '1', '1', '2', '9', '8', '1', '4', '9', '1', '8', '12', '1', '12', '2', '8', '2', '9', '1', '12', '1', '2', '9', '1', '1', '9', '3', '2', '9', '9', '2', '11', '8', '7', '9', '9', '9', '8', '8', '1', '2', '8', '1', '1', '4', '1', '2', '10', '1', '9', '2', '1', '1', '1', '9', '1', '13', '4', '3', '2', '1', '1', '2', '2', '7', '8', '7', '1', '1', '2', '8', '8', '3', '4', '8', '2', '10', '13', '8', '8', '9', '2', '2', '1', '1', '10', '4', '2', '7', '9', '8', '2', '1', '9', '8', '1', '1', '9', '8', '9', '8', '2', '8', '2', '2', '9', '2', '1', '9', '1', '2', '4', '8', '2', '1', '1', '8', '1', '4', '1', '9', '3', '9', '2', '7', '1', '9', '6', '1', '10', '2', '2', '2', '2', '2', '10', '10', '10', '9', '9', '9', '3', '12', '10', '1', '1', '10', '1', '1', '2', '2', '1', '2', '3', '12', '8', '8', '1', '2', '9', '9', '1', '1', '10', '8', '9', '8', '3', '1', '1', '3', '1', '10', '1', '1', '1', '4', '1', '1', '1', '7', '10', '2', '1', '8', '1', '9', '3', '1', '10', '8', '8', '1', '5', '9', '1', '2', '1', '8', '8', '1', '3', '1', '10', '2', '1', '1', '2', '8', '2', '5', '3', '5', '2', '1', '1', '9', '1', '7', '9', '9', '2', '8', '1', '12', '4', '1', '4', '9', '9', '4', '8', '1', '10', '10', '8', '8', '8', '9', '2', '3', '5', '1', '8', '1', '1', '1', '7', '10', '1', '4', '12', '3', '2', '2', '9', '1', '1', '1', '2', '8', '4', '1', '7', '3', '12', '8', '6', '2', '2', '7', '9', '11', '1', '10', '9', '1', '8', '12', '2', '11', '1', '2', '3', '1', '1', '1', '8', '9', '2', '2', '2', '2', '3', '3', '8', '10', '4', '1', '2', '9', '8', '2', '2', '9', '1', '8', '11', '4', '2', '8', '7', '1', '1', '7', '3', '9', '9', '4', '5', '8', '10', '2', '9', '9', '9', '8', '1', '9', '2', '2', '2', '1', '2', '9', '4', '3', '12', '4', '3', '2', '3', '8', '3', '10', '1', '10', '1', '1', '8', '10', '6', '8', '9', '9', '1', '2', '9', '2', '9', '3', '8', '1', '1', '12', '1', '5', '1', '10', '8', '10', '2', '7', '1', '1', '1', '1', '3', '2', '3', '5', '9', '1', '2', '1', '9', '8', '10', '9', '8', '1', '8', '1', '8', '2', '8', '1', '2', '8', '1', '2', '5', '8', '12', '2', '1', '1', '1', '1', '1', '8', '8', '2', '1', '4', '2', '8', '8', '8', '2', '9', '9', '11', '1', '2', '9', '3', '1', '1', '8', '9', '2', '4', '10', '1', '9', '8', '4', '9', '8', '10', '8', '8', '9', '2', '8', '9', '3', '3', '3', '4', '3', '2', '2', '10', '8', '2', '1', '5', '1', '9', '1', '1', '2', '1', '2', '3', '1', '1', '8', '8', '1', '10', '8', '8', '1', '6', '4', '8', '2', '5', '8', '1', '3', '8', '2', '8', '9', '1', '1', '8', '12', '10', '1', '5', '8', '2', '8', '10', '9', '9', '1', '6', '9', '1', '10', '8', '2', '9', '10', '1', '2', '1', '1', '8', '8', '3', '9', '8', '1', '10', '2', '8', '1', '10', '1', '5', '1', '1', '1', '5', '1', '1', '1', '4', '1', '1', '1', '8', '2', '8', '1', '8', '3', '8', '9', '1', '8', '1', '8', '1', '1', '2', '2', '1', '12', '9', '9', '1', '8', '2', '1', '2', '8', '1', '9', '1', '1', '2', '1', '2', '2', '2', '1', '9', '9', '8', '8', '1', '1', '9', '10', '8', '4', '3', '3', '4', '9', '1', '3', '3', '10', '2', '2', '1', '8', '4', '1', '9', '4', '1', '1', '9', '8', '10', '4', '9', '9', '8', '2', '1', '3', '4', '9', '3', '8', '9', '1', '1', '2', '1', '7', '8', '2', '9', '1', '9', '2', '6', '9', '10', '8', '1', '1', '5', '9', '5', '1', '9', '10', '9', '3', '3', '8', '8', '2', '9', '2', '8', '2', '1', '8', '5', '8', '3', '10', '1', '1', '2', '1', '8', '1', '4', '3', '2', '9', '4', '9', '8', '8', '8', '1', '3', '9', '1', '9', '9', '1', '2', '1', '9', '8', '9', '8', '1', '1', '1', '4', '2', '2', '9', '9', '1', '1', '1', '3', '6', '1', '2', '3', '1', '4', '2', '9', '1', '9', '9', '1', '8', '2', '1', '9', '1', '8', '1', '8', '8', '2', '1', '9', '9', '6', '8', '8', '9', '8', '10', '8', '9', '5', '1', '1', '1', '1', '8', '1', '8', '12', '8', '1', '12', '9', '2', '8', '1', '1', '2', '1', '8', '6', '9', '9', '8', '1', '8', '9', '7', '7', '8', '9', '9', '1', '8', '3', '8', '1', '3', '3', '8', '9', '8', '2', '8', '1', '2', '9', '8', '1', '1', '1', '1', '9', '1', '9', '8', '12', '3', '8', '8', '1', '10', '10', '10', '2', '2', '1', '10', '2', '2', '10', '8', '12', '8', '1', '1', '1', '2', '1', '1', '3', '8', '1', '2', '2', '2', '1', '1', '6', '2', '8', '8', '8', '1', '1', '1', '9', '9', '2', '8', '10', '2', '8', '2', '1', '2', '9', '9', '2', '1', '1', '8', '8', '1', '3', '1', '8', '9', '1', '10', '9', '1', '1', '1', '1', '7', '2', '2', '1', '2', '8', '8', '1', '2', '1', '3', '2', '1', '8', '3', '9', '2', '10', '2', '10', '1', '2', '1', '1', '13', '9', '10', '8', '2', '2', '1', '1', '8', '2', '2', '1', '9', '1', '1', '8', '9', '9', '2', '9', '2', '2', '1', '1', '1', '8', '9', '4', '9', '9', '1', '8', '2', '1', '1', '8', '2', '2', '2', '8', '1', '1', '8', '2', '10', '2', '9', '9', '7', '1', '1', '2', '2', '8', '3', '1', '2', '1', '1', '8', '1', '1', '1', '9', '1', '1', '8', '8', '1', '9', '6', '1', '9', '1', '8', '1', '8', '8', '8', '1', '3', '8', '8', '2', '6', '3', '2', '1', '10', '1', '8', '2', '1', '6', '1', '1', '1', '8', '1', '10', '1', '1', '8', '9', '2', '6', '9', '2', '9', '4', '1', '8', '1', '3', '7', '8', '8', '5', '8', '7', '1', '1', '1', '13', '8', '3', '1', '1', '1', '1', '2', '8', '2', '12', '8', '1', '2', '5', '1', '8', '2', '10', '8', '2', '10', '1', '5', '2', '1', '1', '5', '2', '2', '7', '8', '2', '9', '9', '1', '10', '1', '8', '2', '5', '8', '1', '5', '1', '10', '1', '1', '9', '8', '6', '1', '8', '9', '8', '1', '8', '2', '2', '8', '3', '9', '1', '9', '3', '8', '4', '4', '3', '3', '1', '1', '1', '8', '9', '1', '1', '8', '3', '1', '2', '8', '2', '1', '1', '10', '1', '2', '2', '1', '1', '8', '2', '8', '2', '10', '2', '2', '10', '8', '8', '8', '13', '2', '2', '9', '2', '5', '9', '2', '1', '8', '8', '8', '1', '12', '12', '2', '9', '2', '1', '8', '2', '1', '8', '1', '9', '9', '2', '7', '1', '1', '7', '1', '4', '3', '1', '10', '3', '10', '9', '4', '6', '7', '5', '2', '1', '8', '8', '1', '4', '9', '2', '1', '8', '1', '1', '1', '1', '1', '8', '9', '8', '8', '8', '8', '9', '8', '10', '1', '8', '2', '7', '8', '9', '1', '2', '5', '9', '10', '8', '9', '12', '1', '6', '1', '2', '10', '9', '4', '1', '8', '10', '10', '1', '1', '1', '1', '1', '2', '3', '8', '1', '2', '10', '2', '1', '2', '4', '2', '2', '9', '1', '4', '2', '1', '1', '12', '12', '9', '9', '7', '1', '8', '4', '9', '9', '8', '1', '7', '3', '2', '1', '1', '4', '8', '1', '5', '8', '2', '8', '1', '14', '9', '12', '1', '10', '2', '1', '1', '8', '8', '6', '6', '3', '1', '2', '8', '1', '3', '2', '1', '8', '8', '1', '10', '8', '2', '8', '2', '5', '9', '1', '8', '8', '1', '1', '8', '8', '13', '1', '3', '8', '4', '1', '9', '7', '8', '8', '8', '3', '8', '1', '4', '2', '8', '8', '8', '7', '1', '1', '2', '8', '8', '9', '8', '8', '2', '3', '9', '9', '2', '1', '2', '2', '1', '8', '2', '6', '2', '1', '1', '9', '10', '3', '8', '2', '3', '2', '8', '8', '1', '8', '8', '8', '9', '1', '4', '1', '9', '2', '1', '8', '0', '1', '8', '1', '1', '8', '9', '9', '1', '3', '1', '2', '8', '8', '7', '1', '1', '2', '9', '2', '10', '2', '8', '2', '2', '2', '7', '1', '9', '8', '1', '3', '9', '2', '1', '10', '8', '1', '4', '1', '8', '5', '9', '8', '8', '1', '2', '2', '1', '8', '8', '6', '8', '1', '1', '8', '3', '2', '2', '1', '8', '1', '7', '6', '8', '1', '1', '1', '9', '1', '1', '1', '2', '1', '11', '10', '10', '11', '9', '']\n",
            "['8', '1', '8', '2', '8', '8', '8', '9', '7', '8', '1', '2', '1', '1', '8', '4', '8', '8', '12', '3', '3', '7', '3', '12', '1', '8', '8', '8', '8', '8', '8', '8', '1', '9', '5', '9', '9', '9', '11', '8', '8', '8', '4', '8', '8', '8', '8', '1', '3', '9', '3', '7', '1', '2', '9', '9', '7', '8', '8', '1', '10', '7', '8', '8', '9', '8', '7', '9', '9', '12', '7', '2', '8', '1', '11', '11', '1', '7', '7', '12', '1', '9', '8', '10', '12', '7', '8', '2', '8', '9', '9', '1', '8', '9', '1', '7', '12', '10', '10', '10', '8', '3', '7', '9', '8', '9', '1', '8', '8', '2', '7', '2', '9', '9', '11', '8', '8', '12', '12', '7', '8', '12', '4', '9', '3', '1', '12', '1', '1', '8', '8', '3', '8', '8', '8', '8', '9', '1', '8', '8', '10', '1', '8', '2', '8', '8', '7', '3', '8', '2', '4', '4', '9', '8', '10', '12', '12', '1', '1', '9', '1', '1', '1', '8', '2', '2', '8', '1', '1', '2', '2', '1', '2', '8', '1', '9', '9', '8', '8', '4', '2', '9', '9', '8', '3', '4', '3', '1', '8', '8', '2', '1', '9', '7', '8', '8', '1', '12', '3', '8', '2', '4', '2', '9', '12', '1', '4', '1', '8', '8', '8', '2', '2', '8', '9', '8', '8', '8', '10', '9', '8', '7', '9', '1', '1', '9', '4', '2', '4', '2', '2', '1', '7', '8', '11', '11', '3', '9', '2', '4', '8', '9', '1', '8', '1', '1', '4', '9', '1', '1', '8', '8', '2', '1', '8', '4', '2', '8', '9', '8', '8', '2', '8', '8', '8', '7', '1', '1', '1', '2', '1', '1', '8', '7', '8', '8', '12', '2', '12', '12', '8', '10', '12', '8', '3', '3', '12', '10', '1', '8', '12', '1', '8', '8', '2', '8', '4', '7', '8', '7', '10', '8', '10', '9', '8', '12', '12', '1', '8', '8', '3', '8', '8', '8', '8', '8', '1', '9', '8', '11', '1', '1', '1', '9', '8', '1', '9', '2', '3', '11', '8', '9', '9', '9', '2', '1', '8', '8', '9', '7', '1', '4', '9', '4', '8', '8', '4', '8', '12', '9', '4', '8', '2', '10', '10', '10', '8', '9', '9', '8', '8', '12', '7', '1', '8', '8', '8', '4', '1', '1', '1', '1', '1', '1', '8', '1', '9', '8', '9', '9', '4', '8', '12', '9', '8', '8', '2', '8', '8', '8', '6', '9', '8', '3', '7', '8', '8', '4', '12', '8', '8', '9', '12', '12', '9', '8', '2', '9', '2', '3', '1', '12', '8', '10', '9', '9', '9', '10', '10', '3', '8', '12', '1', '4', '2', '1', '10', '8', '2', '8', '4', '8', '9', '1', '9', '9', '10', '10', '1', '4', '9', '2', '4', '9', '1', '1', '3', '10', '3', '3', '8', '7', '3', '8', '9', '9', '12', '4', '8', '12', '2', '2', '4', '1', '9', '9', '4', '1', '4', '2', '8', '12', '2', '3', '10', '10', '9', '8', '9', '9', '1', '12', '8', '8', '8', '12', '4', '1', '8', '8', '1', '9', '8', '8', '2', '1', '8', '9', '8', '3', '3', '3', '1', '8', '8', '9', '1', '10', '9', '9', '9', '9', '5', '9', '9', '8', '8', '8', '8', '8', '8', '8', '8', '11', '12', '8', '8', '1', '8', '9', '11', '2', '2', '2', '2', '3', '1', '2', '2', '8', '2', '4', '9', '1', '2', '9', '8', '2', '8', '9', '9', '3', '10', '9', '9', '2', '8', '9', '8', '12', '12', '1', '3', '8', '8', '8', '2', '7', '7', '7', '7', '3', '9', '1', '9', '8', '9', '9', '1', '1', '1', '2', '9', '9', '9', '11', '1', '8', '8', '9', '1', '9', '8', '8', '8', '1', '1', '8', '7', '1', '1', '8', '8', '9', '4', '4', '8', '2', '2', '8', '8', '8', '8', '8', '8', '11', '8', '2', '9', '4', '9', '3', '9', '9', '1', '3', '9', '3', '1', '12', '8', '9', '12', '1', '8', '4', '2', '1', '4', '8', '3', '3', '8', '2', '8', '9', '7', '8', '8', '8', '5', '8', '3', '9', '8', '8', '13', '12', '1', '1', '2', '8', '4', '1', '9', '9', '12', '8', '9', '12', '9', '1', '9', '9', '9', '9', '3', '2', '9', '9', '4', '8', '12', '2', '4', '9', '3', '1', '9', '7', '8', '9', '9', '8', '4', '8', '8', '7', '9', '10', '3', '8', '8', '8', '1', '1', '1', '1', '8', '8', '4', '1', '10', '1', '5', '7', '7', '1', '8', '9', '3', '7', '2', '7', '7', '2', '4', '8', '12', '7', '4', '2', '9', '9', '12', '6', '10', '8', '2', '4', '12', '9', '9', '3', '8', '8', '1', '2', '10', '9', '9', '8', '4', '12', '2', '1', '8', '8', '8', '12', '10', '10', '9', '3', '8', '8', '9', '2', '8', '10', '1', '1', '1', '1', '2', '1', '1', '1', '1', '9', '8', '12', '9', '4', '8', '8', '9', '1', '9', '3', '9', '8', '8', '1', '7', '7', '10', '1', '8', '8', '1', '9', '8', '10', '3', '1', '7', '1', '8', '8', '12', '8', '8', '1', '8', '7', '1', '7', '7', '8', '2', '1', '8', '8', '2', '10', '8', '8', '8', '8', '8', '10', '1', '8', '8', '12', '8', '3', '3', '2', '2', '2', '10', '8', '8', '8', '2', '9', '1', '8', '9', '3', '2', '8', '10', '8', '6', '1', '1', '8', '4', '1', '9', '10', '8', '1', '7', '1', '2', '8', '1', '1', '1', '12', '1', '9', '12', '8', '12', '12', '12', '8', '8', '12', '4', '8', '8', '8', '8', '9', '9', '1', '3', '3', '3', '3', '1', '12', '12', '9', '10', '8', '8', '1', '9', '2', '2', '13', '9', '8', '9', '2', '1', '9', '1', '8', '8', '8', '4', '8', '1', '1', '1', '12', '12', '7', '2', '2', '2', '8', '3', '8', '9', '2', '10', '7', '8', '9', '2', '1', '2', '12', '12', '8', '8', '9', '2', '2', '9', '11', '1', '8', '1', '10', '9', '2', '1', '4', '7', '7', '7', '7', '7', '12', '8', '8', '8', '1', '1', '10', '1', '12', '1', '8', '2', '1', '1', '12', '8', '7', '9', '12', '8', '9', '3', '9', '8', '8', '8', '8', '3', '11', '2', '2', '9', '8', '8', '10', '8', '2', '7', '3', '1', '4', '7', '8', '8', '1', '8', '3', '7', '12', '8', '10', '9', '9', '8', '8', '2', '8', '9', '9', '1', '2', '8', '8', '9', '8', '3', '8', '1', '8', '10', '9', '8', '9', '9', '12', '4', '4', '8', '9', '9', '8', '2', '10', '1', '2', '8', '9', '1', '9', '9', '9', '7', '12', '12', '8', '1', '1', '1', '1', '8', '3', '1', '1', '8', '1', '8', '8', '7', '8', '8', '8', '8', '3', '2', '2', '10', '10', '10', '7', '8', '1', '2', '12', '7', '9', '8', '7', '8', '12', '2', '8', '9', '2', '6', '6', '7', '9', '8', '1', '8', '9', '8', '1', '12', '1', '2', '8', '7', '7', '7', '8', '2', '2', '8', '1', '2', '2', '9', '9', '1', '8', '8', '4', '3', '3', '1', '6', '3', '3', '12', '3', '8', '9', '1', '4', '3', '1', '8', '3', '9', '2', '8', '2', '8', '8', '8', '1', '1', '1', '9', '9', '8', '1', '9', '8', '1', '1', '3', '10', '8', '1', '1', '3', '9', '1', '4', '4', '1', '8', '9', '9', '2', '0', '0', '1', '8', '3', '1', '8', '8', '9', '8', '8', '1', '1', '8', '9', '8', '8', '8', '7', '9', '8', '8', '8', '10', '9', '8', '1', '2', '6', '1', '9', '9', '8', '12', '12', '12', '8', '8', '2', '8', '1', '2', '2', '2', '1', '9', '8', '2', '12', '2', '8', '12', '8', '9', '8', '8', '9', '7', '1', '1', '1', '1', '1', '8', '8', '1', '8', '8', '1', '1', '3', '2', '8', '8', '9', '10', '10', '2', '2', '1', '9', '2', '9', '9', '4', '12', '12', '12', '10', '7', '3', '3', '4', '2', '2', '9', '2', '8', '4', '2', '4', '1', '10', '9', '7', '8', '7', '1', '1', '3', '3', '1', '1', '3', '3', '3', '1', '1', '1', '1', '8', '2', '3', '1', '1', '2', '8', '8', '12', '8', '8', '8', '8', '11', '9', '1', '8', '9', '2', '8', '8', '8', '3', '9', '1', '9', '2', '7', '2', '8', '2', '8', '10', '8', '1', '10', '1', '1', '9', '9', '8', '8', '1', '8', '8', '8', '12', '8', '8', '8', '1', '8', '8', '8', '1', '9', '1', '1', '8', '1', '8', '9', '8', '2', '12', '9', '9', '0', '1', '8', '8', '1', '8', '12', '8', '8', '10', '8', '8', '8', '7', '8', '1', '8', '7', '3', '10', '1', '8', '9', '1', '8', '8', '8', '10', '1', '10', '3', '9', '1', '8', '9', '2', '8', '3', '3', '9', '9', '7', '9', '1', '1', '9', '2', '1', '1', '1', '7', '1', '1', '8', '8', '1', '1', '8', '1', '8', '3', '12', '9', '3', '3', '8', '8', '8', '8', '3', '1', '3', '3', '1', '11', '0', '8', '8', '7', '8', '12', '1', '8', '9', '8', '9', '8', '8', '3', '8', '8', '1', '1', '1', '9', '2', '2', '2', '8', '7', '12', '8', '8', '9', '10', '10', '7', '8', '1', '9', '8', '7', '3', '1', '3', '8', '2', '2', '3', '9', '8', '4', '4', '8', '9', '2', '1', '1', '7', '8', '9', '9', '7', '8', '7', '7', '8', '2', '2', '8', '4', '9', '7', '10', '0', '9', '8', '3', '7', '8', '1', '1', '8', '9', '9', '2', '2', '10', '1', '9', '10', '10', '10', '8', '3', '2', '12', '9', '9', '10', '12', '9', '12', '12', '9', '1', '2', '4', '12', '12', '7', '8', '9', '7', '7', '7', '3', '9', '8', '9', '1', '12', '8', '9', '4', '1', '3', '12', '12', '12', '12', '8', '8', '2', '1', '1', '2', '1', '1', '1', '12', '12', '8', '12', '2', '2', '12', '3', '3', '12', '8', '2', '8', '8', '12', '2', '1', '10', '3', '2', '8', '7', '1', '8', '1', '3', '7', '8', '9', '8', '3', '1', '1', '7', '8', '8', '9', '8', '2', '9', '2', '2', '9', '8', '1', '8', '8', '1', '3', '3', '1', '1', '10', '1', '2', '8', '1', '1', '1', '1', '9', '1', '4', '1', '7', '7', '7', '7', '2', '2', '8', '8', '12', '1', '9', '1', '7', '3', '3', '1', '8', '8', '10', '8', '9', '2', '9', '1', '3', '8', '8', '3', '12', '2', '8', '12', '2', '9', '1', '3', '3', '3', '3', '2', '8', '7', '9', '8', '3', '3', '1', '7', '8', '3', '1', '1', '12', '8', '9', '1', '2', '3', '8', '1', '1', '3', '3', '9', '1', '1', '1', '12', '1', '7', '3', '3', '1', '8', '8', '8', '1', '2', '4', '8', '1', '10', '2', '5', '3', '3', '12', '10', '9', '9', '12', '9', '0', '2', '8', '8', '9', '9', '9', '8', '1', '3', '1', '1', '4', '8', '1', '10', '8', '7', '2', '8', '2', '8', '4', '7', '8', '1', '9', '1', '9', '8', '2', '8', '2', '7', '9', '2', '2', '9', '1', '8', '12', '1', '8', '1', '4', '1', '9', '9', '1', '10', '12', '4', '8', '1', '7', '3', '9', '2', '12', '7', '8', '8', '2', '1', '12', '9', '8', '1', '2', '2', '8', '10', '2', '1', '7', '7', '7', '12', '3', '3', '8', '3', '8', '8', '3', '9', '8', '9', '1', '1', '8', '7', '9', '3', '3', '8', '1', '0', '9', '9', '9', '1', '8', '9', '9', '10', '1', '8', '7', '8', '8', '8', '9', '9', '8', '9', '10', '4', '9', '3', '7', '12', '1', '9', '9', '8', '9', '1', '9', '3', '1', '8', '4', '12', '10', '9', '8', '7', '10', '8', '12', '12', '3', '10', '8', '12', '2', '1', '2', '3', '9', '8', '8', '7', '1', '1', '2', '2', '1', '2', '2', '7', '1', '3', '9', '9', '3', '8', '8', '8', '8', '8', '7', '8', '8', '10', '8', '1', '8', '2', '8', '2', '2', '2', '2', '2', '1', '7', '12', '10', '1', '2', '8', '1', '4', '7', '8', '1', '12', '8', '7', '9', '2', '2', '2', '8', '1', '8', '1', '1', '0', '2', '2', '6', '1', '8', '2', '1', '8', '1', '1', '2', '2', '3', '2', '2', '10', '10', '9', '1', '7', '7', '8', '8', '1', '12', '10', '12', '3', '8', '8', '8', '3', '8', '3', '10', '2', '2', '2', '1', '2', '2', '1', '0', '1', '8', '9', '1', '1', '1', '2', '2', '2', '2', '2', '1', '1', '1', '1', '12', '9', '9', '1', '10', '11', '12', '9', '1', '1', '8', '8', '1', '2', '12', '8', '3', '7', '2', '10', '9', '7', '8', '1', '2', '2', '7', '9', '12', '2', '2', '1', '8', '9', '2', '3', '2', '2', '2', '9', '9', '8', '9', '2', '3', '8', '8', '9', '7', '3', '10', '8', '8', '9', '1', '1', '2', '8', '1', '10', '8', '8', '9', '7', '1', '7', '12', '8', '1', '7', '2', '1', '3', '9', '2', '8', '8', '1', '2', '3', '9', '3', '8', '9', '7', '7', '1', '10', '2', '8', '8', '8', '1', '8', '2', '8', '8', '1', '2', '8', '10', '2', '1', '0', '8', '3', '8', '10', '10', '12', '3', '8', '9', '3', '10', '8', '8', '8', '1', '8', '8', '2', '2', '2', '2', '2', '2', '1', '1', '10', '1', '10', '8', '2', '1', '2', '2', '8', '3', '3', '2', '2', '1', '8', '1', '3', '1', '9', '2', '2', '1', '3', '2', '2', '3', '2', '2', '3', '2', '1', '1', '2', '1', '2', '7', '8', '2', '1', '12', '12', '3', '1', '9', '9', '7', '8', '7', '2', '7', '9', '2', '2', '2', '8', '9', '7', '9', '2', '8', '9', '9', '2', '9', '1', '1', '9', '8', '3', '3', '8', '2', '7', '8', '11', '2', '8', '9', '1', '8', '3', '1', '2', '1', '2', '2', '2', '2', '8', '3', '9', '2', '10', '9', '9', '2', '7', '7', '7', '1', '3', '8', '8', '1', '1', '8', '7', '1', '8', '8', '9', '3', '3', '3', '2', '4', '8', '12', '8', '1', '12', '9', '2', '8', '2', '12', '2', '1', '3', '8', '12', '12', '11', '8', '10', '9', '2', '9', '4', '10', '1', '12', '12', '9', '3', '8', '9', '12', '9', '9', '2', '2', '2', '3', '5', '1', '1', '1', '7', '7', '8', '2', '2', '8', '2', '1', '7', '3', '2', '9', '8', '9', '1', '1', '9', '8', '9', '3', '3', '2', '9', '4', '9', '8', '8', '1', '8', '10', '2', '0', '3', '8', '8', '8', '9', '12', '9', '8', '2', '2', '8', '10', '9', '8', '2', '2', '4', '1', '9', '9', '8', '8', '10', '3', '1', '1', '2', '8', '7', '8', '2', '1', '1', '8', '12', '2', '9', '4', '8', '3', '3', '3', '8', '2', '12', '12', '8', '9', '12', '8', '2', '12', '7', '9', '2', '2', '1', '3', '1', '3', '11', '9', '8', '1', '10', '2', '12', '3', '8', '4', '1', '2', '8', '8', '3', '9', '3', '8', '7', '1', '1', '3', '1', '8', '2', '9', '8', '12', '1', '1', '1', '9', '9', '1', '8', '8', '8', '2', '2', '12', '3', '1', '1', '1', '9', '9', '3', '2', '1', '1', '9', '9', '1', '9', '9', '1', '9', '3', '9', '1', '8', '1', '9', '8', '3', '8', '8', '9', '9', '8', '2', '8', '1', '2', '2', '10', '1', '2', '1', '1', '9', '1', '9', '3', '7', '7', '2', '10', '3', '3', '1', '1', '9', '1', '2', '1', '3', '2', '1', '1', '1', '1', '1', '1', '1', '1', '1', '2', '8', '8', '7', '1', '1', '8', '8', '2', '1', '12', '1', '8', '1', '9', '8', '8', '2', '1', '9', '9', '2', '2', '8', '1', '7', '7', '9', '8', '1', '6', '8', '8', '2', '2', '8', '3', '8', '3', '2', '3', '8', '1', '2', '2', '2', '8', '9', '9', '9', '12', '1', '9', '2', '1', '9', '2', '9', '9', '10', '8', '1', '1', '9', '2', '1', '7', '1', '3', '7', '1', '1', '1', '3', '8', '8', '8', '9', '9', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '1', '0', '1', '1', '1', '2', '2', '3', '1', '1', '2', '3', '3', '3', '7', '3', '1', '9', '3', '9', '1', '4', '1', '1', '9', '10', '9', '8', '2', '3', '10', '9', '3', '10', '12', '8', '9', '8', '2', '1', '9', '8', '1', '7', '8', '1', '9', '1', '2', '10', '7', '7', '8', '1', '3', '3', '9', '3', '3', '1', '1', '1', '9', '1', '3', '8', '8', '8', '1', '8', '1', '9', '7', '8', '8', '2', '1', '3', '8', '2', '1', '6', '1', '8', '8', '2', '8', '2', '2', '9', '8', '1', '1', '6', '8', '6', '10', '2', '1', '1', '8', '9', '3', '3', '3', '1', '3', '9', '6', '8', '1', '8', '3', '1', '2', '2', '12', '7', '1', '1', '1', '1', '12', '1', '8', '8', '3', '2', '1', '9', '2', '3', '3', '2', '2', '7', '2', '2', '2', '3', '1', '9', '8', '7', '1', '1', '3', '1', '9', '4', '3', '9', '2', '1', '1', '9', '8', '8', '7', '2', '1', '3', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '2', '10', '1', '1', '8', '2', '8', '1', '8', '1', '3', '1', '2', '1', '1', '3', '1', '2', '1', '1', '1', '1', '1', '1', '1', '2', '1', '8', '1', '9', '8', '8', '1', '10', '8', '8', '1', '3', '9', '8', '9', '8', '2', '1', '3', '4', '1', '12', '1', '9', '8', '2', '7', '2', '1', '2', '9', '2', '1', '3', '8', '1', '2', '3', '8', '2', '10', '10', '10', '9', '9', '9', '3', '8', '3', '1', '1', '1', '1', '1', '1', '2', '1', '9', '2', '7', '9', '8', '7', '9', '1', '1', '1', '1', '8', '2', '2', '3', '3', '2', '12', '8', '3', '1', '1', '12', '9', '8', '2', '2', '9', '1', '2', '2', '9', '1', '1', '2', '4', '8', '1', '8', '10', '2', '9', '9', '9', '1', '2', '1', '1', '2', '2', '2', '12', '1', '2', '8', '3', '9', '9', '3', '2', '9', '9', '7', '2', '8', '8', '9', '2', '9', '1', '1', '1', '1', '1', '2', '8', '2', '1', '2', '2', '8', '3', '1', '1', '9', '1', '3', '10', '7', '9', '8', '8', '2', '2', '9', '2', '2', '2', '2', '2', '2', '9', '1', '3', '6', '4', '2', '2', '9', '1', '9', '8', '1', '1', '1', '7', '3', '2', '9', '9', '1', '9', '7', '2', '8', '9', '9', '12', '10', '8', '10', '2', '3', '1', '2', '2', '12', '12', '3', '12', '1', '2', '1', '2', '1', '2', '8', '2', '3', '12', '12', '8', '2', '2', '9', '2', '9', '2', '3', '3', '1', '1', '1', '12', '3', '7', '1', '3', '1', '2', '9', '2', '7', '2', '1', '8', '3', '7', '8', '1', '1', '3', '8', '3', '2', '9', '3', '1', '1', '1', '1', '11', '1', '2', '6', '1', '2', '3', '8', '9', '9', '2', '9', '2', '1', '1', '3', '1', '12', '9', '1', '1', '9', '1', '9', '9', '8', '9', '8', '1', '2', '7', '1', '0', '9', '8', '2', '3', '2', '4', '1', '1', '1', '8', '1', '12', '1', '2', '3', '3', '9', '9', '9', '9', '9', '7', '9', '3', '9', '1', '7', '3', '3', '3', '3', '7', '8', '2', '2', '2', '8', '2', '3', '9', '9', '3', '1', '8', '9', '12', '8', '8', '3', '2', '1', '6', '1', '9', '1', '1', '1', '1', '1', '1', '2', '2', '1', '2', '2', '2', '2', '9', '2', '5', '3', '2', '2', '8', '2', '1', '8', '3', '3', '8', '3', '1', '3', '9', '2', '8', '9', '4', '4', '9', '8', '8', '7', '7', '3', '1', '8', '8', '3', '2', '2', '10', '3', '3', '2', '2', '2', '2', '12', '3', '1', '2', '7', '7', '12', '2', '1', '2', '1', '1', '2', '1', '3', '3', '3', '1', '3', '9', '4', '8', '8', '4', '2', '4', '2', '8', '9', '1', '1', '2', '2', '2', '8', '7', '2', '8', '2', '9', '1', '8', '1', '1', '2', '1', '1', '1', '2', '9', '9', '8', '1', '8', '1', '4', '8', '1', '2', '7', '9', '2', '1', '8', '9', '8', '9', '1', '1', '2', '3', '2', '12', '7', '1', '2', '4', '4', '4', '2', '2', '2', '9', '8', '8', '1', '12', '3', '3', '2', '2', '3', '12', '1', '5', '1', '1', '3', '2', '8', '8', '2', '2', '2', '3', '9', '8', '8', '9', '8', '8', '11', '8', '2', '9', '2', '3', '1', '7', '1', '8', '2', '8', '1', '1', '1', '8', '1', '1', '1', '7', '8', '8', '2', '3', '2', '1', '8', '8', '1', '8', '4', '8', '9', '8', '9', '8', '1', '2', '3', '2', '1', '2', '2', '3', '1', '9', '4', '8', '1', '1', '9', '1', '1', '8', '2', '2', '2', '1', '7', '3', '3', '1', '3', '3', '12', '3', '1', '1', '3', '1', '1', '4', '8', '4', '4', '3', '3', '3', '4', '8', '1', '9', '9', '9', '4', '1', '7', '4', '8', '4', '9', '8', '1', '1', '3', '8', '9', '1', '9', '7', '1', '1', '9', '1', '8', '8', '8', '1', '1', '8', '2', '2', '2', '9', '4', '4', '1', '2', '1', '2', '9', '1', '1', '8', '5', '5', '5', '9', '12', '4', '1', '2', '1', '8', '8', '1', '9', '9', '2', '12', '1', '1', '7', '8', '8', '4', '1', '2', '8', '11', '2', '3', '2', '2', '1', '11', '2', '2', '2', '2', '9', '9', '2', '2', '9', '3', '1', '9', '1', '1', '1', '7', '8', '1', '8', '9', '1', '2', '9', '8', '1', '2', '9', '8', '12', '2', '2', '10', '8', '2', '2', '8', '8', '1', '2', '6', '1', '1', '7', '7', '2', '3', '2', '1', '1', '8', '1', '10', '8', '12', '6', '4', '1', '4', '1', '2', '9', '8', '9', '4', '8', '8', '10', '8', '8', '8', '8', '8', '2', '2', '8', '4', '9', '3', '3', '3', '3', '3', '9', '2', '1', '1', '2', '3', '2', '1', '2', '3', '3', '3', '2', '2', '3', '3', '2', '1', '2', '3', '3', '3', '3', '12', '9', '3', '2', '2', '3', '1', '5', '1', '1', '3', '2', '2', '1', '2', '9', '3', '9', '8', '1', '8', '7', '7', '9', '1', '1', '7', '9', '10', '1', '7', '1', '4', '3', '8', '9', '4', '9', '9', '2', '9', '2', '8', '4', '2', '2', '9', '5', '2', '1', '10', '9', '3', '1', '1', '1', '2', '1', '2', '9', '1', '7', '9', '9', '8', '2', '2', '8', '9', '1', '9', '8', '2', '3', '2', '2', '1', '2', '2', '2', '1', '1', '8', '1', '9', '4', '12', '8', '2', '7', '2', '2', '1', '9', '3', '1', '8', '12', '1', '1', '2', '1', '10', '2', '6', '12', '12', '4', '9', '1', '9', '10', '1', '2', '8', '2', '1', '8', '6', '12', '9', '9', '2', '1', '7', '8', '9', '5', '8', '1', '8', '8', '3', '1', '8', '2', '8', '2', '2', '1', '2', '9', '2', '8', '1', '12', '8', '2', '7', '3', '3', '12', '2', '3', '3', '9', '9', '3', '3', '3', '3', '3', '2', '1', '4', '1', '4', '8', '8', '2', '1', '2', '9', '12', '1', '10', '12', '1', '2', '9', '4', '8', '10', '3', '2', '8', '7', '4', '8', '9', '2', '7', '1', '10', '2', '2', '2', '8', '4', '2', '2', '1', '1', '8', '7', '9', '2', '1', '9', '8', '9', '12', '4', '8', '2', '8', '7', '7', '1', '9', '2', '1', '1', '9', '1', '8', '2', '3', '9', '9', '1', '3', '2', '9', '2', '2', '8', '1', '1', '2', '1', '2', '1', '9', '8', '2', '2', '4', '8', '8', '9', '5', '5', '9', '8', '9', '6', '2', '1', '12', '2', '3', '8', '8', '9', '2', '2', '1', '3', '2', '10', '9', '10', '7', '2', '8', '4', '2', '8', '9', '8', '1', '3', '8', '8', '1', '8', '3', '8', '8', '1', '10', '3', '1', '5', '8', '8', '9', '2', '2', '9', '2', '1', '9', '1', '4', '1', '12', '8', '8', '2', '1', '1', '1', '1', '1', '9', '2', '9', '8', '9', '5', '9', '1', '8', '2', '4', '1', '1', '1', '9', '12', '4', '1', '1', '9', '8', '4', '1', '8', '9', '8', '8', '9', '9', '9', '9', '9', '9', '1', '3', '4', '2', '2', '8', '2', '9', '2', '3', '8', '3', '1', '7', '2', '1', '1', '4', '2', '9', '2', '1', '3', '2', '9', '2', '9', '3', '8', '1', '9', '1', '1', '2', '8', '8', '8', '2', '3', '9', '9', '8', '2', '4', '1', '5', '1', '1', '9', '9', '2', '1', '1', '1', '9', '9', '1', '3', '9', '2', '2', '9', '9', '8', '3', '9', '9', '8', '2', '2', '8', '9', '2', '2', '8', '9', '10', '10', '2', '8', '7', '9', '9', '4', '2', '4', '11', '2', '1', '7', '8', '8', '8', '9', '4', '9', '8', '8', '2', '1', '1', '9', '8', '3', '3', '8', '8', '7', '9', '1', '3', '4', '8', '10', '2', '4', '4', '8', '2', '8', '2', '8', '9', '2', '1', '2', '2', '3', '1', '9', '4', '5', '9', '9', '1', '1', '1', '1', '1', '1', '7', '1', '1', '1', '8', '1', '1', '1', '1', '1', '9', '1', '2', '1', '12', '9', '3', '9', '9', '4', '1', '9', '1', '1', '2', '11', '9', '3', '2', '2', '2', '4', '2', '3', '2', '7', '7', '8', '1', '10', '9', '5', '7', '1', '10', '8', '1', '2', '7', '1', '12', '5', '8', '4', '2', '9', '9', '8', '2', '9', '8', '8', '10', '2', '3', '2', '7', '2', '8', '10', '3', '2', '9', '9', '9', '1', '1', '1', '1', '8', '2', '10', '9', '8', '1', '2', '2', '2', '1', '2', '1', '3', '9', '12', '2', '2', '4', '8', '1', '3', '9', '4', '9', '1', '9', '9', '2', '1', '1', '9', '7', '10', '3', '10', '2', '9', '2', '9', '2', '8', '2', '1', '2', '3', '1', '1', '1', '8', '5', '8', '3', '1', '1', '4', '2', '2', '1', '3', '8', '2', '1', '1', '1', '2', '9', '4', '1', '8', '1', '2', '8', '2', '9', '2', '2', '5', '5', '2', '5', '2', '1', '8', '8', '1', '3', '12', '2', '4', '3', '2', '2', '2', '6', '2', '3', '3', '1', '8', '2', '2', '1', '1', '8', '8', '10', '1', '9', '2', '9', '12', '0', '2', '1', '3', '2', '2', '1', '2', '1', '9', '9', '2', '9', '8', '8', '1', '7', '8', '1', '2', '6', '9', '8', '10', '1', '12', '2', '2', '1', '8', '2', '1', '12', '7', '4', '2', '10', '2', '2', '1', '2', '1', '2', '1', '1', '2', '1', '9', '8', '10', '2', '10', '9', '9', '12', '5', '3', '2', '8', '2', '8', '3', '3', '1', '4', '9', '10', '2', '4', '8', '1', '2', '10', '12', '12', '3', '1', '1', '2', '9', '6', '6', '1', '9', '1', '8', '1', '8', '8', '8', '8', '2', '2', '2', '8', '8', '2', '1', '1', '1', '1', '1', '8', '8', '5', '8', '8', '1', '8', '9', '9', '1', '7', '12', '9', '9', '7', '2', '2', '7', '8', '9', '8', '2', '9', '6', '3', '4', '8', '4', '1', '2', '9', '8', '8', '8', '2', '1', '1', '8', '8', '3', '1', '9', '9', '7', '1', '2', '2', '2', '9', '4', '1', '2', '12', '1', '8', '2', '1', '9', '1', '8', '9', '9', '2', '5', '4', '2', '1', '2', '8', '12', '8', '10', '3', '8', '8', '8', '2', '2', '8', '2', '8', '2', '4', '8', '10', '2', '7', '2', '1', '8', '8', '9', '5', '8', '1', '12', '7', '10', '2', '2', '2', '1', '4', '8', '12', '1', '8', '2', '2', '9', '1', '8', '3', '1', '1', '5', '10', '2', '1', '2', '4', '8', '9', '3', '2', '8', '7', '4', '4', '9', '9', '2', '8', '1', '1', '2', '4', '7', '1', '9', '1', '1', '1', '8', '4', '1', '3', '1', '2', '2', '9', '1', '8', '8', '2', '2', '9', '9', '1', '8', '8', '1', '9', '8', '8', '2', '2', '2', '2', '1', '1', '1', '7', '4', '1', '1', '4', '2', '3', '3', '2', '3', '9', '2', '8', '2', '1', '5', '1', '2', '2', '1', '3', '5', '2', '9', '1', '1', '9', '8', '1', '4', '8', '1', '8', '2', '4', '8', '4', '7', '9', '8', '1', '2', '2', '4', '4', '11', '3', '11', '1', '3', '8', '1', '9', '6', '8', '9', '3', '9', '8', '9', '2', '3', '3', '7', '1', '7', '2', '9', '9', '1', '8', '8', '1', '5', '5', '8', '8', '1', '2', '1', '3', '9', '9', '5', '9', '8', '1', '4', '3', '8', '1', '1', '2', '1', '8', '9', '8', '2', '2', '2', '1', '2', '4', '10', '8', '1', '9', '1', '8', '1', '1', '9', '9', '8', '11', '1', '8', '2', '1', '2', '8', '2', '8', '8', '2', '9', '6', '10', '1', '8', '6', '4', '5', '11', '2', '2', '8', '1', '8', '1', '8', '1', '4', '8', '1', '1', '1', '8', '3', '7', '3', '3', '7', '1', '1', '4', '8', '1', '6', '4', '2', '2', '1', '1', '1', '1', '6', '2', '2', '8', '1', '8', '5', '5', '2', '1', '1', '2', '3', '7', '1', '1', '6', '2', '9', '3', '9', '8', '9', '2', '1', '4', '2', '9', '13', '9', '2', '8', '12', '4', '1', '1', '9', '5', '1', '1', '1', '1', '8', '2', '1', '1', '2', '9', '0', '12', '1', '1', '8', '9', '8', '3', '2', '2', '12', '8', '2', '2', '2', '1', '10', '1', '9', '8', '9', '5', '2', '2', '9', '2', '8', '4', '8', '3', '7', '9', '2', '2', '7', '2', '2', '8', '1', '1', '9', '10', '9', '8', '2', '9', '0', '7', '1', '1', '1', '9', '8', '2', '8', '12', '8', '3', '2', '8', '8', '7', '7', '8', '12', '2', '1', '3', '9', '2', '2', '8', '2', '8', '8', '12', '8', '8', '1', '8', '9', '2', '4', '4', '7', '2', '1', '1', '9', '3', '7', '1', '3', '9', '8', '2', '2', '3', '1', '3', '10', '2', '3', '7', '1', '8', '1', '1', '10', '3', '10', '8', '4', '1', '2', '4', '3', '10', '9', '3', '9', '5', '8', '7', '2', '9', '3', '3', '2', '11', '8', '1', '1', '7', '2', '7', '2', '9', '4', '8', '1', '12', '8', '7', '9', '1', '0', '2', '6', '4', '9', '3', '2', '4', '12', '9', '8', '5', '9', '2', '8', '4', '6', '8', '9', '3', '1', '8', '7', '3', '1', '1', '1', '8', '9', '1', '8', '7', '9', '8', '2', '1', '2', '2', '3', '2', '2', '1', '1', '4', '9', '7', '3', '2', '8', '8', '9', '1', '9', '7', '4', '2', '2', '8', '5', '9', '1', '5', '7', '1', '4', '6', '9', '10', '1', '8', '2', '9', '1', '9', '2', '9', '8', '2', '9', '9', '11', '2', '12', '2', '2', '9', '4', '8', '1', '9', '9', '2', '8', '9', '1', '2', '9', '3', '8', '8', '1', '7', '8', '8', '2', '3', '2', '2', '2', '2', '9', '8', '10', '2', '2', '1', '9', '11', '1', '8', '8', '2', '4', '2', '1', '2', '8', '8', '9', '2', '8', '9', '2', '3', '1', '2', '1', '3', '10', '8', '1', '2', '4', '1', '9', '8', '9', '3', '3', '9', '8', '2', '11', '3', '7', '1', '8', '7', '1', '8', '8', '8', '1', '4', '8', '8', '1', '9', '3', '1', '1', '2', '8', '8', '8', '2', '1', '8', '10', '2', '12', '8', '8', '9', '1', '8', '1', '3', '2', '2', '10', '7', '2', '2', '1', '1', '8', '2', '9', '1', '2', '9', '3', '3', '10', '1', '4', '7', '10', '12', '2', '4', '10', '8', '6', '2', '9', '1', '8', '3', '1', '2', '8', '8', '2', '12', '10', '2', '10', '9', '1', '2', '5', '8', '12', '8', '8', '7', '8', '8', '1', '8', '2', '2', '9', '1', '8', '10', '5', '5', '5', '8', '1', '9', '1', '13', '1', '2', '2', '1', '2', '4', '9', '2', '1', '9', '13', '11', '1', '9', '9', '3', '10', '8', '7', '8', '8', '4', '2', '1', '9', '4', '3', '1', '1', '10', '9', '2', '8', '6', '2', '1', '1', '3', '2', '8', '1', '1', '1', '1', '2', '4', '1', '1', '10', '1', '8', '1', '1', '9', '1', '7', '1', '1', '9', '9', '2', '12', '9', '1', '8', '1', '12', '1', '8', '3', '8', '1', '8', '1', '2', '10', '1', '1', '2', '2', '9', '3', '9', '12', '1', '9', '1', '2', '8', '7', '2', '1', '1', '3', '2', '8', '4', '4', '5', '12', '7', '8', '9', '6', '1', '8', '9', '2', '1', '9', '1', '4', '1', '8', '2', '1', '1', '8', '7', '4', '9', '1', '8', '4', '2', '7', '5', '1', '3', '9', '2', '8', '2', '1', '2', '9', '1', '1', '2', '3', '4', '3', '1', '2', '2', '10', '2', '2', '8', '1', '1', '2', '4', '9', '2', '11', '9', '8', '9', '2', '1', '10', '4', '1', '1', '10', '7', '2', '10', '8', '1', '10', '8', '8', '8', '8', '8', '8', '2', '2', '2', '8', '3', '4', '1', '1', '1', '2', '1', '8', '8', '2', '8', '3', '8', '3', '1', '2', '3', '1', '1', '10', '1', '4', '4', '3', '3', '1', '9', '1', '1', '2', '1', '1', '6', '1', '1', '1', '9', '1', '4', '1', '1', '1', '1', '1', '1', '2', '12', '8', '8', '1', '9', '12', '10', '8', '2', '12', '1', '1', '2', '1', '7', '10', '7', '1', '2', '1', '10', '8', '2', '2', '8', '2', '2', '1', '9', '8', '8', '8', '8', '3', '1', '4', '2', '8', '3', '10', '10', '1', '9', '1', '9', '1', '9', '1', '8', '3', '1', '1', '8', '8', '8', '6', '3', '4', '1', '1', '1', '9', '9', '5', '2', '10', '2', '6', '2', '2', '7', '1', '8', '2', '11', '1', '1', '1', '2', '1', '9', '4', '4', '2', '8', '4', '6', '8', '8', '10', '10', '2', '1', '2', '9', '1', '8', '8', '3', '4', '8', '8', '2', '8', '8', '1', '2', '1', '2', '9', '4', '1', '3', '3', '3', '8', '8', '8', '2', '6', '4', '3', '3', '12', '3', '4', '9', '6', '8', '7', '7', '8', '6', '4', '1', '10', '6', '3', '3', '8', '1', '1', '8', '1', '11', '8', '1', '1', '2', '3', '11', '7', '2', '1', '1', '1', '9', '1', '2', '1', '1', '9', '8', '1', '1', '8', '0', '4', '1', '1', '2', '1', '1', '2', '2', '9', '1', '10', '1', '3', '8', '9', '3', '3', '1', '10', '1', '9', '1', '1', '7', '4', '1', '8', '10', '1', '2', '10', '1', '1', '2', '3', '3', '9', '1', '8', '10', '1', '7', '9', '1', '9', '6', '3', '9', '2', '8', '4', '12', '12', '3', '1', '1', '9', '1', '9', '2', '1', '1', '1', '1', '1', '9', '2', '1', '2', '10', '9', '8', '8', '10', '8', '2', '3', '2', '1', '12', '8', '12', '2', '9', '1', '3', '2', '5', '2', '2', '8', '2', '2', '4', '10', '9', '10', '9', '4', '2', '4', '12', '8', '2', '1', '8', '3', '9', '2', '9', '4', '1', '1', '1', '1', '1', '9', '6', '2', '9', '2', '1', '3', '2', '2', '5', '8', '9', '1', '6', '3', '1', '2', '2', '2', '6', '1', '8', '10', '8', '3', '3', '13', '2', '9', '9', '10', '6', '12', '8', '2', '9', '2', '3', '8', '8', '1', '2', '2', '3', '2', '10', '1', '8', '9', '1', '8', '9', '8', '8', '2', '10', '2', '1', '1', '1', '9', '9', '12', '1', '1', '4', '8', '3', '2', '2', '4', '4', '2', '2', '1', '1', '1', '9', '2', '2', '4', '1', '1', '8', '8', '8', '9', '2', '13', '9', '9', '4', '1', '9', '7', '10', '10', '8', '1', '9', '1', '1', '1', '8', '12', '4', '1', '2', '1', '1', '1', '7', '8', '3', '3', '1', '2', '2', '7', '2', '2', '10', '1', '8', '4', '0', '1', '9', '4', '1', '1', '2', '10', '10', '7', '9', '3', '12', '11', '2', '8', '10', '8', '9', '4', '3', '4', '10', '1', '1', '9', '3', '10', '1', '5', '3', '3', '2', '2', '1', '1', '6', '1', '7', '1', '1', '12', '1', '1', '1', '10', '1', '8', '8', '1', '3', '8', '3', '1', '2', '8', '3', '4', '2', '8', '8', '6', '1', '1', '4', '1', '2', '9', '5', '1', '7', '8', '9', '2', '4', '9', '8', '5', '9', '9', '9', '1', '2', '1', '3', '8', '9', '2', '9', '8', '9', '1', '8', '13', '9', '3', '8', '1', '4', '1', '3', '1', '6', '4', '10', '2', '2', '7', '8', '8', '12', '8', '9', '9', '9', '10', '3', '12', '12', '2', '9', '10', '9', '7', '2', '3', '4', '10', '10', '2', '7', '7', '1', '7', '3', '9', '1', '9', '8', '2', '9', '1', '8', '1', '10', '8', '8', '2', '7', '8', '1', '8', '9', '8', '1', '4', '1', '8', '8', '8', '1', '1', '10', '3', '2', '2', '6', '8', '9', '8', '2', '1', '1', '3', '8', '8', '1', '4', '4', '3', '9', '10', '4', '2', '4', '2', '9', '2', '2', '9', '1', '1', '10', '1', '1', '2', '1', '1', '1', '9', '1', '8', '8', '8', '2', '3', '8', '1', '6', '7', '3', '1', '9', '1', '2', '1', '6', '1', '2', '10', '1', '4', '2', '3', '2', '9', '9', '4', '9', '9', '1', '2', '8', '4', '7', '8', '7', '13', '9', '1', '2', '1', '9', '3', '3', '6', '8', '9', '10', '9', '2', '2', '3', '12', '1', '1', '1', '13', '2', '1', '7', '10', '8', '9', '1', '2', '9', '1', '1', '1', '2', '1', '12', '5', '6', '9', '10', '3', '1', '10', '1', '3', '2', '8', '9', '2', '10', '8', '8', '9', '2', '8', '9', '2', '2', '2', '8', '8', '2', '3', '1', '4', '8', '1', '9', '9', '2', '8', '10', '9', '1', '2', '12', '8', '8', '0', '2', '1', '1', '9', '10', '1', '6', '2', '2', '10', '2', '2', '4', '2', '2', '12', '1', '6', '7', '8', '9', '1', '3', '5', '7', '3', '1', '3', '1', '1', '3', '2', '6', '3', '2', '1', '8', '3', '5', '2', '1', '1', '8', '1', '1', '0', '2', '3', '5', '3', '1', '1', '9', '8', '8', '8', '8', '4', '7', '2', '9', '12', '5', '9', '3', '12', '3', '9', '1', '9', '1', '7', '3', '9', '3', '10', '1', '2', '1', '4', '9', '8', '2', '9', '11', '2', '8', '9', '4', '1', '1', '1', '1', '1', '1', '1', '9', '1', '9', '1', '1', '1', '12', '3', '10', '3', '6', '1', '7', '10', '8', '3', '7', '1', '1', '2', '6', '1', '3', '9', '2', '8', '1', '8', '8', '1', '8', '10', '12', '1', '10', '12', '10', '1', '8', '8', '1', '4', '7', '9', '2', '1', '9', '3', '8', '12', '10', '6', '1', '6', '8', '3', '1', '3', '1', '10', '10', '9', '8', '1', '2', '9', '8', '8', '1', '9', '8', '8', '3', '1', '3', '9', '12', '1', '8', '1', '5', '8', '11', '5', '5', '1', '2', '1', '3', '1', '1', '1', '9', '8', '8', '1', '9', '10', '8', '4', '1', '1', '8', '10', '8', '9', '1', '10', '1', '7', '2', '1', '1', '9', '6', '2', '9', '8', '1', '1', '1', '9', '8', '8', '1', '9', '9', '1', '1', '2', '7', '9', '2', '9', '9', '8', '7', '6', '8', '7', '8', '2', '9', '2', '1', '8', '8', '8', '2', '6', '3', '1', '8', '12', '12', '8', '7', '1', '2', '2', '7', '1', '9', '8', '1', '4', '1', '1', '8', '5', '8', '1', '8', '8', '8', '8', '1', '2', '11', '1', '1', '1', '2', '7', '1', '9', '2', '2', '2', '9', '8', '9', '2', '9', '1', '8', '1', '7', '11', '13', '1', '9', '8', '2', '2', '1', '2', '3', '10', '3', '10', '4', '3', '9', '1', '8', '1', '1', '13', '1', '1', '3', '8', '1', '2', '2', '2', '9', '4', '1', '8', '1', '9', '3', '6', '8', '5', '2', '8', '2', '8', '1', '9', '2', '2', '8', '1', '1', '4', '8', '9', '2', '2', '7', '1', '8', '12', '2', '12', '8', '2', '9', '9', '3', '8', '1', '1', '9', '9', '9', '2', '8', '1', '9', '1', '8', '9', '8', '2', '2', '4', '1', '9', '10', '10', '8', '8', '1', '1', '2', '1', '4', '1', '2', '1', '1', '3', '12', '8', '8', '8', '8', '10', '2', '8', '12', '1', '9', '9', '8', '9', '1', '2', '1', '8', '8', '8', '2', '8', '8', '3', '10', '8', '9', '1', '1', '1', '3', '4', '8', '10', '6', '3', '1', '3', '2', '8', '9', '3', '5', '4', '1', '5', '2', '9', '9', '9', '1', '1', '1', '9', '9', '6', '10', '9', '8', '12', '2', '9', '1', '2', '1', '1', '1', '8', '1', '12', '1', '12', '1', '9', '1', '8', '1', '2', '1', '2', '8', '8', '10', '1', '9', '1', '2', '2', '8', '8', '3', '12', '1', '11', '2', '10', '12', '11', '2', '1', '10', '1', '1', '9', '3', '10', '2', '7', '1', '8', '1', '9', '9', '2', '1', '3', '12', '5', '9', '9', '8', '1', '8', '8', '8', '1', '1', '3', '9', '8', '9', '8', '3', '10', '3', '7', '8', '9', '2', '3', '1', '9', '10', '9', '2', '8', '2', '6', '4', '1', '1', '3', '8', '2', '3', '9', '1', '2', '1', '8', '1', '2', '2', '2', '2', '9', '9', '4', '8', '9', '1', '1', '2', '9', '1', '5', '2', '9', '8', '10', '1', '2', '10', '8', '5', '6', '4', '8', '1', '1', '1', '8', '9', '2', '8', '8', '8', '12', '2', '2', '1', '8', '1', '2', '1', '9', '8', '8', '9', '1', '1', '8', '7', '1', '8', '0', '2', '3', '8', '1', '1', '8', '1', '1', '12', '3', '2', '10', '8', '10', '6', '1', '8', '8', '10', '8', '8', '1', '4', '8', '1', '2', '9', '8', '1', '1', '3', '3', '5', '1', '1', '2', '1', '2', '1', '9', '8', '3', '9', '8', '2', '10', '9', '1', '8', '1', '10', '9', '10', '1', '2', '3', '8', '1', '3', '1', '9', '9', '8', '1', '9', '8', '8', '9', '1', '9', '2', '8', '8', '2', '13', '8', '10', '8', '9', '3', '2', '1', '3', '1', '12', '10', '10', '1', '11', '1', '2', '4', '2', '10', '1', '8', '11', '1', '1', '1', '2', '2', '8', '2', '9', '8', '9', '9', '12', '8', '1', '9', '2', '4', '1', '8', '3', '9', '1', '6', '5', '8', '9', '3', '3', '2', '1', '1', '1', '8', '11', '1', '8', '7', '1', '11', '9', '1', '7', '9', '9', '8', '8', '8', '12', '8', '1', '2', '9', '8', '8', '2', '10', '9', '8', '8', '4', '10', '9', '8', '2', '10', '10', '2', '1', '2', '1', '2', '1', '4', '1', '7', '1', '1', '3', '12', '9', '8', '5', '9', '10', '1', '9', '9', '2', '7', '4', '1', '1', '8', '12', '8', '2', '2', '9', '4', '1', '1', '5', '9', '1', '8', '12', '8', '1', '1', '4', '9', '10', '2', '3', '1', '3', '3', '3', '8', '1', '2', '1', '2', '1', '2', '9', '1', '2', '8', '9', '4', '8', '8', '10', '2', '12', '5', '5', '1', '8', '1', '7', '8', '1', '1', '8', '9', '12', '4', '9', '3', '1', '2', '5', '9', '2', '1', '2', '2', '1', '2', '8', '1', '8', '8', '2', '2', '1', '8', '4', '1', '2', '10', '9', '10', '8', '1', '10', '4', '8', '8', '10', '2', '8', '1', '1', '3', '10', '1', '4', '2', '8', '3', '3', '2', '9', '8', '5', '5', '9', '3', '10', '8', '1', '2', '1', '2', '9', '1', '1', '2', '9', '9', '8', '8', '11', '9', '4', '1', '8', '2', '7', '1', '2', '8', '2', '9', '9', '8', '2', '9', '9', '8', '8', '1', '2', '8', '1', '1', '2', '9', '8', '1', '4', '9', '1', '8', '12', '1', '12', '2', '8', '2', '9', '1', '12', '1', '2', '9', '1', '1', '9', '3', '2', '9', '9', '2', '11', '8', '7', '9', '9', '9', '8', '8', '1', '2', '8', '1', '1', '4', '1', '2', '10', '1', '9', '2', '1', '1', '1', '9', '1', '13', '4', '3', '2', '1', '1', '2', '2', '7', '8', '7', '1', '1', '2', '8', '8', '3', '4', '8', '2', '10', '13', '8', '8', '9', '2', '2', '1', '1', '10', '4', '2', '7', '9', '8', '2', '1', '9', '8', '1', '1', '9', '8', '9', '8', '2', '8', '2', '2', '9', '2', '1', '9', '1', '2', '4', '8', '2', '1', '1', '8', '1', '4', '1', '9', '3', '9', '2', '7', '1', '9', '6', '1', '10', '2', '2', '2', '2', '2', '10', '10', '10', '9', '9', '9', '3', '12', '10', '1', '1', '10', '1', '1', '2', '2', '1', '2', '3', '12', '8', '8', '1', '2', '9', '9', '1', '1', '10', '8', '9', '8', '3', '1', '1', '3', '1', '10', '1', '1', '1', '4', '1', '1', '1', '7', '10', '2', '1', '8', '1', '9', '3', '1', '10', '8', '8', '1', '5', '9', '1', '2', '1', '8', '8', '1', '3', '1', '10', '2', '1', '1', '2', '8', '2', '5', '3', '5', '2', '1', '1', '9', '1', '7', '9', '9', '2', '8', '1', '12', '4', '1', '4', '9', '9', '4', '8', '1', '10', '10', '8', '8', '8', '9', '2', '3', '5', '1', '8', '1', '1', '1', '7', '10', '1', '4', '12', '3', '2', '2', '9', '1', '1', '1', '2', '8', '4', '1', '7', '3', '12', '8', '6', '2', '2', '7', '9', '11', '1', '10', '9', '1', '8', '12', '2', '11', '1', '2', '3', '1', '1', '1', '8', '9', '2', '2', '2', '2', '3', '3', '8', '10', '4', '1', '2', '9', '8', '2', '2', '9', '1', '8', '11', '4', '2', '8', '7', '1', '1', '7', '3', '9', '9', '4', '5', '8', '10', '2', '9', '9', '9', '8', '1', '9', '2', '2', '2', '1', '2', '9', '4', '3', '12', '4', '3', '2', '3', '8', '3', '10', '1', '10', '1', '1', '8', '10', '6', '8', '9', '9', '1', '2', '9', '2', '9', '3', '8', '1', '1', '12', '1', '5', '1', '10', '8', '10', '2', '7', '1', '1', '1', '1', '3', '2', '3', '5', '9', '1', '2', '1', '9', '8', '10', '9', '8', '1', '8', '1', '8', '2', '8', '1', '2', '8', '1', '2', '5', '8', '12', '2', '1', '1', '1', '1', '1', '8', '8', '2', '1', '4', '2', '8', '8', '8', '2', '9', '9', '11', '1', '2', '9', '3', '1', '1', '8', '9', '2', '4', '10', '1', '9', '8', '4', '9', '8', '10', '8', '8', '9', '2', '8', '9', '3', '3', '3', '4', '3', '2', '2', '10', '8', '2', '1', '5', '1', '9', '1', '1', '2', '1', '2', '3', '1', '1', '8', '8', '1', '10', '8', '8', '1', '6', '4', '8', '2', '5', '8', '1', '3', '8', '2', '8', '9', '1', '1', '8', '12', '10', '1', '5', '8', '2', '8', '10', '9', '9', '1', '6', '9', '1', '10', '8', '2', '9', '10', '1', '2', '1', '1', '8', '8', '3', '9', '8', '1', '10', '2', '8', '1', '10', '1', '5', '1', '1', '1', '5', '1', '1', '1', '4', '1', '1', '1', '8', '2', '8', '1', '8', '3', '8', '9', '1', '8', '1', '8', '1', '1', '2', '2', '1', '12', '9', '9', '1', '8', '2', '1', '2', '8', '1', '9', '1', '1', '2', '1', '2', '2', '2', '1', '9', '9', '8', '8', '1', '1', '9', '10', '8', '4', '3', '3', '4', '9', '1', '3', '3', '10', '2', '2', '1', '8', '4', '1', '9', '4', '1', '1', '9', '8', '10', '4', '9', '9', '8', '2', '1', '3', '4', '9', '3', '8', '9', '1', '1', '2', '1', '7', '8', '2', '9', '1', '9', '2', '6', '9', '10', '8', '1', '1', '5', '9', '5', '1', '9', '10', '9', '3', '3', '8', '8', '2', '9', '2', '8', '2', '1', '8', '5', '8', '3', '10', '1', '1', '2', '1', '8', '1', '4', '3', '2', '9', '4', '9', '8', '8', '8', '1', '3', '9', '1', '9', '9', '1', '2', '1', '9', '8', '9', '8', '1', '1', '1', '4', '2', '2', '9', '9', '1', '1', '1', '3', '6', '1', '2', '3', '1', '4', '2', '9', '1', '9', '9', '1', '8', '2', '1', '9', '1', '8', '1', '8', '8', '2', '1', '9', '9', '6', '8', '8', '9', '8', '10', '8', '9', '5', '1', '1', '1', '1', '8', '1', '8', '12', '8', '1', '12', '9', '2', '8', '1', '1', '2', '1', '8', '6', '9', '9', '8', '1', '8', '9', '7', '7', '8', '9', '9', '1', '8', '3', '8', '1', '3', '3', '8', '9', '8', '2', '8', '1', '2', '9', '8', '1', '1', '1', '1', '9', '1', '9', '8', '12', '3', '8', '8', '1', '10', '10', '10', '2', '2', '1', '10', '2', '2', '10', '8', '12', '8', '1', '1', '1', '2', '1', '1', '3', '8', '1', '2', '2', '2', '1', '1', '6', '2', '8', '8', '8', '1', '1', '1', '9', '9', '2', '8', '10', '2', '8', '2', '1', '2', '9', '9', '2', '1', '1', '8', '8', '1', '3', '1', '8', '9', '1', '10', '9', '1', '1', '1', '1', '7', '2', '2', '1', '2', '8', '8', '1', '2', '1', '3', '2', '1', '8', '3', '9', '2', '10', '2', '10', '1', '2', '1', '1', '13', '9', '10', '8', '2', '2', '1', '1', '8', '2', '2', '1', '9', '1', '1', '8', '9', '9', '2', '9', '2', '2', '1', '1', '1', '8', '9', '4', '9', '9', '1', '8', '2', '1', '1', '8', '2', '2', '2', '8', '1', '1', '8', '2', '10', '2', '9', '9', '7', '1', '1', '2', '2', '8', '3', '1', '2', '1', '1', '8', '1', '1', '1', '9', '1', '1', '8', '8', '1', '9', '6', '1', '9', '1', '8', '1', '8', '8', '8', '1', '3', '8', '8', '2', '6', '3', '2', '1', '10', '1', '8', '2', '1', '6', '1', '1', '1', '8', '1', '10', '1', '1', '8', '9', '2', '6', '9', '2', '9', '4', '1', '8', '1', '3', '7', '8', '8', '5', '8', '7', '1', '1', '1', '13', '8', '3', '1', '1', '1', '1', '2', '8', '2', '12', '8', '1', '2', '5', '1', '8', '2', '10', '8', '2', '10', '1', '5', '2', '1', '1', '5', '2', '2', '7', '8', '2', '9', '9', '1', '10', '1', '8', '2', '5', '8', '1', '5', '1', '10', '1', '1', '9', '8', '6', '1', '8', '9', '8', '1', '8', '2', '2', '8', '3', '9', '1', '9', '3', '8', '4', '4', '3', '3', '1', '1', '1', '8', '9', '1', '1', '8', '3', '1', '2', '8', '2', '1', '1', '10', '1', '2', '2', '1', '1', '8', '2', '8', '2', '10', '2', '2', '10', '8', '8', '8', '13', '2', '2', '9', '2', '5', '9', '2', '1', '8', '8', '8', '1', '12', '12', '2', '9', '2', '1', '8', '2', '1', '8', '1', '9', '9', '2', '7', '1', '1', '7', '1', '4', '3', '1', '10', '3', '10', '9', '4', '6', '7', '5', '2', '1', '8', '8', '1', '4', '9', '2', '1', '8', '1', '1', '1', '1', '1', '8', '9', '8', '8', '8', '8', '9', '8', '10', '1', '8', '2', '7', '8', '9', '1', '2', '5', '9', '10', '8', '9', '12', '1', '6', '1', '2', '10', '9', '4', '1', '8', '10', '10', '1', '1', '1', '1', '1', '2', '3', '8', '1', '2', '10', '2', '1', '2', '4', '2', '2', '9', '1', '4', '2', '1', '1', '12', '12', '9', '9', '7', '1', '8', '4', '9', '9', '8', '1', '7', '3', '2', '1', '1', '4', '8', '1', '5', '8', '2', '8', '1', '14', '9', '12', '1', '10', '2', '1', '1', '8', '8', '6', '6', '3', '1', '2', '8', '1', '3', '2', '1', '8', '8', '1', '10', '8', '2', '8', '2', '5', '9', '1', '8', '8', '1', '1', '8', '8', '13', '1', '3', '8', '4', '1', '9', '7', '8', '8', '8', '3', '8', '1', '4', '2', '8', '8', '8', '7', '1', '1', '2', '8', '8', '9', '8', '8', '2', '3', '9', '9', '2', '1', '2', '2', '1', '8', '2', '6', '2', '1', '1', '9', '10', '3', '8', '2', '3', '2', '8', '8', '1', '8', '8', '8', '9', '1', '4', '1', '9', '2', '1', '8', '0', '1', '8', '1', '1', '8', '9', '9', '1', '3', '1', '2', '8', '8', '7', '1', '1', '2', '9', '2', '10', '2', '8', '2', '2', '2', '7', '1', '9', '8', '1', '3', '9', '2', '1', '10', '8', '1', '4', '1', '8', '5', '9', '8', '8', '1', '2', '2', '1', '8', '8', '6', '8', '1', '1', '8', '3', '2', '2', '1', '8', '1', '7', '6', '8', '1', '1', '1', '9', '1', '1', '1', '2', '1', '11', '10', '10', '11', '9']\n",
            "     filenum                                               text  label\n",
            "0          1  [ Halliburton Oil Well Cementing Co. v. Walker...      8\n",
            "1          2  Rehearing Denied Dec. See . Mr.Claude T. Barne...      1\n",
            "2          3  Rehearing Denied Dec. See . Appeal from the Di...      8\n",
            "3          4  Mr. Walter J. Cummings, Jr., of Washington, D....      2\n",
            "4          5  Mr.A. Devitt Vaneck, of Washington, D.C., for ...      8\n",
            "...      ...                                                ...    ...\n",
            "8414    8415  Opinion reported: Ante, p. DECREE It is ordere...     11\n",
            "8415    8416  In this dispute between Utah and the United St...     10\n",
            "8416    8417  The United States, to the exclusion of defenda...     10\n",
            "8417    8418  Louisiana's exception to the portion of the Sp...     11\n",
            "8418    8419  To resolve a dispute over the ownership of cer...      9\n",
            "\n",
            "[8419 rows x 3 columns]\n"
          ]
        }
      ],
      "source": [
        "fh = open('sc_model_0.txt')\n",
        "filenums=[]\n",
        "records=[]\n",
        "for record in fh:\n",
        "    record=record.split(\"---\")\n",
        "    filenum=record[0]\n",
        "    modelnum=record[1]\n",
        "    text=record[2]\n",
        "    filenums.append(filenum)\n",
        "    records.append(text)\n",
        "    \n",
        "print(len(filenums))\n",
        "print(len(records))\n",
        "\n",
        "summarized_data = pd.DataFrame(list(zip(filenums, records)),columns =['filenum','text'])\n",
        "len_list = [len(ele.split()) for ele in records]\n",
        "res = 0 if len(len_list) == 0 else (float(sum(len_list)) / len(len_list))\n",
        "print(\"Average Length %s\" % res)\n",
        "\n",
        "temp_file = open(\"labels_sc.txt\", \"r\")\n",
        "#temp_file = open(\"labels_sc_279.txt\", \"r\")\n",
        "\n",
        "data = temp_file.read()\n",
        "\n",
        "label_list = data.split(\"\\n\")\n",
        "print(label_list)\n",
        "label_list = label_list[0:-1]\n",
        "print(label_list)\n",
        "label_list = [int(i) for i in label_list]\n",
        "temp_file.close()\n",
        "\n",
        "\n",
        "new_label_list=[]\n",
        "for num in filenums:\n",
        "  new_label_list.append(label_list[int(num)-1])\n",
        "summarized_data['label'] = new_label_list\n",
        "\n",
        "print(summarized_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "kEIh_vBxF52U"
      },
      "outputs": [],
      "source": [
        "def create_model():\n",
        "    inps = Input(shape = (max_len,), dtype='int64')\n",
        "    masks= Input(shape = (max_len,), dtype='int64')\n",
        "    dbert_layer = dbert_model(inps, attention_mask=masks)[0][:,0,:]\n",
        "    dense_0 = Dense(512,activation='relu',kernel_regularizer=regularizers.l2(0.01))(dbert_layer)\n",
        "    dropout_0= Dropout(0.5)(dense_0)\n",
        "    pred = Dense(15, activation='softmax',kernel_regularizer=regularizers.l2(0.01))(dropout_0)\n",
        "    model = tf.keras.Model(inputs=[inps,masks], outputs=pred)\n",
        "    print(model.summary())\n",
        "    return model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 947,
          "referenced_widgets": [
            "52fbd984e497486495d47fc4eae74d51",
            "2235f9c65a89430abcde7ef98e00ec57",
            "91485b6e59e24168afe6ee8ba41e0eda",
            "6c006c66bd1144fcb2ab98da512f47de",
            "955453ec403041a58a61520714947e73",
            "9adb12a1141d448ba72cdadf735128c4",
            "3496e8cb43614eb799e7903e9c7af680",
            "5edf3c347b7a4c0f9f6efd10af96c783",
            "e65a4429c4bc432daaada9f1e4e477ad",
            "c32dc3b482d7420380a7b13f262071b8",
            "88f627dc3d424590b20557719e5880ec",
            "13df4bb550f74a448c6aac4a0b5a9685",
            "18edebaedfc546f1b4904998e8ef1253",
            "fb26cf05409549dbb4a1481bc74ee842",
            "8b08515646f349a7b74b5e5e051a0011",
            "1cb807ccb00a478d9f2ee050e4bc03e3",
            "d19867d167fe43aeacb79c4c1ac8b9b9",
            "292bb12525f74aafb81934a9699930ac",
            "5d1617be4e72469da767be64fce1c706",
            "53f90735189944a0b761c929066d6ab6",
            "9e9f14e45fc740e8a18ba23f8662c643",
            "2611f291dd664a75b0600579410456e8",
            "1da7d15a66c445f78b3bf1ba36bf434b",
            "4948d8bac4c74da68127c8369baff037",
            "802601a6a29045418d483721d33c5305",
            "256a4ce646d047ca808e075134fbd1e8",
            "4886d8802a434523a6a4bb95b6d0d814",
            "c60f79f709834c5694476df90f4659e6",
            "079151fd9bc14228ae89603421bf11cc",
            "74fbc094d10a41efb1021730c2212a86",
            "157a062954124a95aaef53e5f888f4f2",
            "9c1ad36c4d06400e9b7cef714c4549fa",
            "eaa69447e8f34d069ccc705bca9bbb15",
            "740efe6ecf6044c3a241a55e3c4234eb",
            "c7eba4ff1d7d45f6837c502aaf104b5c",
            "0a9cc6f9f86b467f91bc8687d5cd08d5",
            "d92767977a2b4f85bb550f1f556e13c7",
            "63e91923708946ce8c97f021cbfe47e9",
            "f4c768be8544465f9008e682c8a3942b",
            "1629c98f99a0471ca3a502178b720bab",
            "4cd9a7b07e87483ba8732fa1596840ad",
            "d6cae863f8cd426ba66b5656b9ebe522",
            "5026e967f4bc4991903e342d18c947d4",
            "9f1923688c174c53acbc2e20963ce68c"
          ]
        },
        "id": "ILY_HaQbtgIj",
        "outputId": "e5af5bcd-e407-47eb-9110-947861f19490"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "52fbd984e497486495d47fc4eae74d51"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "13df4bb550f74a448c6aac4a0b5a9685"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1da7d15a66c445f78b3bf1ba36bf434b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/511M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "740efe6ecf6044c3a241a55e3c4234eb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_1[0][0]',                \n",
            "                                thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_bert_model[0][0]']          \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 15)           7695        ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 8  1  8 ... 10 11  9]\n",
            "[   1    2    3 ... 8417 8418 8419]\n"
          ]
        }
      ],
      "source": [
        "gc.collect()\n",
        "tf.keras.backend.clear_session()\n",
        "dbert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "dbert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
        "max_len=512\n",
        "sentences=summarized_data['text']\n",
        "labels=summarized_data['label']\n",
        "filenumbers=summarized_data['filenum']\n",
        "len(sentences),len(labels),len(filenumbers)\n",
        "\n",
        "model_0=create_model()\n",
        "input_ids=[]\n",
        "attention_masks=[]\n",
        "\n",
        "for sent in sentences:\n",
        "  dbert_inps=dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n",
        "  input_ids.append(dbert_inps['input_ids'])\n",
        "  attention_masks.append(dbert_inps['attention_mask'])\n",
        "input_ids=np.asarray(input_ids)\n",
        "\n",
        "attention_masks=np.array(attention_masks)\n",
        "labels=np.array(labels)\n",
        "filenumbers=[int(i) for i in filenumbers]\n",
        "filenumbers=np.array(filenumbers)\n",
        "print(labels)\n",
        "print(filenumbers)\n",
        "\n",
        "train_inp,val_inp,train_label,val_label,train_mask,val_mask,train_fnum,val_fnum=train_test_split(input_ids,labels,attention_masks,filenumbers,test_size=0.1,random_state=42)\n",
        "\n",
        "new_train_inp_0=[]\n",
        "new_val_inp_0=[]\n",
        "new_train_label_0=[]\n",
        "new_val_label_0=[]\n",
        "new_train_mask_0=[]\n",
        "new_train_fnum_0=[]\n",
        "new_val_fnum_0=[]\n",
        "new_val_mask_0=[]\n",
        "for i in range(len(train_fnum)):\n",
        "  if train_fnum[i] in filenumbers:\n",
        "    new_train_inp_0.append(train_inp[i])\n",
        "    #new_val_inp.append(val_inp[i])\n",
        "    new_train_label_0.append(train_label[i])\n",
        "    #new_val_label.append(val_label[i])\n",
        "    new_train_mask_0.append(train_mask[i])\n",
        "    #new_val_mask.append(val_mask[i])\n",
        "    new_train_fnum_0.append(train_fnum[i])\n",
        "\n",
        "for i in range(len(val_fnum)):\n",
        "  if val_fnum[i] in filenumbers:\n",
        "    #new_train_inp.append(train_inp[i])\n",
        "    new_val_inp_0.append(val_inp[i])\n",
        "    #new_train_label.append(train_label[i])\n",
        "    new_val_label_0.append(val_label[i])\n",
        "    #new_train_mask.append(train_mask[i])\n",
        "    new_val_mask_0.append(val_mask[i])\n",
        "    new_val_fnum_0.append(val_fnum[i])\n",
        "\n",
        "new_train_inp_0=np.array(new_train_inp_0)\n",
        "new_val_inp_0=np.array(new_val_inp_0)\n",
        "new_train_label_0=np.array(new_train_label_0)\n",
        "new_val_label_0=np.array(new_val_label_0)\n",
        "new_train_mask_0=np.array(new_train_mask_0)\n",
        "new_train_fnum_0=np.array(new_train_fnum_0)\n",
        "new_val_fnum_0=np.array(new_val_fnum_0)\n",
        "new_val_mask_0=np.array(new_val_mask_0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2RiZXaKGABO",
        "outputId": "8313b89a-7b2f-4b7f-98da-664891b92e1d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_1[0][0]',                \n",
            "                                thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_bert_model[0][0]']          \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 15)           7695        ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 8  1  8 ... 10 11  9]\n",
            "[   1    2    3 ... 8417 8418 8419]\n",
            "Wed Jun  1 20:35:43 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    39W / 300W |  15237MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "948/948 [==============================] - 340s 342ms/step - loss: 7.0318 - accuracy: 0.6188 - val_loss: 6.1415 - val_accuracy: 0.6841\n",
            "Epoch 2/5\n",
            "948/948 [==============================] - 323s 341ms/step - loss: 5.5065 - accuracy: 0.7599 - val_loss: 5.1476 - val_accuracy: 0.7233\n",
            "Epoch 3/5\n",
            "948/948 [==============================] - 324s 341ms/step - loss: 4.4419 - accuracy: 0.8253 - val_loss: 4.3535 - val_accuracy: 0.7530\n",
            "Epoch 4/5\n",
            "948/948 [==============================] - 319s 337ms/step - loss: 3.5588 - accuracy: 0.8814 - val_loss: 3.7844 - val_accuracy: 0.7411\n",
            "Epoch 5/5\n",
            "948/948 [==============================] - 319s 336ms/step - loss: 2.8043 - accuracy: 0.9248 - val_loss: 3.1761 - val_accuracy: 0.7530\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_3[0][0]',                \n",
            "                                thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_bert_model[1][0]']          \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 15)           7695        ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.7529691211401425\n",
            "Weighted F1: 0.7520917735523328\n",
            "Micro F1: 0.7529691211401426\n",
            "Weighted Precision: 0.7551276057124311\n",
            "Micro Precision: 0.7529691211401425\n",
            "Weighted Recall: 0.7529691211401425\n",
            "Micro Recall: 0.7529691211401425\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_1[0][0]',                \n",
            "                                thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_bert_model[0][0]']          \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 15)           7695        ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 8  1  8 ... 10 11  9]\n",
            "[   1    2    3 ... 8417 8418 8419]\n",
            "Wed Jun  1 21:05:57 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    39W / 300W |  15237MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "948/948 [==============================] - 339s 342ms/step - loss: 7.1130 - accuracy: 0.6059 - val_loss: 6.2135 - val_accuracy: 0.6960\n",
            "Epoch 2/5\n",
            "948/948 [==============================] - 324s 342ms/step - loss: 5.5476 - accuracy: 0.7549 - val_loss: 5.1377 - val_accuracy: 0.7589\n",
            "Epoch 3/5\n",
            "948/948 [==============================] - 319s 337ms/step - loss: 4.4613 - accuracy: 0.8276 - val_loss: 4.3656 - val_accuracy: 0.7530\n",
            "Epoch 4/5\n",
            "948/948 [==============================] - 319s 336ms/step - loss: 3.5447 - accuracy: 0.8815 - val_loss: 3.7300 - val_accuracy: 0.7518\n",
            "Epoch 5/5\n",
            "948/948 [==============================] - 319s 336ms/step - loss: 2.7907 - accuracy: 0.9191 - val_loss: 3.2131 - val_accuracy: 0.7577\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_3[0][0]',                \n",
            "                                thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_bert_model[1][0]']          \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 15)           7695        ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.7589073634204275\n",
            "Weighted F1: 0.7562997066198456\n",
            "Micro F1: 0.7589073634204275\n",
            "Weighted Precision: 0.7622027892195677\n",
            "Micro Precision: 0.7589073634204275\n",
            "Weighted Recall: 0.7589073634204275\n",
            "Micro Recall: 0.7589073634204275\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_1[0][0]',                \n",
            "                                thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_bert_model[0][0]']          \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 15)           7695        ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 8  1  8 ... 10 11  9]\n",
            "[   1    2    3 ... 8417 8418 8419]\n",
            "Wed Jun  1 21:36:05 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    39W / 300W |  15237MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "948/948 [==============================] - 339s 341ms/step - loss: 7.0737 - accuracy: 0.6145 - val_loss: 6.1471 - val_accuracy: 0.7078\n",
            "Epoch 2/5\n",
            "948/948 [==============================] - 323s 341ms/step - loss: 5.5364 - accuracy: 0.7533 - val_loss: 5.1749 - val_accuracy: 0.7268\n",
            "Epoch 3/5\n",
            "948/948 [==============================] - 323s 341ms/step - loss: 4.4721 - accuracy: 0.8224 - val_loss: 4.3605 - val_accuracy: 0.7423\n",
            "Epoch 4/5\n",
            "948/948 [==============================] - 323s 341ms/step - loss: 3.5669 - accuracy: 0.8786 - val_loss: 3.7427 - val_accuracy: 0.7458\n",
            "Epoch 5/5\n",
            "948/948 [==============================] - 323s 341ms/step - loss: 2.8221 - accuracy: 0.9191 - val_loss: 3.1529 - val_accuracy: 0.7601\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_3[0][0]',                \n",
            "                                thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_bert_model[1][0]']          \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 15)           7695        ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.7600950118764845\n",
            "Weighted F1: 0.7589719001292172\n",
            "Micro F1: 0.7600950118764844\n",
            "Weighted Precision: 0.761443928900564\n",
            "Micro Precision: 0.7600950118764845\n",
            "Weighted Recall: 0.7600950118764845\n",
            "Micro Recall: 0.7600950118764845\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_1[0][0]',                \n",
            "                                thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_bert_model[0][0]']          \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 15)           7695        ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 8  1  8 ... 10 11  9]\n",
            "[   1    2    3 ... 8417 8418 8419]\n",
            "Wed Jun  1 22:06:23 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    39W / 300W |  15237MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "948/948 [==============================] - 340s 342ms/step - loss: 7.0573 - accuracy: 0.6167 - val_loss: 6.2278 - val_accuracy: 0.6960\n",
            "Epoch 2/5\n",
            "948/948 [==============================] - 324s 342ms/step - loss: 5.5635 - accuracy: 0.7603 - val_loss: 5.1712 - val_accuracy: 0.7482\n",
            "Epoch 3/5\n",
            "948/948 [==============================] - 320s 337ms/step - loss: 4.5102 - accuracy: 0.8246 - val_loss: 4.4153 - val_accuracy: 0.7363\n",
            "Epoch 4/5\n",
            "948/948 [==============================] - 320s 337ms/step - loss: 3.5832 - accuracy: 0.8840 - val_loss: 3.8165 - val_accuracy: 0.7470\n",
            "Epoch 5/5\n",
            "948/948 [==============================] - 319s 337ms/step - loss: 2.8385 - accuracy: 0.9211 - val_loss: 3.3206 - val_accuracy: 0.7470\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_3[0][0]',                \n",
            "                                thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_bert_model[1][0]']          \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 15)           7695        ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.7482185273159145\n",
            "Weighted F1: 0.7408346225270779\n",
            "Micro F1: 0.7482185273159146\n",
            "Weighted Precision: 0.740785406186586\n",
            "Micro Precision: 0.7482185273159145\n",
            "Weighted Recall: 0.7482185273159145\n",
            "Micro Recall: 0.7482185273159145\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_1[0][0]',                \n",
            "                                thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_bert_model[0][0]']          \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 15)           7695        ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 8  1  8 ... 10 11  9]\n",
            "[   1    2    3 ... 8417 8418 8419]\n",
            "Wed Jun  1 22:36:37 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    39W / 300W |  15237MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "948/948 [==============================] - 340s 342ms/step - loss: 6.9683 - accuracy: 0.6130 - val_loss: 6.0565 - val_accuracy: 0.6995\n",
            "Epoch 2/5\n",
            "948/948 [==============================] - 324s 342ms/step - loss: 5.3653 - accuracy: 0.7594 - val_loss: 5.0282 - val_accuracy: 0.7233\n",
            "Epoch 3/5\n",
            "948/948 [==============================] - 324s 342ms/step - loss: 4.2875 - accuracy: 0.8325 - val_loss: 4.1878 - val_accuracy: 0.7565\n",
            "Epoch 4/5\n",
            "948/948 [==============================] - 320s 338ms/step - loss: 3.4029 - accuracy: 0.8817 - val_loss: 3.6967 - val_accuracy: 0.7387\n",
            "Epoch 5/5\n",
            "948/948 [==============================] - 320s 338ms/step - loss: 2.6597 - accuracy: 0.9236 - val_loss: 3.1482 - val_accuracy: 0.7387\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_3[0][0]',                \n",
            "                                thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_bert_model[1][0]']          \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 15)           7695        ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.7565320665083135\n",
            "Weighted F1: 0.7553445868527437\n",
            "Micro F1: 0.7565320665083135\n",
            "Weighted Precision: 0.7593648221436535\n",
            "Micro Precision: 0.7565320665083135\n",
            "Weighted Recall: 0.7565320665083135\n",
            "Micro Recall: 0.7565320665083135\n",
            "Average Accuracy: 0.7553444180522565\n",
            "Average Weighted F1: 0.7527085179362435\n",
            "Average Micro F1: 0.7553444180522566\n",
            "Average Weighted Precision: 0.7557849104325605\n",
            "Average Micro Precision: 0.7553444180522565\n",
            "Average Weighted Recall: 0.7553444180522565\n",
            "Average Micro Recall: 0.7553444180522565\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "total_accuracy=0\n",
        "total_weighted_f1=0\n",
        "total_micro_f1=0\n",
        "total_weighted_precision=0\n",
        "total_micro_precision=0\n",
        "total_weighted_recall=0\n",
        "total_micro_recall=0\n",
        "\n",
        "for f in range(5):\n",
        "  gc.collect()\n",
        "  tf.keras.backend.clear_session()\n",
        "  dbert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "  dbert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
        "  max_len=512\n",
        "  sentences=summarized_data['text']\n",
        "  labels=summarized_data['label']\n",
        "  filenumbers=summarized_data['filenum']\n",
        "  len(sentences),len(labels),len(filenumbers)\n",
        "\n",
        "  model_0=create_model()\n",
        "  input_ids=[]\n",
        "  attention_masks=[]\n",
        "\n",
        "  for sent in sentences:\n",
        "    dbert_inps=dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n",
        "    input_ids.append(dbert_inps['input_ids'])\n",
        "    attention_masks.append(dbert_inps['attention_mask'])\n",
        "  input_ids=np.asarray(input_ids)\n",
        "\n",
        "  attention_masks=np.array(attention_masks)\n",
        "  labels=np.array(labels)\n",
        "  filenumbers=[int(i) for i in filenumbers]\n",
        "  filenumbers=np.array(filenumbers)\n",
        "  print(labels)\n",
        "  print(filenumbers)\n",
        "\n",
        "  train_inp,val_inp,train_label,val_label,train_mask,val_mask,train_fnum,val_fnum=train_test_split(input_ids,labels,attention_masks,filenumbers,test_size=0.1,random_state=42)\n",
        "\n",
        "  new_train_inp_0=[]\n",
        "  new_val_inp_0=[]\n",
        "  new_train_label_0=[]\n",
        "  new_val_label_0=[]\n",
        "  new_train_mask_0=[]\n",
        "  new_train_fnum_0=[]\n",
        "  new_val_fnum_0=[]\n",
        "  new_val_mask_0=[]\n",
        "  for i in range(len(train_fnum)):\n",
        "    if train_fnum[i] in filenumbers:\n",
        "      new_train_inp_0.append(train_inp[i])\n",
        "      #new_val_inp.append(val_inp[i])\n",
        "      new_train_label_0.append(train_label[i])\n",
        "      #new_val_label.append(val_label[i])\n",
        "      new_train_mask_0.append(train_mask[i])\n",
        "      #new_val_mask.append(val_mask[i])\n",
        "      new_train_fnum_0.append(train_fnum[i])\n",
        "\n",
        "  for i in range(len(val_fnum)):\n",
        "    if val_fnum[i] in filenumbers:\n",
        "      #new_train_inp.append(train_inp[i])\n",
        "      new_val_inp_0.append(val_inp[i])\n",
        "      #new_train_label.append(train_label[i])\n",
        "      new_val_label_0.append(val_label[i])\n",
        "      #new_train_mask.append(train_mask[i])\n",
        "      new_val_mask_0.append(val_mask[i])\n",
        "      new_val_fnum_0.append(val_fnum[i])\n",
        "\n",
        "  # print(new_train_label)\n",
        "  # print(len(new_train_label))\n",
        "  # print(new_val_label)\n",
        "  # print(len(new_val_label))\n",
        "  new_train_inp_0=np.array(new_train_inp_0)\n",
        "  new_val_inp_0=np.array(new_val_inp_0)\n",
        "  new_train_label_0=np.array(new_train_label_0)\n",
        "  new_val_label_0=np.array(new_val_label_0)\n",
        "  new_train_mask_0=np.array(new_train_mask_0)\n",
        "  new_train_fnum_0=np.array(new_train_fnum_0)\n",
        "  new_val_fnum_0=np.array(new_val_fnum_0)\n",
        "  new_val_mask_0=np.array(new_val_mask_0)\n",
        "  \n",
        "  log_dir='dbert_model'\n",
        "\n",
        "  model_save_path='./drive/MyDrive/Ensemble/bert-ensemble-512-model_0-'+str(f)+'-15labels.h5'\n",
        "\n",
        "  loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "  accuracy = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
        "  callbacks= [tf.keras.callbacks.ModelCheckpoint(filepath=model_save_path,monitor='val_accuracy',mode='max',save_best_only=True,save_weights_only=True),keras.callbacks.TensorBoard(log_dir=log_dir)]\n",
        "  model_0.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])\n",
        "  gpu_info = !nvidia-smi\n",
        "  gpu_info = '\\n'.join(gpu_info)\n",
        "  if gpu_info.find('failed') >= 0:\n",
        "    print('Not connected to a GPU')\n",
        "  else:\n",
        "    print(gpu_info)\n",
        "  \n",
        "  history=model_0.fit([new_train_inp_0,new_train_mask_0],new_train_label_0,batch_size=8,epochs=5,validation_data=([new_val_inp_0,new_val_mask_0],new_val_label_0),callbacks=callbacks)\n",
        "\n",
        "  pred_labels=[]\n",
        "\n",
        "  model_saved_0= create_model()\n",
        "  model_saved_0.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])\n",
        "  model_saved_0.load_weights('./drive/MyDrive/Ensemble/bert-ensemble-512-model_0-'+str(f)+'-15labels.h5')\n",
        "\n",
        "  for i in range(0,len(val_inp)):\n",
        "    pred=model_saved_0.predict([val_inp[i].reshape(1,512),val_mask[i].reshape(1,512)])\n",
        "    pred_label = pred.argmax(axis=1)\n",
        "    pred_labels.append(pred_label)\n",
        "  accuracy=accuracy_score(val_label, pred_labels)\n",
        "  print(\"Accuracy: \"+str(accuracy))\n",
        "  total_accuracy=total_accuracy+accuracy\n",
        "  \n",
        "  weighted_f1=f1_score(val_label,pred_labels, average='weighted')\n",
        "  print(\"Weighted F1: \"+ str(weighted_f1))\n",
        "  total_weighted_f1=total_weighted_f1+weighted_f1\n",
        "  micro_f1=f1_score(val_label,pred_labels, average='micro')\n",
        "  print(\"Micro F1: \"+ str(micro_f1))\n",
        "  total_micro_f1=total_micro_f1+micro_f1\n",
        "\n",
        "  weighted_precision=precision_score(val_label, pred_labels, average='weighted')\n",
        "  print(\"Weighted Precision: \" + str(weighted_precision))\n",
        "  total_weighted_precision=total_weighted_precision+weighted_precision\n",
        "  micro_precision=precision_score(val_label, pred_labels, average='micro')\n",
        "  print(\"Micro Precision: \" + str(micro_precision))\n",
        "  total_micro_precision=total_micro_precision+micro_precision\n",
        "\n",
        "  weighted_recall=recall_score(val_label, pred_labels, average='weighted')\n",
        "  print(\"Weighted Recall: \" + str(weighted_recall))\n",
        "  total_weighted_recall=total_weighted_recall+weighted_recall\n",
        "  micro_recall=recall_score(val_label, pred_labels, average='micro')\n",
        "  print(\"Micro Recall: \" + str(micro_recall))\n",
        "  total_micro_recall=total_micro_recall+micro_recall\n",
        "\n",
        "\n",
        "print(\"Average Accuracy: \"+str(total_accuracy/5))\n",
        "print(\"Average Weighted F1: \"+str(total_weighted_f1/5))\n",
        "print(\"Average Micro F1: \"+str(total_micro_f1/5))\n",
        "print(\"Average Weighted Precision: \"+str(total_weighted_precision/5))\n",
        "print(\"Average Micro Precision: \"+str(total_micro_precision/5))\n",
        "print(\"Average Weighted Recall: \"+str(total_weighted_recall/5))\n",
        "print(\"Average Micro Recall: \"+str(total_micro_recall/5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NuZhHmEL6We",
        "outputId": "c19046fa-9a16-41e3-97bf-f949629543de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7332\n",
            "7332\n",
            "Average Length 510.89102564102564\n",
            "['8', '1', '8', '2', '8', '8', '8', '9', '7', '8', '1', '2', '1', '1', '8', '4', '8', '8', '12', '3', '3', '7', '3', '12', '1', '8', '8', '8', '8', '8', '8', '8', '1', '9', '5', '9', '9', '9', '11', '8', '8', '8', '4', '8', '8', '8', '8', '1', '3', '9', '3', '7', '1', '2', '9', '9', '7', '8', '8', '1', '10', '7', '8', '8', '9', '8', '7', '9', '9', '12', '7', '2', '8', '1', '11', '11', '1', '7', '7', '12', '1', '9', '8', '10', '12', '7', '8', '2', '8', '9', '9', '1', '8', '9', '1', '7', '12', '10', '10', '10', '8', '3', '7', '9', '8', '9', '1', '8', '8', '2', '7', '2', '9', '9', '11', '8', '8', '12', '12', '7', '8', '12', '4', '9', '3', '1', '12', '1', '1', '8', '8', '3', '8', '8', '8', '8', '9', '1', '8', '8', '10', '1', '8', '2', '8', '8', '7', '3', '8', '2', '4', '4', '9', '8', '10', '12', '12', '1', '1', '9', '1', '1', '1', '8', '2', '2', '8', '1', '1', '2', '2', '1', '2', '8', '1', '9', '9', '8', '8', '4', '2', '9', '9', '8', '3', '4', '3', '1', '8', '8', '2', '1', '9', '7', '8', '8', '1', '12', '3', '8', '2', '4', '2', '9', '12', '1', '4', '1', '8', '8', '8', '2', '2', '8', '9', '8', '8', '8', '10', '9', '8', '7', '9', '1', '1', '9', '4', '2', '4', '2', '2', '1', '7', '8', '11', '11', '3', '9', '2', '4', '8', '9', '1', '8', '1', '1', '4', '9', '1', '1', '8', '8', '2', '1', '8', '4', '2', '8', '9', '8', '8', '2', '8', '8', '8', '7', '1', '1', '1', '2', '1', '1', '8', '7', '8', '8', '12', '2', '12', '12', '8', '10', '12', '8', '3', '3', '12', '10', '1', '8', '12', '1', '8', '8', '2', '8', '4', '7', '8', '7', '10', '8', '10', '9', '8', '12', '12', '1', '8', '8', '3', '8', '8', '8', '8', '8', '1', '9', '8', '11', '1', '1', '1', '9', '8', '1', '9', '2', '3', '11', '8', '9', '9', '9', '2', '1', '8', '8', '9', '7', '1', '4', '9', '4', '8', '8', '4', '8', '12', '9', '4', '8', '2', '10', '10', '10', '8', '9', '9', '8', '8', '12', '7', '1', '8', '8', '8', '4', '1', '1', '1', '1', '1', '1', '8', '1', '9', '8', '9', '9', '4', '8', '12', '9', '8', '8', '2', '8', '8', '8', '6', '9', '8', '3', '7', '8', '8', '4', '12', '8', '8', '9', '12', '12', '9', '8', '2', '9', '2', '3', '1', '12', '8', '10', '9', '9', '9', '10', '10', '3', '8', '12', '1', '4', '2', '1', '10', '8', '2', '8', '4', '8', '9', '1', '9', '9', '10', '10', '1', '4', '9', '2', '4', '9', '1', '1', '3', '10', '3', '3', '8', '7', '3', '8', '9', '9', '12', '4', '8', '12', '2', '2', '4', '1', '9', '9', '4', '1', '4', '2', '8', '12', '2', '3', '10', '10', '9', '8', '9', '9', '1', '12', '8', '8', '8', '12', '4', '1', '8', '8', '1', '9', '8', '8', '2', '1', '8', '9', '8', '3', '3', '3', '1', '8', '8', '9', '1', '10', '9', '9', '9', '9', '5', '9', '9', '8', '8', '8', '8', '8', '8', '8', '8', '11', '12', '8', '8', '1', '8', '9', '11', '2', '2', '2', '2', '3', '1', '2', '2', '8', '2', '4', '9', '1', '2', '9', '8', '2', '8', '9', '9', '3', '10', '9', '9', '2', '8', '9', '8', '12', '12', '1', '3', '8', '8', '8', '2', '7', '7', '7', '7', '3', '9', '1', '9', '8', '9', '9', '1', '1', '1', '2', '9', '9', '9', '11', '1', '8', '8', '9', '1', '9', '8', '8', '8', '1', '1', '8', '7', '1', '1', '8', '8', '9', '4', '4', '8', '2', '2', '8', '8', '8', '8', '8', '8', '11', '8', '2', '9', '4', '9', '3', '9', '9', '1', '3', '9', '3', '1', '12', '8', '9', '12', '1', '8', '4', '2', '1', '4', '8', '3', '3', '8', '2', '8', '9', '7', '8', '8', '8', '5', '8', '3', '9', '8', '8', '13', '12', '1', '1', '2', '8', '4', '1', '9', '9', '12', '8', '9', '12', '9', '1', '9', '9', '9', '9', '3', '2', '9', '9', '4', '8', '12', '2', '4', '9', '3', '1', '9', '7', '8', '9', '9', '8', '4', '8', '8', '7', '9', '10', '3', '8', '8', '8', '1', '1', '1', '1', '8', '8', '4', '1', '10', '1', '5', '7', '7', '1', '8', '9', '3', '7', '2', '7', '7', '2', '4', '8', '12', '7', '4', '2', '9', '9', '12', '6', '10', '8', '2', '4', '12', '9', '9', '3', '8', '8', '1', '2', '10', '9', '9', '8', '4', '12', '2', '1', '8', '8', '8', '12', '10', '10', '9', '3', '8', '8', '9', '2', '8', '10', '1', '1', '1', '1', '2', '1', '1', '1', '1', '9', '8', '12', '9', '4', '8', '8', '9', '1', '9', '3', '9', '8', '8', '1', '7', '7', '10', '1', '8', '8', '1', '9', '8', '10', '3', '1', '7', '1', '8', '8', '12', '8', '8', '1', '8', '7', '1', '7', '7', '8', '2', '1', '8', '8', '2', '10', '8', '8', '8', '8', '8', '10', '1', '8', '8', '12', '8', '3', '3', '2', '2', '2', '10', '8', '8', '8', '2', '9', '1', '8', '9', '3', '2', '8', '10', '8', '6', '1', '1', '8', '4', '1', '9', '10', '8', '1', '7', '1', '2', '8', '1', '1', '1', '12', '1', '9', '12', '8', '12', '12', '12', '8', '8', '12', '4', '8', '8', '8', '8', '9', '9', '1', '3', '3', '3', '3', '1', '12', '12', '9', '10', '8', '8', '1', '9', '2', '2', '13', '9', '8', '9', '2', '1', '9', '1', '8', '8', '8', '4', '8', '1', '1', '1', '12', '12', '7', '2', '2', '2', '8', '3', '8', '9', '2', '10', '7', '8', '9', '2', '1', '2', '12', '12', '8', '8', '9', '2', '2', '9', '11', '1', '8', '1', '10', '9', '2', '1', '4', '7', '7', '7', '7', '7', '12', '8', '8', '8', '1', '1', '10', '1', '12', '1', '8', '2', '1', '1', '12', '8', '7', '9', '12', '8', '9', '3', '9', '8', '8', '8', '8', '3', '11', '2', '2', '9', '8', '8', '10', '8', '2', '7', '3', '1', '4', '7', '8', '8', '1', '8', '3', '7', '12', '8', '10', '9', '9', '8', '8', '2', '8', '9', '9', '1', '2', '8', '8', '9', '8', '3', '8', '1', '8', '10', '9', '8', '9', '9', '12', '4', '4', '8', '9', '9', '8', '2', '10', '1', '2', '8', '9', '1', '9', '9', '9', '7', '12', '12', '8', '1', '1', '1', '1', '8', '3', '1', '1', '8', '1', '8', '8', '7', '8', '8', '8', '8', '3', '2', '2', '10', '10', '10', '7', '8', '1', '2', '12', '7', '9', '8', '7', '8', '12', '2', '8', '9', '2', '6', '6', '7', '9', '8', '1', '8', '9', '8', '1', '12', '1', '2', '8', '7', '7', '7', '8', '2', '2', '8', '1', '2', '2', '9', '9', '1', '8', '8', '4', '3', '3', '1', '6', '3', '3', '12', '3', '8', '9', '1', '4', '3', '1', '8', '3', '9', '2', '8', '2', '8', '8', '8', '1', '1', '1', '9', '9', '8', '1', '9', '8', '1', '1', '3', '10', '8', '1', '1', '3', '9', '1', '4', '4', '1', '8', '9', '9', '2', '0', '0', '1', '8', '3', '1', '8', '8', '9', '8', '8', '1', '1', '8', '9', '8', '8', '8', '7', '9', '8', '8', '8', '10', '9', '8', '1', '2', '6', '1', '9', '9', '8', '12', '12', '12', '8', '8', '2', '8', '1', '2', '2', '2', '1', '9', '8', '2', '12', '2', '8', '12', '8', '9', '8', '8', '9', '7', '1', '1', '1', '1', '1', '8', '8', '1', '8', '8', '1', '1', '3', '2', '8', '8', '9', '10', '10', '2', '2', '1', '9', '2', '9', '9', '4', '12', '12', '12', '10', '7', '3', '3', '4', '2', '2', '9', '2', '8', '4', '2', '4', '1', '10', '9', '7', '8', '7', '1', '1', '3', '3', '1', '1', '3', '3', '3', '1', '1', '1', '1', '8', '2', '3', '1', '1', '2', '8', '8', '12', '8', '8', '8', '8', '11', '9', '1', '8', '9', '2', '8', '8', '8', '3', '9', '1', '9', '2', '7', '2', '8', '2', '8', '10', '8', '1', '10', '1', '1', '9', '9', '8', '8', '1', '8', '8', '8', '12', '8', '8', '8', '1', '8', '8', '8', '1', '9', '1', '1', '8', '1', '8', '9', '8', '2', '12', '9', '9', '0', '1', '8', '8', '1', '8', '12', '8', '8', '10', '8', '8', '8', '7', '8', '1', '8', '7', '3', '10', '1', '8', '9', '1', '8', '8', '8', '10', '1', '10', '3', '9', '1', '8', '9', '2', '8', '3', '3', '9', '9', '7', '9', '1', '1', '9', '2', '1', '1', '1', '7', '1', '1', '8', '8', '1', '1', '8', '1', '8', '3', '12', '9', '3', '3', '8', '8', '8', '8', '3', '1', '3', '3', '1', '11', '0', '8', '8', '7', '8', '12', '1', '8', '9', '8', '9', '8', '8', '3', '8', '8', '1', '1', '1', '9', '2', '2', '2', '8', '7', '12', '8', '8', '9', '10', '10', '7', '8', '1', '9', '8', '7', '3', '1', '3', '8', '2', '2', '3', '9', '8', '4', '4', '8', '9', '2', '1', '1', '7', '8', '9', '9', '7', '8', '7', '7', '8', '2', '2', '8', '4', '9', '7', '10', '0', '9', '8', '3', '7', '8', '1', '1', '8', '9', '9', '2', '2', '10', '1', '9', '10', '10', '10', '8', '3', '2', '12', '9', '9', '10', '12', '9', '12', '12', '9', '1', '2', '4', '12', '12', '7', '8', '9', '7', '7', '7', '3', '9', '8', '9', '1', '12', '8', '9', '4', '1', '3', '12', '12', '12', '12', '8', '8', '2', '1', '1', '2', '1', '1', '1', '12', '12', '8', '12', '2', '2', '12', '3', '3', '12', '8', '2', '8', '8', '12', '2', '1', '10', '3', '2', '8', '7', '1', '8', '1', '3', '7', '8', '9', '8', '3', '1', '1', '7', '8', '8', '9', '8', '2', '9', '2', '2', '9', '8', '1', '8', '8', '1', '3', '3', '1', '1', '10', '1', '2', '8', '1', '1', '1', '1', '9', '1', '4', '1', '7', '7', '7', '7', '2', '2', '8', '8', '12', '1', '9', '1', '7', '3', '3', '1', '8', '8', '10', '8', '9', '2', '9', '1', '3', '8', '8', '3', '12', '2', '8', '12', '2', '9', '1', '3', '3', '3', '3', '2', '8', '7', '9', '8', '3', '3', '1', '7', '8', '3', '1', '1', '12', '8', '9', '1', '2', '3', '8', '1', '1', '3', '3', '9', '1', '1', '1', '12', '1', '7', '3', '3', '1', '8', '8', '8', '1', '2', '4', '8', '1', '10', '2', '5', '3', '3', '12', '10', '9', '9', '12', '9', '0', '2', '8', '8', '9', '9', '9', '8', '1', '3', '1', '1', '4', '8', '1', '10', '8', '7', '2', '8', '2', '8', '4', '7', '8', '1', '9', '1', '9', '8', '2', '8', '2', '7', '9', '2', '2', '9', '1', '8', '12', '1', '8', '1', '4', '1', '9', '9', '1', '10', '12', '4', '8', '1', '7', '3', '9', '2', '12', '7', '8', '8', '2', '1', '12', '9', '8', '1', '2', '2', '8', '10', '2', '1', '7', '7', '7', '12', '3', '3', '8', '3', '8', '8', '3', '9', '8', '9', '1', '1', '8', '7', '9', '3', '3', '8', '1', '0', '9', '9', '9', '1', '8', '9', '9', '10', '1', '8', '7', '8', '8', '8', '9', '9', '8', '9', '10', '4', '9', '3', '7', '12', '1', '9', '9', '8', '9', '1', '9', '3', '1', '8', '4', '12', '10', '9', '8', '7', '10', '8', '12', '12', '3', '10', '8', '12', '2', '1', '2', '3', '9', '8', '8', '7', '1', '1', '2', '2', '1', '2', '2', '7', '1', '3', '9', '9', '3', '8', '8', '8', '8', '8', '7', '8', '8', '10', '8', '1', '8', '2', '8', '2', '2', '2', '2', '2', '1', '7', '12', '10', '1', '2', '8', '1', '4', '7', '8', '1', '12', '8', '7', '9', '2', '2', '2', '8', '1', '8', '1', '1', '0', '2', '2', '6', '1', '8', '2', '1', '8', '1', '1', '2', '2', '3', '2', '2', '10', '10', '9', '1', '7', '7', '8', '8', '1', '12', '10', '12', '3', '8', '8', '8', '3', '8', '3', '10', '2', '2', '2', '1', '2', '2', '1', '0', '1', '8', '9', '1', '1', '1', '2', '2', '2', '2', '2', '1', '1', '1', '1', '12', '9', '9', '1', '10', '11', '12', '9', '1', '1', '8', '8', '1', '2', '12', '8', '3', '7', '2', '10', '9', '7', '8', '1', '2', '2', '7', '9', '12', '2', '2', '1', '8', '9', '2', '3', '2', '2', '2', '9', '9', '8', '9', '2', '3', '8', '8', '9', '7', '3', '10', '8', '8', '9', '1', '1', '2', '8', '1', '10', '8', '8', '9', '7', '1', '7', '12', '8', '1', '7', '2', '1', '3', '9', '2', '8', '8', '1', '2', '3', '9', '3', '8', '9', '7', '7', '1', '10', '2', '8', '8', '8', '1', '8', '2', '8', '8', '1', '2', '8', '10', '2', '1', '0', '8', '3', '8', '10', '10', '12', '3', '8', '9', '3', '10', '8', '8', '8', '1', '8', '8', '2', '2', '2', '2', '2', '2', '1', '1', '10', '1', '10', '8', '2', '1', '2', '2', '8', '3', '3', '2', '2', '1', '8', '1', '3', '1', '9', '2', '2', '1', '3', '2', '2', '3', '2', '2', '3', '2', '1', '1', '2', '1', '2', '7', '8', '2', '1', '12', '12', '3', '1', '9', '9', '7', '8', '7', '2', '7', '9', '2', '2', '2', '8', '9', '7', '9', '2', '8', '9', '9', '2', '9', '1', '1', '9', '8', '3', '3', '8', '2', '7', '8', '11', '2', '8', '9', '1', '8', '3', '1', '2', '1', '2', '2', '2', '2', '8', '3', '9', '2', '10', '9', '9', '2', '7', '7', '7', '1', '3', '8', '8', '1', '1', '8', '7', '1', '8', '8', '9', '3', '3', '3', '2', '4', '8', '12', '8', '1', '12', '9', '2', '8', '2', '12', '2', '1', '3', '8', '12', '12', '11', '8', '10', '9', '2', '9', '4', '10', '1', '12', '12', '9', '3', '8', '9', '12', '9', '9', '2', '2', '2', '3', '5', '1', '1', '1', '7', '7', '8', '2', '2', '8', '2', '1', '7', '3', '2', '9', '8', '9', '1', '1', '9', '8', '9', '3', '3', '2', '9', '4', '9', '8', '8', '1', '8', '10', '2', '0', '3', '8', '8', '8', '9', '12', '9', '8', '2', '2', '8', '10', '9', '8', '2', '2', '4', '1', '9', '9', '8', '8', '10', '3', '1', '1', '2', '8', '7', '8', '2', '1', '1', '8', '12', '2', '9', '4', '8', '3', '3', '3', '8', '2', '12', '12', '8', '9', '12', '8', '2', '12', '7', '9', '2', '2', '1', '3', '1', '3', '11', '9', '8', '1', '10', '2', '12', '3', '8', '4', '1', '2', '8', '8', '3', '9', '3', '8', '7', '1', '1', '3', '1', '8', '2', '9', '8', '12', '1', '1', '1', '9', '9', '1', '8', '8', '8', '2', '2', '12', '3', '1', '1', '1', '9', '9', '3', '2', '1', '1', '9', '9', '1', '9', '9', '1', '9', '3', '9', '1', '8', '1', '9', '8', '3', '8', '8', '9', '9', '8', '2', '8', '1', '2', '2', '10', '1', '2', '1', '1', '9', '1', '9', '3', '7', '7', '2', '10', '3', '3', '1', '1', '9', '1', '2', '1', '3', '2', '1', '1', '1', '1', '1', '1', '1', '1', '1', '2', '8', '8', '7', '1', '1', '8', '8', '2', '1', '12', '1', '8', '1', '9', '8', '8', '2', '1', '9', '9', '2', '2', '8', '1', '7', '7', '9', '8', '1', '6', '8', '8', '2', '2', '8', '3', '8', '3', '2', '3', '8', '1', '2', '2', '2', '8', '9', '9', '9', '12', '1', '9', '2', '1', '9', '2', '9', '9', '10', '8', '1', '1', '9', '2', '1', '7', '1', '3', '7', '1', '1', '1', '3', '8', '8', '8', '9', '9', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '1', '0', '1', '1', '1', '2', '2', '3', '1', '1', '2', '3', '3', '3', '7', '3', '1', '9', '3', '9', '1', '4', '1', '1', '9', '10', '9', '8', '2', '3', '10', '9', '3', '10', '12', '8', '9', '8', '2', '1', '9', '8', '1', '7', '8', '1', '9', '1', '2', '10', '7', '7', '8', '1', '3', '3', '9', '3', '3', '1', '1', '1', '9', '1', '3', '8', '8', '8', '1', '8', '1', '9', '7', '8', '8', '2', '1', '3', '8', '2', '1', '6', '1', '8', '8', '2', '8', '2', '2', '9', '8', '1', '1', '6', '8', '6', '10', '2', '1', '1', '8', '9', '3', '3', '3', '1', '3', '9', '6', '8', '1', '8', '3', '1', '2', '2', '12', '7', '1', '1', '1', '1', '12', '1', '8', '8', '3', '2', '1', '9', '2', '3', '3', '2', '2', '7', '2', '2', '2', '3', '1', '9', '8', '7', '1', '1', '3', '1', '9', '4', '3', '9', '2', '1', '1', '9', '8', '8', '7', '2', '1', '3', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '2', '10', '1', '1', '8', '2', '8', '1', '8', '1', '3', '1', '2', '1', '1', '3', '1', '2', '1', '1', '1', '1', '1', '1', '1', '2', '1', '8', '1', '9', '8', '8', '1', '10', '8', '8', '1', '3', '9', '8', '9', '8', '2', '1', '3', '4', '1', '12', '1', '9', '8', '2', '7', '2', '1', '2', '9', '2', '1', '3', '8', '1', '2', '3', '8', '2', '10', '10', '10', '9', '9', '9', '3', '8', '3', '1', '1', '1', '1', '1', '1', '2', '1', '9', '2', '7', '9', '8', '7', '9', '1', '1', '1', '1', '8', '2', '2', '3', '3', '2', '12', '8', '3', '1', '1', '12', '9', '8', '2', '2', '9', '1', '2', '2', '9', '1', '1', '2', '4', '8', '1', '8', '10', '2', '9', '9', '9', '1', '2', '1', '1', '2', '2', '2', '12', '1', '2', '8', '3', '9', '9', '3', '2', '9', '9', '7', '2', '8', '8', '9', '2', '9', '1', '1', '1', '1', '1', '2', '8', '2', '1', '2', '2', '8', '3', '1', '1', '9', '1', '3', '10', '7', '9', '8', '8', '2', '2', '9', '2', '2', '2', '2', '2', '2', '9', '1', '3', '6', '4', '2', '2', '9', '1', '9', '8', '1', '1', '1', '7', '3', '2', '9', '9', '1', '9', '7', '2', '8', '9', '9', '12', '10', '8', '10', '2', '3', '1', '2', '2', '12', '12', '3', '12', '1', '2', '1', '2', '1', '2', '8', '2', '3', '12', '12', '8', '2', '2', '9', '2', '9', '2', '3', '3', '1', '1', '1', '12', '3', '7', '1', '3', '1', '2', '9', '2', '7', '2', '1', '8', '3', '7', '8', '1', '1', '3', '8', '3', '2', '9', '3', '1', '1', '1', '1', '11', '1', '2', '6', '1', '2', '3', '8', '9', '9', '2', '9', '2', '1', '1', '3', '1', '12', '9', '1', '1', '9', '1', '9', '9', '8', '9', '8', '1', '2', '7', '1', '0', '9', '8', '2', '3', '2', '4', '1', '1', '1', '8', '1', '12', '1', '2', '3', '3', '9', '9', '9', '9', '9', '7', '9', '3', '9', '1', '7', '3', '3', '3', '3', '7', '8', '2', '2', '2', '8', '2', '3', '9', '9', '3', '1', '8', '9', '12', '8', '8', '3', '2', '1', '6', '1', '9', '1', '1', '1', '1', '1', '1', '2', '2', '1', '2', '2', '2', '2', '9', '2', '5', '3', '2', '2', '8', '2', '1', '8', '3', '3', '8', '3', '1', '3', '9', '2', '8', '9', '4', '4', '9', '8', '8', '7', '7', '3', '1', '8', '8', '3', '2', '2', '10', '3', '3', '2', '2', '2', '2', '12', '3', '1', '2', '7', '7', '12', '2', '1', '2', '1', '1', '2', '1', '3', '3', '3', '1', '3', '9', '4', '8', '8', '4', '2', '4', '2', '8', '9', '1', '1', '2', '2', '2', '8', '7', '2', '8', '2', '9', '1', '8', '1', '1', '2', '1', '1', '1', '2', '9', '9', '8', '1', '8', '1', '4', '8', '1', '2', '7', '9', '2', '1', '8', '9', '8', '9', '1', '1', '2', '3', '2', '12', '7', '1', '2', '4', '4', '4', '2', '2', '2', '9', '8', '8', '1', '12', '3', '3', '2', '2', '3', '12', '1', '5', '1', '1', '3', '2', '8', '8', '2', '2', '2', '3', '9', '8', '8', '9', '8', '8', '11', '8', '2', '9', '2', '3', '1', '7', '1', '8', '2', '8', '1', '1', '1', '8', '1', '1', '1', '7', '8', '8', '2', '3', '2', '1', '8', '8', '1', '8', '4', '8', '9', '8', '9', '8', '1', '2', '3', '2', '1', '2', '2', '3', '1', '9', '4', '8', '1', '1', '9', '1', '1', '8', '2', '2', '2', '1', '7', '3', '3', '1', '3', '3', '12', '3', '1', '1', '3', '1', '1', '4', '8', '4', '4', '3', '3', '3', '4', '8', '1', '9', '9', '9', '4', '1', '7', '4', '8', '4', '9', '8', '1', '1', '3', '8', '9', '1', '9', '7', '1', '1', '9', '1', '8', '8', '8', '1', '1', '8', '2', '2', '2', '9', '4', '4', '1', '2', '1', '2', '9', '1', '1', '8', '5', '5', '5', '9', '12', '4', '1', '2', '1', '8', '8', '1', '9', '9', '2', '12', '1', '1', '7', '8', '8', '4', '1', '2', '8', '11', '2', '3', '2', '2', '1', '11', '2', '2', '2', '2', '9', '9', '2', '2', '9', '3', '1', '9', '1', '1', '1', '7', '8', '1', '8', '9', '1', '2', '9', '8', '1', '2', '9', '8', '12', '2', '2', '10', '8', '2', '2', '8', '8', '1', '2', '6', '1', '1', '7', '7', '2', '3', '2', '1', '1', '8', '1', '10', '8', '12', '6', '4', '1', '4', '1', '2', '9', '8', '9', '4', '8', '8', '10', '8', '8', '8', '8', '8', '2', '2', '8', '4', '9', '3', '3', '3', '3', '3', '9', '2', '1', '1', '2', '3', '2', '1', '2', '3', '3', '3', '2', '2', '3', '3', '2', '1', '2', '3', '3', '3', '3', '12', '9', '3', '2', '2', '3', '1', '5', '1', '1', '3', '2', '2', '1', '2', '9', '3', '9', '8', '1', '8', '7', '7', '9', '1', '1', '7', '9', '10', '1', '7', '1', '4', '3', '8', '9', '4', '9', '9', '2', '9', '2', '8', '4', '2', '2', '9', '5', '2', '1', '10', '9', '3', '1', '1', '1', '2', '1', '2', '9', '1', '7', '9', '9', '8', '2', '2', '8', '9', '1', '9', '8', '2', '3', '2', '2', '1', '2', '2', '2', '1', '1', '8', '1', '9', '4', '12', '8', '2', '7', '2', '2', '1', '9', '3', '1', '8', '12', '1', '1', '2', '1', '10', '2', '6', '12', '12', '4', '9', '1', '9', '10', '1', '2', '8', '2', '1', '8', '6', '12', '9', '9', '2', '1', '7', '8', '9', '5', '8', '1', '8', '8', '3', '1', '8', '2', '8', '2', '2', '1', '2', '9', '2', '8', '1', '12', '8', '2', '7', '3', '3', '12', '2', '3', '3', '9', '9', '3', '3', '3', '3', '3', '2', '1', '4', '1', '4', '8', '8', '2', '1', '2', '9', '12', '1', '10', '12', '1', '2', '9', '4', '8', '10', '3', '2', '8', '7', '4', '8', '9', '2', '7', '1', '10', '2', '2', '2', '8', '4', '2', '2', '1', '1', '8', '7', '9', '2', '1', '9', '8', '9', '12', '4', '8', '2', '8', '7', '7', '1', '9', '2', '1', '1', '9', '1', '8', '2', '3', '9', '9', '1', '3', '2', '9', '2', '2', '8', '1', '1', '2', '1', '2', '1', '9', '8', '2', '2', '4', '8', '8', '9', '5', '5', '9', '8', '9', '6', '2', '1', '12', '2', '3', '8', '8', '9', '2', '2', '1', '3', '2', '10', '9', '10', '7', '2', '8', '4', '2', '8', '9', '8', '1', '3', '8', '8', '1', '8', '3', '8', '8', '1', '10', '3', '1', '5', '8', '8', '9', '2', '2', '9', '2', '1', '9', '1', '4', '1', '12', '8', '8', '2', '1', '1', '1', '1', '1', '9', '2', '9', '8', '9', '5', '9', '1', '8', '2', '4', '1', '1', '1', '9', '12', '4', '1', '1', '9', '8', '4', '1', '8', '9', '8', '8', '9', '9', '9', '9', '9', '9', '1', '3', '4', '2', '2', '8', '2', '9', '2', '3', '8', '3', '1', '7', '2', '1', '1', '4', '2', '9', '2', '1', '3', '2', '9', '2', '9', '3', '8', '1', '9', '1', '1', '2', '8', '8', '8', '2', '3', '9', '9', '8', '2', '4', '1', '5', '1', '1', '9', '9', '2', '1', '1', '1', '9', '9', '1', '3', '9', '2', '2', '9', '9', '8', '3', '9', '9', '8', '2', '2', '8', '9', '2', '2', '8', '9', '10', '10', '2', '8', '7', '9', '9', '4', '2', '4', '11', '2', '1', '7', '8', '8', '8', '9', '4', '9', '8', '8', '2', '1', '1', '9', '8', '3', '3', '8', '8', '7', '9', '1', '3', '4', '8', '10', '2', '4', '4', '8', '2', '8', '2', '8', '9', '2', '1', '2', '2', '3', '1', '9', '4', '5', '9', '9', '1', '1', '1', '1', '1', '1', '7', '1', '1', '1', '8', '1', '1', '1', '1', '1', '9', '1', '2', '1', '12', '9', '3', '9', '9', '4', '1', '9', '1', '1', '2', '11', '9', '3', '2', '2', '2', '4', '2', '3', '2', '7', '7', '8', '1', '10', '9', '5', '7', '1', '10', '8', '1', '2', '7', '1', '12', '5', '8', '4', '2', '9', '9', '8', '2', '9', '8', '8', '10', '2', '3', '2', '7', '2', '8', '10', '3', '2', '9', '9', '9', '1', '1', '1', '1', '8', '2', '10', '9', '8', '1', '2', '2', '2', '1', '2', '1', '3', '9', '12', '2', '2', '4', '8', '1', '3', '9', '4', '9', '1', '9', '9', '2', '1', '1', '9', '7', '10', '3', '10', '2', '9', '2', '9', '2', '8', '2', '1', '2', '3', '1', '1', '1', '8', '5', '8', '3', '1', '1', '4', '2', '2', '1', '3', '8', '2', '1', '1', '1', '2', '9', '4', '1', '8', '1', '2', '8', '2', '9', '2', '2', '5', '5', '2', '5', '2', '1', '8', '8', '1', '3', '12', '2', '4', '3', '2', '2', '2', '6', '2', '3', '3', '1', '8', '2', '2', '1', '1', '8', '8', '10', '1', '9', '2', '9', '12', '0', '2', '1', '3', '2', '2', '1', '2', '1', '9', '9', '2', '9', '8', '8', '1', '7', '8', '1', '2', '6', '9', '8', '10', '1', '12', '2', '2', '1', '8', '2', '1', '12', '7', '4', '2', '10', '2', '2', '1', '2', '1', '2', '1', '1', '2', '1', '9', '8', '10', '2', '10', '9', '9', '12', '5', '3', '2', '8', '2', '8', '3', '3', '1', '4', '9', '10', '2', '4', '8', '1', '2', '10', '12', '12', '3', '1', '1', '2', '9', '6', '6', '1', '9', '1', '8', '1', '8', '8', '8', '8', '2', '2', '2', '8', '8', '2', '1', '1', '1', '1', '1', '8', '8', '5', '8', '8', '1', '8', '9', '9', '1', '7', '12', '9', '9', '7', '2', '2', '7', '8', '9', '8', '2', '9', '6', '3', '4', '8', '4', '1', '2', '9', '8', '8', '8', '2', '1', '1', '8', '8', '3', '1', '9', '9', '7', '1', '2', '2', '2', '9', '4', '1', '2', '12', '1', '8', '2', '1', '9', '1', '8', '9', '9', '2', '5', '4', '2', '1', '2', '8', '12', '8', '10', '3', '8', '8', '8', '2', '2', '8', '2', '8', '2', '4', '8', '10', '2', '7', '2', '1', '8', '8', '9', '5', '8', '1', '12', '7', '10', '2', '2', '2', '1', '4', '8', '12', '1', '8', '2', '2', '9', '1', '8', '3', '1', '1', '5', '10', '2', '1', '2', '4', '8', '9', '3', '2', '8', '7', '4', '4', '9', '9', '2', '8', '1', '1', '2', '4', '7', '1', '9', '1', '1', '1', '8', '4', '1', '3', '1', '2', '2', '9', '1', '8', '8', '2', '2', '9', '9', '1', '8', '8', '1', '9', '8', '8', '2', '2', '2', '2', '1', '1', '1', '7', '4', '1', '1', '4', '2', '3', '3', '2', '3', '9', '2', '8', '2', '1', '5', '1', '2', '2', '1', '3', '5', '2', '9', '1', '1', '9', '8', '1', '4', '8', '1', '8', '2', '4', '8', '4', '7', '9', '8', '1', '2', '2', '4', '4', '11', '3', '11', '1', '3', '8', '1', '9', '6', '8', '9', '3', '9', '8', '9', '2', '3', '3', '7', '1', '7', '2', '9', '9', '1', '8', '8', '1', '5', '5', '8', '8', '1', '2', '1', '3', '9', '9', '5', '9', '8', '1', '4', '3', '8', '1', '1', '2', '1', '8', '9', '8', '2', '2', '2', '1', '2', '4', '10', '8', '1', '9', '1', '8', '1', '1', '9', '9', '8', '11', '1', '8', '2', '1', '2', '8', '2', '8', '8', '2', '9', '6', '10', '1', '8', '6', '4', '5', '11', '2', '2', '8', '1', '8', '1', '8', '1', '4', '8', '1', '1', '1', '8', '3', '7', '3', '3', '7', '1', '1', '4', '8', '1', '6', '4', '2', '2', '1', '1', '1', '1', '6', '2', '2', '8', '1', '8', '5', '5', '2', '1', '1', '2', '3', '7', '1', '1', '6', '2', '9', '3', '9', '8', '9', '2', '1', '4', '2', '9', '13', '9', '2', '8', '12', '4', '1', '1', '9', '5', '1', '1', '1', '1', '8', '2', '1', '1', '2', '9', '0', '12', '1', '1', '8', '9', '8', '3', '2', '2', '12', '8', '2', '2', '2', '1', '10', '1', '9', '8', '9', '5', '2', '2', '9', '2', '8', '4', '8', '3', '7', '9', '2', '2', '7', '2', '2', '8', '1', '1', '9', '10', '9', '8', '2', '9', '0', '7', '1', '1', '1', '9', '8', '2', '8', '12', '8', '3', '2', '8', '8', '7', '7', '8', '12', '2', '1', '3', '9', '2', '2', '8', '2', '8', '8', '12', '8', '8', '1', '8', '9', '2', '4', '4', '7', '2', '1', '1', '9', '3', '7', '1', '3', '9', '8', '2', '2', '3', '1', '3', '10', '2', '3', '7', '1', '8', '1', '1', '10', '3', '10', '8', '4', '1', '2', '4', '3', '10', '9', '3', '9', '5', '8', '7', '2', '9', '3', '3', '2', '11', '8', '1', '1', '7', '2', '7', '2', '9', '4', '8', '1', '12', '8', '7', '9', '1', '0', '2', '6', '4', '9', '3', '2', '4', '12', '9', '8', '5', '9', '2', '8', '4', '6', '8', '9', '3', '1', '8', '7', '3', '1', '1', '1', '8', '9', '1', '8', '7', '9', '8', '2', '1', '2', '2', '3', '2', '2', '1', '1', '4', '9', '7', '3', '2', '8', '8', '9', '1', '9', '7', '4', '2', '2', '8', '5', '9', '1', '5', '7', '1', '4', '6', '9', '10', '1', '8', '2', '9', '1', '9', '2', '9', '8', '2', '9', '9', '11', '2', '12', '2', '2', '9', '4', '8', '1', '9', '9', '2', '8', '9', '1', '2', '9', '3', '8', '8', '1', '7', '8', '8', '2', '3', '2', '2', '2', '2', '9', '8', '10', '2', '2', '1', '9', '11', '1', '8', '8', '2', '4', '2', '1', '2', '8', '8', '9', '2', '8', '9', '2', '3', '1', '2', '1', '3', '10', '8', '1', '2', '4', '1', '9', '8', '9', '3', '3', '9', '8', '2', '11', '3', '7', '1', '8', '7', '1', '8', '8', '8', '1', '4', '8', '8', '1', '9', '3', '1', '1', '2', '8', '8', '8', '2', '1', '8', '10', '2', '12', '8', '8', '9', '1', '8', '1', '3', '2', '2', '10', '7', '2', '2', '1', '1', '8', '2', '9', '1', '2', '9', '3', '3', '10', '1', '4', '7', '10', '12', '2', '4', '10', '8', '6', '2', '9', '1', '8', '3', '1', '2', '8', '8', '2', '12', '10', '2', '10', '9', '1', '2', '5', '8', '12', '8', '8', '7', '8', '8', '1', '8', '2', '2', '9', '1', '8', '10', '5', '5', '5', '8', '1', '9', '1', '13', '1', '2', '2', '1', '2', '4', '9', '2', '1', '9', '13', '11', '1', '9', '9', '3', '10', '8', '7', '8', '8', '4', '2', '1', '9', '4', '3', '1', '1', '10', '9', '2', '8', '6', '2', '1', '1', '3', '2', '8', '1', '1', '1', '1', '2', '4', '1', '1', '10', '1', '8', '1', '1', '9', '1', '7', '1', '1', '9', '9', '2', '12', '9', '1', '8', '1', '12', '1', '8', '3', '8', '1', '8', '1', '2', '10', '1', '1', '2', '2', '9', '3', '9', '12', '1', '9', '1', '2', '8', '7', '2', '1', '1', '3', '2', '8', '4', '4', '5', '12', '7', '8', '9', '6', '1', '8', '9', '2', '1', '9', '1', '4', '1', '8', '2', '1', '1', '8', '7', '4', '9', '1', '8', '4', '2', '7', '5', '1', '3', '9', '2', '8', '2', '1', '2', '9', '1', '1', '2', '3', '4', '3', '1', '2', '2', '10', '2', '2', '8', '1', '1', '2', '4', '9', '2', '11', '9', '8', '9', '2', '1', '10', '4', '1', '1', '10', '7', '2', '10', '8', '1', '10', '8', '8', '8', '8', '8', '8', '2', '2', '2', '8', '3', '4', '1', '1', '1', '2', '1', '8', '8', '2', '8', '3', '8', '3', '1', '2', '3', '1', '1', '10', '1', '4', '4', '3', '3', '1', '9', '1', '1', '2', '1', '1', '6', '1', '1', '1', '9', '1', '4', '1', '1', '1', '1', '1', '1', '2', '12', '8', '8', '1', '9', '12', '10', '8', '2', '12', '1', '1', '2', '1', '7', '10', '7', '1', '2', '1', '10', '8', '2', '2', '8', '2', '2', '1', '9', '8', '8', '8', '8', '3', '1', '4', '2', '8', '3', '10', '10', '1', '9', '1', '9', '1', '9', '1', '8', '3', '1', '1', '8', '8', '8', '6', '3', '4', '1', '1', '1', '9', '9', '5', '2', '10', '2', '6', '2', '2', '7', '1', '8', '2', '11', '1', '1', '1', '2', '1', '9', '4', '4', '2', '8', '4', '6', '8', '8', '10', '10', '2', '1', '2', '9', '1', '8', '8', '3', '4', '8', '8', '2', '8', '8', '1', '2', '1', '2', '9', '4', '1', '3', '3', '3', '8', '8', '8', '2', '6', '4', '3', '3', '12', '3', '4', '9', '6', '8', '7', '7', '8', '6', '4', '1', '10', '6', '3', '3', '8', '1', '1', '8', '1', '11', '8', '1', '1', '2', '3', '11', '7', '2', '1', '1', '1', '9', '1', '2', '1', '1', '9', '8', '1', '1', '8', '0', '4', '1', '1', '2', '1', '1', '2', '2', '9', '1', '10', '1', '3', '8', '9', '3', '3', '1', '10', '1', '9', '1', '1', '7', '4', '1', '8', '10', '1', '2', '10', '1', '1', '2', '3', '3', '9', '1', '8', '10', '1', '7', '9', '1', '9', '6', '3', '9', '2', '8', '4', '12', '12', '3', '1', '1', '9', '1', '9', '2', '1', '1', '1', '1', '1', '9', '2', '1', '2', '10', '9', '8', '8', '10', '8', '2', '3', '2', '1', '12', '8', '12', '2', '9', '1', '3', '2', '5', '2', '2', '8', '2', '2', '4', '10', '9', '10', '9', '4', '2', '4', '12', '8', '2', '1', '8', '3', '9', '2', '9', '4', '1', '1', '1', '1', '1', '9', '6', '2', '9', '2', '1', '3', '2', '2', '5', '8', '9', '1', '6', '3', '1', '2', '2', '2', '6', '1', '8', '10', '8', '3', '3', '13', '2', '9', '9', '10', '6', '12', '8', '2', '9', '2', '3', '8', '8', '1', '2', '2', '3', '2', '10', '1', '8', '9', '1', '8', '9', '8', '8', '2', '10', '2', '1', '1', '1', '9', '9', '12', '1', '1', '4', '8', '3', '2', '2', '4', '4', '2', '2', '1', '1', '1', '9', '2', '2', '4', '1', '1', '8', '8', '8', '9', '2', '13', '9', '9', '4', '1', '9', '7', '10', '10', '8', '1', '9', '1', '1', '1', '8', '12', '4', '1', '2', '1', '1', '1', '7', '8', '3', '3', '1', '2', '2', '7', '2', '2', '10', '1', '8', '4', '0', '1', '9', '4', '1', '1', '2', '10', '10', '7', '9', '3', '12', '11', '2', '8', '10', '8', '9', '4', '3', '4', '10', '1', '1', '9', '3', '10', '1', '5', '3', '3', '2', '2', '1', '1', '6', '1', '7', '1', '1', '12', '1', '1', '1', '10', '1', '8', '8', '1', '3', '8', '3', '1', '2', '8', '3', '4', '2', '8', '8', '6', '1', '1', '4', '1', '2', '9', '5', '1', '7', '8', '9', '2', '4', '9', '8', '5', '9', '9', '9', '1', '2', '1', '3', '8', '9', '2', '9', '8', '9', '1', '8', '13', '9', '3', '8', '1', '4', '1', '3', '1', '6', '4', '10', '2', '2', '7', '8', '8', '12', '8', '9', '9', '9', '10', '3', '12', '12', '2', '9', '10', '9', '7', '2', '3', '4', '10', '10', '2', '7', '7', '1', '7', '3', '9', '1', '9', '8', '2', '9', '1', '8', '1', '10', '8', '8', '2', '7', '8', '1', '8', '9', '8', '1', '4', '1', '8', '8', '8', '1', '1', '10', '3', '2', '2', '6', '8', '9', '8', '2', '1', '1', '3', '8', '8', '1', '4', '4', '3', '9', '10', '4', '2', '4', '2', '9', '2', '2', '9', '1', '1', '10', '1', '1', '2', '1', '1', '1', '9', '1', '8', '8', '8', '2', '3', '8', '1', '6', '7', '3', '1', '9', '1', '2', '1', '6', '1', '2', '10', '1', '4', '2', '3', '2', '9', '9', '4', '9', '9', '1', '2', '8', '4', '7', '8', '7', '13', '9', '1', '2', '1', '9', '3', '3', '6', '8', '9', '10', '9', '2', '2', '3', '12', '1', '1', '1', '13', '2', '1', '7', '10', '8', '9', '1', '2', '9', '1', '1', '1', '2', '1', '12', '5', '6', '9', '10', '3', '1', '10', '1', '3', '2', '8', '9', '2', '10', '8', '8', '9', '2', '8', '9', '2', '2', '2', '8', '8', '2', '3', '1', '4', '8', '1', '9', '9', '2', '8', '10', '9', '1', '2', '12', '8', '8', '0', '2', '1', '1', '9', '10', '1', '6', '2', '2', '10', '2', '2', '4', '2', '2', '12', '1', '6', '7', '8', '9', '1', '3', '5', '7', '3', '1', '3', '1', '1', '3', '2', '6', '3', '2', '1', '8', '3', '5', '2', '1', '1', '8', '1', '1', '0', '2', '3', '5', '3', '1', '1', '9', '8', '8', '8', '8', '4', '7', '2', '9', '12', '5', '9', '3', '12', '3', '9', '1', '9', '1', '7', '3', '9', '3', '10', '1', '2', '1', '4', '9', '8', '2', '9', '11', '2', '8', '9', '4', '1', '1', '1', '1', '1', '1', '1', '9', '1', '9', '1', '1', '1', '12', '3', '10', '3', '6', '1', '7', '10', '8', '3', '7', '1', '1', '2', '6', '1', '3', '9', '2', '8', '1', '8', '8', '1', '8', '10', '12', '1', '10', '12', '10', '1', '8', '8', '1', '4', '7', '9', '2', '1', '9', '3', '8', '12', '10', '6', '1', '6', '8', '3', '1', '3', '1', '10', '10', '9', '8', '1', '2', '9', '8', '8', '1', '9', '8', '8', '3', '1', '3', '9', '12', '1', '8', '1', '5', '8', '11', '5', '5', '1', '2', '1', '3', '1', '1', '1', '9', '8', '8', '1', '9', '10', '8', '4', '1', '1', '8', '10', '8', '9', '1', '10', '1', '7', '2', '1', '1', '9', '6', '2', '9', '8', '1', '1', '1', '9', '8', '8', '1', '9', '9', '1', '1', '2', '7', '9', '2', '9', '9', '8', '7', '6', '8', '7', '8', '2', '9', '2', '1', '8', '8', '8', '2', '6', '3', '1', '8', '12', '12', '8', '7', '1', '2', '2', '7', '1', '9', '8', '1', '4', '1', '1', '8', '5', '8', '1', '8', '8', '8', '8', '1', '2', '11', '1', '1', '1', '2', '7', '1', '9', '2', '2', '2', '9', '8', '9', '2', '9', '1', '8', '1', '7', '11', '13', '1', '9', '8', '2', '2', '1', '2', '3', '10', '3', '10', '4', '3', '9', '1', '8', '1', '1', '13', '1', '1', '3', '8', '1', '2', '2', '2', '9', '4', '1', '8', '1', '9', '3', '6', '8', '5', '2', '8', '2', '8', '1', '9', '2', '2', '8', '1', '1', '4', '8', '9', '2', '2', '7', '1', '8', '12', '2', '12', '8', '2', '9', '9', '3', '8', '1', '1', '9', '9', '9', '2', '8', '1', '9', '1', '8', '9', '8', '2', '2', '4', '1', '9', '10', '10', '8', '8', '1', '1', '2', '1', '4', '1', '2', '1', '1', '3', '12', '8', '8', '8', '8', '10', '2', '8', '12', '1', '9', '9', '8', '9', '1', '2', '1', '8', '8', '8', '2', '8', '8', '3', '10', '8', '9', '1', '1', '1', '3', '4', '8', '10', '6', '3', '1', '3', '2', '8', '9', '3', '5', '4', '1', '5', '2', '9', '9', '9', '1', '1', '1', '9', '9', '6', '10', '9', '8', '12', '2', '9', '1', '2', '1', '1', '1', '8', '1', '12', '1', '12', '1', '9', '1', '8', '1', '2', '1', '2', '8', '8', '10', '1', '9', '1', '2', '2', '8', '8', '3', '12', '1', '11', '2', '10', '12', '11', '2', '1', '10', '1', '1', '9', '3', '10', '2', '7', '1', '8', '1', '9', '9', '2', '1', '3', '12', '5', '9', '9', '8', '1', '8', '8', '8', '1', '1', '3', '9', '8', '9', '8', '3', '10', '3', '7', '8', '9', '2', '3', '1', '9', '10', '9', '2', '8', '2', '6', '4', '1', '1', '3', '8', '2', '3', '9', '1', '2', '1', '8', '1', '2', '2', '2', '2', '9', '9', '4', '8', '9', '1', '1', '2', '9', '1', '5', '2', '9', '8', '10', '1', '2', '10', '8', '5', '6', '4', '8', '1', '1', '1', '8', '9', '2', '8', '8', '8', '12', '2', '2', '1', '8', '1', '2', '1', '9', '8', '8', '9', '1', '1', '8', '7', '1', '8', '0', '2', '3', '8', '1', '1', '8', '1', '1', '12', '3', '2', '10', '8', '10', '6', '1', '8', '8', '10', '8', '8', '1', '4', '8', '1', '2', '9', '8', '1', '1', '3', '3', '5', '1', '1', '2', '1', '2', '1', '9', '8', '3', '9', '8', '2', '10', '9', '1', '8', '1', '10', '9', '10', '1', '2', '3', '8', '1', '3', '1', '9', '9', '8', '1', '9', '8', '8', '9', '1', '9', '2', '8', '8', '2', '13', '8', '10', '8', '9', '3', '2', '1', '3', '1', '12', '10', '10', '1', '11', '1', '2', '4', '2', '10', '1', '8', '11', '1', '1', '1', '2', '2', '8', '2', '9', '8', '9', '9', '12', '8', '1', '9', '2', '4', '1', '8', '3', '9', '1', '6', '5', '8', '9', '3', '3', '2', '1', '1', '1', '8', '11', '1', '8', '7', '1', '11', '9', '1', '7', '9', '9', '8', '8', '8', '12', '8', '1', '2', '9', '8', '8', '2', '10', '9', '8', '8', '4', '10', '9', '8', '2', '10', '10', '2', '1', '2', '1', '2', '1', '4', '1', '7', '1', '1', '3', '12', '9', '8', '5', '9', '10', '1', '9', '9', '2', '7', '4', '1', '1', '8', '12', '8', '2', '2', '9', '4', '1', '1', '5', '9', '1', '8', '12', '8', '1', '1', '4', '9', '10', '2', '3', '1', '3', '3', '3', '8', '1', '2', '1', '2', '1', '2', '9', '1', '2', '8', '9', '4', '8', '8', '10', '2', '12', '5', '5', '1', '8', '1', '7', '8', '1', '1', '8', '9', '12', '4', '9', '3', '1', '2', '5', '9', '2', '1', '2', '2', '1', '2', '8', '1', '8', '8', '2', '2', '1', '8', '4', '1', '2', '10', '9', '10', '8', '1', '10', '4', '8', '8', '10', '2', '8', '1', '1', '3', '10', '1', '4', '2', '8', '3', '3', '2', '9', '8', '5', '5', '9', '3', '10', '8', '1', '2', '1', '2', '9', '1', '1', '2', '9', '9', '8', '8', '11', '9', '4', '1', '8', '2', '7', '1', '2', '8', '2', '9', '9', '8', '2', '9', '9', '8', '8', '1', '2', '8', '1', '1', '2', '9', '8', '1', '4', '9', '1', '8', '12', '1', '12', '2', '8', '2', '9', '1', '12', '1', '2', '9', '1', '1', '9', '3', '2', '9', '9', '2', '11', '8', '7', '9', '9', '9', '8', '8', '1', '2', '8', '1', '1', '4', '1', '2', '10', '1', '9', '2', '1', '1', '1', '9', '1', '13', '4', '3', '2', '1', '1', '2', '2', '7', '8', '7', '1', '1', '2', '8', '8', '3', '4', '8', '2', '10', '13', '8', '8', '9', '2', '2', '1', '1', '10', '4', '2', '7', '9', '8', '2', '1', '9', '8', '1', '1', '9', '8', '9', '8', '2', '8', '2', '2', '9', '2', '1', '9', '1', '2', '4', '8', '2', '1', '1', '8', '1', '4', '1', '9', '3', '9', '2', '7', '1', '9', '6', '1', '10', '2', '2', '2', '2', '2', '10', '10', '10', '9', '9', '9', '3', '12', '10', '1', '1', '10', '1', '1', '2', '2', '1', '2', '3', '12', '8', '8', '1', '2', '9', '9', '1', '1', '10', '8', '9', '8', '3', '1', '1', '3', '1', '10', '1', '1', '1', '4', '1', '1', '1', '7', '10', '2', '1', '8', '1', '9', '3', '1', '10', '8', '8', '1', '5', '9', '1', '2', '1', '8', '8', '1', '3', '1', '10', '2', '1', '1', '2', '8', '2', '5', '3', '5', '2', '1', '1', '9', '1', '7', '9', '9', '2', '8', '1', '12', '4', '1', '4', '9', '9', '4', '8', '1', '10', '10', '8', '8', '8', '9', '2', '3', '5', '1', '8', '1', '1', '1', '7', '10', '1', '4', '12', '3', '2', '2', '9', '1', '1', '1', '2', '8', '4', '1', '7', '3', '12', '8', '6', '2', '2', '7', '9', '11', '1', '10', '9', '1', '8', '12', '2', '11', '1', '2', '3', '1', '1', '1', '8', '9', '2', '2', '2', '2', '3', '3', '8', '10', '4', '1', '2', '9', '8', '2', '2', '9', '1', '8', '11', '4', '2', '8', '7', '1', '1', '7', '3', '9', '9', '4', '5', '8', '10', '2', '9', '9', '9', '8', '1', '9', '2', '2', '2', '1', '2', '9', '4', '3', '12', '4', '3', '2', '3', '8', '3', '10', '1', '10', '1', '1', '8', '10', '6', '8', '9', '9', '1', '2', '9', '2', '9', '3', '8', '1', '1', '12', '1', '5', '1', '10', '8', '10', '2', '7', '1', '1', '1', '1', '3', '2', '3', '5', '9', '1', '2', '1', '9', '8', '10', '9', '8', '1', '8', '1', '8', '2', '8', '1', '2', '8', '1', '2', '5', '8', '12', '2', '1', '1', '1', '1', '1', '8', '8', '2', '1', '4', '2', '8', '8', '8', '2', '9', '9', '11', '1', '2', '9', '3', '1', '1', '8', '9', '2', '4', '10', '1', '9', '8', '4', '9', '8', '10', '8', '8', '9', '2', '8', '9', '3', '3', '3', '4', '3', '2', '2', '10', '8', '2', '1', '5', '1', '9', '1', '1', '2', '1', '2', '3', '1', '1', '8', '8', '1', '10', '8', '8', '1', '6', '4', '8', '2', '5', '8', '1', '3', '8', '2', '8', '9', '1', '1', '8', '12', '10', '1', '5', '8', '2', '8', '10', '9', '9', '1', '6', '9', '1', '10', '8', '2', '9', '10', '1', '2', '1', '1', '8', '8', '3', '9', '8', '1', '10', '2', '8', '1', '10', '1', '5', '1', '1', '1', '5', '1', '1', '1', '4', '1', '1', '1', '8', '2', '8', '1', '8', '3', '8', '9', '1', '8', '1', '8', '1', '1', '2', '2', '1', '12', '9', '9', '1', '8', '2', '1', '2', '8', '1', '9', '1', '1', '2', '1', '2', '2', '2', '1', '9', '9', '8', '8', '1', '1', '9', '10', '8', '4', '3', '3', '4', '9', '1', '3', '3', '10', '2', '2', '1', '8', '4', '1', '9', '4', '1', '1', '9', '8', '10', '4', '9', '9', '8', '2', '1', '3', '4', '9', '3', '8', '9', '1', '1', '2', '1', '7', '8', '2', '9', '1', '9', '2', '6', '9', '10', '8', '1', '1', '5', '9', '5', '1', '9', '10', '9', '3', '3', '8', '8', '2', '9', '2', '8', '2', '1', '8', '5', '8', '3', '10', '1', '1', '2', '1', '8', '1', '4', '3', '2', '9', '4', '9', '8', '8', '8', '1', '3', '9', '1', '9', '9', '1', '2', '1', '9', '8', '9', '8', '1', '1', '1', '4', '2', '2', '9', '9', '1', '1', '1', '3', '6', '1', '2', '3', '1', '4', '2', '9', '1', '9', '9', '1', '8', '2', '1', '9', '1', '8', '1', '8', '8', '2', '1', '9', '9', '6', '8', '8', '9', '8', '10', '8', '9', '5', '1', '1', '1', '1', '8', '1', '8', '12', '8', '1', '12', '9', '2', '8', '1', '1', '2', '1', '8', '6', '9', '9', '8', '1', '8', '9', '7', '7', '8', '9', '9', '1', '8', '3', '8', '1', '3', '3', '8', '9', '8', '2', '8', '1', '2', '9', '8', '1', '1', '1', '1', '9', '1', '9', '8', '12', '3', '8', '8', '1', '10', '10', '10', '2', '2', '1', '10', '2', '2', '10', '8', '12', '8', '1', '1', '1', '2', '1', '1', '3', '8', '1', '2', '2', '2', '1', '1', '6', '2', '8', '8', '8', '1', '1', '1', '9', '9', '2', '8', '10', '2', '8', '2', '1', '2', '9', '9', '2', '1', '1', '8', '8', '1', '3', '1', '8', '9', '1', '10', '9', '1', '1', '1', '1', '7', '2', '2', '1', '2', '8', '8', '1', '2', '1', '3', '2', '1', '8', '3', '9', '2', '10', '2', '10', '1', '2', '1', '1', '13', '9', '10', '8', '2', '2', '1', '1', '8', '2', '2', '1', '9', '1', '1', '8', '9', '9', '2', '9', '2', '2', '1', '1', '1', '8', '9', '4', '9', '9', '1', '8', '2', '1', '1', '8', '2', '2', '2', '8', '1', '1', '8', '2', '10', '2', '9', '9', '7', '1', '1', '2', '2', '8', '3', '1', '2', '1', '1', '8', '1', '1', '1', '9', '1', '1', '8', '8', '1', '9', '6', '1', '9', '1', '8', '1', '8', '8', '8', '1', '3', '8', '8', '2', '6', '3', '2', '1', '10', '1', '8', '2', '1', '6', '1', '1', '1', '8', '1', '10', '1', '1', '8', '9', '2', '6', '9', '2', '9', '4', '1', '8', '1', '3', '7', '8', '8', '5', '8', '7', '1', '1', '1', '13', '8', '3', '1', '1', '1', '1', '2', '8', '2', '12', '8', '1', '2', '5', '1', '8', '2', '10', '8', '2', '10', '1', '5', '2', '1', '1', '5', '2', '2', '7', '8', '2', '9', '9', '1', '10', '1', '8', '2', '5', '8', '1', '5', '1', '10', '1', '1', '9', '8', '6', '1', '8', '9', '8', '1', '8', '2', '2', '8', '3', '9', '1', '9', '3', '8', '4', '4', '3', '3', '1', '1', '1', '8', '9', '1', '1', '8', '3', '1', '2', '8', '2', '1', '1', '10', '1', '2', '2', '1', '1', '8', '2', '8', '2', '10', '2', '2', '10', '8', '8', '8', '13', '2', '2', '9', '2', '5', '9', '2', '1', '8', '8', '8', '1', '12', '12', '2', '9', '2', '1', '8', '2', '1', '8', '1', '9', '9', '2', '7', '1', '1', '7', '1', '4', '3', '1', '10', '3', '10', '9', '4', '6', '7', '5', '2', '1', '8', '8', '1', '4', '9', '2', '1', '8', '1', '1', '1', '1', '1', '8', '9', '8', '8', '8', '8', '9', '8', '10', '1', '8', '2', '7', '8', '9', '1', '2', '5', '9', '10', '8', '9', '12', '1', '6', '1', '2', '10', '9', '4', '1', '8', '10', '10', '1', '1', '1', '1', '1', '2', '3', '8', '1', '2', '10', '2', '1', '2', '4', '2', '2', '9', '1', '4', '2', '1', '1', '12', '12', '9', '9', '7', '1', '8', '4', '9', '9', '8', '1', '7', '3', '2', '1', '1', '4', '8', '1', '5', '8', '2', '8', '1', '14', '9', '12', '1', '10', '2', '1', '1', '8', '8', '6', '6', '3', '1', '2', '8', '1', '3', '2', '1', '8', '8', '1', '10', '8', '2', '8', '2', '5', '9', '1', '8', '8', '1', '1', '8', '8', '13', '1', '3', '8', '4', '1', '9', '7', '8', '8', '8', '3', '8', '1', '4', '2', '8', '8', '8', '7', '1', '1', '2', '8', '8', '9', '8', '8', '2', '3', '9', '9', '2', '1', '2', '2', '1', '8', '2', '6', '2', '1', '1', '9', '10', '3', '8', '2', '3', '2', '8', '8', '1', '8', '8', '8', '9', '1', '4', '1', '9', '2', '1', '8', '0', '1', '8', '1', '1', '8', '9', '9', '1', '3', '1', '2', '8', '8', '7', '1', '1', '2', '9', '2', '10', '2', '8', '2', '2', '2', '7', '1', '9', '8', '1', '3', '9', '2', '1', '10', '8', '1', '4', '1', '8', '5', '9', '8', '8', '1', '2', '2', '1', '8', '8', '6', '8', '1', '1', '8', '3', '2', '2', '1', '8', '1', '7', '6', '8', '1', '1', '1', '9', '1', '1', '1', '2', '1', '11', '10', '10', '11', '9', '']\n",
            "['8', '1', '8', '2', '8', '8', '8', '9', '7', '8', '1', '2', '1', '1', '8', '4', '8', '8', '12', '3', '3', '7', '3', '12', '1', '8', '8', '8', '8', '8', '8', '8', '1', '9', '5', '9', '9', '9', '11', '8', '8', '8', '4', '8', '8', '8', '8', '1', '3', '9', '3', '7', '1', '2', '9', '9', '7', '8', '8', '1', '10', '7', '8', '8', '9', '8', '7', '9', '9', '12', '7', '2', '8', '1', '11', '11', '1', '7', '7', '12', '1', '9', '8', '10', '12', '7', '8', '2', '8', '9', '9', '1', '8', '9', '1', '7', '12', '10', '10', '10', '8', '3', '7', '9', '8', '9', '1', '8', '8', '2', '7', '2', '9', '9', '11', '8', '8', '12', '12', '7', '8', '12', '4', '9', '3', '1', '12', '1', '1', '8', '8', '3', '8', '8', '8', '8', '9', '1', '8', '8', '10', '1', '8', '2', '8', '8', '7', '3', '8', '2', '4', '4', '9', '8', '10', '12', '12', '1', '1', '9', '1', '1', '1', '8', '2', '2', '8', '1', '1', '2', '2', '1', '2', '8', '1', '9', '9', '8', '8', '4', '2', '9', '9', '8', '3', '4', '3', '1', '8', '8', '2', '1', '9', '7', '8', '8', '1', '12', '3', '8', '2', '4', '2', '9', '12', '1', '4', '1', '8', '8', '8', '2', '2', '8', '9', '8', '8', '8', '10', '9', '8', '7', '9', '1', '1', '9', '4', '2', '4', '2', '2', '1', '7', '8', '11', '11', '3', '9', '2', '4', '8', '9', '1', '8', '1', '1', '4', '9', '1', '1', '8', '8', '2', '1', '8', '4', '2', '8', '9', '8', '8', '2', '8', '8', '8', '7', '1', '1', '1', '2', '1', '1', '8', '7', '8', '8', '12', '2', '12', '12', '8', '10', '12', '8', '3', '3', '12', '10', '1', '8', '12', '1', '8', '8', '2', '8', '4', '7', '8', '7', '10', '8', '10', '9', '8', '12', '12', '1', '8', '8', '3', '8', '8', '8', '8', '8', '1', '9', '8', '11', '1', '1', '1', '9', '8', '1', '9', '2', '3', '11', '8', '9', '9', '9', '2', '1', '8', '8', '9', '7', '1', '4', '9', '4', '8', '8', '4', '8', '12', '9', '4', '8', '2', '10', '10', '10', '8', '9', '9', '8', '8', '12', '7', '1', '8', '8', '8', '4', '1', '1', '1', '1', '1', '1', '8', '1', '9', '8', '9', '9', '4', '8', '12', '9', '8', '8', '2', '8', '8', '8', '6', '9', '8', '3', '7', '8', '8', '4', '12', '8', '8', '9', '12', '12', '9', '8', '2', '9', '2', '3', '1', '12', '8', '10', '9', '9', '9', '10', '10', '3', '8', '12', '1', '4', '2', '1', '10', '8', '2', '8', '4', '8', '9', '1', '9', '9', '10', '10', '1', '4', '9', '2', '4', '9', '1', '1', '3', '10', '3', '3', '8', '7', '3', '8', '9', '9', '12', '4', '8', '12', '2', '2', '4', '1', '9', '9', '4', '1', '4', '2', '8', '12', '2', '3', '10', '10', '9', '8', '9', '9', '1', '12', '8', '8', '8', '12', '4', '1', '8', '8', '1', '9', '8', '8', '2', '1', '8', '9', '8', '3', '3', '3', '1', '8', '8', '9', '1', '10', '9', '9', '9', '9', '5', '9', '9', '8', '8', '8', '8', '8', '8', '8', '8', '11', '12', '8', '8', '1', '8', '9', '11', '2', '2', '2', '2', '3', '1', '2', '2', '8', '2', '4', '9', '1', '2', '9', '8', '2', '8', '9', '9', '3', '10', '9', '9', '2', '8', '9', '8', '12', '12', '1', '3', '8', '8', '8', '2', '7', '7', '7', '7', '3', '9', '1', '9', '8', '9', '9', '1', '1', '1', '2', '9', '9', '9', '11', '1', '8', '8', '9', '1', '9', '8', '8', '8', '1', '1', '8', '7', '1', '1', '8', '8', '9', '4', '4', '8', '2', '2', '8', '8', '8', '8', '8', '8', '11', '8', '2', '9', '4', '9', '3', '9', '9', '1', '3', '9', '3', '1', '12', '8', '9', '12', '1', '8', '4', '2', '1', '4', '8', '3', '3', '8', '2', '8', '9', '7', '8', '8', '8', '5', '8', '3', '9', '8', '8', '13', '12', '1', '1', '2', '8', '4', '1', '9', '9', '12', '8', '9', '12', '9', '1', '9', '9', '9', '9', '3', '2', '9', '9', '4', '8', '12', '2', '4', '9', '3', '1', '9', '7', '8', '9', '9', '8', '4', '8', '8', '7', '9', '10', '3', '8', '8', '8', '1', '1', '1', '1', '8', '8', '4', '1', '10', '1', '5', '7', '7', '1', '8', '9', '3', '7', '2', '7', '7', '2', '4', '8', '12', '7', '4', '2', '9', '9', '12', '6', '10', '8', '2', '4', '12', '9', '9', '3', '8', '8', '1', '2', '10', '9', '9', '8', '4', '12', '2', '1', '8', '8', '8', '12', '10', '10', '9', '3', '8', '8', '9', '2', '8', '10', '1', '1', '1', '1', '2', '1', '1', '1', '1', '9', '8', '12', '9', '4', '8', '8', '9', '1', '9', '3', '9', '8', '8', '1', '7', '7', '10', '1', '8', '8', '1', '9', '8', '10', '3', '1', '7', '1', '8', '8', '12', '8', '8', '1', '8', '7', '1', '7', '7', '8', '2', '1', '8', '8', '2', '10', '8', '8', '8', '8', '8', '10', '1', '8', '8', '12', '8', '3', '3', '2', '2', '2', '10', '8', '8', '8', '2', '9', '1', '8', '9', '3', '2', '8', '10', '8', '6', '1', '1', '8', '4', '1', '9', '10', '8', '1', '7', '1', '2', '8', '1', '1', '1', '12', '1', '9', '12', '8', '12', '12', '12', '8', '8', '12', '4', '8', '8', '8', '8', '9', '9', '1', '3', '3', '3', '3', '1', '12', '12', '9', '10', '8', '8', '1', '9', '2', '2', '13', '9', '8', '9', '2', '1', '9', '1', '8', '8', '8', '4', '8', '1', '1', '1', '12', '12', '7', '2', '2', '2', '8', '3', '8', '9', '2', '10', '7', '8', '9', '2', '1', '2', '12', '12', '8', '8', '9', '2', '2', '9', '11', '1', '8', '1', '10', '9', '2', '1', '4', '7', '7', '7', '7', '7', '12', '8', '8', '8', '1', '1', '10', '1', '12', '1', '8', '2', '1', '1', '12', '8', '7', '9', '12', '8', '9', '3', '9', '8', '8', '8', '8', '3', '11', '2', '2', '9', '8', '8', '10', '8', '2', '7', '3', '1', '4', '7', '8', '8', '1', '8', '3', '7', '12', '8', '10', '9', '9', '8', '8', '2', '8', '9', '9', '1', '2', '8', '8', '9', '8', '3', '8', '1', '8', '10', '9', '8', '9', '9', '12', '4', '4', '8', '9', '9', '8', '2', '10', '1', '2', '8', '9', '1', '9', '9', '9', '7', '12', '12', '8', '1', '1', '1', '1', '8', '3', '1', '1', '8', '1', '8', '8', '7', '8', '8', '8', '8', '3', '2', '2', '10', '10', '10', '7', '8', '1', '2', '12', '7', '9', '8', '7', '8', '12', '2', '8', '9', '2', '6', '6', '7', '9', '8', '1', '8', '9', '8', '1', '12', '1', '2', '8', '7', '7', '7', '8', '2', '2', '8', '1', '2', '2', '9', '9', '1', '8', '8', '4', '3', '3', '1', '6', '3', '3', '12', '3', '8', '9', '1', '4', '3', '1', '8', '3', '9', '2', '8', '2', '8', '8', '8', '1', '1', '1', '9', '9', '8', '1', '9', '8', '1', '1', '3', '10', '8', '1', '1', '3', '9', '1', '4', '4', '1', '8', '9', '9', '2', '0', '0', '1', '8', '3', '1', '8', '8', '9', '8', '8', '1', '1', '8', '9', '8', '8', '8', '7', '9', '8', '8', '8', '10', '9', '8', '1', '2', '6', '1', '9', '9', '8', '12', '12', '12', '8', '8', '2', '8', '1', '2', '2', '2', '1', '9', '8', '2', '12', '2', '8', '12', '8', '9', '8', '8', '9', '7', '1', '1', '1', '1', '1', '8', '8', '1', '8', '8', '1', '1', '3', '2', '8', '8', '9', '10', '10', '2', '2', '1', '9', '2', '9', '9', '4', '12', '12', '12', '10', '7', '3', '3', '4', '2', '2', '9', '2', '8', '4', '2', '4', '1', '10', '9', '7', '8', '7', '1', '1', '3', '3', '1', '1', '3', '3', '3', '1', '1', '1', '1', '8', '2', '3', '1', '1', '2', '8', '8', '12', '8', '8', '8', '8', '11', '9', '1', '8', '9', '2', '8', '8', '8', '3', '9', '1', '9', '2', '7', '2', '8', '2', '8', '10', '8', '1', '10', '1', '1', '9', '9', '8', '8', '1', '8', '8', '8', '12', '8', '8', '8', '1', '8', '8', '8', '1', '9', '1', '1', '8', '1', '8', '9', '8', '2', '12', '9', '9', '0', '1', '8', '8', '1', '8', '12', '8', '8', '10', '8', '8', '8', '7', '8', '1', '8', '7', '3', '10', '1', '8', '9', '1', '8', '8', '8', '10', '1', '10', '3', '9', '1', '8', '9', '2', '8', '3', '3', '9', '9', '7', '9', '1', '1', '9', '2', '1', '1', '1', '7', '1', '1', '8', '8', '1', '1', '8', '1', '8', '3', '12', '9', '3', '3', '8', '8', '8', '8', '3', '1', '3', '3', '1', '11', '0', '8', '8', '7', '8', '12', '1', '8', '9', '8', '9', '8', '8', '3', '8', '8', '1', '1', '1', '9', '2', '2', '2', '8', '7', '12', '8', '8', '9', '10', '10', '7', '8', '1', '9', '8', '7', '3', '1', '3', '8', '2', '2', '3', '9', '8', '4', '4', '8', '9', '2', '1', '1', '7', '8', '9', '9', '7', '8', '7', '7', '8', '2', '2', '8', '4', '9', '7', '10', '0', '9', '8', '3', '7', '8', '1', '1', '8', '9', '9', '2', '2', '10', '1', '9', '10', '10', '10', '8', '3', '2', '12', '9', '9', '10', '12', '9', '12', '12', '9', '1', '2', '4', '12', '12', '7', '8', '9', '7', '7', '7', '3', '9', '8', '9', '1', '12', '8', '9', '4', '1', '3', '12', '12', '12', '12', '8', '8', '2', '1', '1', '2', '1', '1', '1', '12', '12', '8', '12', '2', '2', '12', '3', '3', '12', '8', '2', '8', '8', '12', '2', '1', '10', '3', '2', '8', '7', '1', '8', '1', '3', '7', '8', '9', '8', '3', '1', '1', '7', '8', '8', '9', '8', '2', '9', '2', '2', '9', '8', '1', '8', '8', '1', '3', '3', '1', '1', '10', '1', '2', '8', '1', '1', '1', '1', '9', '1', '4', '1', '7', '7', '7', '7', '2', '2', '8', '8', '12', '1', '9', '1', '7', '3', '3', '1', '8', '8', '10', '8', '9', '2', '9', '1', '3', '8', '8', '3', '12', '2', '8', '12', '2', '9', '1', '3', '3', '3', '3', '2', '8', '7', '9', '8', '3', '3', '1', '7', '8', '3', '1', '1', '12', '8', '9', '1', '2', '3', '8', '1', '1', '3', '3', '9', '1', '1', '1', '12', '1', '7', '3', '3', '1', '8', '8', '8', '1', '2', '4', '8', '1', '10', '2', '5', '3', '3', '12', '10', '9', '9', '12', '9', '0', '2', '8', '8', '9', '9', '9', '8', '1', '3', '1', '1', '4', '8', '1', '10', '8', '7', '2', '8', '2', '8', '4', '7', '8', '1', '9', '1', '9', '8', '2', '8', '2', '7', '9', '2', '2', '9', '1', '8', '12', '1', '8', '1', '4', '1', '9', '9', '1', '10', '12', '4', '8', '1', '7', '3', '9', '2', '12', '7', '8', '8', '2', '1', '12', '9', '8', '1', '2', '2', '8', '10', '2', '1', '7', '7', '7', '12', '3', '3', '8', '3', '8', '8', '3', '9', '8', '9', '1', '1', '8', '7', '9', '3', '3', '8', '1', '0', '9', '9', '9', '1', '8', '9', '9', '10', '1', '8', '7', '8', '8', '8', '9', '9', '8', '9', '10', '4', '9', '3', '7', '12', '1', '9', '9', '8', '9', '1', '9', '3', '1', '8', '4', '12', '10', '9', '8', '7', '10', '8', '12', '12', '3', '10', '8', '12', '2', '1', '2', '3', '9', '8', '8', '7', '1', '1', '2', '2', '1', '2', '2', '7', '1', '3', '9', '9', '3', '8', '8', '8', '8', '8', '7', '8', '8', '10', '8', '1', '8', '2', '8', '2', '2', '2', '2', '2', '1', '7', '12', '10', '1', '2', '8', '1', '4', '7', '8', '1', '12', '8', '7', '9', '2', '2', '2', '8', '1', '8', '1', '1', '0', '2', '2', '6', '1', '8', '2', '1', '8', '1', '1', '2', '2', '3', '2', '2', '10', '10', '9', '1', '7', '7', '8', '8', '1', '12', '10', '12', '3', '8', '8', '8', '3', '8', '3', '10', '2', '2', '2', '1', '2', '2', '1', '0', '1', '8', '9', '1', '1', '1', '2', '2', '2', '2', '2', '1', '1', '1', '1', '12', '9', '9', '1', '10', '11', '12', '9', '1', '1', '8', '8', '1', '2', '12', '8', '3', '7', '2', '10', '9', '7', '8', '1', '2', '2', '7', '9', '12', '2', '2', '1', '8', '9', '2', '3', '2', '2', '2', '9', '9', '8', '9', '2', '3', '8', '8', '9', '7', '3', '10', '8', '8', '9', '1', '1', '2', '8', '1', '10', '8', '8', '9', '7', '1', '7', '12', '8', '1', '7', '2', '1', '3', '9', '2', '8', '8', '1', '2', '3', '9', '3', '8', '9', '7', '7', '1', '10', '2', '8', '8', '8', '1', '8', '2', '8', '8', '1', '2', '8', '10', '2', '1', '0', '8', '3', '8', '10', '10', '12', '3', '8', '9', '3', '10', '8', '8', '8', '1', '8', '8', '2', '2', '2', '2', '2', '2', '1', '1', '10', '1', '10', '8', '2', '1', '2', '2', '8', '3', '3', '2', '2', '1', '8', '1', '3', '1', '9', '2', '2', '1', '3', '2', '2', '3', '2', '2', '3', '2', '1', '1', '2', '1', '2', '7', '8', '2', '1', '12', '12', '3', '1', '9', '9', '7', '8', '7', '2', '7', '9', '2', '2', '2', '8', '9', '7', '9', '2', '8', '9', '9', '2', '9', '1', '1', '9', '8', '3', '3', '8', '2', '7', '8', '11', '2', '8', '9', '1', '8', '3', '1', '2', '1', '2', '2', '2', '2', '8', '3', '9', '2', '10', '9', '9', '2', '7', '7', '7', '1', '3', '8', '8', '1', '1', '8', '7', '1', '8', '8', '9', '3', '3', '3', '2', '4', '8', '12', '8', '1', '12', '9', '2', '8', '2', '12', '2', '1', '3', '8', '12', '12', '11', '8', '10', '9', '2', '9', '4', '10', '1', '12', '12', '9', '3', '8', '9', '12', '9', '9', '2', '2', '2', '3', '5', '1', '1', '1', '7', '7', '8', '2', '2', '8', '2', '1', '7', '3', '2', '9', '8', '9', '1', '1', '9', '8', '9', '3', '3', '2', '9', '4', '9', '8', '8', '1', '8', '10', '2', '0', '3', '8', '8', '8', '9', '12', '9', '8', '2', '2', '8', '10', '9', '8', '2', '2', '4', '1', '9', '9', '8', '8', '10', '3', '1', '1', '2', '8', '7', '8', '2', '1', '1', '8', '12', '2', '9', '4', '8', '3', '3', '3', '8', '2', '12', '12', '8', '9', '12', '8', '2', '12', '7', '9', '2', '2', '1', '3', '1', '3', '11', '9', '8', '1', '10', '2', '12', '3', '8', '4', '1', '2', '8', '8', '3', '9', '3', '8', '7', '1', '1', '3', '1', '8', '2', '9', '8', '12', '1', '1', '1', '9', '9', '1', '8', '8', '8', '2', '2', '12', '3', '1', '1', '1', '9', '9', '3', '2', '1', '1', '9', '9', '1', '9', '9', '1', '9', '3', '9', '1', '8', '1', '9', '8', '3', '8', '8', '9', '9', '8', '2', '8', '1', '2', '2', '10', '1', '2', '1', '1', '9', '1', '9', '3', '7', '7', '2', '10', '3', '3', '1', '1', '9', '1', '2', '1', '3', '2', '1', '1', '1', '1', '1', '1', '1', '1', '1', '2', '8', '8', '7', '1', '1', '8', '8', '2', '1', '12', '1', '8', '1', '9', '8', '8', '2', '1', '9', '9', '2', '2', '8', '1', '7', '7', '9', '8', '1', '6', '8', '8', '2', '2', '8', '3', '8', '3', '2', '3', '8', '1', '2', '2', '2', '8', '9', '9', '9', '12', '1', '9', '2', '1', '9', '2', '9', '9', '10', '8', '1', '1', '9', '2', '1', '7', '1', '3', '7', '1', '1', '1', '3', '8', '8', '8', '9', '9', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '1', '0', '1', '1', '1', '2', '2', '3', '1', '1', '2', '3', '3', '3', '7', '3', '1', '9', '3', '9', '1', '4', '1', '1', '9', '10', '9', '8', '2', '3', '10', '9', '3', '10', '12', '8', '9', '8', '2', '1', '9', '8', '1', '7', '8', '1', '9', '1', '2', '10', '7', '7', '8', '1', '3', '3', '9', '3', '3', '1', '1', '1', '9', '1', '3', '8', '8', '8', '1', '8', '1', '9', '7', '8', '8', '2', '1', '3', '8', '2', '1', '6', '1', '8', '8', '2', '8', '2', '2', '9', '8', '1', '1', '6', '8', '6', '10', '2', '1', '1', '8', '9', '3', '3', '3', '1', '3', '9', '6', '8', '1', '8', '3', '1', '2', '2', '12', '7', '1', '1', '1', '1', '12', '1', '8', '8', '3', '2', '1', '9', '2', '3', '3', '2', '2', '7', '2', '2', '2', '3', '1', '9', '8', '7', '1', '1', '3', '1', '9', '4', '3', '9', '2', '1', '1', '9', '8', '8', '7', '2', '1', '3', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '2', '10', '1', '1', '8', '2', '8', '1', '8', '1', '3', '1', '2', '1', '1', '3', '1', '2', '1', '1', '1', '1', '1', '1', '1', '2', '1', '8', '1', '9', '8', '8', '1', '10', '8', '8', '1', '3', '9', '8', '9', '8', '2', '1', '3', '4', '1', '12', '1', '9', '8', '2', '7', '2', '1', '2', '9', '2', '1', '3', '8', '1', '2', '3', '8', '2', '10', '10', '10', '9', '9', '9', '3', '8', '3', '1', '1', '1', '1', '1', '1', '2', '1', '9', '2', '7', '9', '8', '7', '9', '1', '1', '1', '1', '8', '2', '2', '3', '3', '2', '12', '8', '3', '1', '1', '12', '9', '8', '2', '2', '9', '1', '2', '2', '9', '1', '1', '2', '4', '8', '1', '8', '10', '2', '9', '9', '9', '1', '2', '1', '1', '2', '2', '2', '12', '1', '2', '8', '3', '9', '9', '3', '2', '9', '9', '7', '2', '8', '8', '9', '2', '9', '1', '1', '1', '1', '1', '2', '8', '2', '1', '2', '2', '8', '3', '1', '1', '9', '1', '3', '10', '7', '9', '8', '8', '2', '2', '9', '2', '2', '2', '2', '2', '2', '9', '1', '3', '6', '4', '2', '2', '9', '1', '9', '8', '1', '1', '1', '7', '3', '2', '9', '9', '1', '9', '7', '2', '8', '9', '9', '12', '10', '8', '10', '2', '3', '1', '2', '2', '12', '12', '3', '12', '1', '2', '1', '2', '1', '2', '8', '2', '3', '12', '12', '8', '2', '2', '9', '2', '9', '2', '3', '3', '1', '1', '1', '12', '3', '7', '1', '3', '1', '2', '9', '2', '7', '2', '1', '8', '3', '7', '8', '1', '1', '3', '8', '3', '2', '9', '3', '1', '1', '1', '1', '11', '1', '2', '6', '1', '2', '3', '8', '9', '9', '2', '9', '2', '1', '1', '3', '1', '12', '9', '1', '1', '9', '1', '9', '9', '8', '9', '8', '1', '2', '7', '1', '0', '9', '8', '2', '3', '2', '4', '1', '1', '1', '8', '1', '12', '1', '2', '3', '3', '9', '9', '9', '9', '9', '7', '9', '3', '9', '1', '7', '3', '3', '3', '3', '7', '8', '2', '2', '2', '8', '2', '3', '9', '9', '3', '1', '8', '9', '12', '8', '8', '3', '2', '1', '6', '1', '9', '1', '1', '1', '1', '1', '1', '2', '2', '1', '2', '2', '2', '2', '9', '2', '5', '3', '2', '2', '8', '2', '1', '8', '3', '3', '8', '3', '1', '3', '9', '2', '8', '9', '4', '4', '9', '8', '8', '7', '7', '3', '1', '8', '8', '3', '2', '2', '10', '3', '3', '2', '2', '2', '2', '12', '3', '1', '2', '7', '7', '12', '2', '1', '2', '1', '1', '2', '1', '3', '3', '3', '1', '3', '9', '4', '8', '8', '4', '2', '4', '2', '8', '9', '1', '1', '2', '2', '2', '8', '7', '2', '8', '2', '9', '1', '8', '1', '1', '2', '1', '1', '1', '2', '9', '9', '8', '1', '8', '1', '4', '8', '1', '2', '7', '9', '2', '1', '8', '9', '8', '9', '1', '1', '2', '3', '2', '12', '7', '1', '2', '4', '4', '4', '2', '2', '2', '9', '8', '8', '1', '12', '3', '3', '2', '2', '3', '12', '1', '5', '1', '1', '3', '2', '8', '8', '2', '2', '2', '3', '9', '8', '8', '9', '8', '8', '11', '8', '2', '9', '2', '3', '1', '7', '1', '8', '2', '8', '1', '1', '1', '8', '1', '1', '1', '7', '8', '8', '2', '3', '2', '1', '8', '8', '1', '8', '4', '8', '9', '8', '9', '8', '1', '2', '3', '2', '1', '2', '2', '3', '1', '9', '4', '8', '1', '1', '9', '1', '1', '8', '2', '2', '2', '1', '7', '3', '3', '1', '3', '3', '12', '3', '1', '1', '3', '1', '1', '4', '8', '4', '4', '3', '3', '3', '4', '8', '1', '9', '9', '9', '4', '1', '7', '4', '8', '4', '9', '8', '1', '1', '3', '8', '9', '1', '9', '7', '1', '1', '9', '1', '8', '8', '8', '1', '1', '8', '2', '2', '2', '9', '4', '4', '1', '2', '1', '2', '9', '1', '1', '8', '5', '5', '5', '9', '12', '4', '1', '2', '1', '8', '8', '1', '9', '9', '2', '12', '1', '1', '7', '8', '8', '4', '1', '2', '8', '11', '2', '3', '2', '2', '1', '11', '2', '2', '2', '2', '9', '9', '2', '2', '9', '3', '1', '9', '1', '1', '1', '7', '8', '1', '8', '9', '1', '2', '9', '8', '1', '2', '9', '8', '12', '2', '2', '10', '8', '2', '2', '8', '8', '1', '2', '6', '1', '1', '7', '7', '2', '3', '2', '1', '1', '8', '1', '10', '8', '12', '6', '4', '1', '4', '1', '2', '9', '8', '9', '4', '8', '8', '10', '8', '8', '8', '8', '8', '2', '2', '8', '4', '9', '3', '3', '3', '3', '3', '9', '2', '1', '1', '2', '3', '2', '1', '2', '3', '3', '3', '2', '2', '3', '3', '2', '1', '2', '3', '3', '3', '3', '12', '9', '3', '2', '2', '3', '1', '5', '1', '1', '3', '2', '2', '1', '2', '9', '3', '9', '8', '1', '8', '7', '7', '9', '1', '1', '7', '9', '10', '1', '7', '1', '4', '3', '8', '9', '4', '9', '9', '2', '9', '2', '8', '4', '2', '2', '9', '5', '2', '1', '10', '9', '3', '1', '1', '1', '2', '1', '2', '9', '1', '7', '9', '9', '8', '2', '2', '8', '9', '1', '9', '8', '2', '3', '2', '2', '1', '2', '2', '2', '1', '1', '8', '1', '9', '4', '12', '8', '2', '7', '2', '2', '1', '9', '3', '1', '8', '12', '1', '1', '2', '1', '10', '2', '6', '12', '12', '4', '9', '1', '9', '10', '1', '2', '8', '2', '1', '8', '6', '12', '9', '9', '2', '1', '7', '8', '9', '5', '8', '1', '8', '8', '3', '1', '8', '2', '8', '2', '2', '1', '2', '9', '2', '8', '1', '12', '8', '2', '7', '3', '3', '12', '2', '3', '3', '9', '9', '3', '3', '3', '3', '3', '2', '1', '4', '1', '4', '8', '8', '2', '1', '2', '9', '12', '1', '10', '12', '1', '2', '9', '4', '8', '10', '3', '2', '8', '7', '4', '8', '9', '2', '7', '1', '10', '2', '2', '2', '8', '4', '2', '2', '1', '1', '8', '7', '9', '2', '1', '9', '8', '9', '12', '4', '8', '2', '8', '7', '7', '1', '9', '2', '1', '1', '9', '1', '8', '2', '3', '9', '9', '1', '3', '2', '9', '2', '2', '8', '1', '1', '2', '1', '2', '1', '9', '8', '2', '2', '4', '8', '8', '9', '5', '5', '9', '8', '9', '6', '2', '1', '12', '2', '3', '8', '8', '9', '2', '2', '1', '3', '2', '10', '9', '10', '7', '2', '8', '4', '2', '8', '9', '8', '1', '3', '8', '8', '1', '8', '3', '8', '8', '1', '10', '3', '1', '5', '8', '8', '9', '2', '2', '9', '2', '1', '9', '1', '4', '1', '12', '8', '8', '2', '1', '1', '1', '1', '1', '9', '2', '9', '8', '9', '5', '9', '1', '8', '2', '4', '1', '1', '1', '9', '12', '4', '1', '1', '9', '8', '4', '1', '8', '9', '8', '8', '9', '9', '9', '9', '9', '9', '1', '3', '4', '2', '2', '8', '2', '9', '2', '3', '8', '3', '1', '7', '2', '1', '1', '4', '2', '9', '2', '1', '3', '2', '9', '2', '9', '3', '8', '1', '9', '1', '1', '2', '8', '8', '8', '2', '3', '9', '9', '8', '2', '4', '1', '5', '1', '1', '9', '9', '2', '1', '1', '1', '9', '9', '1', '3', '9', '2', '2', '9', '9', '8', '3', '9', '9', '8', '2', '2', '8', '9', '2', '2', '8', '9', '10', '10', '2', '8', '7', '9', '9', '4', '2', '4', '11', '2', '1', '7', '8', '8', '8', '9', '4', '9', '8', '8', '2', '1', '1', '9', '8', '3', '3', '8', '8', '7', '9', '1', '3', '4', '8', '10', '2', '4', '4', '8', '2', '8', '2', '8', '9', '2', '1', '2', '2', '3', '1', '9', '4', '5', '9', '9', '1', '1', '1', '1', '1', '1', '7', '1', '1', '1', '8', '1', '1', '1', '1', '1', '9', '1', '2', '1', '12', '9', '3', '9', '9', '4', '1', '9', '1', '1', '2', '11', '9', '3', '2', '2', '2', '4', '2', '3', '2', '7', '7', '8', '1', '10', '9', '5', '7', '1', '10', '8', '1', '2', '7', '1', '12', '5', '8', '4', '2', '9', '9', '8', '2', '9', '8', '8', '10', '2', '3', '2', '7', '2', '8', '10', '3', '2', '9', '9', '9', '1', '1', '1', '1', '8', '2', '10', '9', '8', '1', '2', '2', '2', '1', '2', '1', '3', '9', '12', '2', '2', '4', '8', '1', '3', '9', '4', '9', '1', '9', '9', '2', '1', '1', '9', '7', '10', '3', '10', '2', '9', '2', '9', '2', '8', '2', '1', '2', '3', '1', '1', '1', '8', '5', '8', '3', '1', '1', '4', '2', '2', '1', '3', '8', '2', '1', '1', '1', '2', '9', '4', '1', '8', '1', '2', '8', '2', '9', '2', '2', '5', '5', '2', '5', '2', '1', '8', '8', '1', '3', '12', '2', '4', '3', '2', '2', '2', '6', '2', '3', '3', '1', '8', '2', '2', '1', '1', '8', '8', '10', '1', '9', '2', '9', '12', '0', '2', '1', '3', '2', '2', '1', '2', '1', '9', '9', '2', '9', '8', '8', '1', '7', '8', '1', '2', '6', '9', '8', '10', '1', '12', '2', '2', '1', '8', '2', '1', '12', '7', '4', '2', '10', '2', '2', '1', '2', '1', '2', '1', '1', '2', '1', '9', '8', '10', '2', '10', '9', '9', '12', '5', '3', '2', '8', '2', '8', '3', '3', '1', '4', '9', '10', '2', '4', '8', '1', '2', '10', '12', '12', '3', '1', '1', '2', '9', '6', '6', '1', '9', '1', '8', '1', '8', '8', '8', '8', '2', '2', '2', '8', '8', '2', '1', '1', '1', '1', '1', '8', '8', '5', '8', '8', '1', '8', '9', '9', '1', '7', '12', '9', '9', '7', '2', '2', '7', '8', '9', '8', '2', '9', '6', '3', '4', '8', '4', '1', '2', '9', '8', '8', '8', '2', '1', '1', '8', '8', '3', '1', '9', '9', '7', '1', '2', '2', '2', '9', '4', '1', '2', '12', '1', '8', '2', '1', '9', '1', '8', '9', '9', '2', '5', '4', '2', '1', '2', '8', '12', '8', '10', '3', '8', '8', '8', '2', '2', '8', '2', '8', '2', '4', '8', '10', '2', '7', '2', '1', '8', '8', '9', '5', '8', '1', '12', '7', '10', '2', '2', '2', '1', '4', '8', '12', '1', '8', '2', '2', '9', '1', '8', '3', '1', '1', '5', '10', '2', '1', '2', '4', '8', '9', '3', '2', '8', '7', '4', '4', '9', '9', '2', '8', '1', '1', '2', '4', '7', '1', '9', '1', '1', '1', '8', '4', '1', '3', '1', '2', '2', '9', '1', '8', '8', '2', '2', '9', '9', '1', '8', '8', '1', '9', '8', '8', '2', '2', '2', '2', '1', '1', '1', '7', '4', '1', '1', '4', '2', '3', '3', '2', '3', '9', '2', '8', '2', '1', '5', '1', '2', '2', '1', '3', '5', '2', '9', '1', '1', '9', '8', '1', '4', '8', '1', '8', '2', '4', '8', '4', '7', '9', '8', '1', '2', '2', '4', '4', '11', '3', '11', '1', '3', '8', '1', '9', '6', '8', '9', '3', '9', '8', '9', '2', '3', '3', '7', '1', '7', '2', '9', '9', '1', '8', '8', '1', '5', '5', '8', '8', '1', '2', '1', '3', '9', '9', '5', '9', '8', '1', '4', '3', '8', '1', '1', '2', '1', '8', '9', '8', '2', '2', '2', '1', '2', '4', '10', '8', '1', '9', '1', '8', '1', '1', '9', '9', '8', '11', '1', '8', '2', '1', '2', '8', '2', '8', '8', '2', '9', '6', '10', '1', '8', '6', '4', '5', '11', '2', '2', '8', '1', '8', '1', '8', '1', '4', '8', '1', '1', '1', '8', '3', '7', '3', '3', '7', '1', '1', '4', '8', '1', '6', '4', '2', '2', '1', '1', '1', '1', '6', '2', '2', '8', '1', '8', '5', '5', '2', '1', '1', '2', '3', '7', '1', '1', '6', '2', '9', '3', '9', '8', '9', '2', '1', '4', '2', '9', '13', '9', '2', '8', '12', '4', '1', '1', '9', '5', '1', '1', '1', '1', '8', '2', '1', '1', '2', '9', '0', '12', '1', '1', '8', '9', '8', '3', '2', '2', '12', '8', '2', '2', '2', '1', '10', '1', '9', '8', '9', '5', '2', '2', '9', '2', '8', '4', '8', '3', '7', '9', '2', '2', '7', '2', '2', '8', '1', '1', '9', '10', '9', '8', '2', '9', '0', '7', '1', '1', '1', '9', '8', '2', '8', '12', '8', '3', '2', '8', '8', '7', '7', '8', '12', '2', '1', '3', '9', '2', '2', '8', '2', '8', '8', '12', '8', '8', '1', '8', '9', '2', '4', '4', '7', '2', '1', '1', '9', '3', '7', '1', '3', '9', '8', '2', '2', '3', '1', '3', '10', '2', '3', '7', '1', '8', '1', '1', '10', '3', '10', '8', '4', '1', '2', '4', '3', '10', '9', '3', '9', '5', '8', '7', '2', '9', '3', '3', '2', '11', '8', '1', '1', '7', '2', '7', '2', '9', '4', '8', '1', '12', '8', '7', '9', '1', '0', '2', '6', '4', '9', '3', '2', '4', '12', '9', '8', '5', '9', '2', '8', '4', '6', '8', '9', '3', '1', '8', '7', '3', '1', '1', '1', '8', '9', '1', '8', '7', '9', '8', '2', '1', '2', '2', '3', '2', '2', '1', '1', '4', '9', '7', '3', '2', '8', '8', '9', '1', '9', '7', '4', '2', '2', '8', '5', '9', '1', '5', '7', '1', '4', '6', '9', '10', '1', '8', '2', '9', '1', '9', '2', '9', '8', '2', '9', '9', '11', '2', '12', '2', '2', '9', '4', '8', '1', '9', '9', '2', '8', '9', '1', '2', '9', '3', '8', '8', '1', '7', '8', '8', '2', '3', '2', '2', '2', '2', '9', '8', '10', '2', '2', '1', '9', '11', '1', '8', '8', '2', '4', '2', '1', '2', '8', '8', '9', '2', '8', '9', '2', '3', '1', '2', '1', '3', '10', '8', '1', '2', '4', '1', '9', '8', '9', '3', '3', '9', '8', '2', '11', '3', '7', '1', '8', '7', '1', '8', '8', '8', '1', '4', '8', '8', '1', '9', '3', '1', '1', '2', '8', '8', '8', '2', '1', '8', '10', '2', '12', '8', '8', '9', '1', '8', '1', '3', '2', '2', '10', '7', '2', '2', '1', '1', '8', '2', '9', '1', '2', '9', '3', '3', '10', '1', '4', '7', '10', '12', '2', '4', '10', '8', '6', '2', '9', '1', '8', '3', '1', '2', '8', '8', '2', '12', '10', '2', '10', '9', '1', '2', '5', '8', '12', '8', '8', '7', '8', '8', '1', '8', '2', '2', '9', '1', '8', '10', '5', '5', '5', '8', '1', '9', '1', '13', '1', '2', '2', '1', '2', '4', '9', '2', '1', '9', '13', '11', '1', '9', '9', '3', '10', '8', '7', '8', '8', '4', '2', '1', '9', '4', '3', '1', '1', '10', '9', '2', '8', '6', '2', '1', '1', '3', '2', '8', '1', '1', '1', '1', '2', '4', '1', '1', '10', '1', '8', '1', '1', '9', '1', '7', '1', '1', '9', '9', '2', '12', '9', '1', '8', '1', '12', '1', '8', '3', '8', '1', '8', '1', '2', '10', '1', '1', '2', '2', '9', '3', '9', '12', '1', '9', '1', '2', '8', '7', '2', '1', '1', '3', '2', '8', '4', '4', '5', '12', '7', '8', '9', '6', '1', '8', '9', '2', '1', '9', '1', '4', '1', '8', '2', '1', '1', '8', '7', '4', '9', '1', '8', '4', '2', '7', '5', '1', '3', '9', '2', '8', '2', '1', '2', '9', '1', '1', '2', '3', '4', '3', '1', '2', '2', '10', '2', '2', '8', '1', '1', '2', '4', '9', '2', '11', '9', '8', '9', '2', '1', '10', '4', '1', '1', '10', '7', '2', '10', '8', '1', '10', '8', '8', '8', '8', '8', '8', '2', '2', '2', '8', '3', '4', '1', '1', '1', '2', '1', '8', '8', '2', '8', '3', '8', '3', '1', '2', '3', '1', '1', '10', '1', '4', '4', '3', '3', '1', '9', '1', '1', '2', '1', '1', '6', '1', '1', '1', '9', '1', '4', '1', '1', '1', '1', '1', '1', '2', '12', '8', '8', '1', '9', '12', '10', '8', '2', '12', '1', '1', '2', '1', '7', '10', '7', '1', '2', '1', '10', '8', '2', '2', '8', '2', '2', '1', '9', '8', '8', '8', '8', '3', '1', '4', '2', '8', '3', '10', '10', '1', '9', '1', '9', '1', '9', '1', '8', '3', '1', '1', '8', '8', '8', '6', '3', '4', '1', '1', '1', '9', '9', '5', '2', '10', '2', '6', '2', '2', '7', '1', '8', '2', '11', '1', '1', '1', '2', '1', '9', '4', '4', '2', '8', '4', '6', '8', '8', '10', '10', '2', '1', '2', '9', '1', '8', '8', '3', '4', '8', '8', '2', '8', '8', '1', '2', '1', '2', '9', '4', '1', '3', '3', '3', '8', '8', '8', '2', '6', '4', '3', '3', '12', '3', '4', '9', '6', '8', '7', '7', '8', '6', '4', '1', '10', '6', '3', '3', '8', '1', '1', '8', '1', '11', '8', '1', '1', '2', '3', '11', '7', '2', '1', '1', '1', '9', '1', '2', '1', '1', '9', '8', '1', '1', '8', '0', '4', '1', '1', '2', '1', '1', '2', '2', '9', '1', '10', '1', '3', '8', '9', '3', '3', '1', '10', '1', '9', '1', '1', '7', '4', '1', '8', '10', '1', '2', '10', '1', '1', '2', '3', '3', '9', '1', '8', '10', '1', '7', '9', '1', '9', '6', '3', '9', '2', '8', '4', '12', '12', '3', '1', '1', '9', '1', '9', '2', '1', '1', '1', '1', '1', '9', '2', '1', '2', '10', '9', '8', '8', '10', '8', '2', '3', '2', '1', '12', '8', '12', '2', '9', '1', '3', '2', '5', '2', '2', '8', '2', '2', '4', '10', '9', '10', '9', '4', '2', '4', '12', '8', '2', '1', '8', '3', '9', '2', '9', '4', '1', '1', '1', '1', '1', '9', '6', '2', '9', '2', '1', '3', '2', '2', '5', '8', '9', '1', '6', '3', '1', '2', '2', '2', '6', '1', '8', '10', '8', '3', '3', '13', '2', '9', '9', '10', '6', '12', '8', '2', '9', '2', '3', '8', '8', '1', '2', '2', '3', '2', '10', '1', '8', '9', '1', '8', '9', '8', '8', '2', '10', '2', '1', '1', '1', '9', '9', '12', '1', '1', '4', '8', '3', '2', '2', '4', '4', '2', '2', '1', '1', '1', '9', '2', '2', '4', '1', '1', '8', '8', '8', '9', '2', '13', '9', '9', '4', '1', '9', '7', '10', '10', '8', '1', '9', '1', '1', '1', '8', '12', '4', '1', '2', '1', '1', '1', '7', '8', '3', '3', '1', '2', '2', '7', '2', '2', '10', '1', '8', '4', '0', '1', '9', '4', '1', '1', '2', '10', '10', '7', '9', '3', '12', '11', '2', '8', '10', '8', '9', '4', '3', '4', '10', '1', '1', '9', '3', '10', '1', '5', '3', '3', '2', '2', '1', '1', '6', '1', '7', '1', '1', '12', '1', '1', '1', '10', '1', '8', '8', '1', '3', '8', '3', '1', '2', '8', '3', '4', '2', '8', '8', '6', '1', '1', '4', '1', '2', '9', '5', '1', '7', '8', '9', '2', '4', '9', '8', '5', '9', '9', '9', '1', '2', '1', '3', '8', '9', '2', '9', '8', '9', '1', '8', '13', '9', '3', '8', '1', '4', '1', '3', '1', '6', '4', '10', '2', '2', '7', '8', '8', '12', '8', '9', '9', '9', '10', '3', '12', '12', '2', '9', '10', '9', '7', '2', '3', '4', '10', '10', '2', '7', '7', '1', '7', '3', '9', '1', '9', '8', '2', '9', '1', '8', '1', '10', '8', '8', '2', '7', '8', '1', '8', '9', '8', '1', '4', '1', '8', '8', '8', '1', '1', '10', '3', '2', '2', '6', '8', '9', '8', '2', '1', '1', '3', '8', '8', '1', '4', '4', '3', '9', '10', '4', '2', '4', '2', '9', '2', '2', '9', '1', '1', '10', '1', '1', '2', '1', '1', '1', '9', '1', '8', '8', '8', '2', '3', '8', '1', '6', '7', '3', '1', '9', '1', '2', '1', '6', '1', '2', '10', '1', '4', '2', '3', '2', '9', '9', '4', '9', '9', '1', '2', '8', '4', '7', '8', '7', '13', '9', '1', '2', '1', '9', '3', '3', '6', '8', '9', '10', '9', '2', '2', '3', '12', '1', '1', '1', '13', '2', '1', '7', '10', '8', '9', '1', '2', '9', '1', '1', '1', '2', '1', '12', '5', '6', '9', '10', '3', '1', '10', '1', '3', '2', '8', '9', '2', '10', '8', '8', '9', '2', '8', '9', '2', '2', '2', '8', '8', '2', '3', '1', '4', '8', '1', '9', '9', '2', '8', '10', '9', '1', '2', '12', '8', '8', '0', '2', '1', '1', '9', '10', '1', '6', '2', '2', '10', '2', '2', '4', '2', '2', '12', '1', '6', '7', '8', '9', '1', '3', '5', '7', '3', '1', '3', '1', '1', '3', '2', '6', '3', '2', '1', '8', '3', '5', '2', '1', '1', '8', '1', '1', '0', '2', '3', '5', '3', '1', '1', '9', '8', '8', '8', '8', '4', '7', '2', '9', '12', '5', '9', '3', '12', '3', '9', '1', '9', '1', '7', '3', '9', '3', '10', '1', '2', '1', '4', '9', '8', '2', '9', '11', '2', '8', '9', '4', '1', '1', '1', '1', '1', '1', '1', '9', '1', '9', '1', '1', '1', '12', '3', '10', '3', '6', '1', '7', '10', '8', '3', '7', '1', '1', '2', '6', '1', '3', '9', '2', '8', '1', '8', '8', '1', '8', '10', '12', '1', '10', '12', '10', '1', '8', '8', '1', '4', '7', '9', '2', '1', '9', '3', '8', '12', '10', '6', '1', '6', '8', '3', '1', '3', '1', '10', '10', '9', '8', '1', '2', '9', '8', '8', '1', '9', '8', '8', '3', '1', '3', '9', '12', '1', '8', '1', '5', '8', '11', '5', '5', '1', '2', '1', '3', '1', '1', '1', '9', '8', '8', '1', '9', '10', '8', '4', '1', '1', '8', '10', '8', '9', '1', '10', '1', '7', '2', '1', '1', '9', '6', '2', '9', '8', '1', '1', '1', '9', '8', '8', '1', '9', '9', '1', '1', '2', '7', '9', '2', '9', '9', '8', '7', '6', '8', '7', '8', '2', '9', '2', '1', '8', '8', '8', '2', '6', '3', '1', '8', '12', '12', '8', '7', '1', '2', '2', '7', '1', '9', '8', '1', '4', '1', '1', '8', '5', '8', '1', '8', '8', '8', '8', '1', '2', '11', '1', '1', '1', '2', '7', '1', '9', '2', '2', '2', '9', '8', '9', '2', '9', '1', '8', '1', '7', '11', '13', '1', '9', '8', '2', '2', '1', '2', '3', '10', '3', '10', '4', '3', '9', '1', '8', '1', '1', '13', '1', '1', '3', '8', '1', '2', '2', '2', '9', '4', '1', '8', '1', '9', '3', '6', '8', '5', '2', '8', '2', '8', '1', '9', '2', '2', '8', '1', '1', '4', '8', '9', '2', '2', '7', '1', '8', '12', '2', '12', '8', '2', '9', '9', '3', '8', '1', '1', '9', '9', '9', '2', '8', '1', '9', '1', '8', '9', '8', '2', '2', '4', '1', '9', '10', '10', '8', '8', '1', '1', '2', '1', '4', '1', '2', '1', '1', '3', '12', '8', '8', '8', '8', '10', '2', '8', '12', '1', '9', '9', '8', '9', '1', '2', '1', '8', '8', '8', '2', '8', '8', '3', '10', '8', '9', '1', '1', '1', '3', '4', '8', '10', '6', '3', '1', '3', '2', '8', '9', '3', '5', '4', '1', '5', '2', '9', '9', '9', '1', '1', '1', '9', '9', '6', '10', '9', '8', '12', '2', '9', '1', '2', '1', '1', '1', '8', '1', '12', '1', '12', '1', '9', '1', '8', '1', '2', '1', '2', '8', '8', '10', '1', '9', '1', '2', '2', '8', '8', '3', '12', '1', '11', '2', '10', '12', '11', '2', '1', '10', '1', '1', '9', '3', '10', '2', '7', '1', '8', '1', '9', '9', '2', '1', '3', '12', '5', '9', '9', '8', '1', '8', '8', '8', '1', '1', '3', '9', '8', '9', '8', '3', '10', '3', '7', '8', '9', '2', '3', '1', '9', '10', '9', '2', '8', '2', '6', '4', '1', '1', '3', '8', '2', '3', '9', '1', '2', '1', '8', '1', '2', '2', '2', '2', '9', '9', '4', '8', '9', '1', '1', '2', '9', '1', '5', '2', '9', '8', '10', '1', '2', '10', '8', '5', '6', '4', '8', '1', '1', '1', '8', '9', '2', '8', '8', '8', '12', '2', '2', '1', '8', '1', '2', '1', '9', '8', '8', '9', '1', '1', '8', '7', '1', '8', '0', '2', '3', '8', '1', '1', '8', '1', '1', '12', '3', '2', '10', '8', '10', '6', '1', '8', '8', '10', '8', '8', '1', '4', '8', '1', '2', '9', '8', '1', '1', '3', '3', '5', '1', '1', '2', '1', '2', '1', '9', '8', '3', '9', '8', '2', '10', '9', '1', '8', '1', '10', '9', '10', '1', '2', '3', '8', '1', '3', '1', '9', '9', '8', '1', '9', '8', '8', '9', '1', '9', '2', '8', '8', '2', '13', '8', '10', '8', '9', '3', '2', '1', '3', '1', '12', '10', '10', '1', '11', '1', '2', '4', '2', '10', '1', '8', '11', '1', '1', '1', '2', '2', '8', '2', '9', '8', '9', '9', '12', '8', '1', '9', '2', '4', '1', '8', '3', '9', '1', '6', '5', '8', '9', '3', '3', '2', '1', '1', '1', '8', '11', '1', '8', '7', '1', '11', '9', '1', '7', '9', '9', '8', '8', '8', '12', '8', '1', '2', '9', '8', '8', '2', '10', '9', '8', '8', '4', '10', '9', '8', '2', '10', '10', '2', '1', '2', '1', '2', '1', '4', '1', '7', '1', '1', '3', '12', '9', '8', '5', '9', '10', '1', '9', '9', '2', '7', '4', '1', '1', '8', '12', '8', '2', '2', '9', '4', '1', '1', '5', '9', '1', '8', '12', '8', '1', '1', '4', '9', '10', '2', '3', '1', '3', '3', '3', '8', '1', '2', '1', '2', '1', '2', '9', '1', '2', '8', '9', '4', '8', '8', '10', '2', '12', '5', '5', '1', '8', '1', '7', '8', '1', '1', '8', '9', '12', '4', '9', '3', '1', '2', '5', '9', '2', '1', '2', '2', '1', '2', '8', '1', '8', '8', '2', '2', '1', '8', '4', '1', '2', '10', '9', '10', '8', '1', '10', '4', '8', '8', '10', '2', '8', '1', '1', '3', '10', '1', '4', '2', '8', '3', '3', '2', '9', '8', '5', '5', '9', '3', '10', '8', '1', '2', '1', '2', '9', '1', '1', '2', '9', '9', '8', '8', '11', '9', '4', '1', '8', '2', '7', '1', '2', '8', '2', '9', '9', '8', '2', '9', '9', '8', '8', '1', '2', '8', '1', '1', '2', '9', '8', '1', '4', '9', '1', '8', '12', '1', '12', '2', '8', '2', '9', '1', '12', '1', '2', '9', '1', '1', '9', '3', '2', '9', '9', '2', '11', '8', '7', '9', '9', '9', '8', '8', '1', '2', '8', '1', '1', '4', '1', '2', '10', '1', '9', '2', '1', '1', '1', '9', '1', '13', '4', '3', '2', '1', '1', '2', '2', '7', '8', '7', '1', '1', '2', '8', '8', '3', '4', '8', '2', '10', '13', '8', '8', '9', '2', '2', '1', '1', '10', '4', '2', '7', '9', '8', '2', '1', '9', '8', '1', '1', '9', '8', '9', '8', '2', '8', '2', '2', '9', '2', '1', '9', '1', '2', '4', '8', '2', '1', '1', '8', '1', '4', '1', '9', '3', '9', '2', '7', '1', '9', '6', '1', '10', '2', '2', '2', '2', '2', '10', '10', '10', '9', '9', '9', '3', '12', '10', '1', '1', '10', '1', '1', '2', '2', '1', '2', '3', '12', '8', '8', '1', '2', '9', '9', '1', '1', '10', '8', '9', '8', '3', '1', '1', '3', '1', '10', '1', '1', '1', '4', '1', '1', '1', '7', '10', '2', '1', '8', '1', '9', '3', '1', '10', '8', '8', '1', '5', '9', '1', '2', '1', '8', '8', '1', '3', '1', '10', '2', '1', '1', '2', '8', '2', '5', '3', '5', '2', '1', '1', '9', '1', '7', '9', '9', '2', '8', '1', '12', '4', '1', '4', '9', '9', '4', '8', '1', '10', '10', '8', '8', '8', '9', '2', '3', '5', '1', '8', '1', '1', '1', '7', '10', '1', '4', '12', '3', '2', '2', '9', '1', '1', '1', '2', '8', '4', '1', '7', '3', '12', '8', '6', '2', '2', '7', '9', '11', '1', '10', '9', '1', '8', '12', '2', '11', '1', '2', '3', '1', '1', '1', '8', '9', '2', '2', '2', '2', '3', '3', '8', '10', '4', '1', '2', '9', '8', '2', '2', '9', '1', '8', '11', '4', '2', '8', '7', '1', '1', '7', '3', '9', '9', '4', '5', '8', '10', '2', '9', '9', '9', '8', '1', '9', '2', '2', '2', '1', '2', '9', '4', '3', '12', '4', '3', '2', '3', '8', '3', '10', '1', '10', '1', '1', '8', '10', '6', '8', '9', '9', '1', '2', '9', '2', '9', '3', '8', '1', '1', '12', '1', '5', '1', '10', '8', '10', '2', '7', '1', '1', '1', '1', '3', '2', '3', '5', '9', '1', '2', '1', '9', '8', '10', '9', '8', '1', '8', '1', '8', '2', '8', '1', '2', '8', '1', '2', '5', '8', '12', '2', '1', '1', '1', '1', '1', '8', '8', '2', '1', '4', '2', '8', '8', '8', '2', '9', '9', '11', '1', '2', '9', '3', '1', '1', '8', '9', '2', '4', '10', '1', '9', '8', '4', '9', '8', '10', '8', '8', '9', '2', '8', '9', '3', '3', '3', '4', '3', '2', '2', '10', '8', '2', '1', '5', '1', '9', '1', '1', '2', '1', '2', '3', '1', '1', '8', '8', '1', '10', '8', '8', '1', '6', '4', '8', '2', '5', '8', '1', '3', '8', '2', '8', '9', '1', '1', '8', '12', '10', '1', '5', '8', '2', '8', '10', '9', '9', '1', '6', '9', '1', '10', '8', '2', '9', '10', '1', '2', '1', '1', '8', '8', '3', '9', '8', '1', '10', '2', '8', '1', '10', '1', '5', '1', '1', '1', '5', '1', '1', '1', '4', '1', '1', '1', '8', '2', '8', '1', '8', '3', '8', '9', '1', '8', '1', '8', '1', '1', '2', '2', '1', '12', '9', '9', '1', '8', '2', '1', '2', '8', '1', '9', '1', '1', '2', '1', '2', '2', '2', '1', '9', '9', '8', '8', '1', '1', '9', '10', '8', '4', '3', '3', '4', '9', '1', '3', '3', '10', '2', '2', '1', '8', '4', '1', '9', '4', '1', '1', '9', '8', '10', '4', '9', '9', '8', '2', '1', '3', '4', '9', '3', '8', '9', '1', '1', '2', '1', '7', '8', '2', '9', '1', '9', '2', '6', '9', '10', '8', '1', '1', '5', '9', '5', '1', '9', '10', '9', '3', '3', '8', '8', '2', '9', '2', '8', '2', '1', '8', '5', '8', '3', '10', '1', '1', '2', '1', '8', '1', '4', '3', '2', '9', '4', '9', '8', '8', '8', '1', '3', '9', '1', '9', '9', '1', '2', '1', '9', '8', '9', '8', '1', '1', '1', '4', '2', '2', '9', '9', '1', '1', '1', '3', '6', '1', '2', '3', '1', '4', '2', '9', '1', '9', '9', '1', '8', '2', '1', '9', '1', '8', '1', '8', '8', '2', '1', '9', '9', '6', '8', '8', '9', '8', '10', '8', '9', '5', '1', '1', '1', '1', '8', '1', '8', '12', '8', '1', '12', '9', '2', '8', '1', '1', '2', '1', '8', '6', '9', '9', '8', '1', '8', '9', '7', '7', '8', '9', '9', '1', '8', '3', '8', '1', '3', '3', '8', '9', '8', '2', '8', '1', '2', '9', '8', '1', '1', '1', '1', '9', '1', '9', '8', '12', '3', '8', '8', '1', '10', '10', '10', '2', '2', '1', '10', '2', '2', '10', '8', '12', '8', '1', '1', '1', '2', '1', '1', '3', '8', '1', '2', '2', '2', '1', '1', '6', '2', '8', '8', '8', '1', '1', '1', '9', '9', '2', '8', '10', '2', '8', '2', '1', '2', '9', '9', '2', '1', '1', '8', '8', '1', '3', '1', '8', '9', '1', '10', '9', '1', '1', '1', '1', '7', '2', '2', '1', '2', '8', '8', '1', '2', '1', '3', '2', '1', '8', '3', '9', '2', '10', '2', '10', '1', '2', '1', '1', '13', '9', '10', '8', '2', '2', '1', '1', '8', '2', '2', '1', '9', '1', '1', '8', '9', '9', '2', '9', '2', '2', '1', '1', '1', '8', '9', '4', '9', '9', '1', '8', '2', '1', '1', '8', '2', '2', '2', '8', '1', '1', '8', '2', '10', '2', '9', '9', '7', '1', '1', '2', '2', '8', '3', '1', '2', '1', '1', '8', '1', '1', '1', '9', '1', '1', '8', '8', '1', '9', '6', '1', '9', '1', '8', '1', '8', '8', '8', '1', '3', '8', '8', '2', '6', '3', '2', '1', '10', '1', '8', '2', '1', '6', '1', '1', '1', '8', '1', '10', '1', '1', '8', '9', '2', '6', '9', '2', '9', '4', '1', '8', '1', '3', '7', '8', '8', '5', '8', '7', '1', '1', '1', '13', '8', '3', '1', '1', '1', '1', '2', '8', '2', '12', '8', '1', '2', '5', '1', '8', '2', '10', '8', '2', '10', '1', '5', '2', '1', '1', '5', '2', '2', '7', '8', '2', '9', '9', '1', '10', '1', '8', '2', '5', '8', '1', '5', '1', '10', '1', '1', '9', '8', '6', '1', '8', '9', '8', '1', '8', '2', '2', '8', '3', '9', '1', '9', '3', '8', '4', '4', '3', '3', '1', '1', '1', '8', '9', '1', '1', '8', '3', '1', '2', '8', '2', '1', '1', '10', '1', '2', '2', '1', '1', '8', '2', '8', '2', '10', '2', '2', '10', '8', '8', '8', '13', '2', '2', '9', '2', '5', '9', '2', '1', '8', '8', '8', '1', '12', '12', '2', '9', '2', '1', '8', '2', '1', '8', '1', '9', '9', '2', '7', '1', '1', '7', '1', '4', '3', '1', '10', '3', '10', '9', '4', '6', '7', '5', '2', '1', '8', '8', '1', '4', '9', '2', '1', '8', '1', '1', '1', '1', '1', '8', '9', '8', '8', '8', '8', '9', '8', '10', '1', '8', '2', '7', '8', '9', '1', '2', '5', '9', '10', '8', '9', '12', '1', '6', '1', '2', '10', '9', '4', '1', '8', '10', '10', '1', '1', '1', '1', '1', '2', '3', '8', '1', '2', '10', '2', '1', '2', '4', '2', '2', '9', '1', '4', '2', '1', '1', '12', '12', '9', '9', '7', '1', '8', '4', '9', '9', '8', '1', '7', '3', '2', '1', '1', '4', '8', '1', '5', '8', '2', '8', '1', '14', '9', '12', '1', '10', '2', '1', '1', '8', '8', '6', '6', '3', '1', '2', '8', '1', '3', '2', '1', '8', '8', '1', '10', '8', '2', '8', '2', '5', '9', '1', '8', '8', '1', '1', '8', '8', '13', '1', '3', '8', '4', '1', '9', '7', '8', '8', '8', '3', '8', '1', '4', '2', '8', '8', '8', '7', '1', '1', '2', '8', '8', '9', '8', '8', '2', '3', '9', '9', '2', '1', '2', '2', '1', '8', '2', '6', '2', '1', '1', '9', '10', '3', '8', '2', '3', '2', '8', '8', '1', '8', '8', '8', '9', '1', '4', '1', '9', '2', '1', '8', '0', '1', '8', '1', '1', '8', '9', '9', '1', '3', '1', '2', '8', '8', '7', '1', '1', '2', '9', '2', '10', '2', '8', '2', '2', '2', '7', '1', '9', '8', '1', '3', '9', '2', '1', '10', '8', '1', '4', '1', '8', '5', '9', '8', '8', '1', '2', '2', '1', '8', '8', '6', '8', '1', '1', '8', '3', '2', '2', '1', '8', '1', '7', '6', '8', '1', '1', '1', '9', '1', '1', '1', '2', '1', '11', '10', '10', '11', '9']\n",
            "     filenum                                               text  label\n",
            "0          1  only had this sound-echo-time method been long...      8\n",
            "1          2  has no such implied limitation. In common unde...      1\n",
            "2          3  to apply its conclusion to Champlin. The contr...      8\n",
            "3          4  size of the reservation; in Congress by statut...      2\n",
            "4          5  to them.' Consequently, the Government cannot ...      8\n",
            "...      ...                                                ...    ...\n",
            "7327    8413  process of law secured by the Fourteenth Amend...      2\n",
            "7328    8414  other shootings and repeatedly expressed an in...      1\n",
            "7329    8417  U.S.C. (b), the United States in April asked l...     10\n",
            "7330    8418  AAA pursuant to agreement of the parties. That...     11\n",
            "7331    8419  Bd. v. Corvallis Sand & Gravel Co., . In the e...      9\n",
            "\n",
            "[7332 rows x 3 columns]\n",
            "0        8\n",
            "1        1\n",
            "2        8\n",
            "3        2\n",
            "4        8\n",
            "        ..\n",
            "7327     2\n",
            "7328     1\n",
            "7329    10\n",
            "7330    11\n",
            "7331     9\n",
            "Name: label, Length: 7332, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "fh = open('sc_model_1.txt')\n",
        "filenums=[]\n",
        "records=[]\n",
        "for record in fh:\n",
        "    record=record.split(\"---\")\n",
        "    filenum=record[0]\n",
        "    modelnum=record[1]\n",
        "    text=record[2]\n",
        "    filenums.append(filenum)\n",
        "    records.append(text)\n",
        "    \n",
        "print(len(filenums))\n",
        "print(len(records))\n",
        "\n",
        "summarized_data = pd.DataFrame(list(zip(filenums, records)),columns =['filenum','text'])\n",
        "len_list = [len(ele.split()) for ele in records]\n",
        "res = 0 if len(len_list) == 0 else (float(sum(len_list)) / len(len_list))\n",
        "print(\"Average Length %s\" % res)\n",
        "\n",
        "temp_file = open(\"labels_sc.txt\", \"r\")\n",
        "#temp_file = open(\"labels_sc_279.txt\", \"r\")\n",
        "\n",
        "data = temp_file.read()\n",
        "\n",
        "label_list = data.split(\"\\n\")\n",
        "print(label_list)\n",
        "label_list = label_list[0:-1]\n",
        "print(label_list)\n",
        "label_list = [int(i) for i in label_list]\n",
        "temp_file.close()\n",
        "\n",
        "\n",
        "new_label_list=[]\n",
        "for num in filenums:\n",
        "  new_label_list.append(label_list[int(num)-1])\n",
        "summarized_data['label'] = new_label_list\n",
        "\n",
        "print(summarized_data)\n",
        "print(summarized_data['label'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQbAYvIIvxo4",
        "outputId": "f0c1d412-6d7e-4876-d886-1245f0a57083"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_1[0][0]',                \n",
            "                                thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_bert_model[0][0]']          \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 15)           7695        ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 8  1  8 ... 10 11  9]\n",
            "[   1    2    3 ... 8417 8418 8419]\n",
            "[6622   35 6713 4989 8233 7444 5061 3942 8344  454 5464  317 2883 4515\n",
            " 3001 3410 6991 2875 1125 3885 6647 4857 4504  214 3230 8340 2969 3847\n",
            " 4609 5092 1346 5993 7358 2752 7651 7301 8188 7073 4782 6069 2346 6445\n",
            " 6483  542 1663 4952 5072 1967 4071 7590  860 6418 5030 7297  626 5463\n",
            " 6029 6302 3819 7256 2950 5667 1235 3993 1439 1323 4263 3835 2134 2624\n",
            " 5801 4293 8129  320 5866 3247 5777   19 6059 6520 4837 3843 5695 1666\n",
            "  858 1418 5409 5274 6205 3167 2356 8418 5997 6685  223 3444 3310 7721\n",
            " 5928 2360 7197 4252 8343  909 4093 4354 4752 1421  649  829 1916 1003\n",
            " 3488 2496 5765 6612 4004 4052 1403 6697 5268 1010 6293 6380 4602 6057\n",
            " 1851 4577 4236 7978 1350 5083 6552 1058  361 7706 4186 2590 4452 4396\n",
            "  749  870 8351 3834  812 5108  463 1743 6600 8264 8064 7121 8279 5556\n",
            " 1845 5119 2516 4673 7947 8215 1882 3350 7617 7152 5199 4282 1092 3342\n",
            " 8003 5025 5051  535  933 3969 1927 3615 1221 8062 1489 3296 7442 5459\n",
            " 2031 7351  234 2313 8015 7416 7921 6574 6257 8396  941 4520 3300 5640\n",
            "  240 7810 7425 1321 6580  928 1170 5404 8273 6742 7005   94 7459    9\n",
            " 7861 7975 1018 5029 2322 2140 1415 7061 7767 4990 8124 5927 6051 3269\n",
            " 6916 5959 2846 6818 7052 7775 7582 8116 4240 4767 5248  430 1812 2175\n",
            " 6055 8331 3916  998 7271 1564 7326 1077 1559  292 3065 6764 6854 4915\n",
            " 5552 3583 1917  868 5902 4229 2144 7565 3712 7224 4502 1406 1651 6256\n",
            " 3827 6525 6318 2686 3616 5369 8235 1704 1011 3351 8089 8021  102  224\n",
            " 5671 8294 1510 7707 2985 6624 5146 1428 7381 3709 6824 6308  701 2281\n",
            " 6837 5436 1741 3520 5995 2649 4886  966 2863 5476 4058 1246 2406 1199\n",
            " 8387  513 5449 1028 2226 6166 4387  350 6123  678 5236 5041  734 3412\n",
            " 6085  587  489 5760 3163 3376 4982 5115 4655 5575 3839 6743 7173 5724\n",
            " 7689 7787  520  713 3214 8302 3150 5713 2247 8128 4607 6660 3505 4610\n",
            " 2866 4161 6905 7096  768 1062 4411 6087 2096   21 4903 7546 6630 2928\n",
            " 4640 7125 1792 2063 1700 2305 7928 8095  737 5849  922 3423 2359 6015\n",
            " 4489 4361 6081 6346 5014 5167 1023 8009 3335  268 4777 5221 4323 6168\n",
            "  759  711 5702 6129 7181 1860 7192 1347 1692 5467 3378 6579   67 2693\n",
            " 6134 4841 4270 3073 8397 4611 2943 6746 7696 3466 3950 4693 6125 8370\n",
            " 6142 6131  995 5548 2186 2112 1721 5067 4015 5587 7092 3156   25 1749\n",
            " 5554 4878 1197 8248  458 8035 6836 1006 5310 2698 6960 2339 1650 6227\n",
            " 4104 5130 7089 6993 2400 1289 4305 5585 2524 7933 6284 7057 5324 3807\n",
            " 5790 8347 2390 5280 2841 7220 1384 7693 8090 2671 4171 5260  133 7451\n",
            " 3957 6793 1719 7379 4743 4746 5435 6216 4858 4417 8355 4492 2648 5104\n",
            " 5278 6539  743 7308  158 7543 2168 5913 7111  864 3414 5788 7592   39\n",
            " 7157 5647 4555 3956 1045 3617 4630 4384 6596   96 4539 6749 7202 4710\n",
            " 2413 4749 5122 5052 1598 5929  252 5565  336 6008 4564 5580 7237 1833\n",
            " 1466 1568 5958 7657 4327 5091 6918 6487 4082 2989 7448 4715   81 3382\n",
            "  467 3784 4624  764 4419 1117 2962 7954 7122 1430 6392  505 3174 1534\n",
            "  323 4164 3659 7526  498 5944 3510 4572 2307 2929 7480 4412  731 4563\n",
            " 7692 1869 2472 6966 4238 4702 6549 1594 2276 7269 2545 2216  605 6587\n",
            " 6315 5297 5282 4816 3200 1619 5504 2536 6399 3459 3917 6342 5754 5721\n",
            " 7385 7909 6126 7382 8382 4383 5502 7130  485 5862 4936 7674 3575 5372\n",
            " 3893 7109 4605 5211 5462 8065   50 4080 3334 8167 8378 5595 1873 8207\n",
            " 7076 4106 3719 4627  395 2685  231 2405 5419 6973 3480 3246 5414 3593\n",
            " 5931 6252 8208 4825 6203 3019 1105 5822   63 5854 6543 5787 4764 6701\n",
            "  334 6562 3822 4861 3051 1315 4805  246 8318 1089 8169 8218 2756 7549\n",
            "  707 1284 1076 2465 3815  540 4636 3188  697   80 1455  381 7025 7127\n",
            " 3992 4026 2777 4961  383 7531  744 6640 1047 6684 4297 7939  150 7384\n",
            "  446 7738 3287 6675 7752 5279 8196 3020 1203 4192 1552 4253 2887 2556\n",
            " 5729 1898 3208 5973 7033 1431 5915 8312 5799 7105  511 6404 2852 1561\n",
            " 4150 4835 1178]\n"
          ]
        }
      ],
      "source": [
        "gc.collect()\n",
        "tf.keras.backend.clear_session()\n",
        "dbert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "dbert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
        "max_len=512\n",
        "sentences=summarized_data['text']\n",
        "labels=summarized_data['label']\n",
        "filenumbers=summarized_data['filenum']\n",
        "len(sentences),len(labels),len(filenumbers)\n",
        "\n",
        "model_1=create_model()\n",
        "input_ids=[]\n",
        "attention_masks=[]\n",
        "\n",
        "for sent in sentences:\n",
        "  dbert_inps=dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n",
        "  input_ids.append(dbert_inps['input_ids'])\n",
        "  attention_masks.append(dbert_inps['attention_mask'])\n",
        "input_ids=np.asarray(input_ids)\n",
        "\n",
        "attention_masks=np.array(attention_masks)\n",
        "labels=np.array(labels)\n",
        "filenumbers=[int(i) for i in filenumbers]\n",
        "filenumbers=np.array(filenumbers)\n",
        "print(labels)\n",
        "print(filenumbers)\n",
        "\n",
        "#train_inp,val_inp,train_label,val_label,train_mask,val_mask,train_fnum,val_fnum=train_test_split(input_ids,labels,attention_masks,filenumbers,test_size=0.1,random_state=42)\n",
        "\n",
        "new_train_inp_1=[]\n",
        "new_val_inp_1=[]\n",
        "new_train_label_1=[]\n",
        "new_val_label_1=[]\n",
        "new_train_mask_1=[]\n",
        "new_train_fnum_1=[]\n",
        "new_val_fnum_1=[]\n",
        "new_val_mask_1=[]\n",
        "for i in range(len(train_fnum)):\n",
        "  if train_fnum[i] in filenumbers:\n",
        "    new_train_inp_1.append(train_inp[i])\n",
        "    #new_val_inp.append(val_inp[i])\n",
        "    new_train_label_1.append(train_label[i])\n",
        "    #new_val_label.append(val_label[i])\n",
        "    new_train_mask_1.append(train_mask[i])\n",
        "    #new_val_mask.append(val_mask[i])\n",
        "    new_train_fnum_1.append(train_fnum[i])\n",
        "\n",
        "for i in range(len(val_fnum)):\n",
        "  if val_fnum[i] in filenumbers:\n",
        "    #new_train_inp.append(train_inp[i])\n",
        "    new_val_inp_1.append(val_inp[i])\n",
        "    #new_train_label.append(train_label[i])\n",
        "    new_val_label_1.append(val_label[i])\n",
        "    #new_train_mask.append(train_mask[i])\n",
        "    new_val_mask_1.append(val_mask[i])\n",
        "    new_val_fnum_1.append(val_fnum[i])\n",
        "\n",
        "# print(new_train_label)\n",
        "# print(len(new_train_label))\n",
        "# print(new_val_label)\n",
        "# print(len(new_val_label))\n",
        "new_train_inp_1=np.array(new_train_inp_1)\n",
        "new_val_inp_1=np.array(new_val_inp_1)\n",
        "new_train_label_1=np.array(new_train_label_1)\n",
        "new_val_label_1=np.array(new_val_label_1)\n",
        "new_train_mask_1=np.array(new_train_mask_1)\n",
        "new_train_fnum_1=np.array(new_train_fnum_1)\n",
        "new_val_fnum_1=np.array(new_val_fnum_1)\n",
        "new_val_mask_1=np.array(new_val_mask_1)\n",
        "\n",
        "print(new_val_fnum_1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "U5WPkKimLztr",
        "outputId": "b7d0eef4-40ef-4609-b360-0eddc84cca92"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_1[0][0]',                \n",
            "                                thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_bert_model[0][0]']          \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 15)           7695        ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 8  1  8 ... 10 11  9]\n",
            "[   1    2    3 ... 8417 8418 8419]\n",
            "[6622   35 6713 4989 8233 7444 5061 3942 8344  454 5464  317 2883 4515\n",
            " 3001 3410 6991 2875 1125 3885 6647 4857 4504  214 3230 8340 2969 3847\n",
            " 4609 5092 1346 5993 7358 2752 7651 7301 8188 7073 4782 6069 2346 6445\n",
            " 6483  542 1663 4952 5072 1967 4071 7590  860 6418 5030 7297  626 5463\n",
            " 6029 6302 3819 7256 2950 5667 1235 3993 1439 1323 4263 3835 2134 2624\n",
            " 5801 4293 8129  320 5866 3247 5777   19 6059 6520 4837 3843 5695 1666\n",
            "  858 1418 5409 5274 6205 3167 2356 8418 5997 6685  223 3444 3310 7721\n",
            " 5928 2360 7197 4252 8343  909 4093 4354 4752 1421  649  829 1916 1003\n",
            " 3488 2496 5765 6612 4004 4052 1403 6697 5268 1010 6293 6380 4602 6057\n",
            " 1851 4577 4236 7978 1350 5083 6552 1058  361 7706 4186 2590 4452 4396\n",
            "  749  870 8351 3834  812 5108  463 1743 6600 8264 8064 7121 8279 5556\n",
            " 1845 5119 2516 4673 7947 8215 1882 3350 7617 7152 5199 4282 1092 3342\n",
            " 8003 5025 5051  535  933 3969 1927 3615 1221 8062 1489 3296 7442 5459\n",
            " 2031 7351  234 2313 8015 7416 7921 6574 6257 8396  941 4520 3300 5640\n",
            "  240 7810 7425 1321 6580  928 1170 5404 8273 6742 7005   94 7459    9\n",
            " 7861 7975 1018 5029 2322 2140 1415 7061 7767 4990 8124 5927 6051 3269\n",
            " 6916 5959 2846 6818 7052 7775 7582 8116 4240 4767 5248  430 1812 2175\n",
            " 6055 8331 3916  998 7271 1564 7326 1077 1559  292 3065 6764 6854 4915\n",
            " 5552 3583 1917  868 5902 4229 2144 7565 3712 7224 4502 1406 1651 6256\n",
            " 3827 6525 6318 2686 3616 5369 8235 1704 1011 3351 8089 8021  102  224\n",
            " 5671 8294 1510 7707 2985 6624 5146 1428 7381 3709 6824 6308  701 2281\n",
            " 6837 5436 1741 3520 5995 2649 4886  966 2863 5476 4058 1246 2406 1199\n",
            " 8387  513 5449 1028 2226 6166 4387  350 6123  678 5236 5041  734 3412\n",
            " 6085  587  489 5760 3163 3376 4982 5115 4655 5575 3839 6743 7173 5724\n",
            " 7689 7787  520  713 3214 8302 3150 5713 2247 8128 4607 6660 3505 4610\n",
            " 2866 4161 6905 7096  768 1062 4411 6087 2096   21 4903 7546 6630 2928\n",
            " 4640 7125 1792 2063 1700 2305 7928 8095  737 5849  922 3423 2359 6015\n",
            " 4489 4361 6081 6346 5014 5167 1023 8009 3335  268 4777 5221 4323 6168\n",
            "  759  711 5702 6129 7181 1860 7192 1347 1692 5467 3378 6579   67 2693\n",
            " 6134 4841 4270 3073 8397 4611 2943 6746 7696 3466 3950 4693 6125 8370\n",
            " 6142 6131  995 5548 2186 2112 1721 5067 4015 5587 7092 3156   25 1749\n",
            " 5554 4878 1197 8248  458 8035 6836 1006 5310 2698 6960 2339 1650 6227\n",
            " 4104 5130 7089 6993 2400 1289 4305 5585 2524 7933 6284 7057 5324 3807\n",
            " 5790 8347 2390 5280 2841 7220 1384 7693 8090 2671 4171 5260  133 7451\n",
            " 3957 6793 1719 7379 4743 4746 5435 6216 4858 4417 8355 4492 2648 5104\n",
            " 5278 6539  743 7308  158 7543 2168 5913 7111  864 3414 5788 7592   39\n",
            " 7157 5647 4555 3956 1045 3617 4630 4384 6596   96 4539 6749 7202 4710\n",
            " 2413 4749 5122 5052 1598 5929  252 5565  336 6008 4564 5580 7237 1833\n",
            " 1466 1568 5958 7657 4327 5091 6918 6487 4082 2989 7448 4715   81 3382\n",
            "  467 3784 4624  764 4419 1117 2962 7954 7122 1430 6392  505 3174 1534\n",
            "  323 4164 3659 7526  498 5944 3510 4572 2307 2929 7480 4412  731 4563\n",
            " 7692 1869 2472 6966 4238 4702 6549 1594 2276 7269 2545 2216  605 6587\n",
            " 6315 5297 5282 4816 3200 1619 5504 2536 6399 3459 3917 6342 5754 5721\n",
            " 7385 7909 6126 7382 8382 4383 5502 7130  485 5862 4936 7674 3575 5372\n",
            " 3893 7109 4605 5211 5462 8065   50 4080 3334 8167 8378 5595 1873 8207\n",
            " 7076 4106 3719 4627  395 2685  231 2405 5419 6973 3480 3246 5414 3593\n",
            " 5931 6252 8208 4825 6203 3019 1105 5822   63 5854 6543 5787 4764 6701\n",
            "  334 6562 3822 4861 3051 1315 4805  246 8318 1089 8169 8218 2756 7549\n",
            "  707 1284 1076 2465 3815  540 4636 3188  697   80 1455  381 7025 7127\n",
            " 3992 4026 2777 4961  383 7531  744 6640 1047 6684 4297 7939  150 7384\n",
            "  446 7738 3287 6675 7752 5279 8196 3020 1203 4192 1552 4253 2887 2556\n",
            " 5729 1898 3208 5973 7033 1431 5915 8312 5799 7105  511 6404 2852 1561\n",
            " 4150 4835 1178]\n",
            "Thu Jun  2 02:48:46 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    39W / 300W |  15237MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "826/826 [==============================] - 297s 342ms/step - loss: 7.0230 - accuracy: 0.6161 - val_loss: 6.0627 - val_accuracy: 0.7086\n",
            "Epoch 2/5\n",
            "826/826 [==============================] - 282s 342ms/step - loss: 5.4383 - accuracy: 0.7714 - val_loss: 5.1384 - val_accuracy: 0.7196\n",
            "Epoch 3/5\n",
            "826/826 [==============================] - 282s 342ms/step - loss: 4.3978 - accuracy: 0.8332 - val_loss: 4.3581 - val_accuracy: 0.7551\n",
            "Epoch 4/5\n",
            "826/826 [==============================] - 282s 342ms/step - loss: 3.5528 - accuracy: 0.8862 - val_loss: 3.6924 - val_accuracy: 0.7606\n",
            "Epoch 5/5\n",
            "826/826 [==============================] - 279s 337ms/step - loss: 2.8395 - accuracy: 0.9261 - val_loss: 3.2905 - val_accuracy: 0.7442\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_3[0][0]',                \n",
            "                                thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_bert_model[1][0]']          \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 15)           7695        ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.7315914489311164\n",
            "Weighted F1: 0.7314658698312698\n",
            "Micro F1: 0.7315914489311163\n",
            "Weighted Precision: 0.735313300455528\n",
            "Micro Precision: 0.7315914489311164\n",
            "Weighted Recall: 0.7315914489311164\n",
            "Micro Recall: 0.7315914489311164\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_1[0][0]',                \n",
            "                                thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_bert_model[0][0]']          \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 15)           7695        ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 8  1  8 ... 10 11  9]\n",
            "[   1    2    3 ... 8417 8418 8419]\n",
            "[6622   35 6713 4989 8233 7444 5061 3942 8344  454 5464  317 2883 4515\n",
            " 3001 3410 6991 2875 1125 3885 6647 4857 4504  214 3230 8340 2969 3847\n",
            " 4609 5092 1346 5993 7358 2752 7651 7301 8188 7073 4782 6069 2346 6445\n",
            " 6483  542 1663 4952 5072 1967 4071 7590  860 6418 5030 7297  626 5463\n",
            " 6029 6302 3819 7256 2950 5667 1235 3993 1439 1323 4263 3835 2134 2624\n",
            " 5801 4293 8129  320 5866 3247 5777   19 6059 6520 4837 3843 5695 1666\n",
            "  858 1418 5409 5274 6205 3167 2356 8418 5997 6685  223 3444 3310 7721\n",
            " 5928 2360 7197 4252 8343  909 4093 4354 4752 1421  649  829 1916 1003\n",
            " 3488 2496 5765 6612 4004 4052 1403 6697 5268 1010 6293 6380 4602 6057\n",
            " 1851 4577 4236 7978 1350 5083 6552 1058  361 7706 4186 2590 4452 4396\n",
            "  749  870 8351 3834  812 5108  463 1743 6600 8264 8064 7121 8279 5556\n",
            " 1845 5119 2516 4673 7947 8215 1882 3350 7617 7152 5199 4282 1092 3342\n",
            " 8003 5025 5051  535  933 3969 1927 3615 1221 8062 1489 3296 7442 5459\n",
            " 2031 7351  234 2313 8015 7416 7921 6574 6257 8396  941 4520 3300 5640\n",
            "  240 7810 7425 1321 6580  928 1170 5404 8273 6742 7005   94 7459    9\n",
            " 7861 7975 1018 5029 2322 2140 1415 7061 7767 4990 8124 5927 6051 3269\n",
            " 6916 5959 2846 6818 7052 7775 7582 8116 4240 4767 5248  430 1812 2175\n",
            " 6055 8331 3916  998 7271 1564 7326 1077 1559  292 3065 6764 6854 4915\n",
            " 5552 3583 1917  868 5902 4229 2144 7565 3712 7224 4502 1406 1651 6256\n",
            " 3827 6525 6318 2686 3616 5369 8235 1704 1011 3351 8089 8021  102  224\n",
            " 5671 8294 1510 7707 2985 6624 5146 1428 7381 3709 6824 6308  701 2281\n",
            " 6837 5436 1741 3520 5995 2649 4886  966 2863 5476 4058 1246 2406 1199\n",
            " 8387  513 5449 1028 2226 6166 4387  350 6123  678 5236 5041  734 3412\n",
            " 6085  587  489 5760 3163 3376 4982 5115 4655 5575 3839 6743 7173 5724\n",
            " 7689 7787  520  713 3214 8302 3150 5713 2247 8128 4607 6660 3505 4610\n",
            " 2866 4161 6905 7096  768 1062 4411 6087 2096   21 4903 7546 6630 2928\n",
            " 4640 7125 1792 2063 1700 2305 7928 8095  737 5849  922 3423 2359 6015\n",
            " 4489 4361 6081 6346 5014 5167 1023 8009 3335  268 4777 5221 4323 6168\n",
            "  759  711 5702 6129 7181 1860 7192 1347 1692 5467 3378 6579   67 2693\n",
            " 6134 4841 4270 3073 8397 4611 2943 6746 7696 3466 3950 4693 6125 8370\n",
            " 6142 6131  995 5548 2186 2112 1721 5067 4015 5587 7092 3156   25 1749\n",
            " 5554 4878 1197 8248  458 8035 6836 1006 5310 2698 6960 2339 1650 6227\n",
            " 4104 5130 7089 6993 2400 1289 4305 5585 2524 7933 6284 7057 5324 3807\n",
            " 5790 8347 2390 5280 2841 7220 1384 7693 8090 2671 4171 5260  133 7451\n",
            " 3957 6793 1719 7379 4743 4746 5435 6216 4858 4417 8355 4492 2648 5104\n",
            " 5278 6539  743 7308  158 7543 2168 5913 7111  864 3414 5788 7592   39\n",
            " 7157 5647 4555 3956 1045 3617 4630 4384 6596   96 4539 6749 7202 4710\n",
            " 2413 4749 5122 5052 1598 5929  252 5565  336 6008 4564 5580 7237 1833\n",
            " 1466 1568 5958 7657 4327 5091 6918 6487 4082 2989 7448 4715   81 3382\n",
            "  467 3784 4624  764 4419 1117 2962 7954 7122 1430 6392  505 3174 1534\n",
            "  323 4164 3659 7526  498 5944 3510 4572 2307 2929 7480 4412  731 4563\n",
            " 7692 1869 2472 6966 4238 4702 6549 1594 2276 7269 2545 2216  605 6587\n",
            " 6315 5297 5282 4816 3200 1619 5504 2536 6399 3459 3917 6342 5754 5721\n",
            " 7385 7909 6126 7382 8382 4383 5502 7130  485 5862 4936 7674 3575 5372\n",
            " 3893 7109 4605 5211 5462 8065   50 4080 3334 8167 8378 5595 1873 8207\n",
            " 7076 4106 3719 4627  395 2685  231 2405 5419 6973 3480 3246 5414 3593\n",
            " 5931 6252 8208 4825 6203 3019 1105 5822   63 5854 6543 5787 4764 6701\n",
            "  334 6562 3822 4861 3051 1315 4805  246 8318 1089 8169 8218 2756 7549\n",
            "  707 1284 1076 2465 3815  540 4636 3188  697   80 1455  381 7025 7127\n",
            " 3992 4026 2777 4961  383 7531  744 6640 1047 6684 4297 7939  150 7384\n",
            "  446 7738 3287 6675 7752 5279 8196 3020 1203 4192 1552 4253 2887 2556\n",
            " 5729 1898 3208 5973 7033 1431 5915 8312 5799 7105  511 6404 2852 1561\n",
            " 4150 4835 1178]\n",
            "Thu Jun  2 03:15:28 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    39W / 300W |  15237MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "826/826 [==============================] - 299s 344ms/step - loss: 7.1120 - accuracy: 0.6261 - val_loss: 6.2619 - val_accuracy: 0.7045\n",
            "Epoch 2/5\n",
            "826/826 [==============================] - 283s 342ms/step - loss: 5.6932 - accuracy: 0.7644 - val_loss: 5.2999 - val_accuracy: 0.7442\n",
            "Epoch 3/5\n",
            "826/826 [==============================] - 279s 338ms/step - loss: 4.6997 - accuracy: 0.8273 - val_loss: 4.7728 - val_accuracy: 0.7291\n",
            "Epoch 4/5\n",
            "826/826 [==============================] - 282s 342ms/step - loss: 3.8323 - accuracy: 0.8809 - val_loss: 4.0289 - val_accuracy: 0.7620\n",
            "Epoch 5/5\n",
            "826/826 [==============================] - 279s 338ms/step - loss: 3.0973 - accuracy: 0.9247 - val_loss: 3.4741 - val_accuracy: 0.7332\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_3[0][0]',                \n",
            "                                thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_bert_model[1][0]']          \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 15)           7695        ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.7375296912114014\n",
            "Weighted F1: 0.7297487080796021\n",
            "Micro F1: 0.7375296912114016\n",
            "Weighted Precision: 0.7415676845628203\n",
            "Micro Precision: 0.7375296912114014\n",
            "Weighted Recall: 0.7375296912114014\n",
            "Micro Recall: 0.7375296912114014\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_1[0][0]',                \n",
            "                                thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_bert_model[0][0]']          \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 15)           7695        ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 8  1  8 ... 10 11  9]\n",
            "[   1    2    3 ... 8417 8418 8419]\n",
            "[6622   35 6713 4989 8233 7444 5061 3942 8344  454 5464  317 2883 4515\n",
            " 3001 3410 6991 2875 1125 3885 6647 4857 4504  214 3230 8340 2969 3847\n",
            " 4609 5092 1346 5993 7358 2752 7651 7301 8188 7073 4782 6069 2346 6445\n",
            " 6483  542 1663 4952 5072 1967 4071 7590  860 6418 5030 7297  626 5463\n",
            " 6029 6302 3819 7256 2950 5667 1235 3993 1439 1323 4263 3835 2134 2624\n",
            " 5801 4293 8129  320 5866 3247 5777   19 6059 6520 4837 3843 5695 1666\n",
            "  858 1418 5409 5274 6205 3167 2356 8418 5997 6685  223 3444 3310 7721\n",
            " 5928 2360 7197 4252 8343  909 4093 4354 4752 1421  649  829 1916 1003\n",
            " 3488 2496 5765 6612 4004 4052 1403 6697 5268 1010 6293 6380 4602 6057\n",
            " 1851 4577 4236 7978 1350 5083 6552 1058  361 7706 4186 2590 4452 4396\n",
            "  749  870 8351 3834  812 5108  463 1743 6600 8264 8064 7121 8279 5556\n",
            " 1845 5119 2516 4673 7947 8215 1882 3350 7617 7152 5199 4282 1092 3342\n",
            " 8003 5025 5051  535  933 3969 1927 3615 1221 8062 1489 3296 7442 5459\n",
            " 2031 7351  234 2313 8015 7416 7921 6574 6257 8396  941 4520 3300 5640\n",
            "  240 7810 7425 1321 6580  928 1170 5404 8273 6742 7005   94 7459    9\n",
            " 7861 7975 1018 5029 2322 2140 1415 7061 7767 4990 8124 5927 6051 3269\n",
            " 6916 5959 2846 6818 7052 7775 7582 8116 4240 4767 5248  430 1812 2175\n",
            " 6055 8331 3916  998 7271 1564 7326 1077 1559  292 3065 6764 6854 4915\n",
            " 5552 3583 1917  868 5902 4229 2144 7565 3712 7224 4502 1406 1651 6256\n",
            " 3827 6525 6318 2686 3616 5369 8235 1704 1011 3351 8089 8021  102  224\n",
            " 5671 8294 1510 7707 2985 6624 5146 1428 7381 3709 6824 6308  701 2281\n",
            " 6837 5436 1741 3520 5995 2649 4886  966 2863 5476 4058 1246 2406 1199\n",
            " 8387  513 5449 1028 2226 6166 4387  350 6123  678 5236 5041  734 3412\n",
            " 6085  587  489 5760 3163 3376 4982 5115 4655 5575 3839 6743 7173 5724\n",
            " 7689 7787  520  713 3214 8302 3150 5713 2247 8128 4607 6660 3505 4610\n",
            " 2866 4161 6905 7096  768 1062 4411 6087 2096   21 4903 7546 6630 2928\n",
            " 4640 7125 1792 2063 1700 2305 7928 8095  737 5849  922 3423 2359 6015\n",
            " 4489 4361 6081 6346 5014 5167 1023 8009 3335  268 4777 5221 4323 6168\n",
            "  759  711 5702 6129 7181 1860 7192 1347 1692 5467 3378 6579   67 2693\n",
            " 6134 4841 4270 3073 8397 4611 2943 6746 7696 3466 3950 4693 6125 8370\n",
            " 6142 6131  995 5548 2186 2112 1721 5067 4015 5587 7092 3156   25 1749\n",
            " 5554 4878 1197 8248  458 8035 6836 1006 5310 2698 6960 2339 1650 6227\n",
            " 4104 5130 7089 6993 2400 1289 4305 5585 2524 7933 6284 7057 5324 3807\n",
            " 5790 8347 2390 5280 2841 7220 1384 7693 8090 2671 4171 5260  133 7451\n",
            " 3957 6793 1719 7379 4743 4746 5435 6216 4858 4417 8355 4492 2648 5104\n",
            " 5278 6539  743 7308  158 7543 2168 5913 7111  864 3414 5788 7592   39\n",
            " 7157 5647 4555 3956 1045 3617 4630 4384 6596   96 4539 6749 7202 4710\n",
            " 2413 4749 5122 5052 1598 5929  252 5565  336 6008 4564 5580 7237 1833\n",
            " 1466 1568 5958 7657 4327 5091 6918 6487 4082 2989 7448 4715   81 3382\n",
            "  467 3784 4624  764 4419 1117 2962 7954 7122 1430 6392  505 3174 1534\n",
            "  323 4164 3659 7526  498 5944 3510 4572 2307 2929 7480 4412  731 4563\n",
            " 7692 1869 2472 6966 4238 4702 6549 1594 2276 7269 2545 2216  605 6587\n",
            " 6315 5297 5282 4816 3200 1619 5504 2536 6399 3459 3917 6342 5754 5721\n",
            " 7385 7909 6126 7382 8382 4383 5502 7130  485 5862 4936 7674 3575 5372\n",
            " 3893 7109 4605 5211 5462 8065   50 4080 3334 8167 8378 5595 1873 8207\n",
            " 7076 4106 3719 4627  395 2685  231 2405 5419 6973 3480 3246 5414 3593\n",
            " 5931 6252 8208 4825 6203 3019 1105 5822   63 5854 6543 5787 4764 6701\n",
            "  334 6562 3822 4861 3051 1315 4805  246 8318 1089 8169 8218 2756 7549\n",
            "  707 1284 1076 2465 3815  540 4636 3188  697   80 1455  381 7025 7127\n",
            " 3992 4026 2777 4961  383 7531  744 6640 1047 6684 4297 7939  150 7384\n",
            "  446 7738 3287 6675 7752 5279 8196 3020 1203 4192 1552 4253 2887 2556\n",
            " 5729 1898 3208 5973 7033 1431 5915 8312 5799 7105  511 6404 2852 1561\n",
            " 4150 4835 1178]\n",
            "Thu Jun  2 03:42:07 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    39W / 300W |  15237MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "826/826 [==============================] - 299s 344ms/step - loss: 7.1060 - accuracy: 0.6235 - val_loss: 6.2317 - val_accuracy: 0.7114\n",
            "Epoch 2/5\n",
            "826/826 [==============================] - 283s 343ms/step - loss: 5.7024 - accuracy: 0.7656 - val_loss: 5.3884 - val_accuracy: 0.7237\n",
            "Epoch 3/5\n",
            "826/826 [==============================] - 283s 343ms/step - loss: 4.6927 - accuracy: 0.8294 - val_loss: 4.7690 - val_accuracy: 0.7305\n",
            "Epoch 4/5\n",
            "826/826 [==============================] - 283s 343ms/step - loss: 3.8265 - accuracy: 0.8827 - val_loss: 4.0082 - val_accuracy: 0.7497\n",
            "Epoch 5/5\n",
            "826/826 [==============================] - 279s 338ms/step - loss: 3.1011 - accuracy: 0.9186 - val_loss: 3.5846 - val_accuracy: 0.7346\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_3[0][0]',                \n",
            "                                thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_bert_model[1][0]']          \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 15)           7695        ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.7280285035629454\n",
            "Weighted F1: 0.7224127775129171\n",
            "Micro F1: 0.7280285035629454\n",
            "Weighted Precision: 0.7281731135828747\n",
            "Micro Precision: 0.7280285035629454\n",
            "Weighted Recall: 0.7280285035629454\n",
            "Micro Recall: 0.7280285035629454\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_1[0][0]',                \n",
            "                                thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_bert_model[0][0]']          \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 15)           7695        ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 8  1  8 ... 10 11  9]\n",
            "[   1    2    3 ... 8417 8418 8419]\n",
            "[6622   35 6713 4989 8233 7444 5061 3942 8344  454 5464  317 2883 4515\n",
            " 3001 3410 6991 2875 1125 3885 6647 4857 4504  214 3230 8340 2969 3847\n",
            " 4609 5092 1346 5993 7358 2752 7651 7301 8188 7073 4782 6069 2346 6445\n",
            " 6483  542 1663 4952 5072 1967 4071 7590  860 6418 5030 7297  626 5463\n",
            " 6029 6302 3819 7256 2950 5667 1235 3993 1439 1323 4263 3835 2134 2624\n",
            " 5801 4293 8129  320 5866 3247 5777   19 6059 6520 4837 3843 5695 1666\n",
            "  858 1418 5409 5274 6205 3167 2356 8418 5997 6685  223 3444 3310 7721\n",
            " 5928 2360 7197 4252 8343  909 4093 4354 4752 1421  649  829 1916 1003\n",
            " 3488 2496 5765 6612 4004 4052 1403 6697 5268 1010 6293 6380 4602 6057\n",
            " 1851 4577 4236 7978 1350 5083 6552 1058  361 7706 4186 2590 4452 4396\n",
            "  749  870 8351 3834  812 5108  463 1743 6600 8264 8064 7121 8279 5556\n",
            " 1845 5119 2516 4673 7947 8215 1882 3350 7617 7152 5199 4282 1092 3342\n",
            " 8003 5025 5051  535  933 3969 1927 3615 1221 8062 1489 3296 7442 5459\n",
            " 2031 7351  234 2313 8015 7416 7921 6574 6257 8396  941 4520 3300 5640\n",
            "  240 7810 7425 1321 6580  928 1170 5404 8273 6742 7005   94 7459    9\n",
            " 7861 7975 1018 5029 2322 2140 1415 7061 7767 4990 8124 5927 6051 3269\n",
            " 6916 5959 2846 6818 7052 7775 7582 8116 4240 4767 5248  430 1812 2175\n",
            " 6055 8331 3916  998 7271 1564 7326 1077 1559  292 3065 6764 6854 4915\n",
            " 5552 3583 1917  868 5902 4229 2144 7565 3712 7224 4502 1406 1651 6256\n",
            " 3827 6525 6318 2686 3616 5369 8235 1704 1011 3351 8089 8021  102  224\n",
            " 5671 8294 1510 7707 2985 6624 5146 1428 7381 3709 6824 6308  701 2281\n",
            " 6837 5436 1741 3520 5995 2649 4886  966 2863 5476 4058 1246 2406 1199\n",
            " 8387  513 5449 1028 2226 6166 4387  350 6123  678 5236 5041  734 3412\n",
            " 6085  587  489 5760 3163 3376 4982 5115 4655 5575 3839 6743 7173 5724\n",
            " 7689 7787  520  713 3214 8302 3150 5713 2247 8128 4607 6660 3505 4610\n",
            " 2866 4161 6905 7096  768 1062 4411 6087 2096   21 4903 7546 6630 2928\n",
            " 4640 7125 1792 2063 1700 2305 7928 8095  737 5849  922 3423 2359 6015\n",
            " 4489 4361 6081 6346 5014 5167 1023 8009 3335  268 4777 5221 4323 6168\n",
            "  759  711 5702 6129 7181 1860 7192 1347 1692 5467 3378 6579   67 2693\n",
            " 6134 4841 4270 3073 8397 4611 2943 6746 7696 3466 3950 4693 6125 8370\n",
            " 6142 6131  995 5548 2186 2112 1721 5067 4015 5587 7092 3156   25 1749\n",
            " 5554 4878 1197 8248  458 8035 6836 1006 5310 2698 6960 2339 1650 6227\n",
            " 4104 5130 7089 6993 2400 1289 4305 5585 2524 7933 6284 7057 5324 3807\n",
            " 5790 8347 2390 5280 2841 7220 1384 7693 8090 2671 4171 5260  133 7451\n",
            " 3957 6793 1719 7379 4743 4746 5435 6216 4858 4417 8355 4492 2648 5104\n",
            " 5278 6539  743 7308  158 7543 2168 5913 7111  864 3414 5788 7592   39\n",
            " 7157 5647 4555 3956 1045 3617 4630 4384 6596   96 4539 6749 7202 4710\n",
            " 2413 4749 5122 5052 1598 5929  252 5565  336 6008 4564 5580 7237 1833\n",
            " 1466 1568 5958 7657 4327 5091 6918 6487 4082 2989 7448 4715   81 3382\n",
            "  467 3784 4624  764 4419 1117 2962 7954 7122 1430 6392  505 3174 1534\n",
            "  323 4164 3659 7526  498 5944 3510 4572 2307 2929 7480 4412  731 4563\n",
            " 7692 1869 2472 6966 4238 4702 6549 1594 2276 7269 2545 2216  605 6587\n",
            " 6315 5297 5282 4816 3200 1619 5504 2536 6399 3459 3917 6342 5754 5721\n",
            " 7385 7909 6126 7382 8382 4383 5502 7130  485 5862 4936 7674 3575 5372\n",
            " 3893 7109 4605 5211 5462 8065   50 4080 3334 8167 8378 5595 1873 8207\n",
            " 7076 4106 3719 4627  395 2685  231 2405 5419 6973 3480 3246 5414 3593\n",
            " 5931 6252 8208 4825 6203 3019 1105 5822   63 5854 6543 5787 4764 6701\n",
            "  334 6562 3822 4861 3051 1315 4805  246 8318 1089 8169 8218 2756 7549\n",
            "  707 1284 1076 2465 3815  540 4636 3188  697   80 1455  381 7025 7127\n",
            " 3992 4026 2777 4961  383 7531  744 6640 1047 6684 4297 7939  150 7384\n",
            "  446 7738 3287 6675 7752 5279 8196 3020 1203 4192 1552 4253 2887 2556\n",
            " 5729 1898 3208 5973 7033 1431 5915 8312 5799 7105  511 6404 2852 1561\n",
            " 4150 4835 1178]\n",
            "Thu Jun  2 04:08:50 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    39W / 300W |  15237MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "826/826 [==============================] - 299s 343ms/step - loss: 7.0339 - accuracy: 0.6198 - val_loss: 6.0983 - val_accuracy: 0.7168\n",
            "Epoch 2/5\n",
            "826/826 [==============================] - 283s 342ms/step - loss: 5.5154 - accuracy: 0.7764 - val_loss: 5.1912 - val_accuracy: 0.7606\n",
            "Epoch 3/5\n",
            "826/826 [==============================] - 279s 338ms/step - loss: 4.5056 - accuracy: 0.8323 - val_loss: 4.6068 - val_accuracy: 0.7360\n",
            "Epoch 4/5\n",
            "826/826 [==============================] - 282s 342ms/step - loss: 3.6422 - accuracy: 0.8902 - val_loss: 3.7863 - val_accuracy: 0.7702\n",
            "Epoch 5/5\n",
            "826/826 [==============================] - 279s 338ms/step - loss: 2.9101 - accuracy: 0.9271 - val_loss: 3.3501 - val_accuracy: 0.7415\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_3[0][0]',                \n",
            "                                thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_bert_model[1][0]']          \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 15)           7695        ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.7458432304038005\n",
            "Weighted F1: 0.7406081366610313\n",
            "Micro F1: 0.7458432304038005\n",
            "Weighted Precision: 0.7519644777550918\n",
            "Micro Precision: 0.7458432304038005\n",
            "Weighted Recall: 0.7458432304038005\n",
            "Micro Recall: 0.7458432304038005\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_1[0][0]',                \n",
            "                                thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_bert_model[0][0]']          \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 15)           7695        ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 8  1  8 ... 10 11  9]\n",
            "[   1    2    3 ... 8417 8418 8419]\n",
            "[6622   35 6713 4989 8233 7444 5061 3942 8344  454 5464  317 2883 4515\n",
            " 3001 3410 6991 2875 1125 3885 6647 4857 4504  214 3230 8340 2969 3847\n",
            " 4609 5092 1346 5993 7358 2752 7651 7301 8188 7073 4782 6069 2346 6445\n",
            " 6483  542 1663 4952 5072 1967 4071 7590  860 6418 5030 7297  626 5463\n",
            " 6029 6302 3819 7256 2950 5667 1235 3993 1439 1323 4263 3835 2134 2624\n",
            " 5801 4293 8129  320 5866 3247 5777   19 6059 6520 4837 3843 5695 1666\n",
            "  858 1418 5409 5274 6205 3167 2356 8418 5997 6685  223 3444 3310 7721\n",
            " 5928 2360 7197 4252 8343  909 4093 4354 4752 1421  649  829 1916 1003\n",
            " 3488 2496 5765 6612 4004 4052 1403 6697 5268 1010 6293 6380 4602 6057\n",
            " 1851 4577 4236 7978 1350 5083 6552 1058  361 7706 4186 2590 4452 4396\n",
            "  749  870 8351 3834  812 5108  463 1743 6600 8264 8064 7121 8279 5556\n",
            " 1845 5119 2516 4673 7947 8215 1882 3350 7617 7152 5199 4282 1092 3342\n",
            " 8003 5025 5051  535  933 3969 1927 3615 1221 8062 1489 3296 7442 5459\n",
            " 2031 7351  234 2313 8015 7416 7921 6574 6257 8396  941 4520 3300 5640\n",
            "  240 7810 7425 1321 6580  928 1170 5404 8273 6742 7005   94 7459    9\n",
            " 7861 7975 1018 5029 2322 2140 1415 7061 7767 4990 8124 5927 6051 3269\n",
            " 6916 5959 2846 6818 7052 7775 7582 8116 4240 4767 5248  430 1812 2175\n",
            " 6055 8331 3916  998 7271 1564 7326 1077 1559  292 3065 6764 6854 4915\n",
            " 5552 3583 1917  868 5902 4229 2144 7565 3712 7224 4502 1406 1651 6256\n",
            " 3827 6525 6318 2686 3616 5369 8235 1704 1011 3351 8089 8021  102  224\n",
            " 5671 8294 1510 7707 2985 6624 5146 1428 7381 3709 6824 6308  701 2281\n",
            " 6837 5436 1741 3520 5995 2649 4886  966 2863 5476 4058 1246 2406 1199\n",
            " 8387  513 5449 1028 2226 6166 4387  350 6123  678 5236 5041  734 3412\n",
            " 6085  587  489 5760 3163 3376 4982 5115 4655 5575 3839 6743 7173 5724\n",
            " 7689 7787  520  713 3214 8302 3150 5713 2247 8128 4607 6660 3505 4610\n",
            " 2866 4161 6905 7096  768 1062 4411 6087 2096   21 4903 7546 6630 2928\n",
            " 4640 7125 1792 2063 1700 2305 7928 8095  737 5849  922 3423 2359 6015\n",
            " 4489 4361 6081 6346 5014 5167 1023 8009 3335  268 4777 5221 4323 6168\n",
            "  759  711 5702 6129 7181 1860 7192 1347 1692 5467 3378 6579   67 2693\n",
            " 6134 4841 4270 3073 8397 4611 2943 6746 7696 3466 3950 4693 6125 8370\n",
            " 6142 6131  995 5548 2186 2112 1721 5067 4015 5587 7092 3156   25 1749\n",
            " 5554 4878 1197 8248  458 8035 6836 1006 5310 2698 6960 2339 1650 6227\n",
            " 4104 5130 7089 6993 2400 1289 4305 5585 2524 7933 6284 7057 5324 3807\n",
            " 5790 8347 2390 5280 2841 7220 1384 7693 8090 2671 4171 5260  133 7451\n",
            " 3957 6793 1719 7379 4743 4746 5435 6216 4858 4417 8355 4492 2648 5104\n",
            " 5278 6539  743 7308  158 7543 2168 5913 7111  864 3414 5788 7592   39\n",
            " 7157 5647 4555 3956 1045 3617 4630 4384 6596   96 4539 6749 7202 4710\n",
            " 2413 4749 5122 5052 1598 5929  252 5565  336 6008 4564 5580 7237 1833\n",
            " 1466 1568 5958 7657 4327 5091 6918 6487 4082 2989 7448 4715   81 3382\n",
            "  467 3784 4624  764 4419 1117 2962 7954 7122 1430 6392  505 3174 1534\n",
            "  323 4164 3659 7526  498 5944 3510 4572 2307 2929 7480 4412  731 4563\n",
            " 7692 1869 2472 6966 4238 4702 6549 1594 2276 7269 2545 2216  605 6587\n",
            " 6315 5297 5282 4816 3200 1619 5504 2536 6399 3459 3917 6342 5754 5721\n",
            " 7385 7909 6126 7382 8382 4383 5502 7130  485 5862 4936 7674 3575 5372\n",
            " 3893 7109 4605 5211 5462 8065   50 4080 3334 8167 8378 5595 1873 8207\n",
            " 7076 4106 3719 4627  395 2685  231 2405 5419 6973 3480 3246 5414 3593\n",
            " 5931 6252 8208 4825 6203 3019 1105 5822   63 5854 6543 5787 4764 6701\n",
            "  334 6562 3822 4861 3051 1315 4805  246 8318 1089 8169 8218 2756 7549\n",
            "  707 1284 1076 2465 3815  540 4636 3188  697   80 1455  381 7025 7127\n",
            " 3992 4026 2777 4961  383 7531  744 6640 1047 6684 4297 7939  150 7384\n",
            "  446 7738 3287 6675 7752 5279 8196 3020 1203 4192 1552 4253 2887 2556\n",
            " 5729 1898 3208 5973 7033 1431 5915 8312 5799 7105  511 6404 2852 1561\n",
            " 4150 4835 1178]\n",
            "Thu Jun  2 04:35:28 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    39W / 300W |  15237MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "826/826 [==============================] - 299s 343ms/step - loss: 7.0508 - accuracy: 0.6207 - val_loss: 6.1678 - val_accuracy: 0.7059\n",
            "Epoch 2/5\n",
            "826/826 [==============================] - 283s 342ms/step - loss: 5.5718 - accuracy: 0.7700 - val_loss: 5.2102 - val_accuracy: 0.7332\n",
            "Epoch 3/5\n",
            "826/826 [==============================] - 282s 342ms/step - loss: 4.5524 - accuracy: 0.8252 - val_loss: 4.5470 - val_accuracy: 0.7497\n",
            "Epoch 4/5\n",
            "826/826 [==============================] - 279s 337ms/step - loss: 3.6867 - accuracy: 0.8867 - val_loss: 3.9171 - val_accuracy: 0.7456\n",
            "Epoch 5/5\n",
            "826/826 [==============================] - 278s 337ms/step - loss: 2.9664 - accuracy: 0.9227 - val_loss: 3.4594 - val_accuracy: 0.7387\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_3[0][0]',                \n",
            "                                thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_bert_model[1][0]']          \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 15)           7695        ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.7268408551068883\n",
            "Weighted F1: 0.7082297867187702\n",
            "Micro F1: 0.7268408551068883\n",
            "Weighted Precision: 0.719524838655942\n",
            "Micro Precision: 0.7268408551068883\n",
            "Weighted Recall: 0.7268408551068883\n",
            "Micro Recall: 0.7268408551068883\n",
            "Average Accuracy: 0.7339667458432304\n",
            "Average Weighted F1: 0.7264930557607181\n",
            "Average Micro F1: 0.7339667458432304\n",
            "Average Weighted Precision: 0.7353086830024512\n",
            "Average Micro Precision: 0.7339667458432304\n",
            "Average Weighted Recall: 0.7339667458432304\n",
            "Average Micro Recall: 0.7339667458432304\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "total_accuracy=0\n",
        "total_weighted_f1=0\n",
        "total_micro_f1=0\n",
        "total_weighted_precision=0\n",
        "total_micro_precision=0\n",
        "total_weighted_recall=0\n",
        "total_micro_recall=0\n",
        "\n",
        "for f in range(5):\n",
        "  gc.collect()\n",
        "  tf.keras.backend.clear_session()\n",
        "  dbert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "  dbert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
        "  max_len=512\n",
        "  sentences=summarized_data['text']\n",
        "  labels=summarized_data['label']\n",
        "  filenumbers=summarized_data['filenum']\n",
        "  len(sentences),len(labels),len(filenumbers)\n",
        "  \n",
        "  model_1=create_model()\n",
        "  input_ids=[]\n",
        "  attention_masks=[]\n",
        "\n",
        "  for sent in sentences:\n",
        "    dbert_inps=dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n",
        "    input_ids.append(dbert_inps['input_ids'])\n",
        "    attention_masks.append(dbert_inps['attention_mask'])\n",
        "  input_ids=np.asarray(input_ids)\n",
        "\n",
        "  attention_masks=np.array(attention_masks)\n",
        "  labels=np.array(labels)\n",
        "  filenumbers=[int(i) for i in filenumbers]\n",
        "  filenumbers=np.array(filenumbers)\n",
        "  print(labels)\n",
        "  print(filenumbers)\n",
        "\n",
        "  #train_inp,val_inp,train_label,val_label,train_mask,val_mask,train_fnum,val_fnum=train_test_split(input_ids,labels,attention_masks,filenumbers,test_size=0.1,random_state=42)\n",
        "\n",
        "  new_train_inp_1=[]\n",
        "  new_val_inp_1=[]\n",
        "  new_train_label_1=[]\n",
        "  new_val_label_1=[]\n",
        "  new_train_mask_1=[]\n",
        "  new_train_fnum_1=[]\n",
        "  new_val_fnum_1=[]\n",
        "  new_val_mask_1=[]\n",
        "  for i in range(len(train_fnum)):\n",
        "    if train_fnum[i] in filenumbers:\n",
        "      new_train_inp_1.append(train_inp[i])\n",
        "      #new_val_inp.append(val_inp[i])\n",
        "      new_train_label_1.append(train_label[i])\n",
        "      #new_val_label.append(val_label[i])\n",
        "      new_train_mask_1.append(train_mask[i])\n",
        "      #new_val_mask.append(val_mask[i])\n",
        "      new_train_fnum_1.append(train_fnum[i])\n",
        "\n",
        "  for i in range(len(val_fnum)):\n",
        "    if val_fnum[i] in filenumbers:\n",
        "      #new_train_inp.append(train_inp[i])\n",
        "      new_val_inp_1.append(val_inp[i])\n",
        "      #new_train_label.append(train_label[i])\n",
        "      new_val_label_1.append(val_label[i])\n",
        "      #new_train_mask.append(train_mask[i])\n",
        "      new_val_mask_1.append(val_mask[i])\n",
        "      new_val_fnum_1.append(val_fnum[i])\n",
        "\n",
        "  # print(new_train_label)\n",
        "  # print(len(new_train_label))\n",
        "  # print(new_val_label)\n",
        "  # print(len(new_val_label))\n",
        "  new_train_inp_1=np.array(new_train_inp_1)\n",
        "  new_val_inp_1=np.array(new_val_inp_1)\n",
        "  new_train_label_1=np.array(new_train_label_1)\n",
        "  new_val_label_1=np.array(new_val_label_1)\n",
        "  new_train_mask_1=np.array(new_train_mask_1)\n",
        "  new_train_fnum_1=np.array(new_train_fnum_1)\n",
        "  new_val_fnum_1=np.array(new_val_fnum_1)\n",
        "  new_val_mask_1=np.array(new_val_mask_1)\n",
        "\n",
        "  print(new_val_fnum_1)\n",
        "  \n",
        "  log_dir='dbert_model'\n",
        "\n",
        "  model_save_path='./drive/MyDrive/Ensemble/bert-ensemble-512-model_1-'+str(f)+'-15labels.h5'\n",
        "\n",
        "  loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "  accuracy = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
        "  callbacks= [tf.keras.callbacks.ModelCheckpoint(filepath=model_save_path,monitor='val_accuracy',mode='max',save_best_only=True,save_weights_only=True),keras.callbacks.TensorBoard(log_dir=log_dir)]\n",
        "  model_1.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])\n",
        "  gpu_info = !nvidia-smi\n",
        "  gpu_info = '\\n'.join(gpu_info)\n",
        "  if gpu_info.find('failed') >= 0:\n",
        "    print('Not connected to a GPU')\n",
        "  else:\n",
        "    print(gpu_info)\n",
        "  \n",
        "  history=model_1.fit([new_train_inp_1,new_train_mask_1],new_train_label_1,batch_size=8,epochs=5,validation_data=([new_val_inp_1,new_val_mask_1],new_val_label_1),callbacks=callbacks)\n",
        "\n",
        "  pred_labels=[]\n",
        "\n",
        "  model_saved_1= create_model()\n",
        "  model_saved_1.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])\n",
        "  model_saved_1.load_weights('./drive/MyDrive/Ensemble/bert-ensemble-512-model_1-'+str(f)+'-15labels.h5')\n",
        "\n",
        "  for i in range(0,len(val_inp)):\n",
        "    pred=model_saved_1.predict([val_inp[i].reshape(1,512),val_mask[i].reshape(1,512)])\n",
        "    pred_label = pred.argmax(axis=1)\n",
        "    pred_labels.append(pred_label)\n",
        "  accuracy=accuracy_score(val_label, pred_labels)\n",
        "  print(\"Accuracy: \"+str(accuracy))\n",
        "  total_accuracy=total_accuracy+accuracy\n",
        "  \n",
        "  weighted_f1=f1_score(val_label,pred_labels, average='weighted')\n",
        "  print(\"Weighted F1: \"+ str(weighted_f1))\n",
        "  total_weighted_f1=total_weighted_f1+weighted_f1\n",
        "  micro_f1=f1_score(val_label,pred_labels, average='micro')\n",
        "  print(\"Micro F1: \"+ str(micro_f1))\n",
        "  total_micro_f1=total_micro_f1+micro_f1\n",
        "\n",
        "  weighted_precision=precision_score(val_label, pred_labels, average='weighted')\n",
        "  print(\"Weighted Precision: \" + str(weighted_precision))\n",
        "  total_weighted_precision=total_weighted_precision+weighted_precision\n",
        "  micro_precision=precision_score(val_label, pred_labels, average='micro')\n",
        "  print(\"Micro Precision: \" + str(micro_precision))\n",
        "  total_micro_precision=total_micro_precision+micro_precision\n",
        "\n",
        "  weighted_recall=recall_score(val_label, pred_labels, average='weighted')\n",
        "  print(\"Weighted Recall: \" + str(weighted_recall))\n",
        "  total_weighted_recall=total_weighted_recall+weighted_recall\n",
        "  micro_recall=recall_score(val_label, pred_labels, average='micro')\n",
        "  print(\"Micro Recall: \" + str(micro_recall))\n",
        "  total_micro_recall=total_micro_recall+micro_recall\n",
        "\n",
        "\n",
        "print(\"Average Accuracy: \"+str(total_accuracy/5))\n",
        "print(\"Average Weighted F1: \"+str(total_weighted_f1/5))\n",
        "print(\"Average Micro F1: \"+str(total_micro_f1/5))\n",
        "print(\"Average Weighted Precision: \"+str(total_weighted_precision/5))\n",
        "print(\"Average Micro Precision: \"+str(total_micro_precision/5))\n",
        "print(\"Average Weighted Recall: \"+str(total_weighted_recall/5))\n",
        "print(\"Average Micro Recall: \"+str(total_micro_recall/5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4Yijn8aNjtR",
        "outputId": "45de18f4-34eb-4101-d2e0-ea912baa7f72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6722\n",
            "6722\n",
            "Average Length 511.5861350788456\n",
            "['8', '1', '8', '2', '8', '8', '8', '9', '7', '8', '1', '2', '1', '1', '8', '4', '8', '8', '12', '3', '3', '7', '3', '12', '1', '8', '8', '8', '8', '8', '8', '8', '1', '9', '5', '9', '9', '9', '11', '8', '8', '8', '4', '8', '8', '8', '8', '1', '3', '9', '3', '7', '1', '2', '9', '9', '7', '8', '8', '1', '10', '7', '8', '8', '9', '8', '7', '9', '9', '12', '7', '2', '8', '1', '11', '11', '1', '7', '7', '12', '1', '9', '8', '10', '12', '7', '8', '2', '8', '9', '9', '1', '8', '9', '1', '7', '12', '10', '10', '10', '8', '3', '7', '9', '8', '9', '1', '8', '8', '2', '7', '2', '9', '9', '11', '8', '8', '12', '12', '7', '8', '12', '4', '9', '3', '1', '12', '1', '1', '8', '8', '3', '8', '8', '8', '8', '9', '1', '8', '8', '10', '1', '8', '2', '8', '8', '7', '3', '8', '2', '4', '4', '9', '8', '10', '12', '12', '1', '1', '9', '1', '1', '1', '8', '2', '2', '8', '1', '1', '2', '2', '1', '2', '8', '1', '9', '9', '8', '8', '4', '2', '9', '9', '8', '3', '4', '3', '1', '8', '8', '2', '1', '9', '7', '8', '8', '1', '12', '3', '8', '2', '4', '2', '9', '12', '1', '4', '1', '8', '8', '8', '2', '2', '8', '9', '8', '8', '8', '10', '9', '8', '7', '9', '1', '1', '9', '4', '2', '4', '2', '2', '1', '7', '8', '11', '11', '3', '9', '2', '4', '8', '9', '1', '8', '1', '1', '4', '9', '1', '1', '8', '8', '2', '1', '8', '4', '2', '8', '9', '8', '8', '2', '8', '8', '8', '7', '1', '1', '1', '2', '1', '1', '8', '7', '8', '8', '12', '2', '12', '12', '8', '10', '12', '8', '3', '3', '12', '10', '1', '8', '12', '1', '8', '8', '2', '8', '4', '7', '8', '7', '10', '8', '10', '9', '8', '12', '12', '1', '8', '8', '3', '8', '8', '8', '8', '8', '1', '9', '8', '11', '1', '1', '1', '9', '8', '1', '9', '2', '3', '11', '8', '9', '9', '9', '2', '1', '8', '8', '9', '7', '1', '4', '9', '4', '8', '8', '4', '8', '12', '9', '4', '8', '2', '10', '10', '10', '8', '9', '9', '8', '8', '12', '7', '1', '8', '8', '8', '4', '1', '1', '1', '1', '1', '1', '8', '1', '9', '8', '9', '9', '4', '8', '12', '9', '8', '8', '2', '8', '8', '8', '6', '9', '8', '3', '7', '8', '8', '4', '12', '8', '8', '9', '12', '12', '9', '8', '2', '9', '2', '3', '1', '12', '8', '10', '9', '9', '9', '10', '10', '3', '8', '12', '1', '4', '2', '1', '10', '8', '2', '8', '4', '8', '9', '1', '9', '9', '10', '10', '1', '4', '9', '2', '4', '9', '1', '1', '3', '10', '3', '3', '8', '7', '3', '8', '9', '9', '12', '4', '8', '12', '2', '2', '4', '1', '9', '9', '4', '1', '4', '2', '8', '12', '2', '3', '10', '10', '9', '8', '9', '9', '1', '12', '8', '8', '8', '12', '4', '1', '8', '8', '1', '9', '8', '8', '2', '1', '8', '9', '8', '3', '3', '3', '1', '8', '8', '9', '1', '10', '9', '9', '9', '9', '5', '9', '9', '8', '8', '8', '8', '8', '8', '8', '8', '11', '12', '8', '8', '1', '8', '9', '11', '2', '2', '2', '2', '3', '1', '2', '2', '8', '2', '4', '9', '1', '2', '9', '8', '2', '8', '9', '9', '3', '10', '9', '9', '2', '8', '9', '8', '12', '12', '1', '3', '8', '8', '8', '2', '7', '7', '7', '7', '3', '9', '1', '9', '8', '9', '9', '1', '1', '1', '2', '9', '9', '9', '11', '1', '8', '8', '9', '1', '9', '8', '8', '8', '1', '1', '8', '7', '1', '1', '8', '8', '9', '4', '4', '8', '2', '2', '8', '8', '8', '8', '8', '8', '11', '8', '2', '9', '4', '9', '3', '9', '9', '1', '3', '9', '3', '1', '12', '8', '9', '12', '1', '8', '4', '2', '1', '4', '8', '3', '3', '8', '2', '8', '9', '7', '8', '8', '8', '5', '8', '3', '9', '8', '8', '13', '12', '1', '1', '2', '8', '4', '1', '9', '9', '12', '8', '9', '12', '9', '1', '9', '9', '9', '9', '3', '2', '9', '9', '4', '8', '12', '2', '4', '9', '3', '1', '9', '7', '8', '9', '9', '8', '4', '8', '8', '7', '9', '10', '3', '8', '8', '8', '1', '1', '1', '1', '8', '8', '4', '1', '10', '1', '5', '7', '7', '1', '8', '9', '3', '7', '2', '7', '7', '2', '4', '8', '12', '7', '4', '2', '9', '9', '12', '6', '10', '8', '2', '4', '12', '9', '9', '3', '8', '8', '1', '2', '10', '9', '9', '8', '4', '12', '2', '1', '8', '8', '8', '12', '10', '10', '9', '3', '8', '8', '9', '2', '8', '10', '1', '1', '1', '1', '2', '1', '1', '1', '1', '9', '8', '12', '9', '4', '8', '8', '9', '1', '9', '3', '9', '8', '8', '1', '7', '7', '10', '1', '8', '8', '1', '9', '8', '10', '3', '1', '7', '1', '8', '8', '12', '8', '8', '1', '8', '7', '1', '7', '7', '8', '2', '1', '8', '8', '2', '10', '8', '8', '8', '8', '8', '10', '1', '8', '8', '12', '8', '3', '3', '2', '2', '2', '10', '8', '8', '8', '2', '9', '1', '8', '9', '3', '2', '8', '10', '8', '6', '1', '1', '8', '4', '1', '9', '10', '8', '1', '7', '1', '2', '8', '1', '1', '1', '12', '1', '9', '12', '8', '12', '12', '12', '8', '8', '12', '4', '8', '8', '8', '8', '9', '9', '1', '3', '3', '3', '3', '1', '12', '12', '9', '10', '8', '8', '1', '9', '2', '2', '13', '9', '8', '9', '2', '1', '9', '1', '8', '8', '8', '4', '8', '1', '1', '1', '12', '12', '7', '2', '2', '2', '8', '3', '8', '9', '2', '10', '7', '8', '9', '2', '1', '2', '12', '12', '8', '8', '9', '2', '2', '9', '11', '1', '8', '1', '10', '9', '2', '1', '4', '7', '7', '7', '7', '7', '12', '8', '8', '8', '1', '1', '10', '1', '12', '1', '8', '2', '1', '1', '12', '8', '7', '9', '12', '8', '9', '3', '9', '8', '8', '8', '8', '3', '11', '2', '2', '9', '8', '8', '10', '8', '2', '7', '3', '1', '4', '7', '8', '8', '1', '8', '3', '7', '12', '8', '10', '9', '9', '8', '8', '2', '8', '9', '9', '1', '2', '8', '8', '9', '8', '3', '8', '1', '8', '10', '9', '8', '9', '9', '12', '4', '4', '8', '9', '9', '8', '2', '10', '1', '2', '8', '9', '1', '9', '9', '9', '7', '12', '12', '8', '1', '1', '1', '1', '8', '3', '1', '1', '8', '1', '8', '8', '7', '8', '8', '8', '8', '3', '2', '2', '10', '10', '10', '7', '8', '1', '2', '12', '7', '9', '8', '7', '8', '12', '2', '8', '9', '2', '6', '6', '7', '9', '8', '1', '8', '9', '8', '1', '12', '1', '2', '8', '7', '7', '7', '8', '2', '2', '8', '1', '2', '2', '9', '9', '1', '8', '8', '4', '3', '3', '1', '6', '3', '3', '12', '3', '8', '9', '1', '4', '3', '1', '8', '3', '9', '2', '8', '2', '8', '8', '8', '1', '1', '1', '9', '9', '8', '1', '9', '8', '1', '1', '3', '10', '8', '1', '1', '3', '9', '1', '4', '4', '1', '8', '9', '9', '2', '0', '0', '1', '8', '3', '1', '8', '8', '9', '8', '8', '1', '1', '8', '9', '8', '8', '8', '7', '9', '8', '8', '8', '10', '9', '8', '1', '2', '6', '1', '9', '9', '8', '12', '12', '12', '8', '8', '2', '8', '1', '2', '2', '2', '1', '9', '8', '2', '12', '2', '8', '12', '8', '9', '8', '8', '9', '7', '1', '1', '1', '1', '1', '8', '8', '1', '8', '8', '1', '1', '3', '2', '8', '8', '9', '10', '10', '2', '2', '1', '9', '2', '9', '9', '4', '12', '12', '12', '10', '7', '3', '3', '4', '2', '2', '9', '2', '8', '4', '2', '4', '1', '10', '9', '7', '8', '7', '1', '1', '3', '3', '1', '1', '3', '3', '3', '1', '1', '1', '1', '8', '2', '3', '1', '1', '2', '8', '8', '12', '8', '8', '8', '8', '11', '9', '1', '8', '9', '2', '8', '8', '8', '3', '9', '1', '9', '2', '7', '2', '8', '2', '8', '10', '8', '1', '10', '1', '1', '9', '9', '8', '8', '1', '8', '8', '8', '12', '8', '8', '8', '1', '8', '8', '8', '1', '9', '1', '1', '8', '1', '8', '9', '8', '2', '12', '9', '9', '0', '1', '8', '8', '1', '8', '12', '8', '8', '10', '8', '8', '8', '7', '8', '1', '8', '7', '3', '10', '1', '8', '9', '1', '8', '8', '8', '10', '1', '10', '3', '9', '1', '8', '9', '2', '8', '3', '3', '9', '9', '7', '9', '1', '1', '9', '2', '1', '1', '1', '7', '1', '1', '8', '8', '1', '1', '8', '1', '8', '3', '12', '9', '3', '3', '8', '8', '8', '8', '3', '1', '3', '3', '1', '11', '0', '8', '8', '7', '8', '12', '1', '8', '9', '8', '9', '8', '8', '3', '8', '8', '1', '1', '1', '9', '2', '2', '2', '8', '7', '12', '8', '8', '9', '10', '10', '7', '8', '1', '9', '8', '7', '3', '1', '3', '8', '2', '2', '3', '9', '8', '4', '4', '8', '9', '2', '1', '1', '7', '8', '9', '9', '7', '8', '7', '7', '8', '2', '2', '8', '4', '9', '7', '10', '0', '9', '8', '3', '7', '8', '1', '1', '8', '9', '9', '2', '2', '10', '1', '9', '10', '10', '10', '8', '3', '2', '12', '9', '9', '10', '12', '9', '12', '12', '9', '1', '2', '4', '12', '12', '7', '8', '9', '7', '7', '7', '3', '9', '8', '9', '1', '12', '8', '9', '4', '1', '3', '12', '12', '12', '12', '8', '8', '2', '1', '1', '2', '1', '1', '1', '12', '12', '8', '12', '2', '2', '12', '3', '3', '12', '8', '2', '8', '8', '12', '2', '1', '10', '3', '2', '8', '7', '1', '8', '1', '3', '7', '8', '9', '8', '3', '1', '1', '7', '8', '8', '9', '8', '2', '9', '2', '2', '9', '8', '1', '8', '8', '1', '3', '3', '1', '1', '10', '1', '2', '8', '1', '1', '1', '1', '9', '1', '4', '1', '7', '7', '7', '7', '2', '2', '8', '8', '12', '1', '9', '1', '7', '3', '3', '1', '8', '8', '10', '8', '9', '2', '9', '1', '3', '8', '8', '3', '12', '2', '8', '12', '2', '9', '1', '3', '3', '3', '3', '2', '8', '7', '9', '8', '3', '3', '1', '7', '8', '3', '1', '1', '12', '8', '9', '1', '2', '3', '8', '1', '1', '3', '3', '9', '1', '1', '1', '12', '1', '7', '3', '3', '1', '8', '8', '8', '1', '2', '4', '8', '1', '10', '2', '5', '3', '3', '12', '10', '9', '9', '12', '9', '0', '2', '8', '8', '9', '9', '9', '8', '1', '3', '1', '1', '4', '8', '1', '10', '8', '7', '2', '8', '2', '8', '4', '7', '8', '1', '9', '1', '9', '8', '2', '8', '2', '7', '9', '2', '2', '9', '1', '8', '12', '1', '8', '1', '4', '1', '9', '9', '1', '10', '12', '4', '8', '1', '7', '3', '9', '2', '12', '7', '8', '8', '2', '1', '12', '9', '8', '1', '2', '2', '8', '10', '2', '1', '7', '7', '7', '12', '3', '3', '8', '3', '8', '8', '3', '9', '8', '9', '1', '1', '8', '7', '9', '3', '3', '8', '1', '0', '9', '9', '9', '1', '8', '9', '9', '10', '1', '8', '7', '8', '8', '8', '9', '9', '8', '9', '10', '4', '9', '3', '7', '12', '1', '9', '9', '8', '9', '1', '9', '3', '1', '8', '4', '12', '10', '9', '8', '7', '10', '8', '12', '12', '3', '10', '8', '12', '2', '1', '2', '3', '9', '8', '8', '7', '1', '1', '2', '2', '1', '2', '2', '7', '1', '3', '9', '9', '3', '8', '8', '8', '8', '8', '7', '8', '8', '10', '8', '1', '8', '2', '8', '2', '2', '2', '2', '2', '1', '7', '12', '10', '1', '2', '8', '1', '4', '7', '8', '1', '12', '8', '7', '9', '2', '2', '2', '8', '1', '8', '1', '1', '0', '2', '2', '6', '1', '8', '2', '1', '8', '1', '1', '2', '2', '3', '2', '2', '10', '10', '9', '1', '7', '7', '8', '8', '1', '12', '10', '12', '3', '8', '8', '8', '3', '8', '3', '10', '2', '2', '2', '1', '2', '2', '1', '0', '1', '8', '9', '1', '1', '1', '2', '2', '2', '2', '2', '1', '1', '1', '1', '12', '9', '9', '1', '10', '11', '12', '9', '1', '1', '8', '8', '1', '2', '12', '8', '3', '7', '2', '10', '9', '7', '8', '1', '2', '2', '7', '9', '12', '2', '2', '1', '8', '9', '2', '3', '2', '2', '2', '9', '9', '8', '9', '2', '3', '8', '8', '9', '7', '3', '10', '8', '8', '9', '1', '1', '2', '8', '1', '10', '8', '8', '9', '7', '1', '7', '12', '8', '1', '7', '2', '1', '3', '9', '2', '8', '8', '1', '2', '3', '9', '3', '8', '9', '7', '7', '1', '10', '2', '8', '8', '8', '1', '8', '2', '8', '8', '1', '2', '8', '10', '2', '1', '0', '8', '3', '8', '10', '10', '12', '3', '8', '9', '3', '10', '8', '8', '8', '1', '8', '8', '2', '2', '2', '2', '2', '2', '1', '1', '10', '1', '10', '8', '2', '1', '2', '2', '8', '3', '3', '2', '2', '1', '8', '1', '3', '1', '9', '2', '2', '1', '3', '2', '2', '3', '2', '2', '3', '2', '1', '1', '2', '1', '2', '7', '8', '2', '1', '12', '12', '3', '1', '9', '9', '7', '8', '7', '2', '7', '9', '2', '2', '2', '8', '9', '7', '9', '2', '8', '9', '9', '2', '9', '1', '1', '9', '8', '3', '3', '8', '2', '7', '8', '11', '2', '8', '9', '1', '8', '3', '1', '2', '1', '2', '2', '2', '2', '8', '3', '9', '2', '10', '9', '9', '2', '7', '7', '7', '1', '3', '8', '8', '1', '1', '8', '7', '1', '8', '8', '9', '3', '3', '3', '2', '4', '8', '12', '8', '1', '12', '9', '2', '8', '2', '12', '2', '1', '3', '8', '12', '12', '11', '8', '10', '9', '2', '9', '4', '10', '1', '12', '12', '9', '3', '8', '9', '12', '9', '9', '2', '2', '2', '3', '5', '1', '1', '1', '7', '7', '8', '2', '2', '8', '2', '1', '7', '3', '2', '9', '8', '9', '1', '1', '9', '8', '9', '3', '3', '2', '9', '4', '9', '8', '8', '1', '8', '10', '2', '0', '3', '8', '8', '8', '9', '12', '9', '8', '2', '2', '8', '10', '9', '8', '2', '2', '4', '1', '9', '9', '8', '8', '10', '3', '1', '1', '2', '8', '7', '8', '2', '1', '1', '8', '12', '2', '9', '4', '8', '3', '3', '3', '8', '2', '12', '12', '8', '9', '12', '8', '2', '12', '7', '9', '2', '2', '1', '3', '1', '3', '11', '9', '8', '1', '10', '2', '12', '3', '8', '4', '1', '2', '8', '8', '3', '9', '3', '8', '7', '1', '1', '3', '1', '8', '2', '9', '8', '12', '1', '1', '1', '9', '9', '1', '8', '8', '8', '2', '2', '12', '3', '1', '1', '1', '9', '9', '3', '2', '1', '1', '9', '9', '1', '9', '9', '1', '9', '3', '9', '1', '8', '1', '9', '8', '3', '8', '8', '9', '9', '8', '2', '8', '1', '2', '2', '10', '1', '2', '1', '1', '9', '1', '9', '3', '7', '7', '2', '10', '3', '3', '1', '1', '9', '1', '2', '1', '3', '2', '1', '1', '1', '1', '1', '1', '1', '1', '1', '2', '8', '8', '7', '1', '1', '8', '8', '2', '1', '12', '1', '8', '1', '9', '8', '8', '2', '1', '9', '9', '2', '2', '8', '1', '7', '7', '9', '8', '1', '6', '8', '8', '2', '2', '8', '3', '8', '3', '2', '3', '8', '1', '2', '2', '2', '8', '9', '9', '9', '12', '1', '9', '2', '1', '9', '2', '9', '9', '10', '8', '1', '1', '9', '2', '1', '7', '1', '3', '7', '1', '1', '1', '3', '8', '8', '8', '9', '9', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '1', '0', '1', '1', '1', '2', '2', '3', '1', '1', '2', '3', '3', '3', '7', '3', '1', '9', '3', '9', '1', '4', '1', '1', '9', '10', '9', '8', '2', '3', '10', '9', '3', '10', '12', '8', '9', '8', '2', '1', '9', '8', '1', '7', '8', '1', '9', '1', '2', '10', '7', '7', '8', '1', '3', '3', '9', '3', '3', '1', '1', '1', '9', '1', '3', '8', '8', '8', '1', '8', '1', '9', '7', '8', '8', '2', '1', '3', '8', '2', '1', '6', '1', '8', '8', '2', '8', '2', '2', '9', '8', '1', '1', '6', '8', '6', '10', '2', '1', '1', '8', '9', '3', '3', '3', '1', '3', '9', '6', '8', '1', '8', '3', '1', '2', '2', '12', '7', '1', '1', '1', '1', '12', '1', '8', '8', '3', '2', '1', '9', '2', '3', '3', '2', '2', '7', '2', '2', '2', '3', '1', '9', '8', '7', '1', '1', '3', '1', '9', '4', '3', '9', '2', '1', '1', '9', '8', '8', '7', '2', '1', '3', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '2', '10', '1', '1', '8', '2', '8', '1', '8', '1', '3', '1', '2', '1', '1', '3', '1', '2', '1', '1', '1', '1', '1', '1', '1', '2', '1', '8', '1', '9', '8', '8', '1', '10', '8', '8', '1', '3', '9', '8', '9', '8', '2', '1', '3', '4', '1', '12', '1', '9', '8', '2', '7', '2', '1', '2', '9', '2', '1', '3', '8', '1', '2', '3', '8', '2', '10', '10', '10', '9', '9', '9', '3', '8', '3', '1', '1', '1', '1', '1', '1', '2', '1', '9', '2', '7', '9', '8', '7', '9', '1', '1', '1', '1', '8', '2', '2', '3', '3', '2', '12', '8', '3', '1', '1', '12', '9', '8', '2', '2', '9', '1', '2', '2', '9', '1', '1', '2', '4', '8', '1', '8', '10', '2', '9', '9', '9', '1', '2', '1', '1', '2', '2', '2', '12', '1', '2', '8', '3', '9', '9', '3', '2', '9', '9', '7', '2', '8', '8', '9', '2', '9', '1', '1', '1', '1', '1', '2', '8', '2', '1', '2', '2', '8', '3', '1', '1', '9', '1', '3', '10', '7', '9', '8', '8', '2', '2', '9', '2', '2', '2', '2', '2', '2', '9', '1', '3', '6', '4', '2', '2', '9', '1', '9', '8', '1', '1', '1', '7', '3', '2', '9', '9', '1', '9', '7', '2', '8', '9', '9', '12', '10', '8', '10', '2', '3', '1', '2', '2', '12', '12', '3', '12', '1', '2', '1', '2', '1', '2', '8', '2', '3', '12', '12', '8', '2', '2', '9', '2', '9', '2', '3', '3', '1', '1', '1', '12', '3', '7', '1', '3', '1', '2', '9', '2', '7', '2', '1', '8', '3', '7', '8', '1', '1', '3', '8', '3', '2', '9', '3', '1', '1', '1', '1', '11', '1', '2', '6', '1', '2', '3', '8', '9', '9', '2', '9', '2', '1', '1', '3', '1', '12', '9', '1', '1', '9', '1', '9', '9', '8', '9', '8', '1', '2', '7', '1', '0', '9', '8', '2', '3', '2', '4', '1', '1', '1', '8', '1', '12', '1', '2', '3', '3', '9', '9', '9', '9', '9', '7', '9', '3', '9', '1', '7', '3', '3', '3', '3', '7', '8', '2', '2', '2', '8', '2', '3', '9', '9', '3', '1', '8', '9', '12', '8', '8', '3', '2', '1', '6', '1', '9', '1', '1', '1', '1', '1', '1', '2', '2', '1', '2', '2', '2', '2', '9', '2', '5', '3', '2', '2', '8', '2', '1', '8', '3', '3', '8', '3', '1', '3', '9', '2', '8', '9', '4', '4', '9', '8', '8', '7', '7', '3', '1', '8', '8', '3', '2', '2', '10', '3', '3', '2', '2', '2', '2', '12', '3', '1', '2', '7', '7', '12', '2', '1', '2', '1', '1', '2', '1', '3', '3', '3', '1', '3', '9', '4', '8', '8', '4', '2', '4', '2', '8', '9', '1', '1', '2', '2', '2', '8', '7', '2', '8', '2', '9', '1', '8', '1', '1', '2', '1', '1', '1', '2', '9', '9', '8', '1', '8', '1', '4', '8', '1', '2', '7', '9', '2', '1', '8', '9', '8', '9', '1', '1', '2', '3', '2', '12', '7', '1', '2', '4', '4', '4', '2', '2', '2', '9', '8', '8', '1', '12', '3', '3', '2', '2', '3', '12', '1', '5', '1', '1', '3', '2', '8', '8', '2', '2', '2', '3', '9', '8', '8', '9', '8', '8', '11', '8', '2', '9', '2', '3', '1', '7', '1', '8', '2', '8', '1', '1', '1', '8', '1', '1', '1', '7', '8', '8', '2', '3', '2', '1', '8', '8', '1', '8', '4', '8', '9', '8', '9', '8', '1', '2', '3', '2', '1', '2', '2', '3', '1', '9', '4', '8', '1', '1', '9', '1', '1', '8', '2', '2', '2', '1', '7', '3', '3', '1', '3', '3', '12', '3', '1', '1', '3', '1', '1', '4', '8', '4', '4', '3', '3', '3', '4', '8', '1', '9', '9', '9', '4', '1', '7', '4', '8', '4', '9', '8', '1', '1', '3', '8', '9', '1', '9', '7', '1', '1', '9', '1', '8', '8', '8', '1', '1', '8', '2', '2', '2', '9', '4', '4', '1', '2', '1', '2', '9', '1', '1', '8', '5', '5', '5', '9', '12', '4', '1', '2', '1', '8', '8', '1', '9', '9', '2', '12', '1', '1', '7', '8', '8', '4', '1', '2', '8', '11', '2', '3', '2', '2', '1', '11', '2', '2', '2', '2', '9', '9', '2', '2', '9', '3', '1', '9', '1', '1', '1', '7', '8', '1', '8', '9', '1', '2', '9', '8', '1', '2', '9', '8', '12', '2', '2', '10', '8', '2', '2', '8', '8', '1', '2', '6', '1', '1', '7', '7', '2', '3', '2', '1', '1', '8', '1', '10', '8', '12', '6', '4', '1', '4', '1', '2', '9', '8', '9', '4', '8', '8', '10', '8', '8', '8', '8', '8', '2', '2', '8', '4', '9', '3', '3', '3', '3', '3', '9', '2', '1', '1', '2', '3', '2', '1', '2', '3', '3', '3', '2', '2', '3', '3', '2', '1', '2', '3', '3', '3', '3', '12', '9', '3', '2', '2', '3', '1', '5', '1', '1', '3', '2', '2', '1', '2', '9', '3', '9', '8', '1', '8', '7', '7', '9', '1', '1', '7', '9', '10', '1', '7', '1', '4', '3', '8', '9', '4', '9', '9', '2', '9', '2', '8', '4', '2', '2', '9', '5', '2', '1', '10', '9', '3', '1', '1', '1', '2', '1', '2', '9', '1', '7', '9', '9', '8', '2', '2', '8', '9', '1', '9', '8', '2', '3', '2', '2', '1', '2', '2', '2', '1', '1', '8', '1', '9', '4', '12', '8', '2', '7', '2', '2', '1', '9', '3', '1', '8', '12', '1', '1', '2', '1', '10', '2', '6', '12', '12', '4', '9', '1', '9', '10', '1', '2', '8', '2', '1', '8', '6', '12', '9', '9', '2', '1', '7', '8', '9', '5', '8', '1', '8', '8', '3', '1', '8', '2', '8', '2', '2', '1', '2', '9', '2', '8', '1', '12', '8', '2', '7', '3', '3', '12', '2', '3', '3', '9', '9', '3', '3', '3', '3', '3', '2', '1', '4', '1', '4', '8', '8', '2', '1', '2', '9', '12', '1', '10', '12', '1', '2', '9', '4', '8', '10', '3', '2', '8', '7', '4', '8', '9', '2', '7', '1', '10', '2', '2', '2', '8', '4', '2', '2', '1', '1', '8', '7', '9', '2', '1', '9', '8', '9', '12', '4', '8', '2', '8', '7', '7', '1', '9', '2', '1', '1', '9', '1', '8', '2', '3', '9', '9', '1', '3', '2', '9', '2', '2', '8', '1', '1', '2', '1', '2', '1', '9', '8', '2', '2', '4', '8', '8', '9', '5', '5', '9', '8', '9', '6', '2', '1', '12', '2', '3', '8', '8', '9', '2', '2', '1', '3', '2', '10', '9', '10', '7', '2', '8', '4', '2', '8', '9', '8', '1', '3', '8', '8', '1', '8', '3', '8', '8', '1', '10', '3', '1', '5', '8', '8', '9', '2', '2', '9', '2', '1', '9', '1', '4', '1', '12', '8', '8', '2', '1', '1', '1', '1', '1', '9', '2', '9', '8', '9', '5', '9', '1', '8', '2', '4', '1', '1', '1', '9', '12', '4', '1', '1', '9', '8', '4', '1', '8', '9', '8', '8', '9', '9', '9', '9', '9', '9', '1', '3', '4', '2', '2', '8', '2', '9', '2', '3', '8', '3', '1', '7', '2', '1', '1', '4', '2', '9', '2', '1', '3', '2', '9', '2', '9', '3', '8', '1', '9', '1', '1', '2', '8', '8', '8', '2', '3', '9', '9', '8', '2', '4', '1', '5', '1', '1', '9', '9', '2', '1', '1', '1', '9', '9', '1', '3', '9', '2', '2', '9', '9', '8', '3', '9', '9', '8', '2', '2', '8', '9', '2', '2', '8', '9', '10', '10', '2', '8', '7', '9', '9', '4', '2', '4', '11', '2', '1', '7', '8', '8', '8', '9', '4', '9', '8', '8', '2', '1', '1', '9', '8', '3', '3', '8', '8', '7', '9', '1', '3', '4', '8', '10', '2', '4', '4', '8', '2', '8', '2', '8', '9', '2', '1', '2', '2', '3', '1', '9', '4', '5', '9', '9', '1', '1', '1', '1', '1', '1', '7', '1', '1', '1', '8', '1', '1', '1', '1', '1', '9', '1', '2', '1', '12', '9', '3', '9', '9', '4', '1', '9', '1', '1', '2', '11', '9', '3', '2', '2', '2', '4', '2', '3', '2', '7', '7', '8', '1', '10', '9', '5', '7', '1', '10', '8', '1', '2', '7', '1', '12', '5', '8', '4', '2', '9', '9', '8', '2', '9', '8', '8', '10', '2', '3', '2', '7', '2', '8', '10', '3', '2', '9', '9', '9', '1', '1', '1', '1', '8', '2', '10', '9', '8', '1', '2', '2', '2', '1', '2', '1', '3', '9', '12', '2', '2', '4', '8', '1', '3', '9', '4', '9', '1', '9', '9', '2', '1', '1', '9', '7', '10', '3', '10', '2', '9', '2', '9', '2', '8', '2', '1', '2', '3', '1', '1', '1', '8', '5', '8', '3', '1', '1', '4', '2', '2', '1', '3', '8', '2', '1', '1', '1', '2', '9', '4', '1', '8', '1', '2', '8', '2', '9', '2', '2', '5', '5', '2', '5', '2', '1', '8', '8', '1', '3', '12', '2', '4', '3', '2', '2', '2', '6', '2', '3', '3', '1', '8', '2', '2', '1', '1', '8', '8', '10', '1', '9', '2', '9', '12', '0', '2', '1', '3', '2', '2', '1', '2', '1', '9', '9', '2', '9', '8', '8', '1', '7', '8', '1', '2', '6', '9', '8', '10', '1', '12', '2', '2', '1', '8', '2', '1', '12', '7', '4', '2', '10', '2', '2', '1', '2', '1', '2', '1', '1', '2', '1', '9', '8', '10', '2', '10', '9', '9', '12', '5', '3', '2', '8', '2', '8', '3', '3', '1', '4', '9', '10', '2', '4', '8', '1', '2', '10', '12', '12', '3', '1', '1', '2', '9', '6', '6', '1', '9', '1', '8', '1', '8', '8', '8', '8', '2', '2', '2', '8', '8', '2', '1', '1', '1', '1', '1', '8', '8', '5', '8', '8', '1', '8', '9', '9', '1', '7', '12', '9', '9', '7', '2', '2', '7', '8', '9', '8', '2', '9', '6', '3', '4', '8', '4', '1', '2', '9', '8', '8', '8', '2', '1', '1', '8', '8', '3', '1', '9', '9', '7', '1', '2', '2', '2', '9', '4', '1', '2', '12', '1', '8', '2', '1', '9', '1', '8', '9', '9', '2', '5', '4', '2', '1', '2', '8', '12', '8', '10', '3', '8', '8', '8', '2', '2', '8', '2', '8', '2', '4', '8', '10', '2', '7', '2', '1', '8', '8', '9', '5', '8', '1', '12', '7', '10', '2', '2', '2', '1', '4', '8', '12', '1', '8', '2', '2', '9', '1', '8', '3', '1', '1', '5', '10', '2', '1', '2', '4', '8', '9', '3', '2', '8', '7', '4', '4', '9', '9', '2', '8', '1', '1', '2', '4', '7', '1', '9', '1', '1', '1', '8', '4', '1', '3', '1', '2', '2', '9', '1', '8', '8', '2', '2', '9', '9', '1', '8', '8', '1', '9', '8', '8', '2', '2', '2', '2', '1', '1', '1', '7', '4', '1', '1', '4', '2', '3', '3', '2', '3', '9', '2', '8', '2', '1', '5', '1', '2', '2', '1', '3', '5', '2', '9', '1', '1', '9', '8', '1', '4', '8', '1', '8', '2', '4', '8', '4', '7', '9', '8', '1', '2', '2', '4', '4', '11', '3', '11', '1', '3', '8', '1', '9', '6', '8', '9', '3', '9', '8', '9', '2', '3', '3', '7', '1', '7', '2', '9', '9', '1', '8', '8', '1', '5', '5', '8', '8', '1', '2', '1', '3', '9', '9', '5', '9', '8', '1', '4', '3', '8', '1', '1', '2', '1', '8', '9', '8', '2', '2', '2', '1', '2', '4', '10', '8', '1', '9', '1', '8', '1', '1', '9', '9', '8', '11', '1', '8', '2', '1', '2', '8', '2', '8', '8', '2', '9', '6', '10', '1', '8', '6', '4', '5', '11', '2', '2', '8', '1', '8', '1', '8', '1', '4', '8', '1', '1', '1', '8', '3', '7', '3', '3', '7', '1', '1', '4', '8', '1', '6', '4', '2', '2', '1', '1', '1', '1', '6', '2', '2', '8', '1', '8', '5', '5', '2', '1', '1', '2', '3', '7', '1', '1', '6', '2', '9', '3', '9', '8', '9', '2', '1', '4', '2', '9', '13', '9', '2', '8', '12', '4', '1', '1', '9', '5', '1', '1', '1', '1', '8', '2', '1', '1', '2', '9', '0', '12', '1', '1', '8', '9', '8', '3', '2', '2', '12', '8', '2', '2', '2', '1', '10', '1', '9', '8', '9', '5', '2', '2', '9', '2', '8', '4', '8', '3', '7', '9', '2', '2', '7', '2', '2', '8', '1', '1', '9', '10', '9', '8', '2', '9', '0', '7', '1', '1', '1', '9', '8', '2', '8', '12', '8', '3', '2', '8', '8', '7', '7', '8', '12', '2', '1', '3', '9', '2', '2', '8', '2', '8', '8', '12', '8', '8', '1', '8', '9', '2', '4', '4', '7', '2', '1', '1', '9', '3', '7', '1', '3', '9', '8', '2', '2', '3', '1', '3', '10', '2', '3', '7', '1', '8', '1', '1', '10', '3', '10', '8', '4', '1', '2', '4', '3', '10', '9', '3', '9', '5', '8', '7', '2', '9', '3', '3', '2', '11', '8', '1', '1', '7', '2', '7', '2', '9', '4', '8', '1', '12', '8', '7', '9', '1', '0', '2', '6', '4', '9', '3', '2', '4', '12', '9', '8', '5', '9', '2', '8', '4', '6', '8', '9', '3', '1', '8', '7', '3', '1', '1', '1', '8', '9', '1', '8', '7', '9', '8', '2', '1', '2', '2', '3', '2', '2', '1', '1', '4', '9', '7', '3', '2', '8', '8', '9', '1', '9', '7', '4', '2', '2', '8', '5', '9', '1', '5', '7', '1', '4', '6', '9', '10', '1', '8', '2', '9', '1', '9', '2', '9', '8', '2', '9', '9', '11', '2', '12', '2', '2', '9', '4', '8', '1', '9', '9', '2', '8', '9', '1', '2', '9', '3', '8', '8', '1', '7', '8', '8', '2', '3', '2', '2', '2', '2', '9', '8', '10', '2', '2', '1', '9', '11', '1', '8', '8', '2', '4', '2', '1', '2', '8', '8', '9', '2', '8', '9', '2', '3', '1', '2', '1', '3', '10', '8', '1', '2', '4', '1', '9', '8', '9', '3', '3', '9', '8', '2', '11', '3', '7', '1', '8', '7', '1', '8', '8', '8', '1', '4', '8', '8', '1', '9', '3', '1', '1', '2', '8', '8', '8', '2', '1', '8', '10', '2', '12', '8', '8', '9', '1', '8', '1', '3', '2', '2', '10', '7', '2', '2', '1', '1', '8', '2', '9', '1', '2', '9', '3', '3', '10', '1', '4', '7', '10', '12', '2', '4', '10', '8', '6', '2', '9', '1', '8', '3', '1', '2', '8', '8', '2', '12', '10', '2', '10', '9', '1', '2', '5', '8', '12', '8', '8', '7', '8', '8', '1', '8', '2', '2', '9', '1', '8', '10', '5', '5', '5', '8', '1', '9', '1', '13', '1', '2', '2', '1', '2', '4', '9', '2', '1', '9', '13', '11', '1', '9', '9', '3', '10', '8', '7', '8', '8', '4', '2', '1', '9', '4', '3', '1', '1', '10', '9', '2', '8', '6', '2', '1', '1', '3', '2', '8', '1', '1', '1', '1', '2', '4', '1', '1', '10', '1', '8', '1', '1', '9', '1', '7', '1', '1', '9', '9', '2', '12', '9', '1', '8', '1', '12', '1', '8', '3', '8', '1', '8', '1', '2', '10', '1', '1', '2', '2', '9', '3', '9', '12', '1', '9', '1', '2', '8', '7', '2', '1', '1', '3', '2', '8', '4', '4', '5', '12', '7', '8', '9', '6', '1', '8', '9', '2', '1', '9', '1', '4', '1', '8', '2', '1', '1', '8', '7', '4', '9', '1', '8', '4', '2', '7', '5', '1', '3', '9', '2', '8', '2', '1', '2', '9', '1', '1', '2', '3', '4', '3', '1', '2', '2', '10', '2', '2', '8', '1', '1', '2', '4', '9', '2', '11', '9', '8', '9', '2', '1', '10', '4', '1', '1', '10', '7', '2', '10', '8', '1', '10', '8', '8', '8', '8', '8', '8', '2', '2', '2', '8', '3', '4', '1', '1', '1', '2', '1', '8', '8', '2', '8', '3', '8', '3', '1', '2', '3', '1', '1', '10', '1', '4', '4', '3', '3', '1', '9', '1', '1', '2', '1', '1', '6', '1', '1', '1', '9', '1', '4', '1', '1', '1', '1', '1', '1', '2', '12', '8', '8', '1', '9', '12', '10', '8', '2', '12', '1', '1', '2', '1', '7', '10', '7', '1', '2', '1', '10', '8', '2', '2', '8', '2', '2', '1', '9', '8', '8', '8', '8', '3', '1', '4', '2', '8', '3', '10', '10', '1', '9', '1', '9', '1', '9', '1', '8', '3', '1', '1', '8', '8', '8', '6', '3', '4', '1', '1', '1', '9', '9', '5', '2', '10', '2', '6', '2', '2', '7', '1', '8', '2', '11', '1', '1', '1', '2', '1', '9', '4', '4', '2', '8', '4', '6', '8', '8', '10', '10', '2', '1', '2', '9', '1', '8', '8', '3', '4', '8', '8', '2', '8', '8', '1', '2', '1', '2', '9', '4', '1', '3', '3', '3', '8', '8', '8', '2', '6', '4', '3', '3', '12', '3', '4', '9', '6', '8', '7', '7', '8', '6', '4', '1', '10', '6', '3', '3', '8', '1', '1', '8', '1', '11', '8', '1', '1', '2', '3', '11', '7', '2', '1', '1', '1', '9', '1', '2', '1', '1', '9', '8', '1', '1', '8', '0', '4', '1', '1', '2', '1', '1', '2', '2', '9', '1', '10', '1', '3', '8', '9', '3', '3', '1', '10', '1', '9', '1', '1', '7', '4', '1', '8', '10', '1', '2', '10', '1', '1', '2', '3', '3', '9', '1', '8', '10', '1', '7', '9', '1', '9', '6', '3', '9', '2', '8', '4', '12', '12', '3', '1', '1', '9', '1', '9', '2', '1', '1', '1', '1', '1', '9', '2', '1', '2', '10', '9', '8', '8', '10', '8', '2', '3', '2', '1', '12', '8', '12', '2', '9', '1', '3', '2', '5', '2', '2', '8', '2', '2', '4', '10', '9', '10', '9', '4', '2', '4', '12', '8', '2', '1', '8', '3', '9', '2', '9', '4', '1', '1', '1', '1', '1', '9', '6', '2', '9', '2', '1', '3', '2', '2', '5', '8', '9', '1', '6', '3', '1', '2', '2', '2', '6', '1', '8', '10', '8', '3', '3', '13', '2', '9', '9', '10', '6', '12', '8', '2', '9', '2', '3', '8', '8', '1', '2', '2', '3', '2', '10', '1', '8', '9', '1', '8', '9', '8', '8', '2', '10', '2', '1', '1', '1', '9', '9', '12', '1', '1', '4', '8', '3', '2', '2', '4', '4', '2', '2', '1', '1', '1', '9', '2', '2', '4', '1', '1', '8', '8', '8', '9', '2', '13', '9', '9', '4', '1', '9', '7', '10', '10', '8', '1', '9', '1', '1', '1', '8', '12', '4', '1', '2', '1', '1', '1', '7', '8', '3', '3', '1', '2', '2', '7', '2', '2', '10', '1', '8', '4', '0', '1', '9', '4', '1', '1', '2', '10', '10', '7', '9', '3', '12', '11', '2', '8', '10', '8', '9', '4', '3', '4', '10', '1', '1', '9', '3', '10', '1', '5', '3', '3', '2', '2', '1', '1', '6', '1', '7', '1', '1', '12', '1', '1', '1', '10', '1', '8', '8', '1', '3', '8', '3', '1', '2', '8', '3', '4', '2', '8', '8', '6', '1', '1', '4', '1', '2', '9', '5', '1', '7', '8', '9', '2', '4', '9', '8', '5', '9', '9', '9', '1', '2', '1', '3', '8', '9', '2', '9', '8', '9', '1', '8', '13', '9', '3', '8', '1', '4', '1', '3', '1', '6', '4', '10', '2', '2', '7', '8', '8', '12', '8', '9', '9', '9', '10', '3', '12', '12', '2', '9', '10', '9', '7', '2', '3', '4', '10', '10', '2', '7', '7', '1', '7', '3', '9', '1', '9', '8', '2', '9', '1', '8', '1', '10', '8', '8', '2', '7', '8', '1', '8', '9', '8', '1', '4', '1', '8', '8', '8', '1', '1', '10', '3', '2', '2', '6', '8', '9', '8', '2', '1', '1', '3', '8', '8', '1', '4', '4', '3', '9', '10', '4', '2', '4', '2', '9', '2', '2', '9', '1', '1', '10', '1', '1', '2', '1', '1', '1', '9', '1', '8', '8', '8', '2', '3', '8', '1', '6', '7', '3', '1', '9', '1', '2', '1', '6', '1', '2', '10', '1', '4', '2', '3', '2', '9', '9', '4', '9', '9', '1', '2', '8', '4', '7', '8', '7', '13', '9', '1', '2', '1', '9', '3', '3', '6', '8', '9', '10', '9', '2', '2', '3', '12', '1', '1', '1', '13', '2', '1', '7', '10', '8', '9', '1', '2', '9', '1', '1', '1', '2', '1', '12', '5', '6', '9', '10', '3', '1', '10', '1', '3', '2', '8', '9', '2', '10', '8', '8', '9', '2', '8', '9', '2', '2', '2', '8', '8', '2', '3', '1', '4', '8', '1', '9', '9', '2', '8', '10', '9', '1', '2', '12', '8', '8', '0', '2', '1', '1', '9', '10', '1', '6', '2', '2', '10', '2', '2', '4', '2', '2', '12', '1', '6', '7', '8', '9', '1', '3', '5', '7', '3', '1', '3', '1', '1', '3', '2', '6', '3', '2', '1', '8', '3', '5', '2', '1', '1', '8', '1', '1', '0', '2', '3', '5', '3', '1', '1', '9', '8', '8', '8', '8', '4', '7', '2', '9', '12', '5', '9', '3', '12', '3', '9', '1', '9', '1', '7', '3', '9', '3', '10', '1', '2', '1', '4', '9', '8', '2', '9', '11', '2', '8', '9', '4', '1', '1', '1', '1', '1', '1', '1', '9', '1', '9', '1', '1', '1', '12', '3', '10', '3', '6', '1', '7', '10', '8', '3', '7', '1', '1', '2', '6', '1', '3', '9', '2', '8', '1', '8', '8', '1', '8', '10', '12', '1', '10', '12', '10', '1', '8', '8', '1', '4', '7', '9', '2', '1', '9', '3', '8', '12', '10', '6', '1', '6', '8', '3', '1', '3', '1', '10', '10', '9', '8', '1', '2', '9', '8', '8', '1', '9', '8', '8', '3', '1', '3', '9', '12', '1', '8', '1', '5', '8', '11', '5', '5', '1', '2', '1', '3', '1', '1', '1', '9', '8', '8', '1', '9', '10', '8', '4', '1', '1', '8', '10', '8', '9', '1', '10', '1', '7', '2', '1', '1', '9', '6', '2', '9', '8', '1', '1', '1', '9', '8', '8', '1', '9', '9', '1', '1', '2', '7', '9', '2', '9', '9', '8', '7', '6', '8', '7', '8', '2', '9', '2', '1', '8', '8', '8', '2', '6', '3', '1', '8', '12', '12', '8', '7', '1', '2', '2', '7', '1', '9', '8', '1', '4', '1', '1', '8', '5', '8', '1', '8', '8', '8', '8', '1', '2', '11', '1', '1', '1', '2', '7', '1', '9', '2', '2', '2', '9', '8', '9', '2', '9', '1', '8', '1', '7', '11', '13', '1', '9', '8', '2', '2', '1', '2', '3', '10', '3', '10', '4', '3', '9', '1', '8', '1', '1', '13', '1', '1', '3', '8', '1', '2', '2', '2', '9', '4', '1', '8', '1', '9', '3', '6', '8', '5', '2', '8', '2', '8', '1', '9', '2', '2', '8', '1', '1', '4', '8', '9', '2', '2', '7', '1', '8', '12', '2', '12', '8', '2', '9', '9', '3', '8', '1', '1', '9', '9', '9', '2', '8', '1', '9', '1', '8', '9', '8', '2', '2', '4', '1', '9', '10', '10', '8', '8', '1', '1', '2', '1', '4', '1', '2', '1', '1', '3', '12', '8', '8', '8', '8', '10', '2', '8', '12', '1', '9', '9', '8', '9', '1', '2', '1', '8', '8', '8', '2', '8', '8', '3', '10', '8', '9', '1', '1', '1', '3', '4', '8', '10', '6', '3', '1', '3', '2', '8', '9', '3', '5', '4', '1', '5', '2', '9', '9', '9', '1', '1', '1', '9', '9', '6', '10', '9', '8', '12', '2', '9', '1', '2', '1', '1', '1', '8', '1', '12', '1', '12', '1', '9', '1', '8', '1', '2', '1', '2', '8', '8', '10', '1', '9', '1', '2', '2', '8', '8', '3', '12', '1', '11', '2', '10', '12', '11', '2', '1', '10', '1', '1', '9', '3', '10', '2', '7', '1', '8', '1', '9', '9', '2', '1', '3', '12', '5', '9', '9', '8', '1', '8', '8', '8', '1', '1', '3', '9', '8', '9', '8', '3', '10', '3', '7', '8', '9', '2', '3', '1', '9', '10', '9', '2', '8', '2', '6', '4', '1', '1', '3', '8', '2', '3', '9', '1', '2', '1', '8', '1', '2', '2', '2', '2', '9', '9', '4', '8', '9', '1', '1', '2', '9', '1', '5', '2', '9', '8', '10', '1', '2', '10', '8', '5', '6', '4', '8', '1', '1', '1', '8', '9', '2', '8', '8', '8', '12', '2', '2', '1', '8', '1', '2', '1', '9', '8', '8', '9', '1', '1', '8', '7', '1', '8', '0', '2', '3', '8', '1', '1', '8', '1', '1', '12', '3', '2', '10', '8', '10', '6', '1', '8', '8', '10', '8', '8', '1', '4', '8', '1', '2', '9', '8', '1', '1', '3', '3', '5', '1', '1', '2', '1', '2', '1', '9', '8', '3', '9', '8', '2', '10', '9', '1', '8', '1', '10', '9', '10', '1', '2', '3', '8', '1', '3', '1', '9', '9', '8', '1', '9', '8', '8', '9', '1', '9', '2', '8', '8', '2', '13', '8', '10', '8', '9', '3', '2', '1', '3', '1', '12', '10', '10', '1', '11', '1', '2', '4', '2', '10', '1', '8', '11', '1', '1', '1', '2', '2', '8', '2', '9', '8', '9', '9', '12', '8', '1', '9', '2', '4', '1', '8', '3', '9', '1', '6', '5', '8', '9', '3', '3', '2', '1', '1', '1', '8', '11', '1', '8', '7', '1', '11', '9', '1', '7', '9', '9', '8', '8', '8', '12', '8', '1', '2', '9', '8', '8', '2', '10', '9', '8', '8', '4', '10', '9', '8', '2', '10', '10', '2', '1', '2', '1', '2', '1', '4', '1', '7', '1', '1', '3', '12', '9', '8', '5', '9', '10', '1', '9', '9', '2', '7', '4', '1', '1', '8', '12', '8', '2', '2', '9', '4', '1', '1', '5', '9', '1', '8', '12', '8', '1', '1', '4', '9', '10', '2', '3', '1', '3', '3', '3', '8', '1', '2', '1', '2', '1', '2', '9', '1', '2', '8', '9', '4', '8', '8', '10', '2', '12', '5', '5', '1', '8', '1', '7', '8', '1', '1', '8', '9', '12', '4', '9', '3', '1', '2', '5', '9', '2', '1', '2', '2', '1', '2', '8', '1', '8', '8', '2', '2', '1', '8', '4', '1', '2', '10', '9', '10', '8', '1', '10', '4', '8', '8', '10', '2', '8', '1', '1', '3', '10', '1', '4', '2', '8', '3', '3', '2', '9', '8', '5', '5', '9', '3', '10', '8', '1', '2', '1', '2', '9', '1', '1', '2', '9', '9', '8', '8', '11', '9', '4', '1', '8', '2', '7', '1', '2', '8', '2', '9', '9', '8', '2', '9', '9', '8', '8', '1', '2', '8', '1', '1', '2', '9', '8', '1', '4', '9', '1', '8', '12', '1', '12', '2', '8', '2', '9', '1', '12', '1', '2', '9', '1', '1', '9', '3', '2', '9', '9', '2', '11', '8', '7', '9', '9', '9', '8', '8', '1', '2', '8', '1', '1', '4', '1', '2', '10', '1', '9', '2', '1', '1', '1', '9', '1', '13', '4', '3', '2', '1', '1', '2', '2', '7', '8', '7', '1', '1', '2', '8', '8', '3', '4', '8', '2', '10', '13', '8', '8', '9', '2', '2', '1', '1', '10', '4', '2', '7', '9', '8', '2', '1', '9', '8', '1', '1', '9', '8', '9', '8', '2', '8', '2', '2', '9', '2', '1', '9', '1', '2', '4', '8', '2', '1', '1', '8', '1', '4', '1', '9', '3', '9', '2', '7', '1', '9', '6', '1', '10', '2', '2', '2', '2', '2', '10', '10', '10', '9', '9', '9', '3', '12', '10', '1', '1', '10', '1', '1', '2', '2', '1', '2', '3', '12', '8', '8', '1', '2', '9', '9', '1', '1', '10', '8', '9', '8', '3', '1', '1', '3', '1', '10', '1', '1', '1', '4', '1', '1', '1', '7', '10', '2', '1', '8', '1', '9', '3', '1', '10', '8', '8', '1', '5', '9', '1', '2', '1', '8', '8', '1', '3', '1', '10', '2', '1', '1', '2', '8', '2', '5', '3', '5', '2', '1', '1', '9', '1', '7', '9', '9', '2', '8', '1', '12', '4', '1', '4', '9', '9', '4', '8', '1', '10', '10', '8', '8', '8', '9', '2', '3', '5', '1', '8', '1', '1', '1', '7', '10', '1', '4', '12', '3', '2', '2', '9', '1', '1', '1', '2', '8', '4', '1', '7', '3', '12', '8', '6', '2', '2', '7', '9', '11', '1', '10', '9', '1', '8', '12', '2', '11', '1', '2', '3', '1', '1', '1', '8', '9', '2', '2', '2', '2', '3', '3', '8', '10', '4', '1', '2', '9', '8', '2', '2', '9', '1', '8', '11', '4', '2', '8', '7', '1', '1', '7', '3', '9', '9', '4', '5', '8', '10', '2', '9', '9', '9', '8', '1', '9', '2', '2', '2', '1', '2', '9', '4', '3', '12', '4', '3', '2', '3', '8', '3', '10', '1', '10', '1', '1', '8', '10', '6', '8', '9', '9', '1', '2', '9', '2', '9', '3', '8', '1', '1', '12', '1', '5', '1', '10', '8', '10', '2', '7', '1', '1', '1', '1', '3', '2', '3', '5', '9', '1', '2', '1', '9', '8', '10', '9', '8', '1', '8', '1', '8', '2', '8', '1', '2', '8', '1', '2', '5', '8', '12', '2', '1', '1', '1', '1', '1', '8', '8', '2', '1', '4', '2', '8', '8', '8', '2', '9', '9', '11', '1', '2', '9', '3', '1', '1', '8', '9', '2', '4', '10', '1', '9', '8', '4', '9', '8', '10', '8', '8', '9', '2', '8', '9', '3', '3', '3', '4', '3', '2', '2', '10', '8', '2', '1', '5', '1', '9', '1', '1', '2', '1', '2', '3', '1', '1', '8', '8', '1', '10', '8', '8', '1', '6', '4', '8', '2', '5', '8', '1', '3', '8', '2', '8', '9', '1', '1', '8', '12', '10', '1', '5', '8', '2', '8', '10', '9', '9', '1', '6', '9', '1', '10', '8', '2', '9', '10', '1', '2', '1', '1', '8', '8', '3', '9', '8', '1', '10', '2', '8', '1', '10', '1', '5', '1', '1', '1', '5', '1', '1', '1', '4', '1', '1', '1', '8', '2', '8', '1', '8', '3', '8', '9', '1', '8', '1', '8', '1', '1', '2', '2', '1', '12', '9', '9', '1', '8', '2', '1', '2', '8', '1', '9', '1', '1', '2', '1', '2', '2', '2', '1', '9', '9', '8', '8', '1', '1', '9', '10', '8', '4', '3', '3', '4', '9', '1', '3', '3', '10', '2', '2', '1', '8', '4', '1', '9', '4', '1', '1', '9', '8', '10', '4', '9', '9', '8', '2', '1', '3', '4', '9', '3', '8', '9', '1', '1', '2', '1', '7', '8', '2', '9', '1', '9', '2', '6', '9', '10', '8', '1', '1', '5', '9', '5', '1', '9', '10', '9', '3', '3', '8', '8', '2', '9', '2', '8', '2', '1', '8', '5', '8', '3', '10', '1', '1', '2', '1', '8', '1', '4', '3', '2', '9', '4', '9', '8', '8', '8', '1', '3', '9', '1', '9', '9', '1', '2', '1', '9', '8', '9', '8', '1', '1', '1', '4', '2', '2', '9', '9', '1', '1', '1', '3', '6', '1', '2', '3', '1', '4', '2', '9', '1', '9', '9', '1', '8', '2', '1', '9', '1', '8', '1', '8', '8', '2', '1', '9', '9', '6', '8', '8', '9', '8', '10', '8', '9', '5', '1', '1', '1', '1', '8', '1', '8', '12', '8', '1', '12', '9', '2', '8', '1', '1', '2', '1', '8', '6', '9', '9', '8', '1', '8', '9', '7', '7', '8', '9', '9', '1', '8', '3', '8', '1', '3', '3', '8', '9', '8', '2', '8', '1', '2', '9', '8', '1', '1', '1', '1', '9', '1', '9', '8', '12', '3', '8', '8', '1', '10', '10', '10', '2', '2', '1', '10', '2', '2', '10', '8', '12', '8', '1', '1', '1', '2', '1', '1', '3', '8', '1', '2', '2', '2', '1', '1', '6', '2', '8', '8', '8', '1', '1', '1', '9', '9', '2', '8', '10', '2', '8', '2', '1', '2', '9', '9', '2', '1', '1', '8', '8', '1', '3', '1', '8', '9', '1', '10', '9', '1', '1', '1', '1', '7', '2', '2', '1', '2', '8', '8', '1', '2', '1', '3', '2', '1', '8', '3', '9', '2', '10', '2', '10', '1', '2', '1', '1', '13', '9', '10', '8', '2', '2', '1', '1', '8', '2', '2', '1', '9', '1', '1', '8', '9', '9', '2', '9', '2', '2', '1', '1', '1', '8', '9', '4', '9', '9', '1', '8', '2', '1', '1', '8', '2', '2', '2', '8', '1', '1', '8', '2', '10', '2', '9', '9', '7', '1', '1', '2', '2', '8', '3', '1', '2', '1', '1', '8', '1', '1', '1', '9', '1', '1', '8', '8', '1', '9', '6', '1', '9', '1', '8', '1', '8', '8', '8', '1', '3', '8', '8', '2', '6', '3', '2', '1', '10', '1', '8', '2', '1', '6', '1', '1', '1', '8', '1', '10', '1', '1', '8', '9', '2', '6', '9', '2', '9', '4', '1', '8', '1', '3', '7', '8', '8', '5', '8', '7', '1', '1', '1', '13', '8', '3', '1', '1', '1', '1', '2', '8', '2', '12', '8', '1', '2', '5', '1', '8', '2', '10', '8', '2', '10', '1', '5', '2', '1', '1', '5', '2', '2', '7', '8', '2', '9', '9', '1', '10', '1', '8', '2', '5', '8', '1', '5', '1', '10', '1', '1', '9', '8', '6', '1', '8', '9', '8', '1', '8', '2', '2', '8', '3', '9', '1', '9', '3', '8', '4', '4', '3', '3', '1', '1', '1', '8', '9', '1', '1', '8', '3', '1', '2', '8', '2', '1', '1', '10', '1', '2', '2', '1', '1', '8', '2', '8', '2', '10', '2', '2', '10', '8', '8', '8', '13', '2', '2', '9', '2', '5', '9', '2', '1', '8', '8', '8', '1', '12', '12', '2', '9', '2', '1', '8', '2', '1', '8', '1', '9', '9', '2', '7', '1', '1', '7', '1', '4', '3', '1', '10', '3', '10', '9', '4', '6', '7', '5', '2', '1', '8', '8', '1', '4', '9', '2', '1', '8', '1', '1', '1', '1', '1', '8', '9', '8', '8', '8', '8', '9', '8', '10', '1', '8', '2', '7', '8', '9', '1', '2', '5', '9', '10', '8', '9', '12', '1', '6', '1', '2', '10', '9', '4', '1', '8', '10', '10', '1', '1', '1', '1', '1', '2', '3', '8', '1', '2', '10', '2', '1', '2', '4', '2', '2', '9', '1', '4', '2', '1', '1', '12', '12', '9', '9', '7', '1', '8', '4', '9', '9', '8', '1', '7', '3', '2', '1', '1', '4', '8', '1', '5', '8', '2', '8', '1', '14', '9', '12', '1', '10', '2', '1', '1', '8', '8', '6', '6', '3', '1', '2', '8', '1', '3', '2', '1', '8', '8', '1', '10', '8', '2', '8', '2', '5', '9', '1', '8', '8', '1', '1', '8', '8', '13', '1', '3', '8', '4', '1', '9', '7', '8', '8', '8', '3', '8', '1', '4', '2', '8', '8', '8', '7', '1', '1', '2', '8', '8', '9', '8', '8', '2', '3', '9', '9', '2', '1', '2', '2', '1', '8', '2', '6', '2', '1', '1', '9', '10', '3', '8', '2', '3', '2', '8', '8', '1', '8', '8', '8', '9', '1', '4', '1', '9', '2', '1', '8', '0', '1', '8', '1', '1', '8', '9', '9', '1', '3', '1', '2', '8', '8', '7', '1', '1', '2', '9', '2', '10', '2', '8', '2', '2', '2', '7', '1', '9', '8', '1', '3', '9', '2', '1', '10', '8', '1', '4', '1', '8', '5', '9', '8', '8', '1', '2', '2', '1', '8', '8', '6', '8', '1', '1', '8', '3', '2', '2', '1', '8', '1', '7', '6', '8', '1', '1', '1', '9', '1', '1', '1', '2', '1', '11', '10', '10', '11', '9', '']\n",
            "['8', '1', '8', '2', '8', '8', '8', '9', '7', '8', '1', '2', '1', '1', '8', '4', '8', '8', '12', '3', '3', '7', '3', '12', '1', '8', '8', '8', '8', '8', '8', '8', '1', '9', '5', '9', '9', '9', '11', '8', '8', '8', '4', '8', '8', '8', '8', '1', '3', '9', '3', '7', '1', '2', '9', '9', '7', '8', '8', '1', '10', '7', '8', '8', '9', '8', '7', '9', '9', '12', '7', '2', '8', '1', '11', '11', '1', '7', '7', '12', '1', '9', '8', '10', '12', '7', '8', '2', '8', '9', '9', '1', '8', '9', '1', '7', '12', '10', '10', '10', '8', '3', '7', '9', '8', '9', '1', '8', '8', '2', '7', '2', '9', '9', '11', '8', '8', '12', '12', '7', '8', '12', '4', '9', '3', '1', '12', '1', '1', '8', '8', '3', '8', '8', '8', '8', '9', '1', '8', '8', '10', '1', '8', '2', '8', '8', '7', '3', '8', '2', '4', '4', '9', '8', '10', '12', '12', '1', '1', '9', '1', '1', '1', '8', '2', '2', '8', '1', '1', '2', '2', '1', '2', '8', '1', '9', '9', '8', '8', '4', '2', '9', '9', '8', '3', '4', '3', '1', '8', '8', '2', '1', '9', '7', '8', '8', '1', '12', '3', '8', '2', '4', '2', '9', '12', '1', '4', '1', '8', '8', '8', '2', '2', '8', '9', '8', '8', '8', '10', '9', '8', '7', '9', '1', '1', '9', '4', '2', '4', '2', '2', '1', '7', '8', '11', '11', '3', '9', '2', '4', '8', '9', '1', '8', '1', '1', '4', '9', '1', '1', '8', '8', '2', '1', '8', '4', '2', '8', '9', '8', '8', '2', '8', '8', '8', '7', '1', '1', '1', '2', '1', '1', '8', '7', '8', '8', '12', '2', '12', '12', '8', '10', '12', '8', '3', '3', '12', '10', '1', '8', '12', '1', '8', '8', '2', '8', '4', '7', '8', '7', '10', '8', '10', '9', '8', '12', '12', '1', '8', '8', '3', '8', '8', '8', '8', '8', '1', '9', '8', '11', '1', '1', '1', '9', '8', '1', '9', '2', '3', '11', '8', '9', '9', '9', '2', '1', '8', '8', '9', '7', '1', '4', '9', '4', '8', '8', '4', '8', '12', '9', '4', '8', '2', '10', '10', '10', '8', '9', '9', '8', '8', '12', '7', '1', '8', '8', '8', '4', '1', '1', '1', '1', '1', '1', '8', '1', '9', '8', '9', '9', '4', '8', '12', '9', '8', '8', '2', '8', '8', '8', '6', '9', '8', '3', '7', '8', '8', '4', '12', '8', '8', '9', '12', '12', '9', '8', '2', '9', '2', '3', '1', '12', '8', '10', '9', '9', '9', '10', '10', '3', '8', '12', '1', '4', '2', '1', '10', '8', '2', '8', '4', '8', '9', '1', '9', '9', '10', '10', '1', '4', '9', '2', '4', '9', '1', '1', '3', '10', '3', '3', '8', '7', '3', '8', '9', '9', '12', '4', '8', '12', '2', '2', '4', '1', '9', '9', '4', '1', '4', '2', '8', '12', '2', '3', '10', '10', '9', '8', '9', '9', '1', '12', '8', '8', '8', '12', '4', '1', '8', '8', '1', '9', '8', '8', '2', '1', '8', '9', '8', '3', '3', '3', '1', '8', '8', '9', '1', '10', '9', '9', '9', '9', '5', '9', '9', '8', '8', '8', '8', '8', '8', '8', '8', '11', '12', '8', '8', '1', '8', '9', '11', '2', '2', '2', '2', '3', '1', '2', '2', '8', '2', '4', '9', '1', '2', '9', '8', '2', '8', '9', '9', '3', '10', '9', '9', '2', '8', '9', '8', '12', '12', '1', '3', '8', '8', '8', '2', '7', '7', '7', '7', '3', '9', '1', '9', '8', '9', '9', '1', '1', '1', '2', '9', '9', '9', '11', '1', '8', '8', '9', '1', '9', '8', '8', '8', '1', '1', '8', '7', '1', '1', '8', '8', '9', '4', '4', '8', '2', '2', '8', '8', '8', '8', '8', '8', '11', '8', '2', '9', '4', '9', '3', '9', '9', '1', '3', '9', '3', '1', '12', '8', '9', '12', '1', '8', '4', '2', '1', '4', '8', '3', '3', '8', '2', '8', '9', '7', '8', '8', '8', '5', '8', '3', '9', '8', '8', '13', '12', '1', '1', '2', '8', '4', '1', '9', '9', '12', '8', '9', '12', '9', '1', '9', '9', '9', '9', '3', '2', '9', '9', '4', '8', '12', '2', '4', '9', '3', '1', '9', '7', '8', '9', '9', '8', '4', '8', '8', '7', '9', '10', '3', '8', '8', '8', '1', '1', '1', '1', '8', '8', '4', '1', '10', '1', '5', '7', '7', '1', '8', '9', '3', '7', '2', '7', '7', '2', '4', '8', '12', '7', '4', '2', '9', '9', '12', '6', '10', '8', '2', '4', '12', '9', '9', '3', '8', '8', '1', '2', '10', '9', '9', '8', '4', '12', '2', '1', '8', '8', '8', '12', '10', '10', '9', '3', '8', '8', '9', '2', '8', '10', '1', '1', '1', '1', '2', '1', '1', '1', '1', '9', '8', '12', '9', '4', '8', '8', '9', '1', '9', '3', '9', '8', '8', '1', '7', '7', '10', '1', '8', '8', '1', '9', '8', '10', '3', '1', '7', '1', '8', '8', '12', '8', '8', '1', '8', '7', '1', '7', '7', '8', '2', '1', '8', '8', '2', '10', '8', '8', '8', '8', '8', '10', '1', '8', '8', '12', '8', '3', '3', '2', '2', '2', '10', '8', '8', '8', '2', '9', '1', '8', '9', '3', '2', '8', '10', '8', '6', '1', '1', '8', '4', '1', '9', '10', '8', '1', '7', '1', '2', '8', '1', '1', '1', '12', '1', '9', '12', '8', '12', '12', '12', '8', '8', '12', '4', '8', '8', '8', '8', '9', '9', '1', '3', '3', '3', '3', '1', '12', '12', '9', '10', '8', '8', '1', '9', '2', '2', '13', '9', '8', '9', '2', '1', '9', '1', '8', '8', '8', '4', '8', '1', '1', '1', '12', '12', '7', '2', '2', '2', '8', '3', '8', '9', '2', '10', '7', '8', '9', '2', '1', '2', '12', '12', '8', '8', '9', '2', '2', '9', '11', '1', '8', '1', '10', '9', '2', '1', '4', '7', '7', '7', '7', '7', '12', '8', '8', '8', '1', '1', '10', '1', '12', '1', '8', '2', '1', '1', '12', '8', '7', '9', '12', '8', '9', '3', '9', '8', '8', '8', '8', '3', '11', '2', '2', '9', '8', '8', '10', '8', '2', '7', '3', '1', '4', '7', '8', '8', '1', '8', '3', '7', '12', '8', '10', '9', '9', '8', '8', '2', '8', '9', '9', '1', '2', '8', '8', '9', '8', '3', '8', '1', '8', '10', '9', '8', '9', '9', '12', '4', '4', '8', '9', '9', '8', '2', '10', '1', '2', '8', '9', '1', '9', '9', '9', '7', '12', '12', '8', '1', '1', '1', '1', '8', '3', '1', '1', '8', '1', '8', '8', '7', '8', '8', '8', '8', '3', '2', '2', '10', '10', '10', '7', '8', '1', '2', '12', '7', '9', '8', '7', '8', '12', '2', '8', '9', '2', '6', '6', '7', '9', '8', '1', '8', '9', '8', '1', '12', '1', '2', '8', '7', '7', '7', '8', '2', '2', '8', '1', '2', '2', '9', '9', '1', '8', '8', '4', '3', '3', '1', '6', '3', '3', '12', '3', '8', '9', '1', '4', '3', '1', '8', '3', '9', '2', '8', '2', '8', '8', '8', '1', '1', '1', '9', '9', '8', '1', '9', '8', '1', '1', '3', '10', '8', '1', '1', '3', '9', '1', '4', '4', '1', '8', '9', '9', '2', '0', '0', '1', '8', '3', '1', '8', '8', '9', '8', '8', '1', '1', '8', '9', '8', '8', '8', '7', '9', '8', '8', '8', '10', '9', '8', '1', '2', '6', '1', '9', '9', '8', '12', '12', '12', '8', '8', '2', '8', '1', '2', '2', '2', '1', '9', '8', '2', '12', '2', '8', '12', '8', '9', '8', '8', '9', '7', '1', '1', '1', '1', '1', '8', '8', '1', '8', '8', '1', '1', '3', '2', '8', '8', '9', '10', '10', '2', '2', '1', '9', '2', '9', '9', '4', '12', '12', '12', '10', '7', '3', '3', '4', '2', '2', '9', '2', '8', '4', '2', '4', '1', '10', '9', '7', '8', '7', '1', '1', '3', '3', '1', '1', '3', '3', '3', '1', '1', '1', '1', '8', '2', '3', '1', '1', '2', '8', '8', '12', '8', '8', '8', '8', '11', '9', '1', '8', '9', '2', '8', '8', '8', '3', '9', '1', '9', '2', '7', '2', '8', '2', '8', '10', '8', '1', '10', '1', '1', '9', '9', '8', '8', '1', '8', '8', '8', '12', '8', '8', '8', '1', '8', '8', '8', '1', '9', '1', '1', '8', '1', '8', '9', '8', '2', '12', '9', '9', '0', '1', '8', '8', '1', '8', '12', '8', '8', '10', '8', '8', '8', '7', '8', '1', '8', '7', '3', '10', '1', '8', '9', '1', '8', '8', '8', '10', '1', '10', '3', '9', '1', '8', '9', '2', '8', '3', '3', '9', '9', '7', '9', '1', '1', '9', '2', '1', '1', '1', '7', '1', '1', '8', '8', '1', '1', '8', '1', '8', '3', '12', '9', '3', '3', '8', '8', '8', '8', '3', '1', '3', '3', '1', '11', '0', '8', '8', '7', '8', '12', '1', '8', '9', '8', '9', '8', '8', '3', '8', '8', '1', '1', '1', '9', '2', '2', '2', '8', '7', '12', '8', '8', '9', '10', '10', '7', '8', '1', '9', '8', '7', '3', '1', '3', '8', '2', '2', '3', '9', '8', '4', '4', '8', '9', '2', '1', '1', '7', '8', '9', '9', '7', '8', '7', '7', '8', '2', '2', '8', '4', '9', '7', '10', '0', '9', '8', '3', '7', '8', '1', '1', '8', '9', '9', '2', '2', '10', '1', '9', '10', '10', '10', '8', '3', '2', '12', '9', '9', '10', '12', '9', '12', '12', '9', '1', '2', '4', '12', '12', '7', '8', '9', '7', '7', '7', '3', '9', '8', '9', '1', '12', '8', '9', '4', '1', '3', '12', '12', '12', '12', '8', '8', '2', '1', '1', '2', '1', '1', '1', '12', '12', '8', '12', '2', '2', '12', '3', '3', '12', '8', '2', '8', '8', '12', '2', '1', '10', '3', '2', '8', '7', '1', '8', '1', '3', '7', '8', '9', '8', '3', '1', '1', '7', '8', '8', '9', '8', '2', '9', '2', '2', '9', '8', '1', '8', '8', '1', '3', '3', '1', '1', '10', '1', '2', '8', '1', '1', '1', '1', '9', '1', '4', '1', '7', '7', '7', '7', '2', '2', '8', '8', '12', '1', '9', '1', '7', '3', '3', '1', '8', '8', '10', '8', '9', '2', '9', '1', '3', '8', '8', '3', '12', '2', '8', '12', '2', '9', '1', '3', '3', '3', '3', '2', '8', '7', '9', '8', '3', '3', '1', '7', '8', '3', '1', '1', '12', '8', '9', '1', '2', '3', '8', '1', '1', '3', '3', '9', '1', '1', '1', '12', '1', '7', '3', '3', '1', '8', '8', '8', '1', '2', '4', '8', '1', '10', '2', '5', '3', '3', '12', '10', '9', '9', '12', '9', '0', '2', '8', '8', '9', '9', '9', '8', '1', '3', '1', '1', '4', '8', '1', '10', '8', '7', '2', '8', '2', '8', '4', '7', '8', '1', '9', '1', '9', '8', '2', '8', '2', '7', '9', '2', '2', '9', '1', '8', '12', '1', '8', '1', '4', '1', '9', '9', '1', '10', '12', '4', '8', '1', '7', '3', '9', '2', '12', '7', '8', '8', '2', '1', '12', '9', '8', '1', '2', '2', '8', '10', '2', '1', '7', '7', '7', '12', '3', '3', '8', '3', '8', '8', '3', '9', '8', '9', '1', '1', '8', '7', '9', '3', '3', '8', '1', '0', '9', '9', '9', '1', '8', '9', '9', '10', '1', '8', '7', '8', '8', '8', '9', '9', '8', '9', '10', '4', '9', '3', '7', '12', '1', '9', '9', '8', '9', '1', '9', '3', '1', '8', '4', '12', '10', '9', '8', '7', '10', '8', '12', '12', '3', '10', '8', '12', '2', '1', '2', '3', '9', '8', '8', '7', '1', '1', '2', '2', '1', '2', '2', '7', '1', '3', '9', '9', '3', '8', '8', '8', '8', '8', '7', '8', '8', '10', '8', '1', '8', '2', '8', '2', '2', '2', '2', '2', '1', '7', '12', '10', '1', '2', '8', '1', '4', '7', '8', '1', '12', '8', '7', '9', '2', '2', '2', '8', '1', '8', '1', '1', '0', '2', '2', '6', '1', '8', '2', '1', '8', '1', '1', '2', '2', '3', '2', '2', '10', '10', '9', '1', '7', '7', '8', '8', '1', '12', '10', '12', '3', '8', '8', '8', '3', '8', '3', '10', '2', '2', '2', '1', '2', '2', '1', '0', '1', '8', '9', '1', '1', '1', '2', '2', '2', '2', '2', '1', '1', '1', '1', '12', '9', '9', '1', '10', '11', '12', '9', '1', '1', '8', '8', '1', '2', '12', '8', '3', '7', '2', '10', '9', '7', '8', '1', '2', '2', '7', '9', '12', '2', '2', '1', '8', '9', '2', '3', '2', '2', '2', '9', '9', '8', '9', '2', '3', '8', '8', '9', '7', '3', '10', '8', '8', '9', '1', '1', '2', '8', '1', '10', '8', '8', '9', '7', '1', '7', '12', '8', '1', '7', '2', '1', '3', '9', '2', '8', '8', '1', '2', '3', '9', '3', '8', '9', '7', '7', '1', '10', '2', '8', '8', '8', '1', '8', '2', '8', '8', '1', '2', '8', '10', '2', '1', '0', '8', '3', '8', '10', '10', '12', '3', '8', '9', '3', '10', '8', '8', '8', '1', '8', '8', '2', '2', '2', '2', '2', '2', '1', '1', '10', '1', '10', '8', '2', '1', '2', '2', '8', '3', '3', '2', '2', '1', '8', '1', '3', '1', '9', '2', '2', '1', '3', '2', '2', '3', '2', '2', '3', '2', '1', '1', '2', '1', '2', '7', '8', '2', '1', '12', '12', '3', '1', '9', '9', '7', '8', '7', '2', '7', '9', '2', '2', '2', '8', '9', '7', '9', '2', '8', '9', '9', '2', '9', '1', '1', '9', '8', '3', '3', '8', '2', '7', '8', '11', '2', '8', '9', '1', '8', '3', '1', '2', '1', '2', '2', '2', '2', '8', '3', '9', '2', '10', '9', '9', '2', '7', '7', '7', '1', '3', '8', '8', '1', '1', '8', '7', '1', '8', '8', '9', '3', '3', '3', '2', '4', '8', '12', '8', '1', '12', '9', '2', '8', '2', '12', '2', '1', '3', '8', '12', '12', '11', '8', '10', '9', '2', '9', '4', '10', '1', '12', '12', '9', '3', '8', '9', '12', '9', '9', '2', '2', '2', '3', '5', '1', '1', '1', '7', '7', '8', '2', '2', '8', '2', '1', '7', '3', '2', '9', '8', '9', '1', '1', '9', '8', '9', '3', '3', '2', '9', '4', '9', '8', '8', '1', '8', '10', '2', '0', '3', '8', '8', '8', '9', '12', '9', '8', '2', '2', '8', '10', '9', '8', '2', '2', '4', '1', '9', '9', '8', '8', '10', '3', '1', '1', '2', '8', '7', '8', '2', '1', '1', '8', '12', '2', '9', '4', '8', '3', '3', '3', '8', '2', '12', '12', '8', '9', '12', '8', '2', '12', '7', '9', '2', '2', '1', '3', '1', '3', '11', '9', '8', '1', '10', '2', '12', '3', '8', '4', '1', '2', '8', '8', '3', '9', '3', '8', '7', '1', '1', '3', '1', '8', '2', '9', '8', '12', '1', '1', '1', '9', '9', '1', '8', '8', '8', '2', '2', '12', '3', '1', '1', '1', '9', '9', '3', '2', '1', '1', '9', '9', '1', '9', '9', '1', '9', '3', '9', '1', '8', '1', '9', '8', '3', '8', '8', '9', '9', '8', '2', '8', '1', '2', '2', '10', '1', '2', '1', '1', '9', '1', '9', '3', '7', '7', '2', '10', '3', '3', '1', '1', '9', '1', '2', '1', '3', '2', '1', '1', '1', '1', '1', '1', '1', '1', '1', '2', '8', '8', '7', '1', '1', '8', '8', '2', '1', '12', '1', '8', '1', '9', '8', '8', '2', '1', '9', '9', '2', '2', '8', '1', '7', '7', '9', '8', '1', '6', '8', '8', '2', '2', '8', '3', '8', '3', '2', '3', '8', '1', '2', '2', '2', '8', '9', '9', '9', '12', '1', '9', '2', '1', '9', '2', '9', '9', '10', '8', '1', '1', '9', '2', '1', '7', '1', '3', '7', '1', '1', '1', '3', '8', '8', '8', '9', '9', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '1', '0', '1', '1', '1', '2', '2', '3', '1', '1', '2', '3', '3', '3', '7', '3', '1', '9', '3', '9', '1', '4', '1', '1', '9', '10', '9', '8', '2', '3', '10', '9', '3', '10', '12', '8', '9', '8', '2', '1', '9', '8', '1', '7', '8', '1', '9', '1', '2', '10', '7', '7', '8', '1', '3', '3', '9', '3', '3', '1', '1', '1', '9', '1', '3', '8', '8', '8', '1', '8', '1', '9', '7', '8', '8', '2', '1', '3', '8', '2', '1', '6', '1', '8', '8', '2', '8', '2', '2', '9', '8', '1', '1', '6', '8', '6', '10', '2', '1', '1', '8', '9', '3', '3', '3', '1', '3', '9', '6', '8', '1', '8', '3', '1', '2', '2', '12', '7', '1', '1', '1', '1', '12', '1', '8', '8', '3', '2', '1', '9', '2', '3', '3', '2', '2', '7', '2', '2', '2', '3', '1', '9', '8', '7', '1', '1', '3', '1', '9', '4', '3', '9', '2', '1', '1', '9', '8', '8', '7', '2', '1', '3', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '2', '10', '1', '1', '8', '2', '8', '1', '8', '1', '3', '1', '2', '1', '1', '3', '1', '2', '1', '1', '1', '1', '1', '1', '1', '2', '1', '8', '1', '9', '8', '8', '1', '10', '8', '8', '1', '3', '9', '8', '9', '8', '2', '1', '3', '4', '1', '12', '1', '9', '8', '2', '7', '2', '1', '2', '9', '2', '1', '3', '8', '1', '2', '3', '8', '2', '10', '10', '10', '9', '9', '9', '3', '8', '3', '1', '1', '1', '1', '1', '1', '2', '1', '9', '2', '7', '9', '8', '7', '9', '1', '1', '1', '1', '8', '2', '2', '3', '3', '2', '12', '8', '3', '1', '1', '12', '9', '8', '2', '2', '9', '1', '2', '2', '9', '1', '1', '2', '4', '8', '1', '8', '10', '2', '9', '9', '9', '1', '2', '1', '1', '2', '2', '2', '12', '1', '2', '8', '3', '9', '9', '3', '2', '9', '9', '7', '2', '8', '8', '9', '2', '9', '1', '1', '1', '1', '1', '2', '8', '2', '1', '2', '2', '8', '3', '1', '1', '9', '1', '3', '10', '7', '9', '8', '8', '2', '2', '9', '2', '2', '2', '2', '2', '2', '9', '1', '3', '6', '4', '2', '2', '9', '1', '9', '8', '1', '1', '1', '7', '3', '2', '9', '9', '1', '9', '7', '2', '8', '9', '9', '12', '10', '8', '10', '2', '3', '1', '2', '2', '12', '12', '3', '12', '1', '2', '1', '2', '1', '2', '8', '2', '3', '12', '12', '8', '2', '2', '9', '2', '9', '2', '3', '3', '1', '1', '1', '12', '3', '7', '1', '3', '1', '2', '9', '2', '7', '2', '1', '8', '3', '7', '8', '1', '1', '3', '8', '3', '2', '9', '3', '1', '1', '1', '1', '11', '1', '2', '6', '1', '2', '3', '8', '9', '9', '2', '9', '2', '1', '1', '3', '1', '12', '9', '1', '1', '9', '1', '9', '9', '8', '9', '8', '1', '2', '7', '1', '0', '9', '8', '2', '3', '2', '4', '1', '1', '1', '8', '1', '12', '1', '2', '3', '3', '9', '9', '9', '9', '9', '7', '9', '3', '9', '1', '7', '3', '3', '3', '3', '7', '8', '2', '2', '2', '8', '2', '3', '9', '9', '3', '1', '8', '9', '12', '8', '8', '3', '2', '1', '6', '1', '9', '1', '1', '1', '1', '1', '1', '2', '2', '1', '2', '2', '2', '2', '9', '2', '5', '3', '2', '2', '8', '2', '1', '8', '3', '3', '8', '3', '1', '3', '9', '2', '8', '9', '4', '4', '9', '8', '8', '7', '7', '3', '1', '8', '8', '3', '2', '2', '10', '3', '3', '2', '2', '2', '2', '12', '3', '1', '2', '7', '7', '12', '2', '1', '2', '1', '1', '2', '1', '3', '3', '3', '1', '3', '9', '4', '8', '8', '4', '2', '4', '2', '8', '9', '1', '1', '2', '2', '2', '8', '7', '2', '8', '2', '9', '1', '8', '1', '1', '2', '1', '1', '1', '2', '9', '9', '8', '1', '8', '1', '4', '8', '1', '2', '7', '9', '2', '1', '8', '9', '8', '9', '1', '1', '2', '3', '2', '12', '7', '1', '2', '4', '4', '4', '2', '2', '2', '9', '8', '8', '1', '12', '3', '3', '2', '2', '3', '12', '1', '5', '1', '1', '3', '2', '8', '8', '2', '2', '2', '3', '9', '8', '8', '9', '8', '8', '11', '8', '2', '9', '2', '3', '1', '7', '1', '8', '2', '8', '1', '1', '1', '8', '1', '1', '1', '7', '8', '8', '2', '3', '2', '1', '8', '8', '1', '8', '4', '8', '9', '8', '9', '8', '1', '2', '3', '2', '1', '2', '2', '3', '1', '9', '4', '8', '1', '1', '9', '1', '1', '8', '2', '2', '2', '1', '7', '3', '3', '1', '3', '3', '12', '3', '1', '1', '3', '1', '1', '4', '8', '4', '4', '3', '3', '3', '4', '8', '1', '9', '9', '9', '4', '1', '7', '4', '8', '4', '9', '8', '1', '1', '3', '8', '9', '1', '9', '7', '1', '1', '9', '1', '8', '8', '8', '1', '1', '8', '2', '2', '2', '9', '4', '4', '1', '2', '1', '2', '9', '1', '1', '8', '5', '5', '5', '9', '12', '4', '1', '2', '1', '8', '8', '1', '9', '9', '2', '12', '1', '1', '7', '8', '8', '4', '1', '2', '8', '11', '2', '3', '2', '2', '1', '11', '2', '2', '2', '2', '9', '9', '2', '2', '9', '3', '1', '9', '1', '1', '1', '7', '8', '1', '8', '9', '1', '2', '9', '8', '1', '2', '9', '8', '12', '2', '2', '10', '8', '2', '2', '8', '8', '1', '2', '6', '1', '1', '7', '7', '2', '3', '2', '1', '1', '8', '1', '10', '8', '12', '6', '4', '1', '4', '1', '2', '9', '8', '9', '4', '8', '8', '10', '8', '8', '8', '8', '8', '2', '2', '8', '4', '9', '3', '3', '3', '3', '3', '9', '2', '1', '1', '2', '3', '2', '1', '2', '3', '3', '3', '2', '2', '3', '3', '2', '1', '2', '3', '3', '3', '3', '12', '9', '3', '2', '2', '3', '1', '5', '1', '1', '3', '2', '2', '1', '2', '9', '3', '9', '8', '1', '8', '7', '7', '9', '1', '1', '7', '9', '10', '1', '7', '1', '4', '3', '8', '9', '4', '9', '9', '2', '9', '2', '8', '4', '2', '2', '9', '5', '2', '1', '10', '9', '3', '1', '1', '1', '2', '1', '2', '9', '1', '7', '9', '9', '8', '2', '2', '8', '9', '1', '9', '8', '2', '3', '2', '2', '1', '2', '2', '2', '1', '1', '8', '1', '9', '4', '12', '8', '2', '7', '2', '2', '1', '9', '3', '1', '8', '12', '1', '1', '2', '1', '10', '2', '6', '12', '12', '4', '9', '1', '9', '10', '1', '2', '8', '2', '1', '8', '6', '12', '9', '9', '2', '1', '7', '8', '9', '5', '8', '1', '8', '8', '3', '1', '8', '2', '8', '2', '2', '1', '2', '9', '2', '8', '1', '12', '8', '2', '7', '3', '3', '12', '2', '3', '3', '9', '9', '3', '3', '3', '3', '3', '2', '1', '4', '1', '4', '8', '8', '2', '1', '2', '9', '12', '1', '10', '12', '1', '2', '9', '4', '8', '10', '3', '2', '8', '7', '4', '8', '9', '2', '7', '1', '10', '2', '2', '2', '8', '4', '2', '2', '1', '1', '8', '7', '9', '2', '1', '9', '8', '9', '12', '4', '8', '2', '8', '7', '7', '1', '9', '2', '1', '1', '9', '1', '8', '2', '3', '9', '9', '1', '3', '2', '9', '2', '2', '8', '1', '1', '2', '1', '2', '1', '9', '8', '2', '2', '4', '8', '8', '9', '5', '5', '9', '8', '9', '6', '2', '1', '12', '2', '3', '8', '8', '9', '2', '2', '1', '3', '2', '10', '9', '10', '7', '2', '8', '4', '2', '8', '9', '8', '1', '3', '8', '8', '1', '8', '3', '8', '8', '1', '10', '3', '1', '5', '8', '8', '9', '2', '2', '9', '2', '1', '9', '1', '4', '1', '12', '8', '8', '2', '1', '1', '1', '1', '1', '9', '2', '9', '8', '9', '5', '9', '1', '8', '2', '4', '1', '1', '1', '9', '12', '4', '1', '1', '9', '8', '4', '1', '8', '9', '8', '8', '9', '9', '9', '9', '9', '9', '1', '3', '4', '2', '2', '8', '2', '9', '2', '3', '8', '3', '1', '7', '2', '1', '1', '4', '2', '9', '2', '1', '3', '2', '9', '2', '9', '3', '8', '1', '9', '1', '1', '2', '8', '8', '8', '2', '3', '9', '9', '8', '2', '4', '1', '5', '1', '1', '9', '9', '2', '1', '1', '1', '9', '9', '1', '3', '9', '2', '2', '9', '9', '8', '3', '9', '9', '8', '2', '2', '8', '9', '2', '2', '8', '9', '10', '10', '2', '8', '7', '9', '9', '4', '2', '4', '11', '2', '1', '7', '8', '8', '8', '9', '4', '9', '8', '8', '2', '1', '1', '9', '8', '3', '3', '8', '8', '7', '9', '1', '3', '4', '8', '10', '2', '4', '4', '8', '2', '8', '2', '8', '9', '2', '1', '2', '2', '3', '1', '9', '4', '5', '9', '9', '1', '1', '1', '1', '1', '1', '7', '1', '1', '1', '8', '1', '1', '1', '1', '1', '9', '1', '2', '1', '12', '9', '3', '9', '9', '4', '1', '9', '1', '1', '2', '11', '9', '3', '2', '2', '2', '4', '2', '3', '2', '7', '7', '8', '1', '10', '9', '5', '7', '1', '10', '8', '1', '2', '7', '1', '12', '5', '8', '4', '2', '9', '9', '8', '2', '9', '8', '8', '10', '2', '3', '2', '7', '2', '8', '10', '3', '2', '9', '9', '9', '1', '1', '1', '1', '8', '2', '10', '9', '8', '1', '2', '2', '2', '1', '2', '1', '3', '9', '12', '2', '2', '4', '8', '1', '3', '9', '4', '9', '1', '9', '9', '2', '1', '1', '9', '7', '10', '3', '10', '2', '9', '2', '9', '2', '8', '2', '1', '2', '3', '1', '1', '1', '8', '5', '8', '3', '1', '1', '4', '2', '2', '1', '3', '8', '2', '1', '1', '1', '2', '9', '4', '1', '8', '1', '2', '8', '2', '9', '2', '2', '5', '5', '2', '5', '2', '1', '8', '8', '1', '3', '12', '2', '4', '3', '2', '2', '2', '6', '2', '3', '3', '1', '8', '2', '2', '1', '1', '8', '8', '10', '1', '9', '2', '9', '12', '0', '2', '1', '3', '2', '2', '1', '2', '1', '9', '9', '2', '9', '8', '8', '1', '7', '8', '1', '2', '6', '9', '8', '10', '1', '12', '2', '2', '1', '8', '2', '1', '12', '7', '4', '2', '10', '2', '2', '1', '2', '1', '2', '1', '1', '2', '1', '9', '8', '10', '2', '10', '9', '9', '12', '5', '3', '2', '8', '2', '8', '3', '3', '1', '4', '9', '10', '2', '4', '8', '1', '2', '10', '12', '12', '3', '1', '1', '2', '9', '6', '6', '1', '9', '1', '8', '1', '8', '8', '8', '8', '2', '2', '2', '8', '8', '2', '1', '1', '1', '1', '1', '8', '8', '5', '8', '8', '1', '8', '9', '9', '1', '7', '12', '9', '9', '7', '2', '2', '7', '8', '9', '8', '2', '9', '6', '3', '4', '8', '4', '1', '2', '9', '8', '8', '8', '2', '1', '1', '8', '8', '3', '1', '9', '9', '7', '1', '2', '2', '2', '9', '4', '1', '2', '12', '1', '8', '2', '1', '9', '1', '8', '9', '9', '2', '5', '4', '2', '1', '2', '8', '12', '8', '10', '3', '8', '8', '8', '2', '2', '8', '2', '8', '2', '4', '8', '10', '2', '7', '2', '1', '8', '8', '9', '5', '8', '1', '12', '7', '10', '2', '2', '2', '1', '4', '8', '12', '1', '8', '2', '2', '9', '1', '8', '3', '1', '1', '5', '10', '2', '1', '2', '4', '8', '9', '3', '2', '8', '7', '4', '4', '9', '9', '2', '8', '1', '1', '2', '4', '7', '1', '9', '1', '1', '1', '8', '4', '1', '3', '1', '2', '2', '9', '1', '8', '8', '2', '2', '9', '9', '1', '8', '8', '1', '9', '8', '8', '2', '2', '2', '2', '1', '1', '1', '7', '4', '1', '1', '4', '2', '3', '3', '2', '3', '9', '2', '8', '2', '1', '5', '1', '2', '2', '1', '3', '5', '2', '9', '1', '1', '9', '8', '1', '4', '8', '1', '8', '2', '4', '8', '4', '7', '9', '8', '1', '2', '2', '4', '4', '11', '3', '11', '1', '3', '8', '1', '9', '6', '8', '9', '3', '9', '8', '9', '2', '3', '3', '7', '1', '7', '2', '9', '9', '1', '8', '8', '1', '5', '5', '8', '8', '1', '2', '1', '3', '9', '9', '5', '9', '8', '1', '4', '3', '8', '1', '1', '2', '1', '8', '9', '8', '2', '2', '2', '1', '2', '4', '10', '8', '1', '9', '1', '8', '1', '1', '9', '9', '8', '11', '1', '8', '2', '1', '2', '8', '2', '8', '8', '2', '9', '6', '10', '1', '8', '6', '4', '5', '11', '2', '2', '8', '1', '8', '1', '8', '1', '4', '8', '1', '1', '1', '8', '3', '7', '3', '3', '7', '1', '1', '4', '8', '1', '6', '4', '2', '2', '1', '1', '1', '1', '6', '2', '2', '8', '1', '8', '5', '5', '2', '1', '1', '2', '3', '7', '1', '1', '6', '2', '9', '3', '9', '8', '9', '2', '1', '4', '2', '9', '13', '9', '2', '8', '12', '4', '1', '1', '9', '5', '1', '1', '1', '1', '8', '2', '1', '1', '2', '9', '0', '12', '1', '1', '8', '9', '8', '3', '2', '2', '12', '8', '2', '2', '2', '1', '10', '1', '9', '8', '9', '5', '2', '2', '9', '2', '8', '4', '8', '3', '7', '9', '2', '2', '7', '2', '2', '8', '1', '1', '9', '10', '9', '8', '2', '9', '0', '7', '1', '1', '1', '9', '8', '2', '8', '12', '8', '3', '2', '8', '8', '7', '7', '8', '12', '2', '1', '3', '9', '2', '2', '8', '2', '8', '8', '12', '8', '8', '1', '8', '9', '2', '4', '4', '7', '2', '1', '1', '9', '3', '7', '1', '3', '9', '8', '2', '2', '3', '1', '3', '10', '2', '3', '7', '1', '8', '1', '1', '10', '3', '10', '8', '4', '1', '2', '4', '3', '10', '9', '3', '9', '5', '8', '7', '2', '9', '3', '3', '2', '11', '8', '1', '1', '7', '2', '7', '2', '9', '4', '8', '1', '12', '8', '7', '9', '1', '0', '2', '6', '4', '9', '3', '2', '4', '12', '9', '8', '5', '9', '2', '8', '4', '6', '8', '9', '3', '1', '8', '7', '3', '1', '1', '1', '8', '9', '1', '8', '7', '9', '8', '2', '1', '2', '2', '3', '2', '2', '1', '1', '4', '9', '7', '3', '2', '8', '8', '9', '1', '9', '7', '4', '2', '2', '8', '5', '9', '1', '5', '7', '1', '4', '6', '9', '10', '1', '8', '2', '9', '1', '9', '2', '9', '8', '2', '9', '9', '11', '2', '12', '2', '2', '9', '4', '8', '1', '9', '9', '2', '8', '9', '1', '2', '9', '3', '8', '8', '1', '7', '8', '8', '2', '3', '2', '2', '2', '2', '9', '8', '10', '2', '2', '1', '9', '11', '1', '8', '8', '2', '4', '2', '1', '2', '8', '8', '9', '2', '8', '9', '2', '3', '1', '2', '1', '3', '10', '8', '1', '2', '4', '1', '9', '8', '9', '3', '3', '9', '8', '2', '11', '3', '7', '1', '8', '7', '1', '8', '8', '8', '1', '4', '8', '8', '1', '9', '3', '1', '1', '2', '8', '8', '8', '2', '1', '8', '10', '2', '12', '8', '8', '9', '1', '8', '1', '3', '2', '2', '10', '7', '2', '2', '1', '1', '8', '2', '9', '1', '2', '9', '3', '3', '10', '1', '4', '7', '10', '12', '2', '4', '10', '8', '6', '2', '9', '1', '8', '3', '1', '2', '8', '8', '2', '12', '10', '2', '10', '9', '1', '2', '5', '8', '12', '8', '8', '7', '8', '8', '1', '8', '2', '2', '9', '1', '8', '10', '5', '5', '5', '8', '1', '9', '1', '13', '1', '2', '2', '1', '2', '4', '9', '2', '1', '9', '13', '11', '1', '9', '9', '3', '10', '8', '7', '8', '8', '4', '2', '1', '9', '4', '3', '1', '1', '10', '9', '2', '8', '6', '2', '1', '1', '3', '2', '8', '1', '1', '1', '1', '2', '4', '1', '1', '10', '1', '8', '1', '1', '9', '1', '7', '1', '1', '9', '9', '2', '12', '9', '1', '8', '1', '12', '1', '8', '3', '8', '1', '8', '1', '2', '10', '1', '1', '2', '2', '9', '3', '9', '12', '1', '9', '1', '2', '8', '7', '2', '1', '1', '3', '2', '8', '4', '4', '5', '12', '7', '8', '9', '6', '1', '8', '9', '2', '1', '9', '1', '4', '1', '8', '2', '1', '1', '8', '7', '4', '9', '1', '8', '4', '2', '7', '5', '1', '3', '9', '2', '8', '2', '1', '2', '9', '1', '1', '2', '3', '4', '3', '1', '2', '2', '10', '2', '2', '8', '1', '1', '2', '4', '9', '2', '11', '9', '8', '9', '2', '1', '10', '4', '1', '1', '10', '7', '2', '10', '8', '1', '10', '8', '8', '8', '8', '8', '8', '2', '2', '2', '8', '3', '4', '1', '1', '1', '2', '1', '8', '8', '2', '8', '3', '8', '3', '1', '2', '3', '1', '1', '10', '1', '4', '4', '3', '3', '1', '9', '1', '1', '2', '1', '1', '6', '1', '1', '1', '9', '1', '4', '1', '1', '1', '1', '1', '1', '2', '12', '8', '8', '1', '9', '12', '10', '8', '2', '12', '1', '1', '2', '1', '7', '10', '7', '1', '2', '1', '10', '8', '2', '2', '8', '2', '2', '1', '9', '8', '8', '8', '8', '3', '1', '4', '2', '8', '3', '10', '10', '1', '9', '1', '9', '1', '9', '1', '8', '3', '1', '1', '8', '8', '8', '6', '3', '4', '1', '1', '1', '9', '9', '5', '2', '10', '2', '6', '2', '2', '7', '1', '8', '2', '11', '1', '1', '1', '2', '1', '9', '4', '4', '2', '8', '4', '6', '8', '8', '10', '10', '2', '1', '2', '9', '1', '8', '8', '3', '4', '8', '8', '2', '8', '8', '1', '2', '1', '2', '9', '4', '1', '3', '3', '3', '8', '8', '8', '2', '6', '4', '3', '3', '12', '3', '4', '9', '6', '8', '7', '7', '8', '6', '4', '1', '10', '6', '3', '3', '8', '1', '1', '8', '1', '11', '8', '1', '1', '2', '3', '11', '7', '2', '1', '1', '1', '9', '1', '2', '1', '1', '9', '8', '1', '1', '8', '0', '4', '1', '1', '2', '1', '1', '2', '2', '9', '1', '10', '1', '3', '8', '9', '3', '3', '1', '10', '1', '9', '1', '1', '7', '4', '1', '8', '10', '1', '2', '10', '1', '1', '2', '3', '3', '9', '1', '8', '10', '1', '7', '9', '1', '9', '6', '3', '9', '2', '8', '4', '12', '12', '3', '1', '1', '9', '1', '9', '2', '1', '1', '1', '1', '1', '9', '2', '1', '2', '10', '9', '8', '8', '10', '8', '2', '3', '2', '1', '12', '8', '12', '2', '9', '1', '3', '2', '5', '2', '2', '8', '2', '2', '4', '10', '9', '10', '9', '4', '2', '4', '12', '8', '2', '1', '8', '3', '9', '2', '9', '4', '1', '1', '1', '1', '1', '9', '6', '2', '9', '2', '1', '3', '2', '2', '5', '8', '9', '1', '6', '3', '1', '2', '2', '2', '6', '1', '8', '10', '8', '3', '3', '13', '2', '9', '9', '10', '6', '12', '8', '2', '9', '2', '3', '8', '8', '1', '2', '2', '3', '2', '10', '1', '8', '9', '1', '8', '9', '8', '8', '2', '10', '2', '1', '1', '1', '9', '9', '12', '1', '1', '4', '8', '3', '2', '2', '4', '4', '2', '2', '1', '1', '1', '9', '2', '2', '4', '1', '1', '8', '8', '8', '9', '2', '13', '9', '9', '4', '1', '9', '7', '10', '10', '8', '1', '9', '1', '1', '1', '8', '12', '4', '1', '2', '1', '1', '1', '7', '8', '3', '3', '1', '2', '2', '7', '2', '2', '10', '1', '8', '4', '0', '1', '9', '4', '1', '1', '2', '10', '10', '7', '9', '3', '12', '11', '2', '8', '10', '8', '9', '4', '3', '4', '10', '1', '1', '9', '3', '10', '1', '5', '3', '3', '2', '2', '1', '1', '6', '1', '7', '1', '1', '12', '1', '1', '1', '10', '1', '8', '8', '1', '3', '8', '3', '1', '2', '8', '3', '4', '2', '8', '8', '6', '1', '1', '4', '1', '2', '9', '5', '1', '7', '8', '9', '2', '4', '9', '8', '5', '9', '9', '9', '1', '2', '1', '3', '8', '9', '2', '9', '8', '9', '1', '8', '13', '9', '3', '8', '1', '4', '1', '3', '1', '6', '4', '10', '2', '2', '7', '8', '8', '12', '8', '9', '9', '9', '10', '3', '12', '12', '2', '9', '10', '9', '7', '2', '3', '4', '10', '10', '2', '7', '7', '1', '7', '3', '9', '1', '9', '8', '2', '9', '1', '8', '1', '10', '8', '8', '2', '7', '8', '1', '8', '9', '8', '1', '4', '1', '8', '8', '8', '1', '1', '10', '3', '2', '2', '6', '8', '9', '8', '2', '1', '1', '3', '8', '8', '1', '4', '4', '3', '9', '10', '4', '2', '4', '2', '9', '2', '2', '9', '1', '1', '10', '1', '1', '2', '1', '1', '1', '9', '1', '8', '8', '8', '2', '3', '8', '1', '6', '7', '3', '1', '9', '1', '2', '1', '6', '1', '2', '10', '1', '4', '2', '3', '2', '9', '9', '4', '9', '9', '1', '2', '8', '4', '7', '8', '7', '13', '9', '1', '2', '1', '9', '3', '3', '6', '8', '9', '10', '9', '2', '2', '3', '12', '1', '1', '1', '13', '2', '1', '7', '10', '8', '9', '1', '2', '9', '1', '1', '1', '2', '1', '12', '5', '6', '9', '10', '3', '1', '10', '1', '3', '2', '8', '9', '2', '10', '8', '8', '9', '2', '8', '9', '2', '2', '2', '8', '8', '2', '3', '1', '4', '8', '1', '9', '9', '2', '8', '10', '9', '1', '2', '12', '8', '8', '0', '2', '1', '1', '9', '10', '1', '6', '2', '2', '10', '2', '2', '4', '2', '2', '12', '1', '6', '7', '8', '9', '1', '3', '5', '7', '3', '1', '3', '1', '1', '3', '2', '6', '3', '2', '1', '8', '3', '5', '2', '1', '1', '8', '1', '1', '0', '2', '3', '5', '3', '1', '1', '9', '8', '8', '8', '8', '4', '7', '2', '9', '12', '5', '9', '3', '12', '3', '9', '1', '9', '1', '7', '3', '9', '3', '10', '1', '2', '1', '4', '9', '8', '2', '9', '11', '2', '8', '9', '4', '1', '1', '1', '1', '1', '1', '1', '9', '1', '9', '1', '1', '1', '12', '3', '10', '3', '6', '1', '7', '10', '8', '3', '7', '1', '1', '2', '6', '1', '3', '9', '2', '8', '1', '8', '8', '1', '8', '10', '12', '1', '10', '12', '10', '1', '8', '8', '1', '4', '7', '9', '2', '1', '9', '3', '8', '12', '10', '6', '1', '6', '8', '3', '1', '3', '1', '10', '10', '9', '8', '1', '2', '9', '8', '8', '1', '9', '8', '8', '3', '1', '3', '9', '12', '1', '8', '1', '5', '8', '11', '5', '5', '1', '2', '1', '3', '1', '1', '1', '9', '8', '8', '1', '9', '10', '8', '4', '1', '1', '8', '10', '8', '9', '1', '10', '1', '7', '2', '1', '1', '9', '6', '2', '9', '8', '1', '1', '1', '9', '8', '8', '1', '9', '9', '1', '1', '2', '7', '9', '2', '9', '9', '8', '7', '6', '8', '7', '8', '2', '9', '2', '1', '8', '8', '8', '2', '6', '3', '1', '8', '12', '12', '8', '7', '1', '2', '2', '7', '1', '9', '8', '1', '4', '1', '1', '8', '5', '8', '1', '8', '8', '8', '8', '1', '2', '11', '1', '1', '1', '2', '7', '1', '9', '2', '2', '2', '9', '8', '9', '2', '9', '1', '8', '1', '7', '11', '13', '1', '9', '8', '2', '2', '1', '2', '3', '10', '3', '10', '4', '3', '9', '1', '8', '1', '1', '13', '1', '1', '3', '8', '1', '2', '2', '2', '9', '4', '1', '8', '1', '9', '3', '6', '8', '5', '2', '8', '2', '8', '1', '9', '2', '2', '8', '1', '1', '4', '8', '9', '2', '2', '7', '1', '8', '12', '2', '12', '8', '2', '9', '9', '3', '8', '1', '1', '9', '9', '9', '2', '8', '1', '9', '1', '8', '9', '8', '2', '2', '4', '1', '9', '10', '10', '8', '8', '1', '1', '2', '1', '4', '1', '2', '1', '1', '3', '12', '8', '8', '8', '8', '10', '2', '8', '12', '1', '9', '9', '8', '9', '1', '2', '1', '8', '8', '8', '2', '8', '8', '3', '10', '8', '9', '1', '1', '1', '3', '4', '8', '10', '6', '3', '1', '3', '2', '8', '9', '3', '5', '4', '1', '5', '2', '9', '9', '9', '1', '1', '1', '9', '9', '6', '10', '9', '8', '12', '2', '9', '1', '2', '1', '1', '1', '8', '1', '12', '1', '12', '1', '9', '1', '8', '1', '2', '1', '2', '8', '8', '10', '1', '9', '1', '2', '2', '8', '8', '3', '12', '1', '11', '2', '10', '12', '11', '2', '1', '10', '1', '1', '9', '3', '10', '2', '7', '1', '8', '1', '9', '9', '2', '1', '3', '12', '5', '9', '9', '8', '1', '8', '8', '8', '1', '1', '3', '9', '8', '9', '8', '3', '10', '3', '7', '8', '9', '2', '3', '1', '9', '10', '9', '2', '8', '2', '6', '4', '1', '1', '3', '8', '2', '3', '9', '1', '2', '1', '8', '1', '2', '2', '2', '2', '9', '9', '4', '8', '9', '1', '1', '2', '9', '1', '5', '2', '9', '8', '10', '1', '2', '10', '8', '5', '6', '4', '8', '1', '1', '1', '8', '9', '2', '8', '8', '8', '12', '2', '2', '1', '8', '1', '2', '1', '9', '8', '8', '9', '1', '1', '8', '7', '1', '8', '0', '2', '3', '8', '1', '1', '8', '1', '1', '12', '3', '2', '10', '8', '10', '6', '1', '8', '8', '10', '8', '8', '1', '4', '8', '1', '2', '9', '8', '1', '1', '3', '3', '5', '1', '1', '2', '1', '2', '1', '9', '8', '3', '9', '8', '2', '10', '9', '1', '8', '1', '10', '9', '10', '1', '2', '3', '8', '1', '3', '1', '9', '9', '8', '1', '9', '8', '8', '9', '1', '9', '2', '8', '8', '2', '13', '8', '10', '8', '9', '3', '2', '1', '3', '1', '12', '10', '10', '1', '11', '1', '2', '4', '2', '10', '1', '8', '11', '1', '1', '1', '2', '2', '8', '2', '9', '8', '9', '9', '12', '8', '1', '9', '2', '4', '1', '8', '3', '9', '1', '6', '5', '8', '9', '3', '3', '2', '1', '1', '1', '8', '11', '1', '8', '7', '1', '11', '9', '1', '7', '9', '9', '8', '8', '8', '12', '8', '1', '2', '9', '8', '8', '2', '10', '9', '8', '8', '4', '10', '9', '8', '2', '10', '10', '2', '1', '2', '1', '2', '1', '4', '1', '7', '1', '1', '3', '12', '9', '8', '5', '9', '10', '1', '9', '9', '2', '7', '4', '1', '1', '8', '12', '8', '2', '2', '9', '4', '1', '1', '5', '9', '1', '8', '12', '8', '1', '1', '4', '9', '10', '2', '3', '1', '3', '3', '3', '8', '1', '2', '1', '2', '1', '2', '9', '1', '2', '8', '9', '4', '8', '8', '10', '2', '12', '5', '5', '1', '8', '1', '7', '8', '1', '1', '8', '9', '12', '4', '9', '3', '1', '2', '5', '9', '2', '1', '2', '2', '1', '2', '8', '1', '8', '8', '2', '2', '1', '8', '4', '1', '2', '10', '9', '10', '8', '1', '10', '4', '8', '8', '10', '2', '8', '1', '1', '3', '10', '1', '4', '2', '8', '3', '3', '2', '9', '8', '5', '5', '9', '3', '10', '8', '1', '2', '1', '2', '9', '1', '1', '2', '9', '9', '8', '8', '11', '9', '4', '1', '8', '2', '7', '1', '2', '8', '2', '9', '9', '8', '2', '9', '9', '8', '8', '1', '2', '8', '1', '1', '2', '9', '8', '1', '4', '9', '1', '8', '12', '1', '12', '2', '8', '2', '9', '1', '12', '1', '2', '9', '1', '1', '9', '3', '2', '9', '9', '2', '11', '8', '7', '9', '9', '9', '8', '8', '1', '2', '8', '1', '1', '4', '1', '2', '10', '1', '9', '2', '1', '1', '1', '9', '1', '13', '4', '3', '2', '1', '1', '2', '2', '7', '8', '7', '1', '1', '2', '8', '8', '3', '4', '8', '2', '10', '13', '8', '8', '9', '2', '2', '1', '1', '10', '4', '2', '7', '9', '8', '2', '1', '9', '8', '1', '1', '9', '8', '9', '8', '2', '8', '2', '2', '9', '2', '1', '9', '1', '2', '4', '8', '2', '1', '1', '8', '1', '4', '1', '9', '3', '9', '2', '7', '1', '9', '6', '1', '10', '2', '2', '2', '2', '2', '10', '10', '10', '9', '9', '9', '3', '12', '10', '1', '1', '10', '1', '1', '2', '2', '1', '2', '3', '12', '8', '8', '1', '2', '9', '9', '1', '1', '10', '8', '9', '8', '3', '1', '1', '3', '1', '10', '1', '1', '1', '4', '1', '1', '1', '7', '10', '2', '1', '8', '1', '9', '3', '1', '10', '8', '8', '1', '5', '9', '1', '2', '1', '8', '8', '1', '3', '1', '10', '2', '1', '1', '2', '8', '2', '5', '3', '5', '2', '1', '1', '9', '1', '7', '9', '9', '2', '8', '1', '12', '4', '1', '4', '9', '9', '4', '8', '1', '10', '10', '8', '8', '8', '9', '2', '3', '5', '1', '8', '1', '1', '1', '7', '10', '1', '4', '12', '3', '2', '2', '9', '1', '1', '1', '2', '8', '4', '1', '7', '3', '12', '8', '6', '2', '2', '7', '9', '11', '1', '10', '9', '1', '8', '12', '2', '11', '1', '2', '3', '1', '1', '1', '8', '9', '2', '2', '2', '2', '3', '3', '8', '10', '4', '1', '2', '9', '8', '2', '2', '9', '1', '8', '11', '4', '2', '8', '7', '1', '1', '7', '3', '9', '9', '4', '5', '8', '10', '2', '9', '9', '9', '8', '1', '9', '2', '2', '2', '1', '2', '9', '4', '3', '12', '4', '3', '2', '3', '8', '3', '10', '1', '10', '1', '1', '8', '10', '6', '8', '9', '9', '1', '2', '9', '2', '9', '3', '8', '1', '1', '12', '1', '5', '1', '10', '8', '10', '2', '7', '1', '1', '1', '1', '3', '2', '3', '5', '9', '1', '2', '1', '9', '8', '10', '9', '8', '1', '8', '1', '8', '2', '8', '1', '2', '8', '1', '2', '5', '8', '12', '2', '1', '1', '1', '1', '1', '8', '8', '2', '1', '4', '2', '8', '8', '8', '2', '9', '9', '11', '1', '2', '9', '3', '1', '1', '8', '9', '2', '4', '10', '1', '9', '8', '4', '9', '8', '10', '8', '8', '9', '2', '8', '9', '3', '3', '3', '4', '3', '2', '2', '10', '8', '2', '1', '5', '1', '9', '1', '1', '2', '1', '2', '3', '1', '1', '8', '8', '1', '10', '8', '8', '1', '6', '4', '8', '2', '5', '8', '1', '3', '8', '2', '8', '9', '1', '1', '8', '12', '10', '1', '5', '8', '2', '8', '10', '9', '9', '1', '6', '9', '1', '10', '8', '2', '9', '10', '1', '2', '1', '1', '8', '8', '3', '9', '8', '1', '10', '2', '8', '1', '10', '1', '5', '1', '1', '1', '5', '1', '1', '1', '4', '1', '1', '1', '8', '2', '8', '1', '8', '3', '8', '9', '1', '8', '1', '8', '1', '1', '2', '2', '1', '12', '9', '9', '1', '8', '2', '1', '2', '8', '1', '9', '1', '1', '2', '1', '2', '2', '2', '1', '9', '9', '8', '8', '1', '1', '9', '10', '8', '4', '3', '3', '4', '9', '1', '3', '3', '10', '2', '2', '1', '8', '4', '1', '9', '4', '1', '1', '9', '8', '10', '4', '9', '9', '8', '2', '1', '3', '4', '9', '3', '8', '9', '1', '1', '2', '1', '7', '8', '2', '9', '1', '9', '2', '6', '9', '10', '8', '1', '1', '5', '9', '5', '1', '9', '10', '9', '3', '3', '8', '8', '2', '9', '2', '8', '2', '1', '8', '5', '8', '3', '10', '1', '1', '2', '1', '8', '1', '4', '3', '2', '9', '4', '9', '8', '8', '8', '1', '3', '9', '1', '9', '9', '1', '2', '1', '9', '8', '9', '8', '1', '1', '1', '4', '2', '2', '9', '9', '1', '1', '1', '3', '6', '1', '2', '3', '1', '4', '2', '9', '1', '9', '9', '1', '8', '2', '1', '9', '1', '8', '1', '8', '8', '2', '1', '9', '9', '6', '8', '8', '9', '8', '10', '8', '9', '5', '1', '1', '1', '1', '8', '1', '8', '12', '8', '1', '12', '9', '2', '8', '1', '1', '2', '1', '8', '6', '9', '9', '8', '1', '8', '9', '7', '7', '8', '9', '9', '1', '8', '3', '8', '1', '3', '3', '8', '9', '8', '2', '8', '1', '2', '9', '8', '1', '1', '1', '1', '9', '1', '9', '8', '12', '3', '8', '8', '1', '10', '10', '10', '2', '2', '1', '10', '2', '2', '10', '8', '12', '8', '1', '1', '1', '2', '1', '1', '3', '8', '1', '2', '2', '2', '1', '1', '6', '2', '8', '8', '8', '1', '1', '1', '9', '9', '2', '8', '10', '2', '8', '2', '1', '2', '9', '9', '2', '1', '1', '8', '8', '1', '3', '1', '8', '9', '1', '10', '9', '1', '1', '1', '1', '7', '2', '2', '1', '2', '8', '8', '1', '2', '1', '3', '2', '1', '8', '3', '9', '2', '10', '2', '10', '1', '2', '1', '1', '13', '9', '10', '8', '2', '2', '1', '1', '8', '2', '2', '1', '9', '1', '1', '8', '9', '9', '2', '9', '2', '2', '1', '1', '1', '8', '9', '4', '9', '9', '1', '8', '2', '1', '1', '8', '2', '2', '2', '8', '1', '1', '8', '2', '10', '2', '9', '9', '7', '1', '1', '2', '2', '8', '3', '1', '2', '1', '1', '8', '1', '1', '1', '9', '1', '1', '8', '8', '1', '9', '6', '1', '9', '1', '8', '1', '8', '8', '8', '1', '3', '8', '8', '2', '6', '3', '2', '1', '10', '1', '8', '2', '1', '6', '1', '1', '1', '8', '1', '10', '1', '1', '8', '9', '2', '6', '9', '2', '9', '4', '1', '8', '1', '3', '7', '8', '8', '5', '8', '7', '1', '1', '1', '13', '8', '3', '1', '1', '1', '1', '2', '8', '2', '12', '8', '1', '2', '5', '1', '8', '2', '10', '8', '2', '10', '1', '5', '2', '1', '1', '5', '2', '2', '7', '8', '2', '9', '9', '1', '10', '1', '8', '2', '5', '8', '1', '5', '1', '10', '1', '1', '9', '8', '6', '1', '8', '9', '8', '1', '8', '2', '2', '8', '3', '9', '1', '9', '3', '8', '4', '4', '3', '3', '1', '1', '1', '8', '9', '1', '1', '8', '3', '1', '2', '8', '2', '1', '1', '10', '1', '2', '2', '1', '1', '8', '2', '8', '2', '10', '2', '2', '10', '8', '8', '8', '13', '2', '2', '9', '2', '5', '9', '2', '1', '8', '8', '8', '1', '12', '12', '2', '9', '2', '1', '8', '2', '1', '8', '1', '9', '9', '2', '7', '1', '1', '7', '1', '4', '3', '1', '10', '3', '10', '9', '4', '6', '7', '5', '2', '1', '8', '8', '1', '4', '9', '2', '1', '8', '1', '1', '1', '1', '1', '8', '9', '8', '8', '8', '8', '9', '8', '10', '1', '8', '2', '7', '8', '9', '1', '2', '5', '9', '10', '8', '9', '12', '1', '6', '1', '2', '10', '9', '4', '1', '8', '10', '10', '1', '1', '1', '1', '1', '2', '3', '8', '1', '2', '10', '2', '1', '2', '4', '2', '2', '9', '1', '4', '2', '1', '1', '12', '12', '9', '9', '7', '1', '8', '4', '9', '9', '8', '1', '7', '3', '2', '1', '1', '4', '8', '1', '5', '8', '2', '8', '1', '14', '9', '12', '1', '10', '2', '1', '1', '8', '8', '6', '6', '3', '1', '2', '8', '1', '3', '2', '1', '8', '8', '1', '10', '8', '2', '8', '2', '5', '9', '1', '8', '8', '1', '1', '8', '8', '13', '1', '3', '8', '4', '1', '9', '7', '8', '8', '8', '3', '8', '1', '4', '2', '8', '8', '8', '7', '1', '1', '2', '8', '8', '9', '8', '8', '2', '3', '9', '9', '2', '1', '2', '2', '1', '8', '2', '6', '2', '1', '1', '9', '10', '3', '8', '2', '3', '2', '8', '8', '1', '8', '8', '8', '9', '1', '4', '1', '9', '2', '1', '8', '0', '1', '8', '1', '1', '8', '9', '9', '1', '3', '1', '2', '8', '8', '7', '1', '1', '2', '9', '2', '10', '2', '8', '2', '2', '2', '7', '1', '9', '8', '1', '3', '9', '2', '1', '10', '8', '1', '4', '1', '8', '5', '9', '8', '8', '1', '2', '2', '1', '8', '8', '6', '8', '1', '1', '8', '3', '2', '2', '1', '8', '1', '7', '6', '8', '1', '1', '1', '9', '1', '1', '1', '2', '1', '11', '10', '10', '11', '9']\n",
            "     filenum                                               text  label\n",
            "0          1  each other. But the section length and therefo...      8\n",
            "1          2  v. United States, L.R.A.,N.S. see Athanasaw v....      1\n",
            "2          3  has made no order which changes the appellant'...      8\n",
            "3          4  to withhold from judicial scrutiny has now bee...      2\n",
            "4          6  instrumentality whose sole purpose is to expor...      8\n",
            "...      ...                                                ...    ...\n",
            "6717    8411  substantially similar to those to which we giv...      1\n",
            "6718    8412  is \"strong evidence\" to the contrary. Ylst v. ...      1\n",
            "6719    8413  \"for the purpose of disturbing the public peac...      2\n",
            "6720    8414  that the sentencing phase of his trial violate...      1\n",
            "6721    8417  detailed exceptions. The controversy is now be...     10\n",
            "\n",
            "[6722 rows x 3 columns]\n"
          ]
        }
      ],
      "source": [
        "fh = open('sc_model_2.txt')\n",
        "filenums=[]\n",
        "records=[]\n",
        "for record in fh:\n",
        "    record=record.split(\"---\")\n",
        "    filenum=record[0]\n",
        "    modelnum=record[1]\n",
        "    text=record[2]\n",
        "    filenums.append(filenum)\n",
        "    records.append(text)\n",
        "    \n",
        "print(len(filenums))\n",
        "print(len(records))\n",
        "\n",
        "summarized_data = pd.DataFrame(list(zip(filenums, records)),columns =['filenum','text'])\n",
        "len_list = [len(ele.split()) for ele in records]\n",
        "res = 0 if len(len_list) == 0 else (float(sum(len_list)) / len(len_list))\n",
        "print(\"Average Length %s\" % res)\n",
        "\n",
        "temp_file = open(\"labels_sc.txt\", \"r\")\n",
        "#temp_file = open(\"labels_sc_279.txt\", \"r\")\n",
        "\n",
        "data = temp_file.read()\n",
        "\n",
        "label_list = data.split(\"\\n\")\n",
        "print(label_list)\n",
        "label_list = label_list[0:-1]\n",
        "print(label_list)\n",
        "label_list = [int(i) for i in label_list]\n",
        "temp_file.close()\n",
        "\n",
        "\n",
        "new_label_list=[]\n",
        "for num in filenums:\n",
        "  new_label_list.append(label_list[int(num)-1])\n",
        "summarized_data['label'] = new_label_list\n",
        "\n",
        "print(summarized_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bK-mvfm1v8-i",
        "outputId": "90ba78c7-53f2-4c00-85dd-5cc3f3b729a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_1[0][0]',                \n",
            "                                thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_bert_model[0][0]']          \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 15)           7695        ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 8  1  8 ...  2  1 10]\n",
            "[   1    2    3 ... 8413 8414 8417]\n",
            "[6622   35 6713 4989 8233 7444 5061 3942 8344  454 5464  317 4515 3001\n",
            " 6991 2875 1125 3885 6647 4857 4504  214 8340 2969 3847 4609 5092 1346\n",
            " 5993 7358 2752 7651 7301 8188 7073 4782 6069 2346 6445 6483 1663 4952\n",
            " 5072 1967 4071 7590  860 6418 5030 7297  626 5463 6029 6302 3819 7256\n",
            " 5667 1235 3993 1439 1323 4263 3835 2624 5801 4293 8129  320 5866 3247\n",
            " 5777   19 6059 4837 3843 1666  858 1418 5409 5274 6205 3167 2356 5997\n",
            " 6685  223 3444 3310 7721 5928 2360 7197 4252 8343  909 4354 4752 1421\n",
            "  649  829 1916 3488 2496 5765 6612 4004 4052 1403 6697 5268 1010 6293\n",
            " 6380 4602 6057 1851 4577 4236 7978 5083 6552 1058  361 7706 4186 4452\n",
            " 4396  749 8351 3834  812 5108  463 1743 6600 8264 8064 7121 8279 5556\n",
            " 5119 4673 7947 1882 3350 7617 7152 5199 4282 1092 8003 5025 5051  933\n",
            " 3969 1927 3615 1221 8062 1489 3296 7442 5459 2031 7351  234 2313 8015\n",
            " 7416 7921 6574 6257 8396  941 4520 3300 5640  240 7810 7425 1321 6580\n",
            "  928 1170 5404 8273 6742 7005   94 7459    9 7861 7975 1018 5029 2140\n",
            " 1415 7061 7767 4990 8124 5927 6051 3269 6916 5959 2846 6818 7052 7775\n",
            " 7582 8116 4240 4767 5248  430 1812 2175 6055 8331 3916  998 1564 7326\n",
            " 1559  292 3065 6764 6854 4915 5552 3583 1917 5902 4229 2144 7565 3712\n",
            " 7224 4502 1406 1651 6256 3827 6525 6318 2686 3616 5369 8235 1704 1011\n",
            " 3351 8089 8021  102  224 5671 8294 7707 2985 6624 5146 1428 7381 3709\n",
            " 6824 6308  701 2281 6837 5436 1741 3520 5995 2649 4886  966 5476 2406\n",
            " 1199 8387  513 5449 1028 2226 6166 4387  350 6123  678 5236 5041  734\n",
            " 3412 6085  587  489 5760 3376 4982 5115 4655 5575 3839 6743 7173 5724\n",
            " 7689 7787  520 8302 3150 5713 2247 8128 4607 6660 3505 4610 2866 4161\n",
            " 6905 7096  768 1062 4411 6087 2096 4903 7546 6630 2928 4640 7125 1792\n",
            " 2063 1700 2305 7928 8095  737 5849  922 2359 6015 4489 4361 6081 6346\n",
            " 5014 5167 1023 8009 3335  268 4777 5221 4323 6168  759  711 5702 6129\n",
            " 7181 1860 7192 1347 5467 3378 6579   67 2693 6134 4841 4270 4611 2943\n",
            " 6746 7696 3466 3950 4693 6125 8370 6142 6131  995 5548 2112 1721 5067\n",
            " 4015 7092 3156   25 1749 5554 4878 8248  458 6836 1006 5310 2698 6960\n",
            " 2339 1650 6227 4104 5130 7089 6993 2400 1289 4305 5585 7933 6284 7057\n",
            " 3807 5790 8347 2390 5280 2841 7220 1384 7693 8090 4171 5260  133 7451\n",
            " 3957 6793 1719 7379 4743 4746 5435 6216 4858 4417 8355 4492 2648 5104\n",
            " 5278 6539  743 7308  158 7543 2168 5913 7111 3414 5788 7592   39 7157\n",
            " 5647 4555 3956 1045 3617 4630 4384 6596   96 4539 6749 7202 2413 4749\n",
            " 5122 5052 1598 5929  252 5565  336 6008 4564 5580 7237 1466 1568 5958\n",
            " 7657 4327 5091 6918 6487 4082 2989 7448 4715 3382  467 3784 4624  764\n",
            " 4419 1117 7954 7122 6392  505 3174 1534  323 4164 3659 7526  498 5944\n",
            " 4572 2307 2929 7480 4412  731 4563 7692 1869 2472 6966 4238 6549 1594\n",
            " 2276 7269 2545 2216  605 6587 6315 5297 5282 4816 1619 5504 2536 6399\n",
            " 3917 6342 5754 5721 7385 7909 6126 7382 8382 4383 5502 7130  485 5862\n",
            " 4936 7674 3575 5372 3893 7109 4605 5211 5462 8065   50 4080 3334 8167\n",
            " 8378 5595 1873 8207 7076 4106 3719 4627  395  231 2405 5419 6973 3480\n",
            " 3246 5414 5931 8208 4825 6203 3019 5822   63 5854 6543 5787 4764 6701\n",
            " 6562 3822 4861 3051 1315 4805 8318 1089 8169 8218 7549  707 1284 1076\n",
            " 2465 3815 4636 3188  697   80 1455  381 7025 7127 3992 4026 2777 4961\n",
            " 7531  744 6640 1047 6684 4297 7939 7384  446 3287 6675 7752 5279 8196\n",
            " 3020 1203 4192 1552 4253 2887 2556 5729 1898 3208 5973 7033 1431 5915\n",
            " 8312 5799 7105  511 6404 2852 1561 4150 4835 1178]\n"
          ]
        }
      ],
      "source": [
        "gc.collect()\n",
        "tf.keras.backend.clear_session()\n",
        "dbert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "dbert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
        "max_len=512\n",
        "sentences=summarized_data['text']\n",
        "labels=summarized_data['label']\n",
        "filenumbers=summarized_data['filenum']\n",
        "len(sentences),len(labels),len(filenumbers)\n",
        "\n",
        "model_2=create_model()\n",
        "input_ids=[]\n",
        "attention_masks=[]\n",
        "\n",
        "for sent in sentences:\n",
        "  dbert_inps=dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n",
        "  input_ids.append(dbert_inps['input_ids'])\n",
        "  attention_masks.append(dbert_inps['attention_mask'])\n",
        "input_ids=np.asarray(input_ids)\n",
        "\n",
        "attention_masks=np.array(attention_masks)\n",
        "labels=np.array(labels)\n",
        "filenumbers=[int(i) for i in filenumbers]\n",
        "filenumbers=np.array(filenumbers)\n",
        "print(labels)\n",
        "print(filenumbers)\n",
        "\n",
        "#train_inp,val_inp,train_label,val_label,train_mask,val_mask,train_fnum,val_fnum=train_test_split(input_ids,labels,attention_masks,filenumbers,test_size=0.1,random_state=42)\n",
        "\n",
        "new_train_inp_2=[]\n",
        "new_val_inp_2=[]\n",
        "new_train_label_2=[]\n",
        "new_val_label_2=[]\n",
        "new_train_mask_2=[]\n",
        "new_train_fnum_2=[]\n",
        "new_val_fnum_2=[]\n",
        "new_val_mask_2=[]\n",
        "for i in range(len(train_fnum)):\n",
        "  if train_fnum[i] in filenumbers:\n",
        "    new_train_inp_2.append(train_inp[i])\n",
        "    #new_val_inp.append(val_inp[i])\n",
        "    new_train_label_2.append(train_label[i])\n",
        "    #new_val_label.append(val_label[i])\n",
        "    new_train_mask_2.append(train_mask[i])\n",
        "    #new_val_mask.append(val_mask[i])\n",
        "    new_train_fnum_2.append(train_fnum[i])\n",
        "\n",
        "for i in range(len(val_fnum)):\n",
        "  if val_fnum[i] in filenumbers:\n",
        "    #new_train_inp.append(train_inp[i])\n",
        "    new_val_inp_2.append(val_inp[i])\n",
        "    #new_train_label.append(train_label[i])\n",
        "    new_val_label_2.append(val_label[i])\n",
        "    #new_train_mask.append(train_mask[i])\n",
        "    new_val_mask_2.append(val_mask[i])\n",
        "    new_val_fnum_2.append(val_fnum[i])\n",
        "\n",
        "# print(new_train_label)\n",
        "# print(len(new_train_label))\n",
        "# print(new_val_label)\n",
        "# print(len(new_val_label))\n",
        "new_train_inp_2=np.array(new_train_inp_2)\n",
        "new_val_inp_2=np.array(new_val_inp_2)\n",
        "new_train_label_2=np.array(new_train_label_2)\n",
        "new_val_label_2=np.array(new_val_label_2)\n",
        "new_train_mask_2=np.array(new_train_mask_2)\n",
        "new_train_fnum_2=np.array(new_train_fnum_2)\n",
        "new_val_fnum_2=np.array(new_val_fnum_2)\n",
        "new_val_mask_2=np.array(new_val_mask_2)\n",
        "\n",
        "print(new_val_fnum_2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFTWfoELNpYx",
        "outputId": "6b434a30-05c5-48dc-a801-53527a53ed2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_1[0][0]',                \n",
            "                                thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_bert_model[0][0]']          \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 15)           7695        ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 8  1  8 ...  2  1 10]\n",
            "[   1    2    3 ... 8413 8414 8417]\n",
            "[6622   35 6713 4989 8233 7444 5061 3942 8344  454 5464  317 4515 3001\n",
            " 6991 2875 1125 3885 6647 4857 4504  214 8340 2969 3847 4609 5092 1346\n",
            " 5993 7358 2752 7651 7301 8188 7073 4782 6069 2346 6445 6483 1663 4952\n",
            " 5072 1967 4071 7590  860 6418 5030 7297  626 5463 6029 6302 3819 7256\n",
            " 5667 1235 3993 1439 1323 4263 3835 2624 5801 4293 8129  320 5866 3247\n",
            " 5777   19 6059 4837 3843 1666  858 1418 5409 5274 6205 3167 2356 5997\n",
            " 6685  223 3444 3310 7721 5928 2360 7197 4252 8343  909 4354 4752 1421\n",
            "  649  829 1916 3488 2496 5765 6612 4004 4052 1403 6697 5268 1010 6293\n",
            " 6380 4602 6057 1851 4577 4236 7978 5083 6552 1058  361 7706 4186 4452\n",
            " 4396  749 8351 3834  812 5108  463 1743 6600 8264 8064 7121 8279 5556\n",
            " 5119 4673 7947 1882 3350 7617 7152 5199 4282 1092 8003 5025 5051  933\n",
            " 3969 1927 3615 1221 8062 1489 3296 7442 5459 2031 7351  234 2313 8015\n",
            " 7416 7921 6574 6257 8396  941 4520 3300 5640  240 7810 7425 1321 6580\n",
            "  928 1170 5404 8273 6742 7005   94 7459    9 7861 7975 1018 5029 2140\n",
            " 1415 7061 7767 4990 8124 5927 6051 3269 6916 5959 2846 6818 7052 7775\n",
            " 7582 8116 4240 4767 5248  430 1812 2175 6055 8331 3916  998 1564 7326\n",
            " 1559  292 3065 6764 6854 4915 5552 3583 1917 5902 4229 2144 7565 3712\n",
            " 7224 4502 1406 1651 6256 3827 6525 6318 2686 3616 5369 8235 1704 1011\n",
            " 3351 8089 8021  102  224 5671 8294 7707 2985 6624 5146 1428 7381 3709\n",
            " 6824 6308  701 2281 6837 5436 1741 3520 5995 2649 4886  966 5476 2406\n",
            " 1199 8387  513 5449 1028 2226 6166 4387  350 6123  678 5236 5041  734\n",
            " 3412 6085  587  489 5760 3376 4982 5115 4655 5575 3839 6743 7173 5724\n",
            " 7689 7787  520 8302 3150 5713 2247 8128 4607 6660 3505 4610 2866 4161\n",
            " 6905 7096  768 1062 4411 6087 2096 4903 7546 6630 2928 4640 7125 1792\n",
            " 2063 1700 2305 7928 8095  737 5849  922 2359 6015 4489 4361 6081 6346\n",
            " 5014 5167 1023 8009 3335  268 4777 5221 4323 6168  759  711 5702 6129\n",
            " 7181 1860 7192 1347 5467 3378 6579   67 2693 6134 4841 4270 4611 2943\n",
            " 6746 7696 3466 3950 4693 6125 8370 6142 6131  995 5548 2112 1721 5067\n",
            " 4015 7092 3156   25 1749 5554 4878 8248  458 6836 1006 5310 2698 6960\n",
            " 2339 1650 6227 4104 5130 7089 6993 2400 1289 4305 5585 7933 6284 7057\n",
            " 3807 5790 8347 2390 5280 2841 7220 1384 7693 8090 4171 5260  133 7451\n",
            " 3957 6793 1719 7379 4743 4746 5435 6216 4858 4417 8355 4492 2648 5104\n",
            " 5278 6539  743 7308  158 7543 2168 5913 7111 3414 5788 7592   39 7157\n",
            " 5647 4555 3956 1045 3617 4630 4384 6596   96 4539 6749 7202 2413 4749\n",
            " 5122 5052 1598 5929  252 5565  336 6008 4564 5580 7237 1466 1568 5958\n",
            " 7657 4327 5091 6918 6487 4082 2989 7448 4715 3382  467 3784 4624  764\n",
            " 4419 1117 7954 7122 6392  505 3174 1534  323 4164 3659 7526  498 5944\n",
            " 4572 2307 2929 7480 4412  731 4563 7692 1869 2472 6966 4238 6549 1594\n",
            " 2276 7269 2545 2216  605 6587 6315 5297 5282 4816 1619 5504 2536 6399\n",
            " 3917 6342 5754 5721 7385 7909 6126 7382 8382 4383 5502 7130  485 5862\n",
            " 4936 7674 3575 5372 3893 7109 4605 5211 5462 8065   50 4080 3334 8167\n",
            " 8378 5595 1873 8207 7076 4106 3719 4627  395  231 2405 5419 6973 3480\n",
            " 3246 5414 5931 8208 4825 6203 3019 5822   63 5854 6543 5787 4764 6701\n",
            " 6562 3822 4861 3051 1315 4805 8318 1089 8169 8218 7549  707 1284 1076\n",
            " 2465 3815 4636 3188  697   80 1455  381 7025 7127 3992 4026 2777 4961\n",
            " 7531  744 6640 1047 6684 4297 7939 7384  446 3287 6675 7752 5279 8196\n",
            " 3020 1203 4192 1552 4253 2887 2556 5729 1898 3208 5973 7033 1431 5915\n",
            " 8312 5799 7105  511 6404 2852 1561 4150 4835 1178]\n",
            "Thu Jun  2 14:16:55 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   42C    P0    34W / 250W |   2517MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "757/757 [==============================] - 472s 599ms/step - loss: 7.1425 - accuracy: 0.6057 - val_loss: 6.2599 - val_accuracy: 0.7141\n",
            "Epoch 2/5\n",
            "757/757 [==============================] - 453s 599ms/step - loss: 5.7069 - accuracy: 0.7706 - val_loss: 5.3341 - val_accuracy: 0.7575\n",
            "Epoch 3/5\n",
            "757/757 [==============================] - 449s 593ms/step - loss: 4.7335 - accuracy: 0.8370 - val_loss: 4.7448 - val_accuracy: 0.7470\n",
            "Epoch 4/5\n",
            "757/757 [==============================] - 449s 593ms/step - loss: 3.9182 - accuracy: 0.8898 - val_loss: 4.1547 - val_accuracy: 0.7395\n",
            "Epoch 5/5\n",
            "757/757 [==============================] - 449s 593ms/step - loss: 3.2167 - accuracy: 0.9250 - val_loss: 3.6822 - val_accuracy: 0.7560\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_3[0][0]',                \n",
            "                                thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_bert_model[1][0]']          \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 15)           7695        ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.7339667458432304\n",
            "Weighted F1: 0.7306929162641994\n",
            "Micro F1: 0.7339667458432304\n",
            "Weighted Precision: 0.7387600990245772\n",
            "Micro Precision: 0.7339667458432304\n",
            "Weighted Recall: 0.7339667458432304\n",
            "Micro Recall: 0.7339667458432304\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_1[0][0]',                \n",
            "                                thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_bert_model[0][0]']          \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 15)           7695        ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 8  1  8 ...  2  1 10]\n",
            "[   1    2    3 ... 8413 8414 8417]\n",
            "[6622   35 6713 4989 8233 7444 5061 3942 8344  454 5464  317 4515 3001\n",
            " 6991 2875 1125 3885 6647 4857 4504  214 8340 2969 3847 4609 5092 1346\n",
            " 5993 7358 2752 7651 7301 8188 7073 4782 6069 2346 6445 6483 1663 4952\n",
            " 5072 1967 4071 7590  860 6418 5030 7297  626 5463 6029 6302 3819 7256\n",
            " 5667 1235 3993 1439 1323 4263 3835 2624 5801 4293 8129  320 5866 3247\n",
            " 5777   19 6059 4837 3843 1666  858 1418 5409 5274 6205 3167 2356 5997\n",
            " 6685  223 3444 3310 7721 5928 2360 7197 4252 8343  909 4354 4752 1421\n",
            "  649  829 1916 3488 2496 5765 6612 4004 4052 1403 6697 5268 1010 6293\n",
            " 6380 4602 6057 1851 4577 4236 7978 5083 6552 1058  361 7706 4186 4452\n",
            " 4396  749 8351 3834  812 5108  463 1743 6600 8264 8064 7121 8279 5556\n",
            " 5119 4673 7947 1882 3350 7617 7152 5199 4282 1092 8003 5025 5051  933\n",
            " 3969 1927 3615 1221 8062 1489 3296 7442 5459 2031 7351  234 2313 8015\n",
            " 7416 7921 6574 6257 8396  941 4520 3300 5640  240 7810 7425 1321 6580\n",
            "  928 1170 5404 8273 6742 7005   94 7459    9 7861 7975 1018 5029 2140\n",
            " 1415 7061 7767 4990 8124 5927 6051 3269 6916 5959 2846 6818 7052 7775\n",
            " 7582 8116 4240 4767 5248  430 1812 2175 6055 8331 3916  998 1564 7326\n",
            " 1559  292 3065 6764 6854 4915 5552 3583 1917 5902 4229 2144 7565 3712\n",
            " 7224 4502 1406 1651 6256 3827 6525 6318 2686 3616 5369 8235 1704 1011\n",
            " 3351 8089 8021  102  224 5671 8294 7707 2985 6624 5146 1428 7381 3709\n",
            " 6824 6308  701 2281 6837 5436 1741 3520 5995 2649 4886  966 5476 2406\n",
            " 1199 8387  513 5449 1028 2226 6166 4387  350 6123  678 5236 5041  734\n",
            " 3412 6085  587  489 5760 3376 4982 5115 4655 5575 3839 6743 7173 5724\n",
            " 7689 7787  520 8302 3150 5713 2247 8128 4607 6660 3505 4610 2866 4161\n",
            " 6905 7096  768 1062 4411 6087 2096 4903 7546 6630 2928 4640 7125 1792\n",
            " 2063 1700 2305 7928 8095  737 5849  922 2359 6015 4489 4361 6081 6346\n",
            " 5014 5167 1023 8009 3335  268 4777 5221 4323 6168  759  711 5702 6129\n",
            " 7181 1860 7192 1347 5467 3378 6579   67 2693 6134 4841 4270 4611 2943\n",
            " 6746 7696 3466 3950 4693 6125 8370 6142 6131  995 5548 2112 1721 5067\n",
            " 4015 7092 3156   25 1749 5554 4878 8248  458 6836 1006 5310 2698 6960\n",
            " 2339 1650 6227 4104 5130 7089 6993 2400 1289 4305 5585 7933 6284 7057\n",
            " 3807 5790 8347 2390 5280 2841 7220 1384 7693 8090 4171 5260  133 7451\n",
            " 3957 6793 1719 7379 4743 4746 5435 6216 4858 4417 8355 4492 2648 5104\n",
            " 5278 6539  743 7308  158 7543 2168 5913 7111 3414 5788 7592   39 7157\n",
            " 5647 4555 3956 1045 3617 4630 4384 6596   96 4539 6749 7202 2413 4749\n",
            " 5122 5052 1598 5929  252 5565  336 6008 4564 5580 7237 1466 1568 5958\n",
            " 7657 4327 5091 6918 6487 4082 2989 7448 4715 3382  467 3784 4624  764\n",
            " 4419 1117 7954 7122 6392  505 3174 1534  323 4164 3659 7526  498 5944\n",
            " 4572 2307 2929 7480 4412  731 4563 7692 1869 2472 6966 4238 6549 1594\n",
            " 2276 7269 2545 2216  605 6587 6315 5297 5282 4816 1619 5504 2536 6399\n",
            " 3917 6342 5754 5721 7385 7909 6126 7382 8382 4383 5502 7130  485 5862\n",
            " 4936 7674 3575 5372 3893 7109 4605 5211 5462 8065   50 4080 3334 8167\n",
            " 8378 5595 1873 8207 7076 4106 3719 4627  395  231 2405 5419 6973 3480\n",
            " 3246 5414 5931 8208 4825 6203 3019 5822   63 5854 6543 5787 4764 6701\n",
            " 6562 3822 4861 3051 1315 4805 8318 1089 8169 8218 7549  707 1284 1076\n",
            " 2465 3815 4636 3188  697   80 1455  381 7025 7127 3992 4026 2777 4961\n",
            " 7531  744 6640 1047 6684 4297 7939 7384  446 3287 6675 7752 5279 8196\n",
            " 3020 1203 4192 1552 4253 2887 2556 5729 1898 3208 5973 7033 1431 5915\n",
            " 8312 5799 7105  511 6404 2852 1561 4150 4835 1178]\n",
            "Thu Jun  2 14:58:07 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   54C    P0    37W / 250W |  15855MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "757/757 [==============================] - 472s 600ms/step - loss: 7.1360 - accuracy: 0.5813 - val_loss: 6.1634 - val_accuracy: 0.7066\n",
            "Epoch 2/5\n",
            "757/757 [==============================] - 453s 599ms/step - loss: 5.6408 - accuracy: 0.7646 - val_loss: 5.2532 - val_accuracy: 0.7515\n",
            "Epoch 3/5\n",
            "757/757 [==============================] - 449s 593ms/step - loss: 4.6750 - accuracy: 0.8292 - val_loss: 4.6415 - val_accuracy: 0.7410\n",
            "Epoch 4/5\n",
            "757/757 [==============================] - 449s 593ms/step - loss: 3.8530 - accuracy: 0.8842 - val_loss: 4.1186 - val_accuracy: 0.7290\n",
            "Epoch 5/5\n",
            "757/757 [==============================] - 449s 593ms/step - loss: 3.1533 - accuracy: 0.9250 - val_loss: 3.5802 - val_accuracy: 0.7395\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_3[0][0]',                \n",
            "                                thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_bert_model[1][0]']          \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 15)           7695        ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.7315914489311164\n",
            "Weighted F1: 0.734252902379029\n",
            "Micro F1: 0.7315914489311163\n",
            "Weighted Precision: 0.7432318080336401\n",
            "Micro Precision: 0.7315914489311164\n",
            "Weighted Recall: 0.7315914489311164\n",
            "Micro Recall: 0.7315914489311164\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_1[0][0]',                \n",
            "                                thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_bert_model[0][0]']          \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 15)           7695        ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 8  1  8 ...  2  1 10]\n",
            "[   1    2    3 ... 8413 8414 8417]\n",
            "[6622   35 6713 4989 8233 7444 5061 3942 8344  454 5464  317 4515 3001\n",
            " 6991 2875 1125 3885 6647 4857 4504  214 8340 2969 3847 4609 5092 1346\n",
            " 5993 7358 2752 7651 7301 8188 7073 4782 6069 2346 6445 6483 1663 4952\n",
            " 5072 1967 4071 7590  860 6418 5030 7297  626 5463 6029 6302 3819 7256\n",
            " 5667 1235 3993 1439 1323 4263 3835 2624 5801 4293 8129  320 5866 3247\n",
            " 5777   19 6059 4837 3843 1666  858 1418 5409 5274 6205 3167 2356 5997\n",
            " 6685  223 3444 3310 7721 5928 2360 7197 4252 8343  909 4354 4752 1421\n",
            "  649  829 1916 3488 2496 5765 6612 4004 4052 1403 6697 5268 1010 6293\n",
            " 6380 4602 6057 1851 4577 4236 7978 5083 6552 1058  361 7706 4186 4452\n",
            " 4396  749 8351 3834  812 5108  463 1743 6600 8264 8064 7121 8279 5556\n",
            " 5119 4673 7947 1882 3350 7617 7152 5199 4282 1092 8003 5025 5051  933\n",
            " 3969 1927 3615 1221 8062 1489 3296 7442 5459 2031 7351  234 2313 8015\n",
            " 7416 7921 6574 6257 8396  941 4520 3300 5640  240 7810 7425 1321 6580\n",
            "  928 1170 5404 8273 6742 7005   94 7459    9 7861 7975 1018 5029 2140\n",
            " 1415 7061 7767 4990 8124 5927 6051 3269 6916 5959 2846 6818 7052 7775\n",
            " 7582 8116 4240 4767 5248  430 1812 2175 6055 8331 3916  998 1564 7326\n",
            " 1559  292 3065 6764 6854 4915 5552 3583 1917 5902 4229 2144 7565 3712\n",
            " 7224 4502 1406 1651 6256 3827 6525 6318 2686 3616 5369 8235 1704 1011\n",
            " 3351 8089 8021  102  224 5671 8294 7707 2985 6624 5146 1428 7381 3709\n",
            " 6824 6308  701 2281 6837 5436 1741 3520 5995 2649 4886  966 5476 2406\n",
            " 1199 8387  513 5449 1028 2226 6166 4387  350 6123  678 5236 5041  734\n",
            " 3412 6085  587  489 5760 3376 4982 5115 4655 5575 3839 6743 7173 5724\n",
            " 7689 7787  520 8302 3150 5713 2247 8128 4607 6660 3505 4610 2866 4161\n",
            " 6905 7096  768 1062 4411 6087 2096 4903 7546 6630 2928 4640 7125 1792\n",
            " 2063 1700 2305 7928 8095  737 5849  922 2359 6015 4489 4361 6081 6346\n",
            " 5014 5167 1023 8009 3335  268 4777 5221 4323 6168  759  711 5702 6129\n",
            " 7181 1860 7192 1347 5467 3378 6579   67 2693 6134 4841 4270 4611 2943\n",
            " 6746 7696 3466 3950 4693 6125 8370 6142 6131  995 5548 2112 1721 5067\n",
            " 4015 7092 3156   25 1749 5554 4878 8248  458 6836 1006 5310 2698 6960\n",
            " 2339 1650 6227 4104 5130 7089 6993 2400 1289 4305 5585 7933 6284 7057\n",
            " 3807 5790 8347 2390 5280 2841 7220 1384 7693 8090 4171 5260  133 7451\n",
            " 3957 6793 1719 7379 4743 4746 5435 6216 4858 4417 8355 4492 2648 5104\n",
            " 5278 6539  743 7308  158 7543 2168 5913 7111 3414 5788 7592   39 7157\n",
            " 5647 4555 3956 1045 3617 4630 4384 6596   96 4539 6749 7202 2413 4749\n",
            " 5122 5052 1598 5929  252 5565  336 6008 4564 5580 7237 1466 1568 5958\n",
            " 7657 4327 5091 6918 6487 4082 2989 7448 4715 3382  467 3784 4624  764\n",
            " 4419 1117 7954 7122 6392  505 3174 1534  323 4164 3659 7526  498 5944\n",
            " 4572 2307 2929 7480 4412  731 4563 7692 1869 2472 6966 4238 6549 1594\n",
            " 2276 7269 2545 2216  605 6587 6315 5297 5282 4816 1619 5504 2536 6399\n",
            " 3917 6342 5754 5721 7385 7909 6126 7382 8382 4383 5502 7130  485 5862\n",
            " 4936 7674 3575 5372 3893 7109 4605 5211 5462 8065   50 4080 3334 8167\n",
            " 8378 5595 1873 8207 7076 4106 3719 4627  395  231 2405 5419 6973 3480\n",
            " 3246 5414 5931 8208 4825 6203 3019 5822   63 5854 6543 5787 4764 6701\n",
            " 6562 3822 4861 3051 1315 4805 8318 1089 8169 8218 7549  707 1284 1076\n",
            " 2465 3815 4636 3188  697   80 1455  381 7025 7127 3992 4026 2777 4961\n",
            " 7531  744 6640 1047 6684 4297 7939 7384  446 3287 6675 7752 5279 8196\n",
            " 3020 1203 4192 1552 4253 2887 2556 5729 1898 3208 5973 7033 1431 5915\n",
            " 8312 5799 7105  511 6404 2852 1561 4150 4835 1178]\n",
            "Thu Jun  2 15:39:16 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   60C    P0    39W / 250W |  15855MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "757/757 [==============================] - 472s 599ms/step - loss: 7.1801 - accuracy: 0.6212 - val_loss: 6.3630 - val_accuracy: 0.7081\n",
            "Epoch 2/5\n",
            "757/757 [==============================] - 454s 599ms/step - loss: 5.8165 - accuracy: 0.7717 - val_loss: 5.4865 - val_accuracy: 0.7425\n",
            "Epoch 3/5\n",
            "757/757 [==============================] - 449s 593ms/step - loss: 4.8778 - accuracy: 0.8266 - val_loss: 4.8425 - val_accuracy: 0.7410\n",
            "Epoch 4/5\n",
            "757/757 [==============================] - 449s 593ms/step - loss: 4.0391 - accuracy: 0.8834 - val_loss: 4.3496 - val_accuracy: 0.7290\n",
            "Epoch 5/5\n",
            "757/757 [==============================] - 453s 599ms/step - loss: 3.3203 - accuracy: 0.9207 - val_loss: 3.6391 - val_accuracy: 0.7635\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_3[0][0]',                \n",
            "                                thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_bert_model[1][0]']          \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 15)           7695        ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.7434679334916865\n",
            "Weighted F1: 0.740004991290582\n",
            "Micro F1: 0.7434679334916865\n",
            "Weighted Precision: 0.742030459468069\n",
            "Micro Precision: 0.7434679334916865\n",
            "Weighted Recall: 0.7434679334916865\n",
            "Micro Recall: 0.7434679334916865\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_1[0][0]',                \n",
            "                                thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_bert_model[0][0]']          \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 15)           7695        ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 8  1  8 ...  2  1 10]\n",
            "[   1    2    3 ... 8413 8414 8417]\n",
            "[6622   35 6713 4989 8233 7444 5061 3942 8344  454 5464  317 4515 3001\n",
            " 6991 2875 1125 3885 6647 4857 4504  214 8340 2969 3847 4609 5092 1346\n",
            " 5993 7358 2752 7651 7301 8188 7073 4782 6069 2346 6445 6483 1663 4952\n",
            " 5072 1967 4071 7590  860 6418 5030 7297  626 5463 6029 6302 3819 7256\n",
            " 5667 1235 3993 1439 1323 4263 3835 2624 5801 4293 8129  320 5866 3247\n",
            " 5777   19 6059 4837 3843 1666  858 1418 5409 5274 6205 3167 2356 5997\n",
            " 6685  223 3444 3310 7721 5928 2360 7197 4252 8343  909 4354 4752 1421\n",
            "  649  829 1916 3488 2496 5765 6612 4004 4052 1403 6697 5268 1010 6293\n",
            " 6380 4602 6057 1851 4577 4236 7978 5083 6552 1058  361 7706 4186 4452\n",
            " 4396  749 8351 3834  812 5108  463 1743 6600 8264 8064 7121 8279 5556\n",
            " 5119 4673 7947 1882 3350 7617 7152 5199 4282 1092 8003 5025 5051  933\n",
            " 3969 1927 3615 1221 8062 1489 3296 7442 5459 2031 7351  234 2313 8015\n",
            " 7416 7921 6574 6257 8396  941 4520 3300 5640  240 7810 7425 1321 6580\n",
            "  928 1170 5404 8273 6742 7005   94 7459    9 7861 7975 1018 5029 2140\n",
            " 1415 7061 7767 4990 8124 5927 6051 3269 6916 5959 2846 6818 7052 7775\n",
            " 7582 8116 4240 4767 5248  430 1812 2175 6055 8331 3916  998 1564 7326\n",
            " 1559  292 3065 6764 6854 4915 5552 3583 1917 5902 4229 2144 7565 3712\n",
            " 7224 4502 1406 1651 6256 3827 6525 6318 2686 3616 5369 8235 1704 1011\n",
            " 3351 8089 8021  102  224 5671 8294 7707 2985 6624 5146 1428 7381 3709\n",
            " 6824 6308  701 2281 6837 5436 1741 3520 5995 2649 4886  966 5476 2406\n",
            " 1199 8387  513 5449 1028 2226 6166 4387  350 6123  678 5236 5041  734\n",
            " 3412 6085  587  489 5760 3376 4982 5115 4655 5575 3839 6743 7173 5724\n",
            " 7689 7787  520 8302 3150 5713 2247 8128 4607 6660 3505 4610 2866 4161\n",
            " 6905 7096  768 1062 4411 6087 2096 4903 7546 6630 2928 4640 7125 1792\n",
            " 2063 1700 2305 7928 8095  737 5849  922 2359 6015 4489 4361 6081 6346\n",
            " 5014 5167 1023 8009 3335  268 4777 5221 4323 6168  759  711 5702 6129\n",
            " 7181 1860 7192 1347 5467 3378 6579   67 2693 6134 4841 4270 4611 2943\n",
            " 6746 7696 3466 3950 4693 6125 8370 6142 6131  995 5548 2112 1721 5067\n",
            " 4015 7092 3156   25 1749 5554 4878 8248  458 6836 1006 5310 2698 6960\n",
            " 2339 1650 6227 4104 5130 7089 6993 2400 1289 4305 5585 7933 6284 7057\n",
            " 3807 5790 8347 2390 5280 2841 7220 1384 7693 8090 4171 5260  133 7451\n",
            " 3957 6793 1719 7379 4743 4746 5435 6216 4858 4417 8355 4492 2648 5104\n",
            " 5278 6539  743 7308  158 7543 2168 5913 7111 3414 5788 7592   39 7157\n",
            " 5647 4555 3956 1045 3617 4630 4384 6596   96 4539 6749 7202 2413 4749\n",
            " 5122 5052 1598 5929  252 5565  336 6008 4564 5580 7237 1466 1568 5958\n",
            " 7657 4327 5091 6918 6487 4082 2989 7448 4715 3382  467 3784 4624  764\n",
            " 4419 1117 7954 7122 6392  505 3174 1534  323 4164 3659 7526  498 5944\n",
            " 4572 2307 2929 7480 4412  731 4563 7692 1869 2472 6966 4238 6549 1594\n",
            " 2276 7269 2545 2216  605 6587 6315 5297 5282 4816 1619 5504 2536 6399\n",
            " 3917 6342 5754 5721 7385 7909 6126 7382 8382 4383 5502 7130  485 5862\n",
            " 4936 7674 3575 5372 3893 7109 4605 5211 5462 8065   50 4080 3334 8167\n",
            " 8378 5595 1873 8207 7076 4106 3719 4627  395  231 2405 5419 6973 3480\n",
            " 3246 5414 5931 8208 4825 6203 3019 5822   63 5854 6543 5787 4764 6701\n",
            " 6562 3822 4861 3051 1315 4805 8318 1089 8169 8218 7549  707 1284 1076\n",
            " 2465 3815 4636 3188  697   80 1455  381 7025 7127 3992 4026 2777 4961\n",
            " 7531  744 6640 1047 6684 4297 7939 7384  446 3287 6675 7752 5279 8196\n",
            " 3020 1203 4192 1552 4253 2887 2556 5729 1898 3208 5973 7033 1431 5915\n",
            " 8312 5799 7105  511 6404 2852 1561 4150 4835 1178]\n",
            "Thu Jun  2 16:20:25 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   60C    P0    39W / 250W |  15855MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "757/757 [==============================] - 472s 599ms/step - loss: 7.2334 - accuracy: 0.6092 - val_loss: 6.3690 - val_accuracy: 0.7066\n",
            "Epoch 2/5\n",
            "757/757 [==============================] - 452s 598ms/step - loss: 5.8763 - accuracy: 0.7560 - val_loss: 5.5109 - val_accuracy: 0.7290\n",
            "Epoch 3/5\n",
            "757/757 [==============================] - 448s 592ms/step - loss: 4.9055 - accuracy: 0.8201 - val_loss: 4.8706 - val_accuracy: 0.7290\n",
            "Epoch 4/5\n",
            "757/757 [==============================] - 452s 598ms/step - loss: 4.0861 - accuracy: 0.8702 - val_loss: 4.1901 - val_accuracy: 0.7440\n",
            "Epoch 5/5\n",
            "757/757 [==============================] - 452s 597ms/step - loss: 3.3585 - accuracy: 0.9139 - val_loss: 3.7182 - val_accuracy: 0.7470\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_3[0][0]',                \n",
            "                                thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_bert_model[1][0]']          \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 15)           7695        ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.7327790973871734\n",
            "Weighted F1: 0.7294136774811579\n",
            "Micro F1: 0.7327790973871734\n",
            "Weighted Precision: 0.7337084577880272\n",
            "Micro Precision: 0.7327790973871734\n",
            "Weighted Recall: 0.7327790973871734\n",
            "Micro Recall: 0.7327790973871734\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_1[0][0]',                \n",
            "                                thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_bert_model[0][0]']          \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 15)           7695        ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 8  1  8 ...  2  1 10]\n",
            "[   1    2    3 ... 8413 8414 8417]\n",
            "[6622   35 6713 4989 8233 7444 5061 3942 8344  454 5464  317 4515 3001\n",
            " 6991 2875 1125 3885 6647 4857 4504  214 8340 2969 3847 4609 5092 1346\n",
            " 5993 7358 2752 7651 7301 8188 7073 4782 6069 2346 6445 6483 1663 4952\n",
            " 5072 1967 4071 7590  860 6418 5030 7297  626 5463 6029 6302 3819 7256\n",
            " 5667 1235 3993 1439 1323 4263 3835 2624 5801 4293 8129  320 5866 3247\n",
            " 5777   19 6059 4837 3843 1666  858 1418 5409 5274 6205 3167 2356 5997\n",
            " 6685  223 3444 3310 7721 5928 2360 7197 4252 8343  909 4354 4752 1421\n",
            "  649  829 1916 3488 2496 5765 6612 4004 4052 1403 6697 5268 1010 6293\n",
            " 6380 4602 6057 1851 4577 4236 7978 5083 6552 1058  361 7706 4186 4452\n",
            " 4396  749 8351 3834  812 5108  463 1743 6600 8264 8064 7121 8279 5556\n",
            " 5119 4673 7947 1882 3350 7617 7152 5199 4282 1092 8003 5025 5051  933\n",
            " 3969 1927 3615 1221 8062 1489 3296 7442 5459 2031 7351  234 2313 8015\n",
            " 7416 7921 6574 6257 8396  941 4520 3300 5640  240 7810 7425 1321 6580\n",
            "  928 1170 5404 8273 6742 7005   94 7459    9 7861 7975 1018 5029 2140\n",
            " 1415 7061 7767 4990 8124 5927 6051 3269 6916 5959 2846 6818 7052 7775\n",
            " 7582 8116 4240 4767 5248  430 1812 2175 6055 8331 3916  998 1564 7326\n",
            " 1559  292 3065 6764 6854 4915 5552 3583 1917 5902 4229 2144 7565 3712\n",
            " 7224 4502 1406 1651 6256 3827 6525 6318 2686 3616 5369 8235 1704 1011\n",
            " 3351 8089 8021  102  224 5671 8294 7707 2985 6624 5146 1428 7381 3709\n",
            " 6824 6308  701 2281 6837 5436 1741 3520 5995 2649 4886  966 5476 2406\n",
            " 1199 8387  513 5449 1028 2226 6166 4387  350 6123  678 5236 5041  734\n",
            " 3412 6085  587  489 5760 3376 4982 5115 4655 5575 3839 6743 7173 5724\n",
            " 7689 7787  520 8302 3150 5713 2247 8128 4607 6660 3505 4610 2866 4161\n",
            " 6905 7096  768 1062 4411 6087 2096 4903 7546 6630 2928 4640 7125 1792\n",
            " 2063 1700 2305 7928 8095  737 5849  922 2359 6015 4489 4361 6081 6346\n",
            " 5014 5167 1023 8009 3335  268 4777 5221 4323 6168  759  711 5702 6129\n",
            " 7181 1860 7192 1347 5467 3378 6579   67 2693 6134 4841 4270 4611 2943\n",
            " 6746 7696 3466 3950 4693 6125 8370 6142 6131  995 5548 2112 1721 5067\n",
            " 4015 7092 3156   25 1749 5554 4878 8248  458 6836 1006 5310 2698 6960\n",
            " 2339 1650 6227 4104 5130 7089 6993 2400 1289 4305 5585 7933 6284 7057\n",
            " 3807 5790 8347 2390 5280 2841 7220 1384 7693 8090 4171 5260  133 7451\n",
            " 3957 6793 1719 7379 4743 4746 5435 6216 4858 4417 8355 4492 2648 5104\n",
            " 5278 6539  743 7308  158 7543 2168 5913 7111 3414 5788 7592   39 7157\n",
            " 5647 4555 3956 1045 3617 4630 4384 6596   96 4539 6749 7202 2413 4749\n",
            " 5122 5052 1598 5929  252 5565  336 6008 4564 5580 7237 1466 1568 5958\n",
            " 7657 4327 5091 6918 6487 4082 2989 7448 4715 3382  467 3784 4624  764\n",
            " 4419 1117 7954 7122 6392  505 3174 1534  323 4164 3659 7526  498 5944\n",
            " 4572 2307 2929 7480 4412  731 4563 7692 1869 2472 6966 4238 6549 1594\n",
            " 2276 7269 2545 2216  605 6587 6315 5297 5282 4816 1619 5504 2536 6399\n",
            " 3917 6342 5754 5721 7385 7909 6126 7382 8382 4383 5502 7130  485 5862\n",
            " 4936 7674 3575 5372 3893 7109 4605 5211 5462 8065   50 4080 3334 8167\n",
            " 8378 5595 1873 8207 7076 4106 3719 4627  395  231 2405 5419 6973 3480\n",
            " 3246 5414 5931 8208 4825 6203 3019 5822   63 5854 6543 5787 4764 6701\n",
            " 6562 3822 4861 3051 1315 4805 8318 1089 8169 8218 7549  707 1284 1076\n",
            " 2465 3815 4636 3188  697   80 1455  381 7025 7127 3992 4026 2777 4961\n",
            " 7531  744 6640 1047 6684 4297 7939 7384  446 3287 6675 7752 5279 8196\n",
            " 3020 1203 4192 1552 4253 2887 2556 5729 1898 3208 5973 7033 1431 5915\n",
            " 8312 5799 7105  511 6404 2852 1561 4150 4835 1178]\n",
            "Thu Jun  2 17:01:37 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   59C    P0    39W / 250W |  15855MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "757/757 [==============================] - 472s 598ms/step - loss: 7.0811 - accuracy: 0.6041 - val_loss: 6.1484 - val_accuracy: 0.7111\n",
            "Epoch 2/5\n",
            "757/757 [==============================] - 453s 598ms/step - loss: 5.6117 - accuracy: 0.7564 - val_loss: 5.3031 - val_accuracy: 0.7201\n",
            "Epoch 3/5\n",
            "757/757 [==============================] - 452s 598ms/step - loss: 4.6277 - accuracy: 0.8247 - val_loss: 4.5649 - val_accuracy: 0.7470\n",
            "Epoch 4/5\n",
            "757/757 [==============================] - 449s 593ms/step - loss: 3.8186 - accuracy: 0.8743 - val_loss: 4.0654 - val_accuracy: 0.7470\n",
            "Epoch 5/5\n",
            "757/757 [==============================] - 453s 598ms/step - loss: 3.1256 - accuracy: 0.9134 - val_loss: 3.5866 - val_accuracy: 0.7560\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_3[0][0]',                \n",
            "                                thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_bert_model[1][0]']          \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 15)           7695        ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.7268408551068883\n",
            "Weighted F1: 0.7191069988139133\n",
            "Micro F1: 0.7268408551068883\n",
            "Weighted Precision: 0.7295655851842087\n",
            "Micro Precision: 0.7268408551068883\n",
            "Weighted Recall: 0.7268408551068883\n",
            "Micro Recall: 0.7268408551068883\n",
            "Average Accuracy: 0.7337292161520189\n",
            "Average Weighted F1: 0.7306942972457764\n",
            "Average Micro F1: 0.7337292161520189\n",
            "Average Weighted Precision: 0.7374592818997044\n",
            "Average Micro Precision: 0.7337292161520189\n",
            "Average Weighted Recall: 0.7337292161520189\n",
            "Average Micro Recall: 0.7337292161520189\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "total_accuracy=0\n",
        "total_weighted_f1=0\n",
        "total_micro_f1=0\n",
        "total_weighted_precision=0\n",
        "total_micro_precision=0\n",
        "total_weighted_recall=0\n",
        "total_micro_recall=0\n",
        "\n",
        "for f in range(5):\n",
        "  gc.collect()\n",
        "  tf.keras.backend.clear_session()\n",
        "  dbert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "  dbert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
        "  max_len=512\n",
        "  sentences=summarized_data['text']\n",
        "  labels=summarized_data['label']\n",
        "  filenumbers=summarized_data['filenum']\n",
        "  len(sentences),len(labels),len(filenumbers)\n",
        "  \n",
        "  model_2=create_model()\n",
        "  input_ids=[]\n",
        "  attention_masks=[]\n",
        "\n",
        "  for sent in sentences:\n",
        "    dbert_inps=dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n",
        "    input_ids.append(dbert_inps['input_ids'])\n",
        "    attention_masks.append(dbert_inps['attention_mask'])\n",
        "  input_ids=np.asarray(input_ids)\n",
        "\n",
        "  attention_masks=np.array(attention_masks)\n",
        "  labels=np.array(labels)\n",
        "  filenumbers=[int(i) for i in filenumbers]\n",
        "  filenumbers=np.array(filenumbers)\n",
        "  print(labels)\n",
        "  print(filenumbers)\n",
        "\n",
        "  #train_inp,val_inp,train_label,val_label,train_mask,val_mask,train_fnum,val_fnum=train_test_split(input_ids,labels,attention_masks,filenumbers,test_size=0.1,random_state=42)\n",
        "\n",
        "  new_train_inp_2=[]\n",
        "  new_val_inp_2=[]\n",
        "  new_train_label_2=[]\n",
        "  new_val_label_2=[]\n",
        "  new_train_mask_2=[]\n",
        "  new_train_fnum_2=[]\n",
        "  new_val_fnum_2=[]\n",
        "  new_val_mask_2=[]\n",
        "  for i in range(len(train_fnum)):\n",
        "    if train_fnum[i] in filenumbers:\n",
        "      new_train_inp_2.append(train_inp[i])\n",
        "      #new_val_inp.append(val_inp[i])\n",
        "      new_train_label_2.append(train_label[i])\n",
        "      #new_val_label.append(val_label[i])\n",
        "      new_train_mask_2.append(train_mask[i])\n",
        "      #new_val_mask.append(val_mask[i])\n",
        "      new_train_fnum_2.append(train_fnum[i])\n",
        "\n",
        "  for i in range(len(val_fnum)):\n",
        "    if val_fnum[i] in filenumbers:\n",
        "      #new_train_inp.append(train_inp[i])\n",
        "      new_val_inp_2.append(val_inp[i])\n",
        "      #new_train_label.append(train_label[i])\n",
        "      new_val_label_2.append(val_label[i])\n",
        "      #new_train_mask.append(train_mask[i])\n",
        "      new_val_mask_2.append(val_mask[i])\n",
        "      new_val_fnum_2.append(val_fnum[i])\n",
        "\n",
        "  # print(new_train_label)\n",
        "  # print(len(new_train_label))\n",
        "  # print(new_val_label)\n",
        "  # print(len(new_val_label))\n",
        "  new_train_inp_2=np.array(new_train_inp_2)\n",
        "  new_val_inp_2=np.array(new_val_inp_2)\n",
        "  new_train_label_2=np.array(new_train_label_2)\n",
        "  new_val_label_2=np.array(new_val_label_2)\n",
        "  new_train_mask_2=np.array(new_train_mask_2)\n",
        "  new_train_fnum_2=np.array(new_train_fnum_2)\n",
        "  new_val_fnum_2=np.array(new_val_fnum_2)\n",
        "  new_val_mask_2=np.array(new_val_mask_2)\n",
        "  \n",
        "  print(new_val_fnum_2)\n",
        "  \n",
        "  log_dir='dbert_model'\n",
        "\n",
        "  model_save_path='./drive/MyDrive/Ensemble/bert-ensemble-512-model_2-'+str(f)+'-15labels.h5'\n",
        "\n",
        "  loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "  accuracy = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
        "  callbacks= [tf.keras.callbacks.ModelCheckpoint(filepath=model_save_path,monitor='val_accuracy',mode='max',save_best_only=True,save_weights_only=True),keras.callbacks.TensorBoard(log_dir=log_dir)]\n",
        "  model_2.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])\n",
        "  gpu_info = !nvidia-smi\n",
        "  gpu_info = '\\n'.join(gpu_info)\n",
        "  if gpu_info.find('failed') >= 0:\n",
        "    print('Not connected to a GPU')\n",
        "  else:\n",
        "    print(gpu_info)\n",
        "  \n",
        "  history=model_2.fit([new_train_inp_2,new_train_mask_2],new_train_label_2,batch_size=8,epochs=5,validation_data=([new_val_inp_2,new_val_mask_2],new_val_label_2),callbacks=callbacks)\n",
        "\n",
        "  pred_labels=[]\n",
        "\n",
        "  model_saved_2= create_model()\n",
        "  model_saved_2.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])\n",
        "  model_saved_2.load_weights('./drive/MyDrive/Ensemble/bert-ensemble-512-model_2-'+str(f)+'-15labels.h5')\n",
        "\n",
        "  for i in range(0,len(val_inp)):\n",
        "    pred=model_saved_2.predict([val_inp[i].reshape(1,512),val_mask[i].reshape(1,512)])\n",
        "    pred_label = pred.argmax(axis=1)\n",
        "    pred_labels.append(pred_label)\n",
        "  accuracy=accuracy_score(val_label, pred_labels)\n",
        "  print(\"Accuracy: \"+str(accuracy))\n",
        "  total_accuracy=total_accuracy+accuracy\n",
        "  \n",
        "  weighted_f1=f1_score(val_label,pred_labels, average='weighted')\n",
        "  print(\"Weighted F1: \"+ str(weighted_f1))\n",
        "  total_weighted_f1=total_weighted_f1+weighted_f1\n",
        "  micro_f1=f1_score(val_label,pred_labels, average='micro')\n",
        "  print(\"Micro F1: \"+ str(micro_f1))\n",
        "  total_micro_f1=total_micro_f1+micro_f1\n",
        "\n",
        "  weighted_precision=precision_score(val_label, pred_labels, average='weighted')\n",
        "  print(\"Weighted Precision: \" + str(weighted_precision))\n",
        "  total_weighted_precision=total_weighted_precision+weighted_precision\n",
        "  micro_precision=precision_score(val_label, pred_labels, average='micro')\n",
        "  print(\"Micro Precision: \" + str(micro_precision))\n",
        "  total_micro_precision=total_micro_precision+micro_precision\n",
        "\n",
        "  weighted_recall=recall_score(val_label, pred_labels, average='weighted')\n",
        "  print(\"Weighted Recall: \" + str(weighted_recall))\n",
        "  total_weighted_recall=total_weighted_recall+weighted_recall\n",
        "  micro_recall=recall_score(val_label, pred_labels, average='micro')\n",
        "  print(\"Micro Recall: \" + str(micro_recall))\n",
        "  total_micro_recall=total_micro_recall+micro_recall\n",
        "\n",
        "\n",
        "print(\"Average Accuracy: \"+str(total_accuracy/5))\n",
        "print(\"Average Weighted F1: \"+str(total_weighted_f1/5))\n",
        "print(\"Average Micro F1: \"+str(total_micro_f1/5))\n",
        "print(\"Average Weighted Precision: \"+str(total_weighted_precision/5))\n",
        "print(\"Average Micro Precision: \"+str(total_micro_precision/5))\n",
        "print(\"Average Weighted Recall: \"+str(total_weighted_recall/5))\n",
        "print(\"Average Micro Recall: \"+str(total_micro_recall/5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "u5uUYcxMONKM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59998fee-5137-42dd-8f7e-f53a4b3ddb07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6005\n",
            "6005\n",
            "Average Length 511.7708576186511\n",
            "['8', '1', '8', '2', '8', '8', '8', '9', '7', '8', '1', '2', '1', '1', '8', '4', '8', '8', '12', '3', '3', '7', '3', '12', '1', '8', '8', '8', '8', '8', '8', '8', '1', '9', '5', '9', '9', '9', '11', '8', '8', '8', '4', '8', '8', '8', '8', '1', '3', '9', '3', '7', '1', '2', '9', '9', '7', '8', '8', '1', '10', '7', '8', '8', '9', '8', '7', '9', '9', '12', '7', '2', '8', '1', '11', '11', '1', '7', '7', '12', '1', '9', '8', '10', '12', '7', '8', '2', '8', '9', '9', '1', '8', '9', '1', '7', '12', '10', '10', '10', '8', '3', '7', '9', '8', '9', '1', '8', '8', '2', '7', '2', '9', '9', '11', '8', '8', '12', '12', '7', '8', '12', '4', '9', '3', '1', '12', '1', '1', '8', '8', '3', '8', '8', '8', '8', '9', '1', '8', '8', '10', '1', '8', '2', '8', '8', '7', '3', '8', '2', '4', '4', '9', '8', '10', '12', '12', '1', '1', '9', '1', '1', '1', '8', '2', '2', '8', '1', '1', '2', '2', '1', '2', '8', '1', '9', '9', '8', '8', '4', '2', '9', '9', '8', '3', '4', '3', '1', '8', '8', '2', '1', '9', '7', '8', '8', '1', '12', '3', '8', '2', '4', '2', '9', '12', '1', '4', '1', '8', '8', '8', '2', '2', '8', '9', '8', '8', '8', '10', '9', '8', '7', '9', '1', '1', '9', '4', '2', '4', '2', '2', '1', '7', '8', '11', '11', '3', '9', '2', '4', '8', '9', '1', '8', '1', '1', '4', '9', '1', '1', '8', '8', '2', '1', '8', '4', '2', '8', '9', '8', '8', '2', '8', '8', '8', '7', '1', '1', '1', '2', '1', '1', '8', '7', '8', '8', '12', '2', '12', '12', '8', '10', '12', '8', '3', '3', '12', '10', '1', '8', '12', '1', '8', '8', '2', '8', '4', '7', '8', '7', '10', '8', '10', '9', '8', '12', '12', '1', '8', '8', '3', '8', '8', '8', '8', '8', '1', '9', '8', '11', '1', '1', '1', '9', '8', '1', '9', '2', '3', '11', '8', '9', '9', '9', '2', '1', '8', '8', '9', '7', '1', '4', '9', '4', '8', '8', '4', '8', '12', '9', '4', '8', '2', '10', '10', '10', '8', '9', '9', '8', '8', '12', '7', '1', '8', '8', '8', '4', '1', '1', '1', '1', '1', '1', '8', '1', '9', '8', '9', '9', '4', '8', '12', '9', '8', '8', '2', '8', '8', '8', '6', '9', '8', '3', '7', '8', '8', '4', '12', '8', '8', '9', '12', '12', '9', '8', '2', '9', '2', '3', '1', '12', '8', '10', '9', '9', '9', '10', '10', '3', '8', '12', '1', '4', '2', '1', '10', '8', '2', '8', '4', '8', '9', '1', '9', '9', '10', '10', '1', '4', '9', '2', '4', '9', '1', '1', '3', '10', '3', '3', '8', '7', '3', '8', '9', '9', '12', '4', '8', '12', '2', '2', '4', '1', '9', '9', '4', '1', '4', '2', '8', '12', '2', '3', '10', '10', '9', '8', '9', '9', '1', '12', '8', '8', '8', '12', '4', '1', '8', '8', '1', '9', '8', '8', '2', '1', '8', '9', '8', '3', '3', '3', '1', '8', '8', '9', '1', '10', '9', '9', '9', '9', '5', '9', '9', '8', '8', '8', '8', '8', '8', '8', '8', '11', '12', '8', '8', '1', '8', '9', '11', '2', '2', '2', '2', '3', '1', '2', '2', '8', '2', '4', '9', '1', '2', '9', '8', '2', '8', '9', '9', '3', '10', '9', '9', '2', '8', '9', '8', '12', '12', '1', '3', '8', '8', '8', '2', '7', '7', '7', '7', '3', '9', '1', '9', '8', '9', '9', '1', '1', '1', '2', '9', '9', '9', '11', '1', '8', '8', '9', '1', '9', '8', '8', '8', '1', '1', '8', '7', '1', '1', '8', '8', '9', '4', '4', '8', '2', '2', '8', '8', '8', '8', '8', '8', '11', '8', '2', '9', '4', '9', '3', '9', '9', '1', '3', '9', '3', '1', '12', '8', '9', '12', '1', '8', '4', '2', '1', '4', '8', '3', '3', '8', '2', '8', '9', '7', '8', '8', '8', '5', '8', '3', '9', '8', '8', '13', '12', '1', '1', '2', '8', '4', '1', '9', '9', '12', '8', '9', '12', '9', '1', '9', '9', '9', '9', '3', '2', '9', '9', '4', '8', '12', '2', '4', '9', '3', '1', '9', '7', '8', '9', '9', '8', '4', '8', '8', '7', '9', '10', '3', '8', '8', '8', '1', '1', '1', '1', '8', '8', '4', '1', '10', '1', '5', '7', '7', '1', '8', '9', '3', '7', '2', '7', '7', '2', '4', '8', '12', '7', '4', '2', '9', '9', '12', '6', '10', '8', '2', '4', '12', '9', '9', '3', '8', '8', '1', '2', '10', '9', '9', '8', '4', '12', '2', '1', '8', '8', '8', '12', '10', '10', '9', '3', '8', '8', '9', '2', '8', '10', '1', '1', '1', '1', '2', '1', '1', '1', '1', '9', '8', '12', '9', '4', '8', '8', '9', '1', '9', '3', '9', '8', '8', '1', '7', '7', '10', '1', '8', '8', '1', '9', '8', '10', '3', '1', '7', '1', '8', '8', '12', '8', '8', '1', '8', '7', '1', '7', '7', '8', '2', '1', '8', '8', '2', '10', '8', '8', '8', '8', '8', '10', '1', '8', '8', '12', '8', '3', '3', '2', '2', '2', '10', '8', '8', '8', '2', '9', '1', '8', '9', '3', '2', '8', '10', '8', '6', '1', '1', '8', '4', '1', '9', '10', '8', '1', '7', '1', '2', '8', '1', '1', '1', '12', '1', '9', '12', '8', '12', '12', '12', '8', '8', '12', '4', '8', '8', '8', '8', '9', '9', '1', '3', '3', '3', '3', '1', '12', '12', '9', '10', '8', '8', '1', '9', '2', '2', '13', '9', '8', '9', '2', '1', '9', '1', '8', '8', '8', '4', '8', '1', '1', '1', '12', '12', '7', '2', '2', '2', '8', '3', '8', '9', '2', '10', '7', '8', '9', '2', '1', '2', '12', '12', '8', '8', '9', '2', '2', '9', '11', '1', '8', '1', '10', '9', '2', '1', '4', '7', '7', '7', '7', '7', '12', '8', '8', '8', '1', '1', '10', '1', '12', '1', '8', '2', '1', '1', '12', '8', '7', '9', '12', '8', '9', '3', '9', '8', '8', '8', '8', '3', '11', '2', '2', '9', '8', '8', '10', '8', '2', '7', '3', '1', '4', '7', '8', '8', '1', '8', '3', '7', '12', '8', '10', '9', '9', '8', '8', '2', '8', '9', '9', '1', '2', '8', '8', '9', '8', '3', '8', '1', '8', '10', '9', '8', '9', '9', '12', '4', '4', '8', '9', '9', '8', '2', '10', '1', '2', '8', '9', '1', '9', '9', '9', '7', '12', '12', '8', '1', '1', '1', '1', '8', '3', '1', '1', '8', '1', '8', '8', '7', '8', '8', '8', '8', '3', '2', '2', '10', '10', '10', '7', '8', '1', '2', '12', '7', '9', '8', '7', '8', '12', '2', '8', '9', '2', '6', '6', '7', '9', '8', '1', '8', '9', '8', '1', '12', '1', '2', '8', '7', '7', '7', '8', '2', '2', '8', '1', '2', '2', '9', '9', '1', '8', '8', '4', '3', '3', '1', '6', '3', '3', '12', '3', '8', '9', '1', '4', '3', '1', '8', '3', '9', '2', '8', '2', '8', '8', '8', '1', '1', '1', '9', '9', '8', '1', '9', '8', '1', '1', '3', '10', '8', '1', '1', '3', '9', '1', '4', '4', '1', '8', '9', '9', '2', '0', '0', '1', '8', '3', '1', '8', '8', '9', '8', '8', '1', '1', '8', '9', '8', '8', '8', '7', '9', '8', '8', '8', '10', '9', '8', '1', '2', '6', '1', '9', '9', '8', '12', '12', '12', '8', '8', '2', '8', '1', '2', '2', '2', '1', '9', '8', '2', '12', '2', '8', '12', '8', '9', '8', '8', '9', '7', '1', '1', '1', '1', '1', '8', '8', '1', '8', '8', '1', '1', '3', '2', '8', '8', '9', '10', '10', '2', '2', '1', '9', '2', '9', '9', '4', '12', '12', '12', '10', '7', '3', '3', '4', '2', '2', '9', '2', '8', '4', '2', '4', '1', '10', '9', '7', '8', '7', '1', '1', '3', '3', '1', '1', '3', '3', '3', '1', '1', '1', '1', '8', '2', '3', '1', '1', '2', '8', '8', '12', '8', '8', '8', '8', '11', '9', '1', '8', '9', '2', '8', '8', '8', '3', '9', '1', '9', '2', '7', '2', '8', '2', '8', '10', '8', '1', '10', '1', '1', '9', '9', '8', '8', '1', '8', '8', '8', '12', '8', '8', '8', '1', '8', '8', '8', '1', '9', '1', '1', '8', '1', '8', '9', '8', '2', '12', '9', '9', '0', '1', '8', '8', '1', '8', '12', '8', '8', '10', '8', '8', '8', '7', '8', '1', '8', '7', '3', '10', '1', '8', '9', '1', '8', '8', '8', '10', '1', '10', '3', '9', '1', '8', '9', '2', '8', '3', '3', '9', '9', '7', '9', '1', '1', '9', '2', '1', '1', '1', '7', '1', '1', '8', '8', '1', '1', '8', '1', '8', '3', '12', '9', '3', '3', '8', '8', '8', '8', '3', '1', '3', '3', '1', '11', '0', '8', '8', '7', '8', '12', '1', '8', '9', '8', '9', '8', '8', '3', '8', '8', '1', '1', '1', '9', '2', '2', '2', '8', '7', '12', '8', '8', '9', '10', '10', '7', '8', '1', '9', '8', '7', '3', '1', '3', '8', '2', '2', '3', '9', '8', '4', '4', '8', '9', '2', '1', '1', '7', '8', '9', '9', '7', '8', '7', '7', '8', '2', '2', '8', '4', '9', '7', '10', '0', '9', '8', '3', '7', '8', '1', '1', '8', '9', '9', '2', '2', '10', '1', '9', '10', '10', '10', '8', '3', '2', '12', '9', '9', '10', '12', '9', '12', '12', '9', '1', '2', '4', '12', '12', '7', '8', '9', '7', '7', '7', '3', '9', '8', '9', '1', '12', '8', '9', '4', '1', '3', '12', '12', '12', '12', '8', '8', '2', '1', '1', '2', '1', '1', '1', '12', '12', '8', '12', '2', '2', '12', '3', '3', '12', '8', '2', '8', '8', '12', '2', '1', '10', '3', '2', '8', '7', '1', '8', '1', '3', '7', '8', '9', '8', '3', '1', '1', '7', '8', '8', '9', '8', '2', '9', '2', '2', '9', '8', '1', '8', '8', '1', '3', '3', '1', '1', '10', '1', '2', '8', '1', '1', '1', '1', '9', '1', '4', '1', '7', '7', '7', '7', '2', '2', '8', '8', '12', '1', '9', '1', '7', '3', '3', '1', '8', '8', '10', '8', '9', '2', '9', '1', '3', '8', '8', '3', '12', '2', '8', '12', '2', '9', '1', '3', '3', '3', '3', '2', '8', '7', '9', '8', '3', '3', '1', '7', '8', '3', '1', '1', '12', '8', '9', '1', '2', '3', '8', '1', '1', '3', '3', '9', '1', '1', '1', '12', '1', '7', '3', '3', '1', '8', '8', '8', '1', '2', '4', '8', '1', '10', '2', '5', '3', '3', '12', '10', '9', '9', '12', '9', '0', '2', '8', '8', '9', '9', '9', '8', '1', '3', '1', '1', '4', '8', '1', '10', '8', '7', '2', '8', '2', '8', '4', '7', '8', '1', '9', '1', '9', '8', '2', '8', '2', '7', '9', '2', '2', '9', '1', '8', '12', '1', '8', '1', '4', '1', '9', '9', '1', '10', '12', '4', '8', '1', '7', '3', '9', '2', '12', '7', '8', '8', '2', '1', '12', '9', '8', '1', '2', '2', '8', '10', '2', '1', '7', '7', '7', '12', '3', '3', '8', '3', '8', '8', '3', '9', '8', '9', '1', '1', '8', '7', '9', '3', '3', '8', '1', '0', '9', '9', '9', '1', '8', '9', '9', '10', '1', '8', '7', '8', '8', '8', '9', '9', '8', '9', '10', '4', '9', '3', '7', '12', '1', '9', '9', '8', '9', '1', '9', '3', '1', '8', '4', '12', '10', '9', '8', '7', '10', '8', '12', '12', '3', '10', '8', '12', '2', '1', '2', '3', '9', '8', '8', '7', '1', '1', '2', '2', '1', '2', '2', '7', '1', '3', '9', '9', '3', '8', '8', '8', '8', '8', '7', '8', '8', '10', '8', '1', '8', '2', '8', '2', '2', '2', '2', '2', '1', '7', '12', '10', '1', '2', '8', '1', '4', '7', '8', '1', '12', '8', '7', '9', '2', '2', '2', '8', '1', '8', '1', '1', '0', '2', '2', '6', '1', '8', '2', '1', '8', '1', '1', '2', '2', '3', '2', '2', '10', '10', '9', '1', '7', '7', '8', '8', '1', '12', '10', '12', '3', '8', '8', '8', '3', '8', '3', '10', '2', '2', '2', '1', '2', '2', '1', '0', '1', '8', '9', '1', '1', '1', '2', '2', '2', '2', '2', '1', '1', '1', '1', '12', '9', '9', '1', '10', '11', '12', '9', '1', '1', '8', '8', '1', '2', '12', '8', '3', '7', '2', '10', '9', '7', '8', '1', '2', '2', '7', '9', '12', '2', '2', '1', '8', '9', '2', '3', '2', '2', '2', '9', '9', '8', '9', '2', '3', '8', '8', '9', '7', '3', '10', '8', '8', '9', '1', '1', '2', '8', '1', '10', '8', '8', '9', '7', '1', '7', '12', '8', '1', '7', '2', '1', '3', '9', '2', '8', '8', '1', '2', '3', '9', '3', '8', '9', '7', '7', '1', '10', '2', '8', '8', '8', '1', '8', '2', '8', '8', '1', '2', '8', '10', '2', '1', '0', '8', '3', '8', '10', '10', '12', '3', '8', '9', '3', '10', '8', '8', '8', '1', '8', '8', '2', '2', '2', '2', '2', '2', '1', '1', '10', '1', '10', '8', '2', '1', '2', '2', '8', '3', '3', '2', '2', '1', '8', '1', '3', '1', '9', '2', '2', '1', '3', '2', '2', '3', '2', '2', '3', '2', '1', '1', '2', '1', '2', '7', '8', '2', '1', '12', '12', '3', '1', '9', '9', '7', '8', '7', '2', '7', '9', '2', '2', '2', '8', '9', '7', '9', '2', '8', '9', '9', '2', '9', '1', '1', '9', '8', '3', '3', '8', '2', '7', '8', '11', '2', '8', '9', '1', '8', '3', '1', '2', '1', '2', '2', '2', '2', '8', '3', '9', '2', '10', '9', '9', '2', '7', '7', '7', '1', '3', '8', '8', '1', '1', '8', '7', '1', '8', '8', '9', '3', '3', '3', '2', '4', '8', '12', '8', '1', '12', '9', '2', '8', '2', '12', '2', '1', '3', '8', '12', '12', '11', '8', '10', '9', '2', '9', '4', '10', '1', '12', '12', '9', '3', '8', '9', '12', '9', '9', '2', '2', '2', '3', '5', '1', '1', '1', '7', '7', '8', '2', '2', '8', '2', '1', '7', '3', '2', '9', '8', '9', '1', '1', '9', '8', '9', '3', '3', '2', '9', '4', '9', '8', '8', '1', '8', '10', '2', '0', '3', '8', '8', '8', '9', '12', '9', '8', '2', '2', '8', '10', '9', '8', '2', '2', '4', '1', '9', '9', '8', '8', '10', '3', '1', '1', '2', '8', '7', '8', '2', '1', '1', '8', '12', '2', '9', '4', '8', '3', '3', '3', '8', '2', '12', '12', '8', '9', '12', '8', '2', '12', '7', '9', '2', '2', '1', '3', '1', '3', '11', '9', '8', '1', '10', '2', '12', '3', '8', '4', '1', '2', '8', '8', '3', '9', '3', '8', '7', '1', '1', '3', '1', '8', '2', '9', '8', '12', '1', '1', '1', '9', '9', '1', '8', '8', '8', '2', '2', '12', '3', '1', '1', '1', '9', '9', '3', '2', '1', '1', '9', '9', '1', '9', '9', '1', '9', '3', '9', '1', '8', '1', '9', '8', '3', '8', '8', '9', '9', '8', '2', '8', '1', '2', '2', '10', '1', '2', '1', '1', '9', '1', '9', '3', '7', '7', '2', '10', '3', '3', '1', '1', '9', '1', '2', '1', '3', '2', '1', '1', '1', '1', '1', '1', '1', '1', '1', '2', '8', '8', '7', '1', '1', '8', '8', '2', '1', '12', '1', '8', '1', '9', '8', '8', '2', '1', '9', '9', '2', '2', '8', '1', '7', '7', '9', '8', '1', '6', '8', '8', '2', '2', '8', '3', '8', '3', '2', '3', '8', '1', '2', '2', '2', '8', '9', '9', '9', '12', '1', '9', '2', '1', '9', '2', '9', '9', '10', '8', '1', '1', '9', '2', '1', '7', '1', '3', '7', '1', '1', '1', '3', '8', '8', '8', '9', '9', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '1', '0', '1', '1', '1', '2', '2', '3', '1', '1', '2', '3', '3', '3', '7', '3', '1', '9', '3', '9', '1', '4', '1', '1', '9', '10', '9', '8', '2', '3', '10', '9', '3', '10', '12', '8', '9', '8', '2', '1', '9', '8', '1', '7', '8', '1', '9', '1', '2', '10', '7', '7', '8', '1', '3', '3', '9', '3', '3', '1', '1', '1', '9', '1', '3', '8', '8', '8', '1', '8', '1', '9', '7', '8', '8', '2', '1', '3', '8', '2', '1', '6', '1', '8', '8', '2', '8', '2', '2', '9', '8', '1', '1', '6', '8', '6', '10', '2', '1', '1', '8', '9', '3', '3', '3', '1', '3', '9', '6', '8', '1', '8', '3', '1', '2', '2', '12', '7', '1', '1', '1', '1', '12', '1', '8', '8', '3', '2', '1', '9', '2', '3', '3', '2', '2', '7', '2', '2', '2', '3', '1', '9', '8', '7', '1', '1', '3', '1', '9', '4', '3', '9', '2', '1', '1', '9', '8', '8', '7', '2', '1', '3', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '2', '10', '1', '1', '8', '2', '8', '1', '8', '1', '3', '1', '2', '1', '1', '3', '1', '2', '1', '1', '1', '1', '1', '1', '1', '2', '1', '8', '1', '9', '8', '8', '1', '10', '8', '8', '1', '3', '9', '8', '9', '8', '2', '1', '3', '4', '1', '12', '1', '9', '8', '2', '7', '2', '1', '2', '9', '2', '1', '3', '8', '1', '2', '3', '8', '2', '10', '10', '10', '9', '9', '9', '3', '8', '3', '1', '1', '1', '1', '1', '1', '2', '1', '9', '2', '7', '9', '8', '7', '9', '1', '1', '1', '1', '8', '2', '2', '3', '3', '2', '12', '8', '3', '1', '1', '12', '9', '8', '2', '2', '9', '1', '2', '2', '9', '1', '1', '2', '4', '8', '1', '8', '10', '2', '9', '9', '9', '1', '2', '1', '1', '2', '2', '2', '12', '1', '2', '8', '3', '9', '9', '3', '2', '9', '9', '7', '2', '8', '8', '9', '2', '9', '1', '1', '1', '1', '1', '2', '8', '2', '1', '2', '2', '8', '3', '1', '1', '9', '1', '3', '10', '7', '9', '8', '8', '2', '2', '9', '2', '2', '2', '2', '2', '2', '9', '1', '3', '6', '4', '2', '2', '9', '1', '9', '8', '1', '1', '1', '7', '3', '2', '9', '9', '1', '9', '7', '2', '8', '9', '9', '12', '10', '8', '10', '2', '3', '1', '2', '2', '12', '12', '3', '12', '1', '2', '1', '2', '1', '2', '8', '2', '3', '12', '12', '8', '2', '2', '9', '2', '9', '2', '3', '3', '1', '1', '1', '12', '3', '7', '1', '3', '1', '2', '9', '2', '7', '2', '1', '8', '3', '7', '8', '1', '1', '3', '8', '3', '2', '9', '3', '1', '1', '1', '1', '11', '1', '2', '6', '1', '2', '3', '8', '9', '9', '2', '9', '2', '1', '1', '3', '1', '12', '9', '1', '1', '9', '1', '9', '9', '8', '9', '8', '1', '2', '7', '1', '0', '9', '8', '2', '3', '2', '4', '1', '1', '1', '8', '1', '12', '1', '2', '3', '3', '9', '9', '9', '9', '9', '7', '9', '3', '9', '1', '7', '3', '3', '3', '3', '7', '8', '2', '2', '2', '8', '2', '3', '9', '9', '3', '1', '8', '9', '12', '8', '8', '3', '2', '1', '6', '1', '9', '1', '1', '1', '1', '1', '1', '2', '2', '1', '2', '2', '2', '2', '9', '2', '5', '3', '2', '2', '8', '2', '1', '8', '3', '3', '8', '3', '1', '3', '9', '2', '8', '9', '4', '4', '9', '8', '8', '7', '7', '3', '1', '8', '8', '3', '2', '2', '10', '3', '3', '2', '2', '2', '2', '12', '3', '1', '2', '7', '7', '12', '2', '1', '2', '1', '1', '2', '1', '3', '3', '3', '1', '3', '9', '4', '8', '8', '4', '2', '4', '2', '8', '9', '1', '1', '2', '2', '2', '8', '7', '2', '8', '2', '9', '1', '8', '1', '1', '2', '1', '1', '1', '2', '9', '9', '8', '1', '8', '1', '4', '8', '1', '2', '7', '9', '2', '1', '8', '9', '8', '9', '1', '1', '2', '3', '2', '12', '7', '1', '2', '4', '4', '4', '2', '2', '2', '9', '8', '8', '1', '12', '3', '3', '2', '2', '3', '12', '1', '5', '1', '1', '3', '2', '8', '8', '2', '2', '2', '3', '9', '8', '8', '9', '8', '8', '11', '8', '2', '9', '2', '3', '1', '7', '1', '8', '2', '8', '1', '1', '1', '8', '1', '1', '1', '7', '8', '8', '2', '3', '2', '1', '8', '8', '1', '8', '4', '8', '9', '8', '9', '8', '1', '2', '3', '2', '1', '2', '2', '3', '1', '9', '4', '8', '1', '1', '9', '1', '1', '8', '2', '2', '2', '1', '7', '3', '3', '1', '3', '3', '12', '3', '1', '1', '3', '1', '1', '4', '8', '4', '4', '3', '3', '3', '4', '8', '1', '9', '9', '9', '4', '1', '7', '4', '8', '4', '9', '8', '1', '1', '3', '8', '9', '1', '9', '7', '1', '1', '9', '1', '8', '8', '8', '1', '1', '8', '2', '2', '2', '9', '4', '4', '1', '2', '1', '2', '9', '1', '1', '8', '5', '5', '5', '9', '12', '4', '1', '2', '1', '8', '8', '1', '9', '9', '2', '12', '1', '1', '7', '8', '8', '4', '1', '2', '8', '11', '2', '3', '2', '2', '1', '11', '2', '2', '2', '2', '9', '9', '2', '2', '9', '3', '1', '9', '1', '1', '1', '7', '8', '1', '8', '9', '1', '2', '9', '8', '1', '2', '9', '8', '12', '2', '2', '10', '8', '2', '2', '8', '8', '1', '2', '6', '1', '1', '7', '7', '2', '3', '2', '1', '1', '8', '1', '10', '8', '12', '6', '4', '1', '4', '1', '2', '9', '8', '9', '4', '8', '8', '10', '8', '8', '8', '8', '8', '2', '2', '8', '4', '9', '3', '3', '3', '3', '3', '9', '2', '1', '1', '2', '3', '2', '1', '2', '3', '3', '3', '2', '2', '3', '3', '2', '1', '2', '3', '3', '3', '3', '12', '9', '3', '2', '2', '3', '1', '5', '1', '1', '3', '2', '2', '1', '2', '9', '3', '9', '8', '1', '8', '7', '7', '9', '1', '1', '7', '9', '10', '1', '7', '1', '4', '3', '8', '9', '4', '9', '9', '2', '9', '2', '8', '4', '2', '2', '9', '5', '2', '1', '10', '9', '3', '1', '1', '1', '2', '1', '2', '9', '1', '7', '9', '9', '8', '2', '2', '8', '9', '1', '9', '8', '2', '3', '2', '2', '1', '2', '2', '2', '1', '1', '8', '1', '9', '4', '12', '8', '2', '7', '2', '2', '1', '9', '3', '1', '8', '12', '1', '1', '2', '1', '10', '2', '6', '12', '12', '4', '9', '1', '9', '10', '1', '2', '8', '2', '1', '8', '6', '12', '9', '9', '2', '1', '7', '8', '9', '5', '8', '1', '8', '8', '3', '1', '8', '2', '8', '2', '2', '1', '2', '9', '2', '8', '1', '12', '8', '2', '7', '3', '3', '12', '2', '3', '3', '9', '9', '3', '3', '3', '3', '3', '2', '1', '4', '1', '4', '8', '8', '2', '1', '2', '9', '12', '1', '10', '12', '1', '2', '9', '4', '8', '10', '3', '2', '8', '7', '4', '8', '9', '2', '7', '1', '10', '2', '2', '2', '8', '4', '2', '2', '1', '1', '8', '7', '9', '2', '1', '9', '8', '9', '12', '4', '8', '2', '8', '7', '7', '1', '9', '2', '1', '1', '9', '1', '8', '2', '3', '9', '9', '1', '3', '2', '9', '2', '2', '8', '1', '1', '2', '1', '2', '1', '9', '8', '2', '2', '4', '8', '8', '9', '5', '5', '9', '8', '9', '6', '2', '1', '12', '2', '3', '8', '8', '9', '2', '2', '1', '3', '2', '10', '9', '10', '7', '2', '8', '4', '2', '8', '9', '8', '1', '3', '8', '8', '1', '8', '3', '8', '8', '1', '10', '3', '1', '5', '8', '8', '9', '2', '2', '9', '2', '1', '9', '1', '4', '1', '12', '8', '8', '2', '1', '1', '1', '1', '1', '9', '2', '9', '8', '9', '5', '9', '1', '8', '2', '4', '1', '1', '1', '9', '12', '4', '1', '1', '9', '8', '4', '1', '8', '9', '8', '8', '9', '9', '9', '9', '9', '9', '1', '3', '4', '2', '2', '8', '2', '9', '2', '3', '8', '3', '1', '7', '2', '1', '1', '4', '2', '9', '2', '1', '3', '2', '9', '2', '9', '3', '8', '1', '9', '1', '1', '2', '8', '8', '8', '2', '3', '9', '9', '8', '2', '4', '1', '5', '1', '1', '9', '9', '2', '1', '1', '1', '9', '9', '1', '3', '9', '2', '2', '9', '9', '8', '3', '9', '9', '8', '2', '2', '8', '9', '2', '2', '8', '9', '10', '10', '2', '8', '7', '9', '9', '4', '2', '4', '11', '2', '1', '7', '8', '8', '8', '9', '4', '9', '8', '8', '2', '1', '1', '9', '8', '3', '3', '8', '8', '7', '9', '1', '3', '4', '8', '10', '2', '4', '4', '8', '2', '8', '2', '8', '9', '2', '1', '2', '2', '3', '1', '9', '4', '5', '9', '9', '1', '1', '1', '1', '1', '1', '7', '1', '1', '1', '8', '1', '1', '1', '1', '1', '9', '1', '2', '1', '12', '9', '3', '9', '9', '4', '1', '9', '1', '1', '2', '11', '9', '3', '2', '2', '2', '4', '2', '3', '2', '7', '7', '8', '1', '10', '9', '5', '7', '1', '10', '8', '1', '2', '7', '1', '12', '5', '8', '4', '2', '9', '9', '8', '2', '9', '8', '8', '10', '2', '3', '2', '7', '2', '8', '10', '3', '2', '9', '9', '9', '1', '1', '1', '1', '8', '2', '10', '9', '8', '1', '2', '2', '2', '1', '2', '1', '3', '9', '12', '2', '2', '4', '8', '1', '3', '9', '4', '9', '1', '9', '9', '2', '1', '1', '9', '7', '10', '3', '10', '2', '9', '2', '9', '2', '8', '2', '1', '2', '3', '1', '1', '1', '8', '5', '8', '3', '1', '1', '4', '2', '2', '1', '3', '8', '2', '1', '1', '1', '2', '9', '4', '1', '8', '1', '2', '8', '2', '9', '2', '2', '5', '5', '2', '5', '2', '1', '8', '8', '1', '3', '12', '2', '4', '3', '2', '2', '2', '6', '2', '3', '3', '1', '8', '2', '2', '1', '1', '8', '8', '10', '1', '9', '2', '9', '12', '0', '2', '1', '3', '2', '2', '1', '2', '1', '9', '9', '2', '9', '8', '8', '1', '7', '8', '1', '2', '6', '9', '8', '10', '1', '12', '2', '2', '1', '8', '2', '1', '12', '7', '4', '2', '10', '2', '2', '1', '2', '1', '2', '1', '1', '2', '1', '9', '8', '10', '2', '10', '9', '9', '12', '5', '3', '2', '8', '2', '8', '3', '3', '1', '4', '9', '10', '2', '4', '8', '1', '2', '10', '12', '12', '3', '1', '1', '2', '9', '6', '6', '1', '9', '1', '8', '1', '8', '8', '8', '8', '2', '2', '2', '8', '8', '2', '1', '1', '1', '1', '1', '8', '8', '5', '8', '8', '1', '8', '9', '9', '1', '7', '12', '9', '9', '7', '2', '2', '7', '8', '9', '8', '2', '9', '6', '3', '4', '8', '4', '1', '2', '9', '8', '8', '8', '2', '1', '1', '8', '8', '3', '1', '9', '9', '7', '1', '2', '2', '2', '9', '4', '1', '2', '12', '1', '8', '2', '1', '9', '1', '8', '9', '9', '2', '5', '4', '2', '1', '2', '8', '12', '8', '10', '3', '8', '8', '8', '2', '2', '8', '2', '8', '2', '4', '8', '10', '2', '7', '2', '1', '8', '8', '9', '5', '8', '1', '12', '7', '10', '2', '2', '2', '1', '4', '8', '12', '1', '8', '2', '2', '9', '1', '8', '3', '1', '1', '5', '10', '2', '1', '2', '4', '8', '9', '3', '2', '8', '7', '4', '4', '9', '9', '2', '8', '1', '1', '2', '4', '7', '1', '9', '1', '1', '1', '8', '4', '1', '3', '1', '2', '2', '9', '1', '8', '8', '2', '2', '9', '9', '1', '8', '8', '1', '9', '8', '8', '2', '2', '2', '2', '1', '1', '1', '7', '4', '1', '1', '4', '2', '3', '3', '2', '3', '9', '2', '8', '2', '1', '5', '1', '2', '2', '1', '3', '5', '2', '9', '1', '1', '9', '8', '1', '4', '8', '1', '8', '2', '4', '8', '4', '7', '9', '8', '1', '2', '2', '4', '4', '11', '3', '11', '1', '3', '8', '1', '9', '6', '8', '9', '3', '9', '8', '9', '2', '3', '3', '7', '1', '7', '2', '9', '9', '1', '8', '8', '1', '5', '5', '8', '8', '1', '2', '1', '3', '9', '9', '5', '9', '8', '1', '4', '3', '8', '1', '1', '2', '1', '8', '9', '8', '2', '2', '2', '1', '2', '4', '10', '8', '1', '9', '1', '8', '1', '1', '9', '9', '8', '11', '1', '8', '2', '1', '2', '8', '2', '8', '8', '2', '9', '6', '10', '1', '8', '6', '4', '5', '11', '2', '2', '8', '1', '8', '1', '8', '1', '4', '8', '1', '1', '1', '8', '3', '7', '3', '3', '7', '1', '1', '4', '8', '1', '6', '4', '2', '2', '1', '1', '1', '1', '6', '2', '2', '8', '1', '8', '5', '5', '2', '1', '1', '2', '3', '7', '1', '1', '6', '2', '9', '3', '9', '8', '9', '2', '1', '4', '2', '9', '13', '9', '2', '8', '12', '4', '1', '1', '9', '5', '1', '1', '1', '1', '8', '2', '1', '1', '2', '9', '0', '12', '1', '1', '8', '9', '8', '3', '2', '2', '12', '8', '2', '2', '2', '1', '10', '1', '9', '8', '9', '5', '2', '2', '9', '2', '8', '4', '8', '3', '7', '9', '2', '2', '7', '2', '2', '8', '1', '1', '9', '10', '9', '8', '2', '9', '0', '7', '1', '1', '1', '9', '8', '2', '8', '12', '8', '3', '2', '8', '8', '7', '7', '8', '12', '2', '1', '3', '9', '2', '2', '8', '2', '8', '8', '12', '8', '8', '1', '8', '9', '2', '4', '4', '7', '2', '1', '1', '9', '3', '7', '1', '3', '9', '8', '2', '2', '3', '1', '3', '10', '2', '3', '7', '1', '8', '1', '1', '10', '3', '10', '8', '4', '1', '2', '4', '3', '10', '9', '3', '9', '5', '8', '7', '2', '9', '3', '3', '2', '11', '8', '1', '1', '7', '2', '7', '2', '9', '4', '8', '1', '12', '8', '7', '9', '1', '0', '2', '6', '4', '9', '3', '2', '4', '12', '9', '8', '5', '9', '2', '8', '4', '6', '8', '9', '3', '1', '8', '7', '3', '1', '1', '1', '8', '9', '1', '8', '7', '9', '8', '2', '1', '2', '2', '3', '2', '2', '1', '1', '4', '9', '7', '3', '2', '8', '8', '9', '1', '9', '7', '4', '2', '2', '8', '5', '9', '1', '5', '7', '1', '4', '6', '9', '10', '1', '8', '2', '9', '1', '9', '2', '9', '8', '2', '9', '9', '11', '2', '12', '2', '2', '9', '4', '8', '1', '9', '9', '2', '8', '9', '1', '2', '9', '3', '8', '8', '1', '7', '8', '8', '2', '3', '2', '2', '2', '2', '9', '8', '10', '2', '2', '1', '9', '11', '1', '8', '8', '2', '4', '2', '1', '2', '8', '8', '9', '2', '8', '9', '2', '3', '1', '2', '1', '3', '10', '8', '1', '2', '4', '1', '9', '8', '9', '3', '3', '9', '8', '2', '11', '3', '7', '1', '8', '7', '1', '8', '8', '8', '1', '4', '8', '8', '1', '9', '3', '1', '1', '2', '8', '8', '8', '2', '1', '8', '10', '2', '12', '8', '8', '9', '1', '8', '1', '3', '2', '2', '10', '7', '2', '2', '1', '1', '8', '2', '9', '1', '2', '9', '3', '3', '10', '1', '4', '7', '10', '12', '2', '4', '10', '8', '6', '2', '9', '1', '8', '3', '1', '2', '8', '8', '2', '12', '10', '2', '10', '9', '1', '2', '5', '8', '12', '8', '8', '7', '8', '8', '1', '8', '2', '2', '9', '1', '8', '10', '5', '5', '5', '8', '1', '9', '1', '13', '1', '2', '2', '1', '2', '4', '9', '2', '1', '9', '13', '11', '1', '9', '9', '3', '10', '8', '7', '8', '8', '4', '2', '1', '9', '4', '3', '1', '1', '10', '9', '2', '8', '6', '2', '1', '1', '3', '2', '8', '1', '1', '1', '1', '2', '4', '1', '1', '10', '1', '8', '1', '1', '9', '1', '7', '1', '1', '9', '9', '2', '12', '9', '1', '8', '1', '12', '1', '8', '3', '8', '1', '8', '1', '2', '10', '1', '1', '2', '2', '9', '3', '9', '12', '1', '9', '1', '2', '8', '7', '2', '1', '1', '3', '2', '8', '4', '4', '5', '12', '7', '8', '9', '6', '1', '8', '9', '2', '1', '9', '1', '4', '1', '8', '2', '1', '1', '8', '7', '4', '9', '1', '8', '4', '2', '7', '5', '1', '3', '9', '2', '8', '2', '1', '2', '9', '1', '1', '2', '3', '4', '3', '1', '2', '2', '10', '2', '2', '8', '1', '1', '2', '4', '9', '2', '11', '9', '8', '9', '2', '1', '10', '4', '1', '1', '10', '7', '2', '10', '8', '1', '10', '8', '8', '8', '8', '8', '8', '2', '2', '2', '8', '3', '4', '1', '1', '1', '2', '1', '8', '8', '2', '8', '3', '8', '3', '1', '2', '3', '1', '1', '10', '1', '4', '4', '3', '3', '1', '9', '1', '1', '2', '1', '1', '6', '1', '1', '1', '9', '1', '4', '1', '1', '1', '1', '1', '1', '2', '12', '8', '8', '1', '9', '12', '10', '8', '2', '12', '1', '1', '2', '1', '7', '10', '7', '1', '2', '1', '10', '8', '2', '2', '8', '2', '2', '1', '9', '8', '8', '8', '8', '3', '1', '4', '2', '8', '3', '10', '10', '1', '9', '1', '9', '1', '9', '1', '8', '3', '1', '1', '8', '8', '8', '6', '3', '4', '1', '1', '1', '9', '9', '5', '2', '10', '2', '6', '2', '2', '7', '1', '8', '2', '11', '1', '1', '1', '2', '1', '9', '4', '4', '2', '8', '4', '6', '8', '8', '10', '10', '2', '1', '2', '9', '1', '8', '8', '3', '4', '8', '8', '2', '8', '8', '1', '2', '1', '2', '9', '4', '1', '3', '3', '3', '8', '8', '8', '2', '6', '4', '3', '3', '12', '3', '4', '9', '6', '8', '7', '7', '8', '6', '4', '1', '10', '6', '3', '3', '8', '1', '1', '8', '1', '11', '8', '1', '1', '2', '3', '11', '7', '2', '1', '1', '1', '9', '1', '2', '1', '1', '9', '8', '1', '1', '8', '0', '4', '1', '1', '2', '1', '1', '2', '2', '9', '1', '10', '1', '3', '8', '9', '3', '3', '1', '10', '1', '9', '1', '1', '7', '4', '1', '8', '10', '1', '2', '10', '1', '1', '2', '3', '3', '9', '1', '8', '10', '1', '7', '9', '1', '9', '6', '3', '9', '2', '8', '4', '12', '12', '3', '1', '1', '9', '1', '9', '2', '1', '1', '1', '1', '1', '9', '2', '1', '2', '10', '9', '8', '8', '10', '8', '2', '3', '2', '1', '12', '8', '12', '2', '9', '1', '3', '2', '5', '2', '2', '8', '2', '2', '4', '10', '9', '10', '9', '4', '2', '4', '12', '8', '2', '1', '8', '3', '9', '2', '9', '4', '1', '1', '1', '1', '1', '9', '6', '2', '9', '2', '1', '3', '2', '2', '5', '8', '9', '1', '6', '3', '1', '2', '2', '2', '6', '1', '8', '10', '8', '3', '3', '13', '2', '9', '9', '10', '6', '12', '8', '2', '9', '2', '3', '8', '8', '1', '2', '2', '3', '2', '10', '1', '8', '9', '1', '8', '9', '8', '8', '2', '10', '2', '1', '1', '1', '9', '9', '12', '1', '1', '4', '8', '3', '2', '2', '4', '4', '2', '2', '1', '1', '1', '9', '2', '2', '4', '1', '1', '8', '8', '8', '9', '2', '13', '9', '9', '4', '1', '9', '7', '10', '10', '8', '1', '9', '1', '1', '1', '8', '12', '4', '1', '2', '1', '1', '1', '7', '8', '3', '3', '1', '2', '2', '7', '2', '2', '10', '1', '8', '4', '0', '1', '9', '4', '1', '1', '2', '10', '10', '7', '9', '3', '12', '11', '2', '8', '10', '8', '9', '4', '3', '4', '10', '1', '1', '9', '3', '10', '1', '5', '3', '3', '2', '2', '1', '1', '6', '1', '7', '1', '1', '12', '1', '1', '1', '10', '1', '8', '8', '1', '3', '8', '3', '1', '2', '8', '3', '4', '2', '8', '8', '6', '1', '1', '4', '1', '2', '9', '5', '1', '7', '8', '9', '2', '4', '9', '8', '5', '9', '9', '9', '1', '2', '1', '3', '8', '9', '2', '9', '8', '9', '1', '8', '13', '9', '3', '8', '1', '4', '1', '3', '1', '6', '4', '10', '2', '2', '7', '8', '8', '12', '8', '9', '9', '9', '10', '3', '12', '12', '2', '9', '10', '9', '7', '2', '3', '4', '10', '10', '2', '7', '7', '1', '7', '3', '9', '1', '9', '8', '2', '9', '1', '8', '1', '10', '8', '8', '2', '7', '8', '1', '8', '9', '8', '1', '4', '1', '8', '8', '8', '1', '1', '10', '3', '2', '2', '6', '8', '9', '8', '2', '1', '1', '3', '8', '8', '1', '4', '4', '3', '9', '10', '4', '2', '4', '2', '9', '2', '2', '9', '1', '1', '10', '1', '1', '2', '1', '1', '1', '9', '1', '8', '8', '8', '2', '3', '8', '1', '6', '7', '3', '1', '9', '1', '2', '1', '6', '1', '2', '10', '1', '4', '2', '3', '2', '9', '9', '4', '9', '9', '1', '2', '8', '4', '7', '8', '7', '13', '9', '1', '2', '1', '9', '3', '3', '6', '8', '9', '10', '9', '2', '2', '3', '12', '1', '1', '1', '13', '2', '1', '7', '10', '8', '9', '1', '2', '9', '1', '1', '1', '2', '1', '12', '5', '6', '9', '10', '3', '1', '10', '1', '3', '2', '8', '9', '2', '10', '8', '8', '9', '2', '8', '9', '2', '2', '2', '8', '8', '2', '3', '1', '4', '8', '1', '9', '9', '2', '8', '10', '9', '1', '2', '12', '8', '8', '0', '2', '1', '1', '9', '10', '1', '6', '2', '2', '10', '2', '2', '4', '2', '2', '12', '1', '6', '7', '8', '9', '1', '3', '5', '7', '3', '1', '3', '1', '1', '3', '2', '6', '3', '2', '1', '8', '3', '5', '2', '1', '1', '8', '1', '1', '0', '2', '3', '5', '3', '1', '1', '9', '8', '8', '8', '8', '4', '7', '2', '9', '12', '5', '9', '3', '12', '3', '9', '1', '9', '1', '7', '3', '9', '3', '10', '1', '2', '1', '4', '9', '8', '2', '9', '11', '2', '8', '9', '4', '1', '1', '1', '1', '1', '1', '1', '9', '1', '9', '1', '1', '1', '12', '3', '10', '3', '6', '1', '7', '10', '8', '3', '7', '1', '1', '2', '6', '1', '3', '9', '2', '8', '1', '8', '8', '1', '8', '10', '12', '1', '10', '12', '10', '1', '8', '8', '1', '4', '7', '9', '2', '1', '9', '3', '8', '12', '10', '6', '1', '6', '8', '3', '1', '3', '1', '10', '10', '9', '8', '1', '2', '9', '8', '8', '1', '9', '8', '8', '3', '1', '3', '9', '12', '1', '8', '1', '5', '8', '11', '5', '5', '1', '2', '1', '3', '1', '1', '1', '9', '8', '8', '1', '9', '10', '8', '4', '1', '1', '8', '10', '8', '9', '1', '10', '1', '7', '2', '1', '1', '9', '6', '2', '9', '8', '1', '1', '1', '9', '8', '8', '1', '9', '9', '1', '1', '2', '7', '9', '2', '9', '9', '8', '7', '6', '8', '7', '8', '2', '9', '2', '1', '8', '8', '8', '2', '6', '3', '1', '8', '12', '12', '8', '7', '1', '2', '2', '7', '1', '9', '8', '1', '4', '1', '1', '8', '5', '8', '1', '8', '8', '8', '8', '1', '2', '11', '1', '1', '1', '2', '7', '1', '9', '2', '2', '2', '9', '8', '9', '2', '9', '1', '8', '1', '7', '11', '13', '1', '9', '8', '2', '2', '1', '2', '3', '10', '3', '10', '4', '3', '9', '1', '8', '1', '1', '13', '1', '1', '3', '8', '1', '2', '2', '2', '9', '4', '1', '8', '1', '9', '3', '6', '8', '5', '2', '8', '2', '8', '1', '9', '2', '2', '8', '1', '1', '4', '8', '9', '2', '2', '7', '1', '8', '12', '2', '12', '8', '2', '9', '9', '3', '8', '1', '1', '9', '9', '9', '2', '8', '1', '9', '1', '8', '9', '8', '2', '2', '4', '1', '9', '10', '10', '8', '8', '1', '1', '2', '1', '4', '1', '2', '1', '1', '3', '12', '8', '8', '8', '8', '10', '2', '8', '12', '1', '9', '9', '8', '9', '1', '2', '1', '8', '8', '8', '2', '8', '8', '3', '10', '8', '9', '1', '1', '1', '3', '4', '8', '10', '6', '3', '1', '3', '2', '8', '9', '3', '5', '4', '1', '5', '2', '9', '9', '9', '1', '1', '1', '9', '9', '6', '10', '9', '8', '12', '2', '9', '1', '2', '1', '1', '1', '8', '1', '12', '1', '12', '1', '9', '1', '8', '1', '2', '1', '2', '8', '8', '10', '1', '9', '1', '2', '2', '8', '8', '3', '12', '1', '11', '2', '10', '12', '11', '2', '1', '10', '1', '1', '9', '3', '10', '2', '7', '1', '8', '1', '9', '9', '2', '1', '3', '12', '5', '9', '9', '8', '1', '8', '8', '8', '1', '1', '3', '9', '8', '9', '8', '3', '10', '3', '7', '8', '9', '2', '3', '1', '9', '10', '9', '2', '8', '2', '6', '4', '1', '1', '3', '8', '2', '3', '9', '1', '2', '1', '8', '1', '2', '2', '2', '2', '9', '9', '4', '8', '9', '1', '1', '2', '9', '1', '5', '2', '9', '8', '10', '1', '2', '10', '8', '5', '6', '4', '8', '1', '1', '1', '8', '9', '2', '8', '8', '8', '12', '2', '2', '1', '8', '1', '2', '1', '9', '8', '8', '9', '1', '1', '8', '7', '1', '8', '0', '2', '3', '8', '1', '1', '8', '1', '1', '12', '3', '2', '10', '8', '10', '6', '1', '8', '8', '10', '8', '8', '1', '4', '8', '1', '2', '9', '8', '1', '1', '3', '3', '5', '1', '1', '2', '1', '2', '1', '9', '8', '3', '9', '8', '2', '10', '9', '1', '8', '1', '10', '9', '10', '1', '2', '3', '8', '1', '3', '1', '9', '9', '8', '1', '9', '8', '8', '9', '1', '9', '2', '8', '8', '2', '13', '8', '10', '8', '9', '3', '2', '1', '3', '1', '12', '10', '10', '1', '11', '1', '2', '4', '2', '10', '1', '8', '11', '1', '1', '1', '2', '2', '8', '2', '9', '8', '9', '9', '12', '8', '1', '9', '2', '4', '1', '8', '3', '9', '1', '6', '5', '8', '9', '3', '3', '2', '1', '1', '1', '8', '11', '1', '8', '7', '1', '11', '9', '1', '7', '9', '9', '8', '8', '8', '12', '8', '1', '2', '9', '8', '8', '2', '10', '9', '8', '8', '4', '10', '9', '8', '2', '10', '10', '2', '1', '2', '1', '2', '1', '4', '1', '7', '1', '1', '3', '12', '9', '8', '5', '9', '10', '1', '9', '9', '2', '7', '4', '1', '1', '8', '12', '8', '2', '2', '9', '4', '1', '1', '5', '9', '1', '8', '12', '8', '1', '1', '4', '9', '10', '2', '3', '1', '3', '3', '3', '8', '1', '2', '1', '2', '1', '2', '9', '1', '2', '8', '9', '4', '8', '8', '10', '2', '12', '5', '5', '1', '8', '1', '7', '8', '1', '1', '8', '9', '12', '4', '9', '3', '1', '2', '5', '9', '2', '1', '2', '2', '1', '2', '8', '1', '8', '8', '2', '2', '1', '8', '4', '1', '2', '10', '9', '10', '8', '1', '10', '4', '8', '8', '10', '2', '8', '1', '1', '3', '10', '1', '4', '2', '8', '3', '3', '2', '9', '8', '5', '5', '9', '3', '10', '8', '1', '2', '1', '2', '9', '1', '1', '2', '9', '9', '8', '8', '11', '9', '4', '1', '8', '2', '7', '1', '2', '8', '2', '9', '9', '8', '2', '9', '9', '8', '8', '1', '2', '8', '1', '1', '2', '9', '8', '1', '4', '9', '1', '8', '12', '1', '12', '2', '8', '2', '9', '1', '12', '1', '2', '9', '1', '1', '9', '3', '2', '9', '9', '2', '11', '8', '7', '9', '9', '9', '8', '8', '1', '2', '8', '1', '1', '4', '1', '2', '10', '1', '9', '2', '1', '1', '1', '9', '1', '13', '4', '3', '2', '1', '1', '2', '2', '7', '8', '7', '1', '1', '2', '8', '8', '3', '4', '8', '2', '10', '13', '8', '8', '9', '2', '2', '1', '1', '10', '4', '2', '7', '9', '8', '2', '1', '9', '8', '1', '1', '9', '8', '9', '8', '2', '8', '2', '2', '9', '2', '1', '9', '1', '2', '4', '8', '2', '1', '1', '8', '1', '4', '1', '9', '3', '9', '2', '7', '1', '9', '6', '1', '10', '2', '2', '2', '2', '2', '10', '10', '10', '9', '9', '9', '3', '12', '10', '1', '1', '10', '1', '1', '2', '2', '1', '2', '3', '12', '8', '8', '1', '2', '9', '9', '1', '1', '10', '8', '9', '8', '3', '1', '1', '3', '1', '10', '1', '1', '1', '4', '1', '1', '1', '7', '10', '2', '1', '8', '1', '9', '3', '1', '10', '8', '8', '1', '5', '9', '1', '2', '1', '8', '8', '1', '3', '1', '10', '2', '1', '1', '2', '8', '2', '5', '3', '5', '2', '1', '1', '9', '1', '7', '9', '9', '2', '8', '1', '12', '4', '1', '4', '9', '9', '4', '8', '1', '10', '10', '8', '8', '8', '9', '2', '3', '5', '1', '8', '1', '1', '1', '7', '10', '1', '4', '12', '3', '2', '2', '9', '1', '1', '1', '2', '8', '4', '1', '7', '3', '12', '8', '6', '2', '2', '7', '9', '11', '1', '10', '9', '1', '8', '12', '2', '11', '1', '2', '3', '1', '1', '1', '8', '9', '2', '2', '2', '2', '3', '3', '8', '10', '4', '1', '2', '9', '8', '2', '2', '9', '1', '8', '11', '4', '2', '8', '7', '1', '1', '7', '3', '9', '9', '4', '5', '8', '10', '2', '9', '9', '9', '8', '1', '9', '2', '2', '2', '1', '2', '9', '4', '3', '12', '4', '3', '2', '3', '8', '3', '10', '1', '10', '1', '1', '8', '10', '6', '8', '9', '9', '1', '2', '9', '2', '9', '3', '8', '1', '1', '12', '1', '5', '1', '10', '8', '10', '2', '7', '1', '1', '1', '1', '3', '2', '3', '5', '9', '1', '2', '1', '9', '8', '10', '9', '8', '1', '8', '1', '8', '2', '8', '1', '2', '8', '1', '2', '5', '8', '12', '2', '1', '1', '1', '1', '1', '8', '8', '2', '1', '4', '2', '8', '8', '8', '2', '9', '9', '11', '1', '2', '9', '3', '1', '1', '8', '9', '2', '4', '10', '1', '9', '8', '4', '9', '8', '10', '8', '8', '9', '2', '8', '9', '3', '3', '3', '4', '3', '2', '2', '10', '8', '2', '1', '5', '1', '9', '1', '1', '2', '1', '2', '3', '1', '1', '8', '8', '1', '10', '8', '8', '1', '6', '4', '8', '2', '5', '8', '1', '3', '8', '2', '8', '9', '1', '1', '8', '12', '10', '1', '5', '8', '2', '8', '10', '9', '9', '1', '6', '9', '1', '10', '8', '2', '9', '10', '1', '2', '1', '1', '8', '8', '3', '9', '8', '1', '10', '2', '8', '1', '10', '1', '5', '1', '1', '1', '5', '1', '1', '1', '4', '1', '1', '1', '8', '2', '8', '1', '8', '3', '8', '9', '1', '8', '1', '8', '1', '1', '2', '2', '1', '12', '9', '9', '1', '8', '2', '1', '2', '8', '1', '9', '1', '1', '2', '1', '2', '2', '2', '1', '9', '9', '8', '8', '1', '1', '9', '10', '8', '4', '3', '3', '4', '9', '1', '3', '3', '10', '2', '2', '1', '8', '4', '1', '9', '4', '1', '1', '9', '8', '10', '4', '9', '9', '8', '2', '1', '3', '4', '9', '3', '8', '9', '1', '1', '2', '1', '7', '8', '2', '9', '1', '9', '2', '6', '9', '10', '8', '1', '1', '5', '9', '5', '1', '9', '10', '9', '3', '3', '8', '8', '2', '9', '2', '8', '2', '1', '8', '5', '8', '3', '10', '1', '1', '2', '1', '8', '1', '4', '3', '2', '9', '4', '9', '8', '8', '8', '1', '3', '9', '1', '9', '9', '1', '2', '1', '9', '8', '9', '8', '1', '1', '1', '4', '2', '2', '9', '9', '1', '1', '1', '3', '6', '1', '2', '3', '1', '4', '2', '9', '1', '9', '9', '1', '8', '2', '1', '9', '1', '8', '1', '8', '8', '2', '1', '9', '9', '6', '8', '8', '9', '8', '10', '8', '9', '5', '1', '1', '1', '1', '8', '1', '8', '12', '8', '1', '12', '9', '2', '8', '1', '1', '2', '1', '8', '6', '9', '9', '8', '1', '8', '9', '7', '7', '8', '9', '9', '1', '8', '3', '8', '1', '3', '3', '8', '9', '8', '2', '8', '1', '2', '9', '8', '1', '1', '1', '1', '9', '1', '9', '8', '12', '3', '8', '8', '1', '10', '10', '10', '2', '2', '1', '10', '2', '2', '10', '8', '12', '8', '1', '1', '1', '2', '1', '1', '3', '8', '1', '2', '2', '2', '1', '1', '6', '2', '8', '8', '8', '1', '1', '1', '9', '9', '2', '8', '10', '2', '8', '2', '1', '2', '9', '9', '2', '1', '1', '8', '8', '1', '3', '1', '8', '9', '1', '10', '9', '1', '1', '1', '1', '7', '2', '2', '1', '2', '8', '8', '1', '2', '1', '3', '2', '1', '8', '3', '9', '2', '10', '2', '10', '1', '2', '1', '1', '13', '9', '10', '8', '2', '2', '1', '1', '8', '2', '2', '1', '9', '1', '1', '8', '9', '9', '2', '9', '2', '2', '1', '1', '1', '8', '9', '4', '9', '9', '1', '8', '2', '1', '1', '8', '2', '2', '2', '8', '1', '1', '8', '2', '10', '2', '9', '9', '7', '1', '1', '2', '2', '8', '3', '1', '2', '1', '1', '8', '1', '1', '1', '9', '1', '1', '8', '8', '1', '9', '6', '1', '9', '1', '8', '1', '8', '8', '8', '1', '3', '8', '8', '2', '6', '3', '2', '1', '10', '1', '8', '2', '1', '6', '1', '1', '1', '8', '1', '10', '1', '1', '8', '9', '2', '6', '9', '2', '9', '4', '1', '8', '1', '3', '7', '8', '8', '5', '8', '7', '1', '1', '1', '13', '8', '3', '1', '1', '1', '1', '2', '8', '2', '12', '8', '1', '2', '5', '1', '8', '2', '10', '8', '2', '10', '1', '5', '2', '1', '1', '5', '2', '2', '7', '8', '2', '9', '9', '1', '10', '1', '8', '2', '5', '8', '1', '5', '1', '10', '1', '1', '9', '8', '6', '1', '8', '9', '8', '1', '8', '2', '2', '8', '3', '9', '1', '9', '3', '8', '4', '4', '3', '3', '1', '1', '1', '8', '9', '1', '1', '8', '3', '1', '2', '8', '2', '1', '1', '10', '1', '2', '2', '1', '1', '8', '2', '8', '2', '10', '2', '2', '10', '8', '8', '8', '13', '2', '2', '9', '2', '5', '9', '2', '1', '8', '8', '8', '1', '12', '12', '2', '9', '2', '1', '8', '2', '1', '8', '1', '9', '9', '2', '7', '1', '1', '7', '1', '4', '3', '1', '10', '3', '10', '9', '4', '6', '7', '5', '2', '1', '8', '8', '1', '4', '9', '2', '1', '8', '1', '1', '1', '1', '1', '8', '9', '8', '8', '8', '8', '9', '8', '10', '1', '8', '2', '7', '8', '9', '1', '2', '5', '9', '10', '8', '9', '12', '1', '6', '1', '2', '10', '9', '4', '1', '8', '10', '10', '1', '1', '1', '1', '1', '2', '3', '8', '1', '2', '10', '2', '1', '2', '4', '2', '2', '9', '1', '4', '2', '1', '1', '12', '12', '9', '9', '7', '1', '8', '4', '9', '9', '8', '1', '7', '3', '2', '1', '1', '4', '8', '1', '5', '8', '2', '8', '1', '14', '9', '12', '1', '10', '2', '1', '1', '8', '8', '6', '6', '3', '1', '2', '8', '1', '3', '2', '1', '8', '8', '1', '10', '8', '2', '8', '2', '5', '9', '1', '8', '8', '1', '1', '8', '8', '13', '1', '3', '8', '4', '1', '9', '7', '8', '8', '8', '3', '8', '1', '4', '2', '8', '8', '8', '7', '1', '1', '2', '8', '8', '9', '8', '8', '2', '3', '9', '9', '2', '1', '2', '2', '1', '8', '2', '6', '2', '1', '1', '9', '10', '3', '8', '2', '3', '2', '8', '8', '1', '8', '8', '8', '9', '1', '4', '1', '9', '2', '1', '8', '0', '1', '8', '1', '1', '8', '9', '9', '1', '3', '1', '2', '8', '8', '7', '1', '1', '2', '9', '2', '10', '2', '8', '2', '2', '2', '7', '1', '9', '8', '1', '3', '9', '2', '1', '10', '8', '1', '4', '1', '8', '5', '9', '8', '8', '1', '2', '2', '1', '8', '8', '6', '8', '1', '1', '8', '3', '2', '2', '1', '8', '1', '7', '6', '8', '1', '1', '1', '9', '1', '1', '1', '2', '1', '11', '10', '10', '11', '9', '']\n",
            "['8', '1', '8', '2', '8', '8', '8', '9', '7', '8', '1', '2', '1', '1', '8', '4', '8', '8', '12', '3', '3', '7', '3', '12', '1', '8', '8', '8', '8', '8', '8', '8', '1', '9', '5', '9', '9', '9', '11', '8', '8', '8', '4', '8', '8', '8', '8', '1', '3', '9', '3', '7', '1', '2', '9', '9', '7', '8', '8', '1', '10', '7', '8', '8', '9', '8', '7', '9', '9', '12', '7', '2', '8', '1', '11', '11', '1', '7', '7', '12', '1', '9', '8', '10', '12', '7', '8', '2', '8', '9', '9', '1', '8', '9', '1', '7', '12', '10', '10', '10', '8', '3', '7', '9', '8', '9', '1', '8', '8', '2', '7', '2', '9', '9', '11', '8', '8', '12', '12', '7', '8', '12', '4', '9', '3', '1', '12', '1', '1', '8', '8', '3', '8', '8', '8', '8', '9', '1', '8', '8', '10', '1', '8', '2', '8', '8', '7', '3', '8', '2', '4', '4', '9', '8', '10', '12', '12', '1', '1', '9', '1', '1', '1', '8', '2', '2', '8', '1', '1', '2', '2', '1', '2', '8', '1', '9', '9', '8', '8', '4', '2', '9', '9', '8', '3', '4', '3', '1', '8', '8', '2', '1', '9', '7', '8', '8', '1', '12', '3', '8', '2', '4', '2', '9', '12', '1', '4', '1', '8', '8', '8', '2', '2', '8', '9', '8', '8', '8', '10', '9', '8', '7', '9', '1', '1', '9', '4', '2', '4', '2', '2', '1', '7', '8', '11', '11', '3', '9', '2', '4', '8', '9', '1', '8', '1', '1', '4', '9', '1', '1', '8', '8', '2', '1', '8', '4', '2', '8', '9', '8', '8', '2', '8', '8', '8', '7', '1', '1', '1', '2', '1', '1', '8', '7', '8', '8', '12', '2', '12', '12', '8', '10', '12', '8', '3', '3', '12', '10', '1', '8', '12', '1', '8', '8', '2', '8', '4', '7', '8', '7', '10', '8', '10', '9', '8', '12', '12', '1', '8', '8', '3', '8', '8', '8', '8', '8', '1', '9', '8', '11', '1', '1', '1', '9', '8', '1', '9', '2', '3', '11', '8', '9', '9', '9', '2', '1', '8', '8', '9', '7', '1', '4', '9', '4', '8', '8', '4', '8', '12', '9', '4', '8', '2', '10', '10', '10', '8', '9', '9', '8', '8', '12', '7', '1', '8', '8', '8', '4', '1', '1', '1', '1', '1', '1', '8', '1', '9', '8', '9', '9', '4', '8', '12', '9', '8', '8', '2', '8', '8', '8', '6', '9', '8', '3', '7', '8', '8', '4', '12', '8', '8', '9', '12', '12', '9', '8', '2', '9', '2', '3', '1', '12', '8', '10', '9', '9', '9', '10', '10', '3', '8', '12', '1', '4', '2', '1', '10', '8', '2', '8', '4', '8', '9', '1', '9', '9', '10', '10', '1', '4', '9', '2', '4', '9', '1', '1', '3', '10', '3', '3', '8', '7', '3', '8', '9', '9', '12', '4', '8', '12', '2', '2', '4', '1', '9', '9', '4', '1', '4', '2', '8', '12', '2', '3', '10', '10', '9', '8', '9', '9', '1', '12', '8', '8', '8', '12', '4', '1', '8', '8', '1', '9', '8', '8', '2', '1', '8', '9', '8', '3', '3', '3', '1', '8', '8', '9', '1', '10', '9', '9', '9', '9', '5', '9', '9', '8', '8', '8', '8', '8', '8', '8', '8', '11', '12', '8', '8', '1', '8', '9', '11', '2', '2', '2', '2', '3', '1', '2', '2', '8', '2', '4', '9', '1', '2', '9', '8', '2', '8', '9', '9', '3', '10', '9', '9', '2', '8', '9', '8', '12', '12', '1', '3', '8', '8', '8', '2', '7', '7', '7', '7', '3', '9', '1', '9', '8', '9', '9', '1', '1', '1', '2', '9', '9', '9', '11', '1', '8', '8', '9', '1', '9', '8', '8', '8', '1', '1', '8', '7', '1', '1', '8', '8', '9', '4', '4', '8', '2', '2', '8', '8', '8', '8', '8', '8', '11', '8', '2', '9', '4', '9', '3', '9', '9', '1', '3', '9', '3', '1', '12', '8', '9', '12', '1', '8', '4', '2', '1', '4', '8', '3', '3', '8', '2', '8', '9', '7', '8', '8', '8', '5', '8', '3', '9', '8', '8', '13', '12', '1', '1', '2', '8', '4', '1', '9', '9', '12', '8', '9', '12', '9', '1', '9', '9', '9', '9', '3', '2', '9', '9', '4', '8', '12', '2', '4', '9', '3', '1', '9', '7', '8', '9', '9', '8', '4', '8', '8', '7', '9', '10', '3', '8', '8', '8', '1', '1', '1', '1', '8', '8', '4', '1', '10', '1', '5', '7', '7', '1', '8', '9', '3', '7', '2', '7', '7', '2', '4', '8', '12', '7', '4', '2', '9', '9', '12', '6', '10', '8', '2', '4', '12', '9', '9', '3', '8', '8', '1', '2', '10', '9', '9', '8', '4', '12', '2', '1', '8', '8', '8', '12', '10', '10', '9', '3', '8', '8', '9', '2', '8', '10', '1', '1', '1', '1', '2', '1', '1', '1', '1', '9', '8', '12', '9', '4', '8', '8', '9', '1', '9', '3', '9', '8', '8', '1', '7', '7', '10', '1', '8', '8', '1', '9', '8', '10', '3', '1', '7', '1', '8', '8', '12', '8', '8', '1', '8', '7', '1', '7', '7', '8', '2', '1', '8', '8', '2', '10', '8', '8', '8', '8', '8', '10', '1', '8', '8', '12', '8', '3', '3', '2', '2', '2', '10', '8', '8', '8', '2', '9', '1', '8', '9', '3', '2', '8', '10', '8', '6', '1', '1', '8', '4', '1', '9', '10', '8', '1', '7', '1', '2', '8', '1', '1', '1', '12', '1', '9', '12', '8', '12', '12', '12', '8', '8', '12', '4', '8', '8', '8', '8', '9', '9', '1', '3', '3', '3', '3', '1', '12', '12', '9', '10', '8', '8', '1', '9', '2', '2', '13', '9', '8', '9', '2', '1', '9', '1', '8', '8', '8', '4', '8', '1', '1', '1', '12', '12', '7', '2', '2', '2', '8', '3', '8', '9', '2', '10', '7', '8', '9', '2', '1', '2', '12', '12', '8', '8', '9', '2', '2', '9', '11', '1', '8', '1', '10', '9', '2', '1', '4', '7', '7', '7', '7', '7', '12', '8', '8', '8', '1', '1', '10', '1', '12', '1', '8', '2', '1', '1', '12', '8', '7', '9', '12', '8', '9', '3', '9', '8', '8', '8', '8', '3', '11', '2', '2', '9', '8', '8', '10', '8', '2', '7', '3', '1', '4', '7', '8', '8', '1', '8', '3', '7', '12', '8', '10', '9', '9', '8', '8', '2', '8', '9', '9', '1', '2', '8', '8', '9', '8', '3', '8', '1', '8', '10', '9', '8', '9', '9', '12', '4', '4', '8', '9', '9', '8', '2', '10', '1', '2', '8', '9', '1', '9', '9', '9', '7', '12', '12', '8', '1', '1', '1', '1', '8', '3', '1', '1', '8', '1', '8', '8', '7', '8', '8', '8', '8', '3', '2', '2', '10', '10', '10', '7', '8', '1', '2', '12', '7', '9', '8', '7', '8', '12', '2', '8', '9', '2', '6', '6', '7', '9', '8', '1', '8', '9', '8', '1', '12', '1', '2', '8', '7', '7', '7', '8', '2', '2', '8', '1', '2', '2', '9', '9', '1', '8', '8', '4', '3', '3', '1', '6', '3', '3', '12', '3', '8', '9', '1', '4', '3', '1', '8', '3', '9', '2', '8', '2', '8', '8', '8', '1', '1', '1', '9', '9', '8', '1', '9', '8', '1', '1', '3', '10', '8', '1', '1', '3', '9', '1', '4', '4', '1', '8', '9', '9', '2', '0', '0', '1', '8', '3', '1', '8', '8', '9', '8', '8', '1', '1', '8', '9', '8', '8', '8', '7', '9', '8', '8', '8', '10', '9', '8', '1', '2', '6', '1', '9', '9', '8', '12', '12', '12', '8', '8', '2', '8', '1', '2', '2', '2', '1', '9', '8', '2', '12', '2', '8', '12', '8', '9', '8', '8', '9', '7', '1', '1', '1', '1', '1', '8', '8', '1', '8', '8', '1', '1', '3', '2', '8', '8', '9', '10', '10', '2', '2', '1', '9', '2', '9', '9', '4', '12', '12', '12', '10', '7', '3', '3', '4', '2', '2', '9', '2', '8', '4', '2', '4', '1', '10', '9', '7', '8', '7', '1', '1', '3', '3', '1', '1', '3', '3', '3', '1', '1', '1', '1', '8', '2', '3', '1', '1', '2', '8', '8', '12', '8', '8', '8', '8', '11', '9', '1', '8', '9', '2', '8', '8', '8', '3', '9', '1', '9', '2', '7', '2', '8', '2', '8', '10', '8', '1', '10', '1', '1', '9', '9', '8', '8', '1', '8', '8', '8', '12', '8', '8', '8', '1', '8', '8', '8', '1', '9', '1', '1', '8', '1', '8', '9', '8', '2', '12', '9', '9', '0', '1', '8', '8', '1', '8', '12', '8', '8', '10', '8', '8', '8', '7', '8', '1', '8', '7', '3', '10', '1', '8', '9', '1', '8', '8', '8', '10', '1', '10', '3', '9', '1', '8', '9', '2', '8', '3', '3', '9', '9', '7', '9', '1', '1', '9', '2', '1', '1', '1', '7', '1', '1', '8', '8', '1', '1', '8', '1', '8', '3', '12', '9', '3', '3', '8', '8', '8', '8', '3', '1', '3', '3', '1', '11', '0', '8', '8', '7', '8', '12', '1', '8', '9', '8', '9', '8', '8', '3', '8', '8', '1', '1', '1', '9', '2', '2', '2', '8', '7', '12', '8', '8', '9', '10', '10', '7', '8', '1', '9', '8', '7', '3', '1', '3', '8', '2', '2', '3', '9', '8', '4', '4', '8', '9', '2', '1', '1', '7', '8', '9', '9', '7', '8', '7', '7', '8', '2', '2', '8', '4', '9', '7', '10', '0', '9', '8', '3', '7', '8', '1', '1', '8', '9', '9', '2', '2', '10', '1', '9', '10', '10', '10', '8', '3', '2', '12', '9', '9', '10', '12', '9', '12', '12', '9', '1', '2', '4', '12', '12', '7', '8', '9', '7', '7', '7', '3', '9', '8', '9', '1', '12', '8', '9', '4', '1', '3', '12', '12', '12', '12', '8', '8', '2', '1', '1', '2', '1', '1', '1', '12', '12', '8', '12', '2', '2', '12', '3', '3', '12', '8', '2', '8', '8', '12', '2', '1', '10', '3', '2', '8', '7', '1', '8', '1', '3', '7', '8', '9', '8', '3', '1', '1', '7', '8', '8', '9', '8', '2', '9', '2', '2', '9', '8', '1', '8', '8', '1', '3', '3', '1', '1', '10', '1', '2', '8', '1', '1', '1', '1', '9', '1', '4', '1', '7', '7', '7', '7', '2', '2', '8', '8', '12', '1', '9', '1', '7', '3', '3', '1', '8', '8', '10', '8', '9', '2', '9', '1', '3', '8', '8', '3', '12', '2', '8', '12', '2', '9', '1', '3', '3', '3', '3', '2', '8', '7', '9', '8', '3', '3', '1', '7', '8', '3', '1', '1', '12', '8', '9', '1', '2', '3', '8', '1', '1', '3', '3', '9', '1', '1', '1', '12', '1', '7', '3', '3', '1', '8', '8', '8', '1', '2', '4', '8', '1', '10', '2', '5', '3', '3', '12', '10', '9', '9', '12', '9', '0', '2', '8', '8', '9', '9', '9', '8', '1', '3', '1', '1', '4', '8', '1', '10', '8', '7', '2', '8', '2', '8', '4', '7', '8', '1', '9', '1', '9', '8', '2', '8', '2', '7', '9', '2', '2', '9', '1', '8', '12', '1', '8', '1', '4', '1', '9', '9', '1', '10', '12', '4', '8', '1', '7', '3', '9', '2', '12', '7', '8', '8', '2', '1', '12', '9', '8', '1', '2', '2', '8', '10', '2', '1', '7', '7', '7', '12', '3', '3', '8', '3', '8', '8', '3', '9', '8', '9', '1', '1', '8', '7', '9', '3', '3', '8', '1', '0', '9', '9', '9', '1', '8', '9', '9', '10', '1', '8', '7', '8', '8', '8', '9', '9', '8', '9', '10', '4', '9', '3', '7', '12', '1', '9', '9', '8', '9', '1', '9', '3', '1', '8', '4', '12', '10', '9', '8', '7', '10', '8', '12', '12', '3', '10', '8', '12', '2', '1', '2', '3', '9', '8', '8', '7', '1', '1', '2', '2', '1', '2', '2', '7', '1', '3', '9', '9', '3', '8', '8', '8', '8', '8', '7', '8', '8', '10', '8', '1', '8', '2', '8', '2', '2', '2', '2', '2', '1', '7', '12', '10', '1', '2', '8', '1', '4', '7', '8', '1', '12', '8', '7', '9', '2', '2', '2', '8', '1', '8', '1', '1', '0', '2', '2', '6', '1', '8', '2', '1', '8', '1', '1', '2', '2', '3', '2', '2', '10', '10', '9', '1', '7', '7', '8', '8', '1', '12', '10', '12', '3', '8', '8', '8', '3', '8', '3', '10', '2', '2', '2', '1', '2', '2', '1', '0', '1', '8', '9', '1', '1', '1', '2', '2', '2', '2', '2', '1', '1', '1', '1', '12', '9', '9', '1', '10', '11', '12', '9', '1', '1', '8', '8', '1', '2', '12', '8', '3', '7', '2', '10', '9', '7', '8', '1', '2', '2', '7', '9', '12', '2', '2', '1', '8', '9', '2', '3', '2', '2', '2', '9', '9', '8', '9', '2', '3', '8', '8', '9', '7', '3', '10', '8', '8', '9', '1', '1', '2', '8', '1', '10', '8', '8', '9', '7', '1', '7', '12', '8', '1', '7', '2', '1', '3', '9', '2', '8', '8', '1', '2', '3', '9', '3', '8', '9', '7', '7', '1', '10', '2', '8', '8', '8', '1', '8', '2', '8', '8', '1', '2', '8', '10', '2', '1', '0', '8', '3', '8', '10', '10', '12', '3', '8', '9', '3', '10', '8', '8', '8', '1', '8', '8', '2', '2', '2', '2', '2', '2', '1', '1', '10', '1', '10', '8', '2', '1', '2', '2', '8', '3', '3', '2', '2', '1', '8', '1', '3', '1', '9', '2', '2', '1', '3', '2', '2', '3', '2', '2', '3', '2', '1', '1', '2', '1', '2', '7', '8', '2', '1', '12', '12', '3', '1', '9', '9', '7', '8', '7', '2', '7', '9', '2', '2', '2', '8', '9', '7', '9', '2', '8', '9', '9', '2', '9', '1', '1', '9', '8', '3', '3', '8', '2', '7', '8', '11', '2', '8', '9', '1', '8', '3', '1', '2', '1', '2', '2', '2', '2', '8', '3', '9', '2', '10', '9', '9', '2', '7', '7', '7', '1', '3', '8', '8', '1', '1', '8', '7', '1', '8', '8', '9', '3', '3', '3', '2', '4', '8', '12', '8', '1', '12', '9', '2', '8', '2', '12', '2', '1', '3', '8', '12', '12', '11', '8', '10', '9', '2', '9', '4', '10', '1', '12', '12', '9', '3', '8', '9', '12', '9', '9', '2', '2', '2', '3', '5', '1', '1', '1', '7', '7', '8', '2', '2', '8', '2', '1', '7', '3', '2', '9', '8', '9', '1', '1', '9', '8', '9', '3', '3', '2', '9', '4', '9', '8', '8', '1', '8', '10', '2', '0', '3', '8', '8', '8', '9', '12', '9', '8', '2', '2', '8', '10', '9', '8', '2', '2', '4', '1', '9', '9', '8', '8', '10', '3', '1', '1', '2', '8', '7', '8', '2', '1', '1', '8', '12', '2', '9', '4', '8', '3', '3', '3', '8', '2', '12', '12', '8', '9', '12', '8', '2', '12', '7', '9', '2', '2', '1', '3', '1', '3', '11', '9', '8', '1', '10', '2', '12', '3', '8', '4', '1', '2', '8', '8', '3', '9', '3', '8', '7', '1', '1', '3', '1', '8', '2', '9', '8', '12', '1', '1', '1', '9', '9', '1', '8', '8', '8', '2', '2', '12', '3', '1', '1', '1', '9', '9', '3', '2', '1', '1', '9', '9', '1', '9', '9', '1', '9', '3', '9', '1', '8', '1', '9', '8', '3', '8', '8', '9', '9', '8', '2', '8', '1', '2', '2', '10', '1', '2', '1', '1', '9', '1', '9', '3', '7', '7', '2', '10', '3', '3', '1', '1', '9', '1', '2', '1', '3', '2', '1', '1', '1', '1', '1', '1', '1', '1', '1', '2', '8', '8', '7', '1', '1', '8', '8', '2', '1', '12', '1', '8', '1', '9', '8', '8', '2', '1', '9', '9', '2', '2', '8', '1', '7', '7', '9', '8', '1', '6', '8', '8', '2', '2', '8', '3', '8', '3', '2', '3', '8', '1', '2', '2', '2', '8', '9', '9', '9', '12', '1', '9', '2', '1', '9', '2', '9', '9', '10', '8', '1', '1', '9', '2', '1', '7', '1', '3', '7', '1', '1', '1', '3', '8', '8', '8', '9', '9', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '1', '0', '1', '1', '1', '2', '2', '3', '1', '1', '2', '3', '3', '3', '7', '3', '1', '9', '3', '9', '1', '4', '1', '1', '9', '10', '9', '8', '2', '3', '10', '9', '3', '10', '12', '8', '9', '8', '2', '1', '9', '8', '1', '7', '8', '1', '9', '1', '2', '10', '7', '7', '8', '1', '3', '3', '9', '3', '3', '1', '1', '1', '9', '1', '3', '8', '8', '8', '1', '8', '1', '9', '7', '8', '8', '2', '1', '3', '8', '2', '1', '6', '1', '8', '8', '2', '8', '2', '2', '9', '8', '1', '1', '6', '8', '6', '10', '2', '1', '1', '8', '9', '3', '3', '3', '1', '3', '9', '6', '8', '1', '8', '3', '1', '2', '2', '12', '7', '1', '1', '1', '1', '12', '1', '8', '8', '3', '2', '1', '9', '2', '3', '3', '2', '2', '7', '2', '2', '2', '3', '1', '9', '8', '7', '1', '1', '3', '1', '9', '4', '3', '9', '2', '1', '1', '9', '8', '8', '7', '2', '1', '3', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '2', '10', '1', '1', '8', '2', '8', '1', '8', '1', '3', '1', '2', '1', '1', '3', '1', '2', '1', '1', '1', '1', '1', '1', '1', '2', '1', '8', '1', '9', '8', '8', '1', '10', '8', '8', '1', '3', '9', '8', '9', '8', '2', '1', '3', '4', '1', '12', '1', '9', '8', '2', '7', '2', '1', '2', '9', '2', '1', '3', '8', '1', '2', '3', '8', '2', '10', '10', '10', '9', '9', '9', '3', '8', '3', '1', '1', '1', '1', '1', '1', '2', '1', '9', '2', '7', '9', '8', '7', '9', '1', '1', '1', '1', '8', '2', '2', '3', '3', '2', '12', '8', '3', '1', '1', '12', '9', '8', '2', '2', '9', '1', '2', '2', '9', '1', '1', '2', '4', '8', '1', '8', '10', '2', '9', '9', '9', '1', '2', '1', '1', '2', '2', '2', '12', '1', '2', '8', '3', '9', '9', '3', '2', '9', '9', '7', '2', '8', '8', '9', '2', '9', '1', '1', '1', '1', '1', '2', '8', '2', '1', '2', '2', '8', '3', '1', '1', '9', '1', '3', '10', '7', '9', '8', '8', '2', '2', '9', '2', '2', '2', '2', '2', '2', '9', '1', '3', '6', '4', '2', '2', '9', '1', '9', '8', '1', '1', '1', '7', '3', '2', '9', '9', '1', '9', '7', '2', '8', '9', '9', '12', '10', '8', '10', '2', '3', '1', '2', '2', '12', '12', '3', '12', '1', '2', '1', '2', '1', '2', '8', '2', '3', '12', '12', '8', '2', '2', '9', '2', '9', '2', '3', '3', '1', '1', '1', '12', '3', '7', '1', '3', '1', '2', '9', '2', '7', '2', '1', '8', '3', '7', '8', '1', '1', '3', '8', '3', '2', '9', '3', '1', '1', '1', '1', '11', '1', '2', '6', '1', '2', '3', '8', '9', '9', '2', '9', '2', '1', '1', '3', '1', '12', '9', '1', '1', '9', '1', '9', '9', '8', '9', '8', '1', '2', '7', '1', '0', '9', '8', '2', '3', '2', '4', '1', '1', '1', '8', '1', '12', '1', '2', '3', '3', '9', '9', '9', '9', '9', '7', '9', '3', '9', '1', '7', '3', '3', '3', '3', '7', '8', '2', '2', '2', '8', '2', '3', '9', '9', '3', '1', '8', '9', '12', '8', '8', '3', '2', '1', '6', '1', '9', '1', '1', '1', '1', '1', '1', '2', '2', '1', '2', '2', '2', '2', '9', '2', '5', '3', '2', '2', '8', '2', '1', '8', '3', '3', '8', '3', '1', '3', '9', '2', '8', '9', '4', '4', '9', '8', '8', '7', '7', '3', '1', '8', '8', '3', '2', '2', '10', '3', '3', '2', '2', '2', '2', '12', '3', '1', '2', '7', '7', '12', '2', '1', '2', '1', '1', '2', '1', '3', '3', '3', '1', '3', '9', '4', '8', '8', '4', '2', '4', '2', '8', '9', '1', '1', '2', '2', '2', '8', '7', '2', '8', '2', '9', '1', '8', '1', '1', '2', '1', '1', '1', '2', '9', '9', '8', '1', '8', '1', '4', '8', '1', '2', '7', '9', '2', '1', '8', '9', '8', '9', '1', '1', '2', '3', '2', '12', '7', '1', '2', '4', '4', '4', '2', '2', '2', '9', '8', '8', '1', '12', '3', '3', '2', '2', '3', '12', '1', '5', '1', '1', '3', '2', '8', '8', '2', '2', '2', '3', '9', '8', '8', '9', '8', '8', '11', '8', '2', '9', '2', '3', '1', '7', '1', '8', '2', '8', '1', '1', '1', '8', '1', '1', '1', '7', '8', '8', '2', '3', '2', '1', '8', '8', '1', '8', '4', '8', '9', '8', '9', '8', '1', '2', '3', '2', '1', '2', '2', '3', '1', '9', '4', '8', '1', '1', '9', '1', '1', '8', '2', '2', '2', '1', '7', '3', '3', '1', '3', '3', '12', '3', '1', '1', '3', '1', '1', '4', '8', '4', '4', '3', '3', '3', '4', '8', '1', '9', '9', '9', '4', '1', '7', '4', '8', '4', '9', '8', '1', '1', '3', '8', '9', '1', '9', '7', '1', '1', '9', '1', '8', '8', '8', '1', '1', '8', '2', '2', '2', '9', '4', '4', '1', '2', '1', '2', '9', '1', '1', '8', '5', '5', '5', '9', '12', '4', '1', '2', '1', '8', '8', '1', '9', '9', '2', '12', '1', '1', '7', '8', '8', '4', '1', '2', '8', '11', '2', '3', '2', '2', '1', '11', '2', '2', '2', '2', '9', '9', '2', '2', '9', '3', '1', '9', '1', '1', '1', '7', '8', '1', '8', '9', '1', '2', '9', '8', '1', '2', '9', '8', '12', '2', '2', '10', '8', '2', '2', '8', '8', '1', '2', '6', '1', '1', '7', '7', '2', '3', '2', '1', '1', '8', '1', '10', '8', '12', '6', '4', '1', '4', '1', '2', '9', '8', '9', '4', '8', '8', '10', '8', '8', '8', '8', '8', '2', '2', '8', '4', '9', '3', '3', '3', '3', '3', '9', '2', '1', '1', '2', '3', '2', '1', '2', '3', '3', '3', '2', '2', '3', '3', '2', '1', '2', '3', '3', '3', '3', '12', '9', '3', '2', '2', '3', '1', '5', '1', '1', '3', '2', '2', '1', '2', '9', '3', '9', '8', '1', '8', '7', '7', '9', '1', '1', '7', '9', '10', '1', '7', '1', '4', '3', '8', '9', '4', '9', '9', '2', '9', '2', '8', '4', '2', '2', '9', '5', '2', '1', '10', '9', '3', '1', '1', '1', '2', '1', '2', '9', '1', '7', '9', '9', '8', '2', '2', '8', '9', '1', '9', '8', '2', '3', '2', '2', '1', '2', '2', '2', '1', '1', '8', '1', '9', '4', '12', '8', '2', '7', '2', '2', '1', '9', '3', '1', '8', '12', '1', '1', '2', '1', '10', '2', '6', '12', '12', '4', '9', '1', '9', '10', '1', '2', '8', '2', '1', '8', '6', '12', '9', '9', '2', '1', '7', '8', '9', '5', '8', '1', '8', '8', '3', '1', '8', '2', '8', '2', '2', '1', '2', '9', '2', '8', '1', '12', '8', '2', '7', '3', '3', '12', '2', '3', '3', '9', '9', '3', '3', '3', '3', '3', '2', '1', '4', '1', '4', '8', '8', '2', '1', '2', '9', '12', '1', '10', '12', '1', '2', '9', '4', '8', '10', '3', '2', '8', '7', '4', '8', '9', '2', '7', '1', '10', '2', '2', '2', '8', '4', '2', '2', '1', '1', '8', '7', '9', '2', '1', '9', '8', '9', '12', '4', '8', '2', '8', '7', '7', '1', '9', '2', '1', '1', '9', '1', '8', '2', '3', '9', '9', '1', '3', '2', '9', '2', '2', '8', '1', '1', '2', '1', '2', '1', '9', '8', '2', '2', '4', '8', '8', '9', '5', '5', '9', '8', '9', '6', '2', '1', '12', '2', '3', '8', '8', '9', '2', '2', '1', '3', '2', '10', '9', '10', '7', '2', '8', '4', '2', '8', '9', '8', '1', '3', '8', '8', '1', '8', '3', '8', '8', '1', '10', '3', '1', '5', '8', '8', '9', '2', '2', '9', '2', '1', '9', '1', '4', '1', '12', '8', '8', '2', '1', '1', '1', '1', '1', '9', '2', '9', '8', '9', '5', '9', '1', '8', '2', '4', '1', '1', '1', '9', '12', '4', '1', '1', '9', '8', '4', '1', '8', '9', '8', '8', '9', '9', '9', '9', '9', '9', '1', '3', '4', '2', '2', '8', '2', '9', '2', '3', '8', '3', '1', '7', '2', '1', '1', '4', '2', '9', '2', '1', '3', '2', '9', '2', '9', '3', '8', '1', '9', '1', '1', '2', '8', '8', '8', '2', '3', '9', '9', '8', '2', '4', '1', '5', '1', '1', '9', '9', '2', '1', '1', '1', '9', '9', '1', '3', '9', '2', '2', '9', '9', '8', '3', '9', '9', '8', '2', '2', '8', '9', '2', '2', '8', '9', '10', '10', '2', '8', '7', '9', '9', '4', '2', '4', '11', '2', '1', '7', '8', '8', '8', '9', '4', '9', '8', '8', '2', '1', '1', '9', '8', '3', '3', '8', '8', '7', '9', '1', '3', '4', '8', '10', '2', '4', '4', '8', '2', '8', '2', '8', '9', '2', '1', '2', '2', '3', '1', '9', '4', '5', '9', '9', '1', '1', '1', '1', '1', '1', '7', '1', '1', '1', '8', '1', '1', '1', '1', '1', '9', '1', '2', '1', '12', '9', '3', '9', '9', '4', '1', '9', '1', '1', '2', '11', '9', '3', '2', '2', '2', '4', '2', '3', '2', '7', '7', '8', '1', '10', '9', '5', '7', '1', '10', '8', '1', '2', '7', '1', '12', '5', '8', '4', '2', '9', '9', '8', '2', '9', '8', '8', '10', '2', '3', '2', '7', '2', '8', '10', '3', '2', '9', '9', '9', '1', '1', '1', '1', '8', '2', '10', '9', '8', '1', '2', '2', '2', '1', '2', '1', '3', '9', '12', '2', '2', '4', '8', '1', '3', '9', '4', '9', '1', '9', '9', '2', '1', '1', '9', '7', '10', '3', '10', '2', '9', '2', '9', '2', '8', '2', '1', '2', '3', '1', '1', '1', '8', '5', '8', '3', '1', '1', '4', '2', '2', '1', '3', '8', '2', '1', '1', '1', '2', '9', '4', '1', '8', '1', '2', '8', '2', '9', '2', '2', '5', '5', '2', '5', '2', '1', '8', '8', '1', '3', '12', '2', '4', '3', '2', '2', '2', '6', '2', '3', '3', '1', '8', '2', '2', '1', '1', '8', '8', '10', '1', '9', '2', '9', '12', '0', '2', '1', '3', '2', '2', '1', '2', '1', '9', '9', '2', '9', '8', '8', '1', '7', '8', '1', '2', '6', '9', '8', '10', '1', '12', '2', '2', '1', '8', '2', '1', '12', '7', '4', '2', '10', '2', '2', '1', '2', '1', '2', '1', '1', '2', '1', '9', '8', '10', '2', '10', '9', '9', '12', '5', '3', '2', '8', '2', '8', '3', '3', '1', '4', '9', '10', '2', '4', '8', '1', '2', '10', '12', '12', '3', '1', '1', '2', '9', '6', '6', '1', '9', '1', '8', '1', '8', '8', '8', '8', '2', '2', '2', '8', '8', '2', '1', '1', '1', '1', '1', '8', '8', '5', '8', '8', '1', '8', '9', '9', '1', '7', '12', '9', '9', '7', '2', '2', '7', '8', '9', '8', '2', '9', '6', '3', '4', '8', '4', '1', '2', '9', '8', '8', '8', '2', '1', '1', '8', '8', '3', '1', '9', '9', '7', '1', '2', '2', '2', '9', '4', '1', '2', '12', '1', '8', '2', '1', '9', '1', '8', '9', '9', '2', '5', '4', '2', '1', '2', '8', '12', '8', '10', '3', '8', '8', '8', '2', '2', '8', '2', '8', '2', '4', '8', '10', '2', '7', '2', '1', '8', '8', '9', '5', '8', '1', '12', '7', '10', '2', '2', '2', '1', '4', '8', '12', '1', '8', '2', '2', '9', '1', '8', '3', '1', '1', '5', '10', '2', '1', '2', '4', '8', '9', '3', '2', '8', '7', '4', '4', '9', '9', '2', '8', '1', '1', '2', '4', '7', '1', '9', '1', '1', '1', '8', '4', '1', '3', '1', '2', '2', '9', '1', '8', '8', '2', '2', '9', '9', '1', '8', '8', '1', '9', '8', '8', '2', '2', '2', '2', '1', '1', '1', '7', '4', '1', '1', '4', '2', '3', '3', '2', '3', '9', '2', '8', '2', '1', '5', '1', '2', '2', '1', '3', '5', '2', '9', '1', '1', '9', '8', '1', '4', '8', '1', '8', '2', '4', '8', '4', '7', '9', '8', '1', '2', '2', '4', '4', '11', '3', '11', '1', '3', '8', '1', '9', '6', '8', '9', '3', '9', '8', '9', '2', '3', '3', '7', '1', '7', '2', '9', '9', '1', '8', '8', '1', '5', '5', '8', '8', '1', '2', '1', '3', '9', '9', '5', '9', '8', '1', '4', '3', '8', '1', '1', '2', '1', '8', '9', '8', '2', '2', '2', '1', '2', '4', '10', '8', '1', '9', '1', '8', '1', '1', '9', '9', '8', '11', '1', '8', '2', '1', '2', '8', '2', '8', '8', '2', '9', '6', '10', '1', '8', '6', '4', '5', '11', '2', '2', '8', '1', '8', '1', '8', '1', '4', '8', '1', '1', '1', '8', '3', '7', '3', '3', '7', '1', '1', '4', '8', '1', '6', '4', '2', '2', '1', '1', '1', '1', '6', '2', '2', '8', '1', '8', '5', '5', '2', '1', '1', '2', '3', '7', '1', '1', '6', '2', '9', '3', '9', '8', '9', '2', '1', '4', '2', '9', '13', '9', '2', '8', '12', '4', '1', '1', '9', '5', '1', '1', '1', '1', '8', '2', '1', '1', '2', '9', '0', '12', '1', '1', '8', '9', '8', '3', '2', '2', '12', '8', '2', '2', '2', '1', '10', '1', '9', '8', '9', '5', '2', '2', '9', '2', '8', '4', '8', '3', '7', '9', '2', '2', '7', '2', '2', '8', '1', '1', '9', '10', '9', '8', '2', '9', '0', '7', '1', '1', '1', '9', '8', '2', '8', '12', '8', '3', '2', '8', '8', '7', '7', '8', '12', '2', '1', '3', '9', '2', '2', '8', '2', '8', '8', '12', '8', '8', '1', '8', '9', '2', '4', '4', '7', '2', '1', '1', '9', '3', '7', '1', '3', '9', '8', '2', '2', '3', '1', '3', '10', '2', '3', '7', '1', '8', '1', '1', '10', '3', '10', '8', '4', '1', '2', '4', '3', '10', '9', '3', '9', '5', '8', '7', '2', '9', '3', '3', '2', '11', '8', '1', '1', '7', '2', '7', '2', '9', '4', '8', '1', '12', '8', '7', '9', '1', '0', '2', '6', '4', '9', '3', '2', '4', '12', '9', '8', '5', '9', '2', '8', '4', '6', '8', '9', '3', '1', '8', '7', '3', '1', '1', '1', '8', '9', '1', '8', '7', '9', '8', '2', '1', '2', '2', '3', '2', '2', '1', '1', '4', '9', '7', '3', '2', '8', '8', '9', '1', '9', '7', '4', '2', '2', '8', '5', '9', '1', '5', '7', '1', '4', '6', '9', '10', '1', '8', '2', '9', '1', '9', '2', '9', '8', '2', '9', '9', '11', '2', '12', '2', '2', '9', '4', '8', '1', '9', '9', '2', '8', '9', '1', '2', '9', '3', '8', '8', '1', '7', '8', '8', '2', '3', '2', '2', '2', '2', '9', '8', '10', '2', '2', '1', '9', '11', '1', '8', '8', '2', '4', '2', '1', '2', '8', '8', '9', '2', '8', '9', '2', '3', '1', '2', '1', '3', '10', '8', '1', '2', '4', '1', '9', '8', '9', '3', '3', '9', '8', '2', '11', '3', '7', '1', '8', '7', '1', '8', '8', '8', '1', '4', '8', '8', '1', '9', '3', '1', '1', '2', '8', '8', '8', '2', '1', '8', '10', '2', '12', '8', '8', '9', '1', '8', '1', '3', '2', '2', '10', '7', '2', '2', '1', '1', '8', '2', '9', '1', '2', '9', '3', '3', '10', '1', '4', '7', '10', '12', '2', '4', '10', '8', '6', '2', '9', '1', '8', '3', '1', '2', '8', '8', '2', '12', '10', '2', '10', '9', '1', '2', '5', '8', '12', '8', '8', '7', '8', '8', '1', '8', '2', '2', '9', '1', '8', '10', '5', '5', '5', '8', '1', '9', '1', '13', '1', '2', '2', '1', '2', '4', '9', '2', '1', '9', '13', '11', '1', '9', '9', '3', '10', '8', '7', '8', '8', '4', '2', '1', '9', '4', '3', '1', '1', '10', '9', '2', '8', '6', '2', '1', '1', '3', '2', '8', '1', '1', '1', '1', '2', '4', '1', '1', '10', '1', '8', '1', '1', '9', '1', '7', '1', '1', '9', '9', '2', '12', '9', '1', '8', '1', '12', '1', '8', '3', '8', '1', '8', '1', '2', '10', '1', '1', '2', '2', '9', '3', '9', '12', '1', '9', '1', '2', '8', '7', '2', '1', '1', '3', '2', '8', '4', '4', '5', '12', '7', '8', '9', '6', '1', '8', '9', '2', '1', '9', '1', '4', '1', '8', '2', '1', '1', '8', '7', '4', '9', '1', '8', '4', '2', '7', '5', '1', '3', '9', '2', '8', '2', '1', '2', '9', '1', '1', '2', '3', '4', '3', '1', '2', '2', '10', '2', '2', '8', '1', '1', '2', '4', '9', '2', '11', '9', '8', '9', '2', '1', '10', '4', '1', '1', '10', '7', '2', '10', '8', '1', '10', '8', '8', '8', '8', '8', '8', '2', '2', '2', '8', '3', '4', '1', '1', '1', '2', '1', '8', '8', '2', '8', '3', '8', '3', '1', '2', '3', '1', '1', '10', '1', '4', '4', '3', '3', '1', '9', '1', '1', '2', '1', '1', '6', '1', '1', '1', '9', '1', '4', '1', '1', '1', '1', '1', '1', '2', '12', '8', '8', '1', '9', '12', '10', '8', '2', '12', '1', '1', '2', '1', '7', '10', '7', '1', '2', '1', '10', '8', '2', '2', '8', '2', '2', '1', '9', '8', '8', '8', '8', '3', '1', '4', '2', '8', '3', '10', '10', '1', '9', '1', '9', '1', '9', '1', '8', '3', '1', '1', '8', '8', '8', '6', '3', '4', '1', '1', '1', '9', '9', '5', '2', '10', '2', '6', '2', '2', '7', '1', '8', '2', '11', '1', '1', '1', '2', '1', '9', '4', '4', '2', '8', '4', '6', '8', '8', '10', '10', '2', '1', '2', '9', '1', '8', '8', '3', '4', '8', '8', '2', '8', '8', '1', '2', '1', '2', '9', '4', '1', '3', '3', '3', '8', '8', '8', '2', '6', '4', '3', '3', '12', '3', '4', '9', '6', '8', '7', '7', '8', '6', '4', '1', '10', '6', '3', '3', '8', '1', '1', '8', '1', '11', '8', '1', '1', '2', '3', '11', '7', '2', '1', '1', '1', '9', '1', '2', '1', '1', '9', '8', '1', '1', '8', '0', '4', '1', '1', '2', '1', '1', '2', '2', '9', '1', '10', '1', '3', '8', '9', '3', '3', '1', '10', '1', '9', '1', '1', '7', '4', '1', '8', '10', '1', '2', '10', '1', '1', '2', '3', '3', '9', '1', '8', '10', '1', '7', '9', '1', '9', '6', '3', '9', '2', '8', '4', '12', '12', '3', '1', '1', '9', '1', '9', '2', '1', '1', '1', '1', '1', '9', '2', '1', '2', '10', '9', '8', '8', '10', '8', '2', '3', '2', '1', '12', '8', '12', '2', '9', '1', '3', '2', '5', '2', '2', '8', '2', '2', '4', '10', '9', '10', '9', '4', '2', '4', '12', '8', '2', '1', '8', '3', '9', '2', '9', '4', '1', '1', '1', '1', '1', '9', '6', '2', '9', '2', '1', '3', '2', '2', '5', '8', '9', '1', '6', '3', '1', '2', '2', '2', '6', '1', '8', '10', '8', '3', '3', '13', '2', '9', '9', '10', '6', '12', '8', '2', '9', '2', '3', '8', '8', '1', '2', '2', '3', '2', '10', '1', '8', '9', '1', '8', '9', '8', '8', '2', '10', '2', '1', '1', '1', '9', '9', '12', '1', '1', '4', '8', '3', '2', '2', '4', '4', '2', '2', '1', '1', '1', '9', '2', '2', '4', '1', '1', '8', '8', '8', '9', '2', '13', '9', '9', '4', '1', '9', '7', '10', '10', '8', '1', '9', '1', '1', '1', '8', '12', '4', '1', '2', '1', '1', '1', '7', '8', '3', '3', '1', '2', '2', '7', '2', '2', '10', '1', '8', '4', '0', '1', '9', '4', '1', '1', '2', '10', '10', '7', '9', '3', '12', '11', '2', '8', '10', '8', '9', '4', '3', '4', '10', '1', '1', '9', '3', '10', '1', '5', '3', '3', '2', '2', '1', '1', '6', '1', '7', '1', '1', '12', '1', '1', '1', '10', '1', '8', '8', '1', '3', '8', '3', '1', '2', '8', '3', '4', '2', '8', '8', '6', '1', '1', '4', '1', '2', '9', '5', '1', '7', '8', '9', '2', '4', '9', '8', '5', '9', '9', '9', '1', '2', '1', '3', '8', '9', '2', '9', '8', '9', '1', '8', '13', '9', '3', '8', '1', '4', '1', '3', '1', '6', '4', '10', '2', '2', '7', '8', '8', '12', '8', '9', '9', '9', '10', '3', '12', '12', '2', '9', '10', '9', '7', '2', '3', '4', '10', '10', '2', '7', '7', '1', '7', '3', '9', '1', '9', '8', '2', '9', '1', '8', '1', '10', '8', '8', '2', '7', '8', '1', '8', '9', '8', '1', '4', '1', '8', '8', '8', '1', '1', '10', '3', '2', '2', '6', '8', '9', '8', '2', '1', '1', '3', '8', '8', '1', '4', '4', '3', '9', '10', '4', '2', '4', '2', '9', '2', '2', '9', '1', '1', '10', '1', '1', '2', '1', '1', '1', '9', '1', '8', '8', '8', '2', '3', '8', '1', '6', '7', '3', '1', '9', '1', '2', '1', '6', '1', '2', '10', '1', '4', '2', '3', '2', '9', '9', '4', '9', '9', '1', '2', '8', '4', '7', '8', '7', '13', '9', '1', '2', '1', '9', '3', '3', '6', '8', '9', '10', '9', '2', '2', '3', '12', '1', '1', '1', '13', '2', '1', '7', '10', '8', '9', '1', '2', '9', '1', '1', '1', '2', '1', '12', '5', '6', '9', '10', '3', '1', '10', '1', '3', '2', '8', '9', '2', '10', '8', '8', '9', '2', '8', '9', '2', '2', '2', '8', '8', '2', '3', '1', '4', '8', '1', '9', '9', '2', '8', '10', '9', '1', '2', '12', '8', '8', '0', '2', '1', '1', '9', '10', '1', '6', '2', '2', '10', '2', '2', '4', '2', '2', '12', '1', '6', '7', '8', '9', '1', '3', '5', '7', '3', '1', '3', '1', '1', '3', '2', '6', '3', '2', '1', '8', '3', '5', '2', '1', '1', '8', '1', '1', '0', '2', '3', '5', '3', '1', '1', '9', '8', '8', '8', '8', '4', '7', '2', '9', '12', '5', '9', '3', '12', '3', '9', '1', '9', '1', '7', '3', '9', '3', '10', '1', '2', '1', '4', '9', '8', '2', '9', '11', '2', '8', '9', '4', '1', '1', '1', '1', '1', '1', '1', '9', '1', '9', '1', '1', '1', '12', '3', '10', '3', '6', '1', '7', '10', '8', '3', '7', '1', '1', '2', '6', '1', '3', '9', '2', '8', '1', '8', '8', '1', '8', '10', '12', '1', '10', '12', '10', '1', '8', '8', '1', '4', '7', '9', '2', '1', '9', '3', '8', '12', '10', '6', '1', '6', '8', '3', '1', '3', '1', '10', '10', '9', '8', '1', '2', '9', '8', '8', '1', '9', '8', '8', '3', '1', '3', '9', '12', '1', '8', '1', '5', '8', '11', '5', '5', '1', '2', '1', '3', '1', '1', '1', '9', '8', '8', '1', '9', '10', '8', '4', '1', '1', '8', '10', '8', '9', '1', '10', '1', '7', '2', '1', '1', '9', '6', '2', '9', '8', '1', '1', '1', '9', '8', '8', '1', '9', '9', '1', '1', '2', '7', '9', '2', '9', '9', '8', '7', '6', '8', '7', '8', '2', '9', '2', '1', '8', '8', '8', '2', '6', '3', '1', '8', '12', '12', '8', '7', '1', '2', '2', '7', '1', '9', '8', '1', '4', '1', '1', '8', '5', '8', '1', '8', '8', '8', '8', '1', '2', '11', '1', '1', '1', '2', '7', '1', '9', '2', '2', '2', '9', '8', '9', '2', '9', '1', '8', '1', '7', '11', '13', '1', '9', '8', '2', '2', '1', '2', '3', '10', '3', '10', '4', '3', '9', '1', '8', '1', '1', '13', '1', '1', '3', '8', '1', '2', '2', '2', '9', '4', '1', '8', '1', '9', '3', '6', '8', '5', '2', '8', '2', '8', '1', '9', '2', '2', '8', '1', '1', '4', '8', '9', '2', '2', '7', '1', '8', '12', '2', '12', '8', '2', '9', '9', '3', '8', '1', '1', '9', '9', '9', '2', '8', '1', '9', '1', '8', '9', '8', '2', '2', '4', '1', '9', '10', '10', '8', '8', '1', '1', '2', '1', '4', '1', '2', '1', '1', '3', '12', '8', '8', '8', '8', '10', '2', '8', '12', '1', '9', '9', '8', '9', '1', '2', '1', '8', '8', '8', '2', '8', '8', '3', '10', '8', '9', '1', '1', '1', '3', '4', '8', '10', '6', '3', '1', '3', '2', '8', '9', '3', '5', '4', '1', '5', '2', '9', '9', '9', '1', '1', '1', '9', '9', '6', '10', '9', '8', '12', '2', '9', '1', '2', '1', '1', '1', '8', '1', '12', '1', '12', '1', '9', '1', '8', '1', '2', '1', '2', '8', '8', '10', '1', '9', '1', '2', '2', '8', '8', '3', '12', '1', '11', '2', '10', '12', '11', '2', '1', '10', '1', '1', '9', '3', '10', '2', '7', '1', '8', '1', '9', '9', '2', '1', '3', '12', '5', '9', '9', '8', '1', '8', '8', '8', '1', '1', '3', '9', '8', '9', '8', '3', '10', '3', '7', '8', '9', '2', '3', '1', '9', '10', '9', '2', '8', '2', '6', '4', '1', '1', '3', '8', '2', '3', '9', '1', '2', '1', '8', '1', '2', '2', '2', '2', '9', '9', '4', '8', '9', '1', '1', '2', '9', '1', '5', '2', '9', '8', '10', '1', '2', '10', '8', '5', '6', '4', '8', '1', '1', '1', '8', '9', '2', '8', '8', '8', '12', '2', '2', '1', '8', '1', '2', '1', '9', '8', '8', '9', '1', '1', '8', '7', '1', '8', '0', '2', '3', '8', '1', '1', '8', '1', '1', '12', '3', '2', '10', '8', '10', '6', '1', '8', '8', '10', '8', '8', '1', '4', '8', '1', '2', '9', '8', '1', '1', '3', '3', '5', '1', '1', '2', '1', '2', '1', '9', '8', '3', '9', '8', '2', '10', '9', '1', '8', '1', '10', '9', '10', '1', '2', '3', '8', '1', '3', '1', '9', '9', '8', '1', '9', '8', '8', '9', '1', '9', '2', '8', '8', '2', '13', '8', '10', '8', '9', '3', '2', '1', '3', '1', '12', '10', '10', '1', '11', '1', '2', '4', '2', '10', '1', '8', '11', '1', '1', '1', '2', '2', '8', '2', '9', '8', '9', '9', '12', '8', '1', '9', '2', '4', '1', '8', '3', '9', '1', '6', '5', '8', '9', '3', '3', '2', '1', '1', '1', '8', '11', '1', '8', '7', '1', '11', '9', '1', '7', '9', '9', '8', '8', '8', '12', '8', '1', '2', '9', '8', '8', '2', '10', '9', '8', '8', '4', '10', '9', '8', '2', '10', '10', '2', '1', '2', '1', '2', '1', '4', '1', '7', '1', '1', '3', '12', '9', '8', '5', '9', '10', '1', '9', '9', '2', '7', '4', '1', '1', '8', '12', '8', '2', '2', '9', '4', '1', '1', '5', '9', '1', '8', '12', '8', '1', '1', '4', '9', '10', '2', '3', '1', '3', '3', '3', '8', '1', '2', '1', '2', '1', '2', '9', '1', '2', '8', '9', '4', '8', '8', '10', '2', '12', '5', '5', '1', '8', '1', '7', '8', '1', '1', '8', '9', '12', '4', '9', '3', '1', '2', '5', '9', '2', '1', '2', '2', '1', '2', '8', '1', '8', '8', '2', '2', '1', '8', '4', '1', '2', '10', '9', '10', '8', '1', '10', '4', '8', '8', '10', '2', '8', '1', '1', '3', '10', '1', '4', '2', '8', '3', '3', '2', '9', '8', '5', '5', '9', '3', '10', '8', '1', '2', '1', '2', '9', '1', '1', '2', '9', '9', '8', '8', '11', '9', '4', '1', '8', '2', '7', '1', '2', '8', '2', '9', '9', '8', '2', '9', '9', '8', '8', '1', '2', '8', '1', '1', '2', '9', '8', '1', '4', '9', '1', '8', '12', '1', '12', '2', '8', '2', '9', '1', '12', '1', '2', '9', '1', '1', '9', '3', '2', '9', '9', '2', '11', '8', '7', '9', '9', '9', '8', '8', '1', '2', '8', '1', '1', '4', '1', '2', '10', '1', '9', '2', '1', '1', '1', '9', '1', '13', '4', '3', '2', '1', '1', '2', '2', '7', '8', '7', '1', '1', '2', '8', '8', '3', '4', '8', '2', '10', '13', '8', '8', '9', '2', '2', '1', '1', '10', '4', '2', '7', '9', '8', '2', '1', '9', '8', '1', '1', '9', '8', '9', '8', '2', '8', '2', '2', '9', '2', '1', '9', '1', '2', '4', '8', '2', '1', '1', '8', '1', '4', '1', '9', '3', '9', '2', '7', '1', '9', '6', '1', '10', '2', '2', '2', '2', '2', '10', '10', '10', '9', '9', '9', '3', '12', '10', '1', '1', '10', '1', '1', '2', '2', '1', '2', '3', '12', '8', '8', '1', '2', '9', '9', '1', '1', '10', '8', '9', '8', '3', '1', '1', '3', '1', '10', '1', '1', '1', '4', '1', '1', '1', '7', '10', '2', '1', '8', '1', '9', '3', '1', '10', '8', '8', '1', '5', '9', '1', '2', '1', '8', '8', '1', '3', '1', '10', '2', '1', '1', '2', '8', '2', '5', '3', '5', '2', '1', '1', '9', '1', '7', '9', '9', '2', '8', '1', '12', '4', '1', '4', '9', '9', '4', '8', '1', '10', '10', '8', '8', '8', '9', '2', '3', '5', '1', '8', '1', '1', '1', '7', '10', '1', '4', '12', '3', '2', '2', '9', '1', '1', '1', '2', '8', '4', '1', '7', '3', '12', '8', '6', '2', '2', '7', '9', '11', '1', '10', '9', '1', '8', '12', '2', '11', '1', '2', '3', '1', '1', '1', '8', '9', '2', '2', '2', '2', '3', '3', '8', '10', '4', '1', '2', '9', '8', '2', '2', '9', '1', '8', '11', '4', '2', '8', '7', '1', '1', '7', '3', '9', '9', '4', '5', '8', '10', '2', '9', '9', '9', '8', '1', '9', '2', '2', '2', '1', '2', '9', '4', '3', '12', '4', '3', '2', '3', '8', '3', '10', '1', '10', '1', '1', '8', '10', '6', '8', '9', '9', '1', '2', '9', '2', '9', '3', '8', '1', '1', '12', '1', '5', '1', '10', '8', '10', '2', '7', '1', '1', '1', '1', '3', '2', '3', '5', '9', '1', '2', '1', '9', '8', '10', '9', '8', '1', '8', '1', '8', '2', '8', '1', '2', '8', '1', '2', '5', '8', '12', '2', '1', '1', '1', '1', '1', '8', '8', '2', '1', '4', '2', '8', '8', '8', '2', '9', '9', '11', '1', '2', '9', '3', '1', '1', '8', '9', '2', '4', '10', '1', '9', '8', '4', '9', '8', '10', '8', '8', '9', '2', '8', '9', '3', '3', '3', '4', '3', '2', '2', '10', '8', '2', '1', '5', '1', '9', '1', '1', '2', '1', '2', '3', '1', '1', '8', '8', '1', '10', '8', '8', '1', '6', '4', '8', '2', '5', '8', '1', '3', '8', '2', '8', '9', '1', '1', '8', '12', '10', '1', '5', '8', '2', '8', '10', '9', '9', '1', '6', '9', '1', '10', '8', '2', '9', '10', '1', '2', '1', '1', '8', '8', '3', '9', '8', '1', '10', '2', '8', '1', '10', '1', '5', '1', '1', '1', '5', '1', '1', '1', '4', '1', '1', '1', '8', '2', '8', '1', '8', '3', '8', '9', '1', '8', '1', '8', '1', '1', '2', '2', '1', '12', '9', '9', '1', '8', '2', '1', '2', '8', '1', '9', '1', '1', '2', '1', '2', '2', '2', '1', '9', '9', '8', '8', '1', '1', '9', '10', '8', '4', '3', '3', '4', '9', '1', '3', '3', '10', '2', '2', '1', '8', '4', '1', '9', '4', '1', '1', '9', '8', '10', '4', '9', '9', '8', '2', '1', '3', '4', '9', '3', '8', '9', '1', '1', '2', '1', '7', '8', '2', '9', '1', '9', '2', '6', '9', '10', '8', '1', '1', '5', '9', '5', '1', '9', '10', '9', '3', '3', '8', '8', '2', '9', '2', '8', '2', '1', '8', '5', '8', '3', '10', '1', '1', '2', '1', '8', '1', '4', '3', '2', '9', '4', '9', '8', '8', '8', '1', '3', '9', '1', '9', '9', '1', '2', '1', '9', '8', '9', '8', '1', '1', '1', '4', '2', '2', '9', '9', '1', '1', '1', '3', '6', '1', '2', '3', '1', '4', '2', '9', '1', '9', '9', '1', '8', '2', '1', '9', '1', '8', '1', '8', '8', '2', '1', '9', '9', '6', '8', '8', '9', '8', '10', '8', '9', '5', '1', '1', '1', '1', '8', '1', '8', '12', '8', '1', '12', '9', '2', '8', '1', '1', '2', '1', '8', '6', '9', '9', '8', '1', '8', '9', '7', '7', '8', '9', '9', '1', '8', '3', '8', '1', '3', '3', '8', '9', '8', '2', '8', '1', '2', '9', '8', '1', '1', '1', '1', '9', '1', '9', '8', '12', '3', '8', '8', '1', '10', '10', '10', '2', '2', '1', '10', '2', '2', '10', '8', '12', '8', '1', '1', '1', '2', '1', '1', '3', '8', '1', '2', '2', '2', '1', '1', '6', '2', '8', '8', '8', '1', '1', '1', '9', '9', '2', '8', '10', '2', '8', '2', '1', '2', '9', '9', '2', '1', '1', '8', '8', '1', '3', '1', '8', '9', '1', '10', '9', '1', '1', '1', '1', '7', '2', '2', '1', '2', '8', '8', '1', '2', '1', '3', '2', '1', '8', '3', '9', '2', '10', '2', '10', '1', '2', '1', '1', '13', '9', '10', '8', '2', '2', '1', '1', '8', '2', '2', '1', '9', '1', '1', '8', '9', '9', '2', '9', '2', '2', '1', '1', '1', '8', '9', '4', '9', '9', '1', '8', '2', '1', '1', '8', '2', '2', '2', '8', '1', '1', '8', '2', '10', '2', '9', '9', '7', '1', '1', '2', '2', '8', '3', '1', '2', '1', '1', '8', '1', '1', '1', '9', '1', '1', '8', '8', '1', '9', '6', '1', '9', '1', '8', '1', '8', '8', '8', '1', '3', '8', '8', '2', '6', '3', '2', '1', '10', '1', '8', '2', '1', '6', '1', '1', '1', '8', '1', '10', '1', '1', '8', '9', '2', '6', '9', '2', '9', '4', '1', '8', '1', '3', '7', '8', '8', '5', '8', '7', '1', '1', '1', '13', '8', '3', '1', '1', '1', '1', '2', '8', '2', '12', '8', '1', '2', '5', '1', '8', '2', '10', '8', '2', '10', '1', '5', '2', '1', '1', '5', '2', '2', '7', '8', '2', '9', '9', '1', '10', '1', '8', '2', '5', '8', '1', '5', '1', '10', '1', '1', '9', '8', '6', '1', '8', '9', '8', '1', '8', '2', '2', '8', '3', '9', '1', '9', '3', '8', '4', '4', '3', '3', '1', '1', '1', '8', '9', '1', '1', '8', '3', '1', '2', '8', '2', '1', '1', '10', '1', '2', '2', '1', '1', '8', '2', '8', '2', '10', '2', '2', '10', '8', '8', '8', '13', '2', '2', '9', '2', '5', '9', '2', '1', '8', '8', '8', '1', '12', '12', '2', '9', '2', '1', '8', '2', '1', '8', '1', '9', '9', '2', '7', '1', '1', '7', '1', '4', '3', '1', '10', '3', '10', '9', '4', '6', '7', '5', '2', '1', '8', '8', '1', '4', '9', '2', '1', '8', '1', '1', '1', '1', '1', '8', '9', '8', '8', '8', '8', '9', '8', '10', '1', '8', '2', '7', '8', '9', '1', '2', '5', '9', '10', '8', '9', '12', '1', '6', '1', '2', '10', '9', '4', '1', '8', '10', '10', '1', '1', '1', '1', '1', '2', '3', '8', '1', '2', '10', '2', '1', '2', '4', '2', '2', '9', '1', '4', '2', '1', '1', '12', '12', '9', '9', '7', '1', '8', '4', '9', '9', '8', '1', '7', '3', '2', '1', '1', '4', '8', '1', '5', '8', '2', '8', '1', '14', '9', '12', '1', '10', '2', '1', '1', '8', '8', '6', '6', '3', '1', '2', '8', '1', '3', '2', '1', '8', '8', '1', '10', '8', '2', '8', '2', '5', '9', '1', '8', '8', '1', '1', '8', '8', '13', '1', '3', '8', '4', '1', '9', '7', '8', '8', '8', '3', '8', '1', '4', '2', '8', '8', '8', '7', '1', '1', '2', '8', '8', '9', '8', '8', '2', '3', '9', '9', '2', '1', '2', '2', '1', '8', '2', '6', '2', '1', '1', '9', '10', '3', '8', '2', '3', '2', '8', '8', '1', '8', '8', '8', '9', '1', '4', '1', '9', '2', '1', '8', '0', '1', '8', '1', '1', '8', '9', '9', '1', '3', '1', '2', '8', '8', '7', '1', '1', '2', '9', '2', '10', '2', '8', '2', '2', '2', '7', '1', '9', '8', '1', '3', '9', '2', '1', '10', '8', '1', '4', '1', '8', '5', '9', '8', '8', '1', '2', '2', '1', '8', '8', '6', '8', '1', '1', '8', '3', '2', '2', '1', '8', '1', '7', '6', '8', '1', '1', '1', '9', '1', '1', '1', '2', '1', '11', '10', '10', '11', '9']\n",
            "     filenum                                               text  label\n",
            "0          1  novelty.' General Electric Co. v. Wabash Appli...      8\n",
            "1          2  Court properly to shift to Congress the respon...      1\n",
            "2          3  would sell' at the carrier's price. In the Val...      8\n",
            "3          4  not to coerce the surrender of lands without c...      2\n",
            "4          6  for the privilege of doing it, which place an ...      8\n",
            "...      ...                                                ...    ...\n",
            "6000    8411  too here, \"[w]e see no reason to reject Califo...      1\n",
            "6001    8412  the courts of the State.\" U. S. C. If the stat...      1\n",
            "6002    8413  abandoned. A mere recital in briefs of the exi...      2\n",
            "6003    8414  sentence less than death.\" ' \" U. S., at (quot...      1\n",
            "6004    8417  \"As we pointed out in United States v. Califor...     10\n",
            "\n",
            "[6005 rows x 3 columns]\n"
          ]
        }
      ],
      "source": [
        "fh = open('sc_model_3.txt')\n",
        "filenums=[]\n",
        "records=[]\n",
        "for record in fh:\n",
        "    record=record.split(\"---\")\n",
        "    filenum=record[0]\n",
        "    modelnum=record[1]\n",
        "    text=record[2]\n",
        "    filenums.append(filenum)\n",
        "    records.append(text)\n",
        "    \n",
        "print(len(filenums))\n",
        "print(len(records))\n",
        "\n",
        "summarized_data = pd.DataFrame(list(zip(filenums, records)),columns =['filenum','text'])\n",
        "len_list = [len(ele.split()) for ele in records]\n",
        "res = 0 if len(len_list) == 0 else (float(sum(len_list)) / len(len_list))\n",
        "print(\"Average Length %s\" % res)\n",
        "\n",
        "temp_file = open(\"labels_sc.txt\", \"r\")\n",
        "#temp_file = open(\"labels_sc_279.txt\", \"r\")\n",
        "\n",
        "data = temp_file.read()\n",
        "\n",
        "label_list = data.split(\"\\n\")\n",
        "print(label_list)\n",
        "label_list = label_list[0:-1]\n",
        "print(label_list)\n",
        "label_list = [int(i) for i in label_list]\n",
        "temp_file.close()\n",
        "\n",
        "\n",
        "new_label_list=[]\n",
        "for num in filenums:\n",
        "  new_label_list.append(label_list[int(num)-1])\n",
        "summarized_data['label'] = new_label_list\n",
        "\n",
        "print(summarized_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBg7k7sMwIiY",
        "outputId": "3c959170-41dc-45c5-953c-8ba925b9a11a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_1[0][0]',                \n",
            "                                thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_bert_model[0][0]']          \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 15)           7695        ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 8  1  8 ...  2  1 10]\n",
            "[   1    2    3 ... 8413 8414 8417]\n",
            "[6622   35 6713 4989 8233 7444 5061 3942 8344 5464  317 4515 3001 6991\n",
            " 2875 1125 3885 6647 4857 4504  214 8340 2969 3847 4609 1346 5993 7358\n",
            " 2752 7651 8188 7073 4782 6069 2346 6483 1663 4952 5072 1967 4071 7590\n",
            "  860 6418 5030 7297  626 5463 6029 6302 7256 5667 1235 3993 1323 4263\n",
            " 3835 2624 5801 4293 8129 5866 5777 6059 4837 3843 1418 5409 5274 6205\n",
            " 2356 5997 6685 3444 3310 7721 5928 2360 7197 4252 8343  909 4354 4752\n",
            "  649 1916 3488 2496 5765 6612 4004 4052 1403 6697 5268 6293 6380 4602\n",
            " 6057 1851 4577 4236 7978 5083 6552  361 7706 4186 4452 4396 8351 3834\n",
            "  812 5108  463 6600 8264 8064 7121 8279 5556 5119 7947 3350 7152 5199\n",
            " 4282 1092 5025 5051  933 1927 3615 1221 8062 1489 7442 5459 2031 7351\n",
            "  234 8015 7416 7921 6574 6257 8396  941 4520 3300 5640  240 7810 7425\n",
            " 1321 6580  928 1170 5404 8273 6742   94    9 7861 7975 1018 2140 1415\n",
            " 7061 7767 4990 8124 5927 6051 3269 6916 5959 2846 6818 7052 7775 7582\n",
            " 8116 4240 4767 5248 1812 2175 6055 8331  998 1564 7326 1559  292 3065\n",
            " 6764 6854 4915 5552 3583 5902 4229 2144 7565 3712 7224 4502 1406 1651\n",
            " 6256 3827 6525 6318 2686 3616 5369 8235 1704 1011 3351 8089 8021  102\n",
            "  224 5671 8294 7707 2985 6624 5146 7381 3709 6824 6308 2281 6837 5436\n",
            " 1741 3520 2649 4886  966 5476 1199 8387 5449 1028 2226 6166 4387  350\n",
            " 6123 5236 5041  734 3412 6085  587  489 5760 4982 5115 5575 3839 6743\n",
            " 7173 5724 7689 7787 8302 3150 2247 8128 6660 3505 4610 2866 4161 6905\n",
            " 7096  768 1062 4411 6087 2096 4903 7546 6630 2928 4640 7125 1792 2063\n",
            " 1700 7928 8095  737 5849  922 2359 6015 4489 4361 6081 6346 5014 5167\n",
            " 1023 8009 3335  268 4777 5221 4323 6168  711 5702 6129 7181 1860 7192\n",
            " 1347 5467 6579   67 2693 6134 4841 4270 4611 2943 6746 7696 3466 3950\n",
            " 6125 8370 6142 6131  995 5548 1721 5067 4015 7092 3156   25 1749 5554\n",
            " 4878 8248 6836 5310 2698 6960 2339 1650 6227 4104 5130 7089 6993 2400\n",
            " 1289 4305 5585 7933 6284 7057 3807 5790 8347 5280 2841 7220 1384 7693\n",
            " 8090 4171 5260  133 7451 3957 1719 7379 4743 4746 5435 6216 4858 4417\n",
            " 8355 4492 2648 5104 5278  743 7308  158 7543 2168 5913 7111 3414 7592\n",
            "   39 7157 5647 4555 3956 3617 4630 4384 6596   96 4539 6749 7202 2413\n",
            " 4749 5122 5052 5929  252 5565  336 6008 4564 5580 7237 1466 1568 5958\n",
            " 7657 4327 5091 6918 6487 4082 2989 7448 4715 3382  467 3784 4624 4419\n",
            " 1117 7954 7122 6392 3174 1534 4164 3659 7526  498 5944 4572 2307 2929\n",
            " 7480 4412  731 4563 7692 1869 2472 6966 4238 6549 1594 2276 7269 2545\n",
            " 2216  605 6587 6315 5297 5282 1619 5504 6399 3917 6342 5754 5721 7385\n",
            " 7909 6126 7382 8382 4383 5502 7130  485 4936 7674 5372 3893 7109 4605\n",
            " 5211 5462 8065   50 4080 3334 8167 8378 5595 1873 8207 7076 4106 3719\n",
            " 4627  395  231 2405 6973 3480 3246 5414 5931 8208 4825 6203 3019 5822\n",
            "   63 5854 6543 5787 4764 6701 6562 3822 4861 3051 1315 8318 1089 8169\n",
            " 8218 7549  707 1284 1076 2465 3815 4636   80 7025 7127 4026 2777 4961\n",
            " 7531  744 6640 6684 4297 7939 7384  446 3287 6675 7752 5279 8196 3020\n",
            " 1203 4192 4253 2887 2556 5729 1898 3208 5973 7033 1431 5915 8312 5799\n",
            " 7105  511 6404 2852 1561 4150 1178]\n"
          ]
        }
      ],
      "source": [
        "gc.collect()\n",
        "tf.keras.backend.clear_session()\n",
        "dbert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "dbert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
        "max_len=512\n",
        "sentences=summarized_data['text']\n",
        "labels=summarized_data['label']\n",
        "filenumbers=summarized_data['filenum']\n",
        "len(sentences),len(labels),len(filenumbers)\n",
        "\n",
        "model_3=create_model()\n",
        "input_ids=[]\n",
        "attention_masks=[]\n",
        "\n",
        "for sent in sentences:\n",
        "  dbert_inps=dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n",
        "  input_ids.append(dbert_inps['input_ids'])\n",
        "  attention_masks.append(dbert_inps['attention_mask'])\n",
        "input_ids=np.asarray(input_ids)\n",
        "\n",
        "attention_masks=np.array(attention_masks)\n",
        "labels=np.array(labels)\n",
        "filenumbers=[int(i) for i in filenumbers]\n",
        "filenumbers=np.array(filenumbers)\n",
        "print(labels)\n",
        "print(filenumbers)\n",
        "\n",
        "#train_inp,val_inp,train_label,val_label,train_mask,val_mask,train_fnum,val_fnum=train_test_split(input_ids,labels,attention_masks,filenumbers,test_size=0.1,random_state=42)\n",
        "\n",
        "new_train_inp_3=[]\n",
        "new_val_inp_3=[]\n",
        "new_train_label_3=[]\n",
        "new_val_label_3=[]\n",
        "new_train_mask_3=[]\n",
        "new_train_fnum_3=[]\n",
        "new_val_fnum_3=[]\n",
        "new_val_mask_3=[]\n",
        "for i in range(len(train_fnum)):\n",
        "  if train_fnum[i] in filenumbers:\n",
        "    new_train_inp_3.append(train_inp[i])\n",
        "    #new_val_inp.append(val_inp[i])\n",
        "    new_train_label_3.append(train_label[i])\n",
        "    #new_val_label.append(val_label[i])\n",
        "    new_train_mask_3.append(train_mask[i])\n",
        "    #new_val_mask.append(val_mask[i])\n",
        "    new_train_fnum_3.append(train_fnum[i])\n",
        "\n",
        "for i in range(len(val_fnum)):\n",
        "  if val_fnum[i] in filenumbers:\n",
        "    #new_train_inp.append(train_inp[i])\n",
        "    new_val_inp_3.append(val_inp[i])\n",
        "    #new_train_label.append(train_label[i])\n",
        "    new_val_label_3.append(val_label[i])\n",
        "    #new_train_mask.append(train_mask[i])\n",
        "    new_val_mask_3.append(val_mask[i])\n",
        "    new_val_fnum_3.append(val_fnum[i])\n",
        "\n",
        "# print(new_train_label)\n",
        "# print(len(new_train_label))\n",
        "# print(new_val_label)\n",
        "# print(len(new_val_label))\n",
        "new_train_inp_3=np.array(new_train_inp_3)\n",
        "new_val_inp_3=np.array(new_val_inp_3)\n",
        "new_train_label_3=np.array(new_train_label_3)\n",
        "new_val_label_3=np.array(new_val_label_3)\n",
        "new_train_mask_3=np.array(new_train_mask_3)\n",
        "new_train_fnum_3=np.array(new_train_fnum_3)\n",
        "new_val_fnum_3=np.array(new_val_fnum_3)\n",
        "new_val_mask_3=np.array(new_val_mask_3)\n",
        "\n",
        "print(new_val_fnum_3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "EYIS7zLEOS8u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6539e29-acb7-4d47-e65c-f08a2ccf623b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_1[0][0]',                \n",
            "                                thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_bert_model[0][0]']          \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 15)           7695        ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 8  1  8 ...  2  1 10]\n",
            "[   1    2    3 ... 8413 8414 8417]\n",
            "[6622   35 6713 4989 8233 7444 5061 3942 8344 5464  317 4515 3001 6991\n",
            " 2875 1125 3885 6647 4857 4504  214 8340 2969 3847 4609 1346 5993 7358\n",
            " 2752 7651 8188 7073 4782 6069 2346 6483 1663 4952 5072 1967 4071 7590\n",
            "  860 6418 5030 7297  626 5463 6029 6302 7256 5667 1235 3993 1323 4263\n",
            " 3835 2624 5801 4293 8129 5866 5777 6059 4837 3843 1418 5409 5274 6205\n",
            " 2356 5997 6685 3444 3310 7721 5928 2360 7197 4252 8343  909 4354 4752\n",
            "  649 1916 3488 2496 5765 6612 4004 4052 1403 6697 5268 6293 6380 4602\n",
            " 6057 1851 4577 4236 7978 5083 6552  361 7706 4186 4452 4396 8351 3834\n",
            "  812 5108  463 6600 8264 8064 7121 8279 5556 5119 7947 3350 7152 5199\n",
            " 4282 1092 5025 5051  933 1927 3615 1221 8062 1489 7442 5459 2031 7351\n",
            "  234 8015 7416 7921 6574 6257 8396  941 4520 3300 5640  240 7810 7425\n",
            " 1321 6580  928 1170 5404 8273 6742   94    9 7861 7975 1018 2140 1415\n",
            " 7061 7767 4990 8124 5927 6051 3269 6916 5959 2846 6818 7052 7775 7582\n",
            " 8116 4240 4767 5248 1812 2175 6055 8331  998 1564 7326 1559  292 3065\n",
            " 6764 6854 4915 5552 3583 5902 4229 2144 7565 3712 7224 4502 1406 1651\n",
            " 6256 3827 6525 6318 2686 3616 5369 8235 1704 1011 3351 8089 8021  102\n",
            "  224 5671 8294 7707 2985 6624 5146 7381 3709 6824 6308 2281 6837 5436\n",
            " 1741 3520 2649 4886  966 5476 1199 8387 5449 1028 2226 6166 4387  350\n",
            " 6123 5236 5041  734 3412 6085  587  489 5760 4982 5115 5575 3839 6743\n",
            " 7173 5724 7689 7787 8302 3150 2247 8128 6660 3505 4610 2866 4161 6905\n",
            " 7096  768 1062 4411 6087 2096 4903 7546 6630 2928 4640 7125 1792 2063\n",
            " 1700 7928 8095  737 5849  922 2359 6015 4489 4361 6081 6346 5014 5167\n",
            " 1023 8009 3335  268 4777 5221 4323 6168  711 5702 6129 7181 1860 7192\n",
            " 1347 5467 6579   67 2693 6134 4841 4270 4611 2943 6746 7696 3466 3950\n",
            " 6125 8370 6142 6131  995 5548 1721 5067 4015 7092 3156   25 1749 5554\n",
            " 4878 8248 6836 5310 2698 6960 2339 1650 6227 4104 5130 7089 6993 2400\n",
            " 1289 4305 5585 7933 6284 7057 3807 5790 8347 5280 2841 7220 1384 7693\n",
            " 8090 4171 5260  133 7451 3957 1719 7379 4743 4746 5435 6216 4858 4417\n",
            " 8355 4492 2648 5104 5278  743 7308  158 7543 2168 5913 7111 3414 7592\n",
            "   39 7157 5647 4555 3956 3617 4630 4384 6596   96 4539 6749 7202 2413\n",
            " 4749 5122 5052 5929  252 5565  336 6008 4564 5580 7237 1466 1568 5958\n",
            " 7657 4327 5091 6918 6487 4082 2989 7448 4715 3382  467 3784 4624 4419\n",
            " 1117 7954 7122 6392 3174 1534 4164 3659 7526  498 5944 4572 2307 2929\n",
            " 7480 4412  731 4563 7692 1869 2472 6966 4238 6549 1594 2276 7269 2545\n",
            " 2216  605 6587 6315 5297 5282 1619 5504 6399 3917 6342 5754 5721 7385\n",
            " 7909 6126 7382 8382 4383 5502 7130  485 4936 7674 5372 3893 7109 4605\n",
            " 5211 5462 8065   50 4080 3334 8167 8378 5595 1873 8207 7076 4106 3719\n",
            " 4627  395  231 2405 6973 3480 3246 5414 5931 8208 4825 6203 3019 5822\n",
            "   63 5854 6543 5787 4764 6701 6562 3822 4861 3051 1315 8318 1089 8169\n",
            " 8218 7549  707 1284 1076 2465 3815 4636   80 7025 7127 4026 2777 4961\n",
            " 7531  744 6640 6684 4297 7939 7384  446 3287 6675 7752 5279 8196 3020\n",
            " 1203 4192 4253 2887 2556 5729 1898 3208 5973 7033 1431 5915 8312 5799\n",
            " 7105  511 6404 2852 1561 4150 1178]\n",
            "Thu Jun  2 17:53:12 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   59C    P0    39W / 250W |  15855MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "677/677 [==============================] - 425s 601ms/step - loss: 7.3032 - accuracy: 0.5726 - val_loss: 6.4494 - val_accuracy: 0.6857\n",
            "Epoch 2/5\n",
            "677/677 [==============================] - 405s 598ms/step - loss: 5.9081 - accuracy: 0.7457 - val_loss: 5.5196 - val_accuracy: 0.7412\n",
            "Epoch 3/5\n",
            "677/677 [==============================] - 401s 592ms/step - loss: 4.9683 - accuracy: 0.8183 - val_loss: 4.9593 - val_accuracy: 0.7244\n",
            "Epoch 4/5\n",
            "677/677 [==============================] - 401s 592ms/step - loss: 4.1942 - accuracy: 0.8723 - val_loss: 4.3830 - val_accuracy: 0.7261\n",
            "Epoch 5/5\n",
            "677/677 [==============================] - 405s 598ms/step - loss: 3.5036 - accuracy: 0.9140 - val_loss: 3.9214 - val_accuracy: 0.7513\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_3[0][0]',                \n",
            "                                thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_bert_model[1][0]']          \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 15)           7695        ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.7315914489311164\n",
            "Weighted F1: 0.7270421779420299\n",
            "Micro F1: 0.7315914489311163\n",
            "Weighted Precision: 0.7274394705405105\n",
            "Micro Precision: 0.7315914489311164\n",
            "Weighted Recall: 0.7315914489311164\n",
            "Micro Recall: 0.7315914489311164\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_1[0][0]',                \n",
            "                                thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_bert_model[0][0]']          \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 15)           7695        ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 8  1  8 ...  2  1 10]\n",
            "[   1    2    3 ... 8413 8414 8417]\n",
            "[6622   35 6713 4989 8233 7444 5061 3942 8344 5464  317 4515 3001 6991\n",
            " 2875 1125 3885 6647 4857 4504  214 8340 2969 3847 4609 1346 5993 7358\n",
            " 2752 7651 8188 7073 4782 6069 2346 6483 1663 4952 5072 1967 4071 7590\n",
            "  860 6418 5030 7297  626 5463 6029 6302 7256 5667 1235 3993 1323 4263\n",
            " 3835 2624 5801 4293 8129 5866 5777 6059 4837 3843 1418 5409 5274 6205\n",
            " 2356 5997 6685 3444 3310 7721 5928 2360 7197 4252 8343  909 4354 4752\n",
            "  649 1916 3488 2496 5765 6612 4004 4052 1403 6697 5268 6293 6380 4602\n",
            " 6057 1851 4577 4236 7978 5083 6552  361 7706 4186 4452 4396 8351 3834\n",
            "  812 5108  463 6600 8264 8064 7121 8279 5556 5119 7947 3350 7152 5199\n",
            " 4282 1092 5025 5051  933 1927 3615 1221 8062 1489 7442 5459 2031 7351\n",
            "  234 8015 7416 7921 6574 6257 8396  941 4520 3300 5640  240 7810 7425\n",
            " 1321 6580  928 1170 5404 8273 6742   94    9 7861 7975 1018 2140 1415\n",
            " 7061 7767 4990 8124 5927 6051 3269 6916 5959 2846 6818 7052 7775 7582\n",
            " 8116 4240 4767 5248 1812 2175 6055 8331  998 1564 7326 1559  292 3065\n",
            " 6764 6854 4915 5552 3583 5902 4229 2144 7565 3712 7224 4502 1406 1651\n",
            " 6256 3827 6525 6318 2686 3616 5369 8235 1704 1011 3351 8089 8021  102\n",
            "  224 5671 8294 7707 2985 6624 5146 7381 3709 6824 6308 2281 6837 5436\n",
            " 1741 3520 2649 4886  966 5476 1199 8387 5449 1028 2226 6166 4387  350\n",
            " 6123 5236 5041  734 3412 6085  587  489 5760 4982 5115 5575 3839 6743\n",
            " 7173 5724 7689 7787 8302 3150 2247 8128 6660 3505 4610 2866 4161 6905\n",
            " 7096  768 1062 4411 6087 2096 4903 7546 6630 2928 4640 7125 1792 2063\n",
            " 1700 7928 8095  737 5849  922 2359 6015 4489 4361 6081 6346 5014 5167\n",
            " 1023 8009 3335  268 4777 5221 4323 6168  711 5702 6129 7181 1860 7192\n",
            " 1347 5467 6579   67 2693 6134 4841 4270 4611 2943 6746 7696 3466 3950\n",
            " 6125 8370 6142 6131  995 5548 1721 5067 4015 7092 3156   25 1749 5554\n",
            " 4878 8248 6836 5310 2698 6960 2339 1650 6227 4104 5130 7089 6993 2400\n",
            " 1289 4305 5585 7933 6284 7057 3807 5790 8347 5280 2841 7220 1384 7693\n",
            " 8090 4171 5260  133 7451 3957 1719 7379 4743 4746 5435 6216 4858 4417\n",
            " 8355 4492 2648 5104 5278  743 7308  158 7543 2168 5913 7111 3414 7592\n",
            "   39 7157 5647 4555 3956 3617 4630 4384 6596   96 4539 6749 7202 2413\n",
            " 4749 5122 5052 5929  252 5565  336 6008 4564 5580 7237 1466 1568 5958\n",
            " 7657 4327 5091 6918 6487 4082 2989 7448 4715 3382  467 3784 4624 4419\n",
            " 1117 7954 7122 6392 3174 1534 4164 3659 7526  498 5944 4572 2307 2929\n",
            " 7480 4412  731 4563 7692 1869 2472 6966 4238 6549 1594 2276 7269 2545\n",
            " 2216  605 6587 6315 5297 5282 1619 5504 6399 3917 6342 5754 5721 7385\n",
            " 7909 6126 7382 8382 4383 5502 7130  485 4936 7674 5372 3893 7109 4605\n",
            " 5211 5462 8065   50 4080 3334 8167 8378 5595 1873 8207 7076 4106 3719\n",
            " 4627  395  231 2405 6973 3480 3246 5414 5931 8208 4825 6203 3019 5822\n",
            "   63 5854 6543 5787 4764 6701 6562 3822 4861 3051 1315 8318 1089 8169\n",
            " 8218 7549  707 1284 1076 2465 3815 4636   80 7025 7127 4026 2777 4961\n",
            " 7531  744 6640 6684 4297 7939 7384  446 3287 6675 7752 5279 8196 3020\n",
            " 1203 4192 4253 2887 2556 5729 1898 3208 5973 7033 1431 5915 8312 5799\n",
            " 7105  511 6404 2852 1561 4150 1178]\n",
            "Thu Jun  2 18:30:18 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   59C    P0    39W / 250W |  15855MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "677/677 [==============================] - 426s 600ms/step - loss: 7.1617 - accuracy: 0.6024 - val_loss: 6.2869 - val_accuracy: 0.7176\n",
            "Epoch 2/5\n",
            "677/677 [==============================] - 405s 599ms/step - loss: 5.7441 - accuracy: 0.7732 - val_loss: 5.4925 - val_accuracy: 0.7345\n",
            "Epoch 3/5\n",
            "677/677 [==============================] - 401s 593ms/step - loss: 4.8398 - accuracy: 0.8370 - val_loss: 4.8604 - val_accuracy: 0.7328\n",
            "Epoch 4/5\n",
            "677/677 [==============================] - 405s 598ms/step - loss: 4.0548 - accuracy: 0.8913 - val_loss: 4.3571 - val_accuracy: 0.7361\n",
            "Epoch 5/5\n",
            "677/677 [==============================] - 401s 593ms/step - loss: 3.3738 - accuracy: 0.9340 - val_loss: 3.9240 - val_accuracy: 0.7277\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_3[0][0]',                \n",
            "                                thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_bert_model[1][0]']          \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 15)           7695        ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.7161520190023754\n",
            "Weighted F1: 0.718636617967178\n",
            "Micro F1: 0.7161520190023754\n",
            "Weighted Precision: 0.7267010531174871\n",
            "Micro Precision: 0.7161520190023754\n",
            "Weighted Recall: 0.7161520190023754\n",
            "Micro Recall: 0.7161520190023754\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_1[0][0]',                \n",
            "                                thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_bert_model[0][0]']          \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 15)           7695        ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 8  1  8 ...  2  1 10]\n",
            "[   1    2    3 ... 8413 8414 8417]\n",
            "[6622   35 6713 4989 8233 7444 5061 3942 8344 5464  317 4515 3001 6991\n",
            " 2875 1125 3885 6647 4857 4504  214 8340 2969 3847 4609 1346 5993 7358\n",
            " 2752 7651 8188 7073 4782 6069 2346 6483 1663 4952 5072 1967 4071 7590\n",
            "  860 6418 5030 7297  626 5463 6029 6302 7256 5667 1235 3993 1323 4263\n",
            " 3835 2624 5801 4293 8129 5866 5777 6059 4837 3843 1418 5409 5274 6205\n",
            " 2356 5997 6685 3444 3310 7721 5928 2360 7197 4252 8343  909 4354 4752\n",
            "  649 1916 3488 2496 5765 6612 4004 4052 1403 6697 5268 6293 6380 4602\n",
            " 6057 1851 4577 4236 7978 5083 6552  361 7706 4186 4452 4396 8351 3834\n",
            "  812 5108  463 6600 8264 8064 7121 8279 5556 5119 7947 3350 7152 5199\n",
            " 4282 1092 5025 5051  933 1927 3615 1221 8062 1489 7442 5459 2031 7351\n",
            "  234 8015 7416 7921 6574 6257 8396  941 4520 3300 5640  240 7810 7425\n",
            " 1321 6580  928 1170 5404 8273 6742   94    9 7861 7975 1018 2140 1415\n",
            " 7061 7767 4990 8124 5927 6051 3269 6916 5959 2846 6818 7052 7775 7582\n",
            " 8116 4240 4767 5248 1812 2175 6055 8331  998 1564 7326 1559  292 3065\n",
            " 6764 6854 4915 5552 3583 5902 4229 2144 7565 3712 7224 4502 1406 1651\n",
            " 6256 3827 6525 6318 2686 3616 5369 8235 1704 1011 3351 8089 8021  102\n",
            "  224 5671 8294 7707 2985 6624 5146 7381 3709 6824 6308 2281 6837 5436\n",
            " 1741 3520 2649 4886  966 5476 1199 8387 5449 1028 2226 6166 4387  350\n",
            " 6123 5236 5041  734 3412 6085  587  489 5760 4982 5115 5575 3839 6743\n",
            " 7173 5724 7689 7787 8302 3150 2247 8128 6660 3505 4610 2866 4161 6905\n",
            " 7096  768 1062 4411 6087 2096 4903 7546 6630 2928 4640 7125 1792 2063\n",
            " 1700 7928 8095  737 5849  922 2359 6015 4489 4361 6081 6346 5014 5167\n",
            " 1023 8009 3335  268 4777 5221 4323 6168  711 5702 6129 7181 1860 7192\n",
            " 1347 5467 6579   67 2693 6134 4841 4270 4611 2943 6746 7696 3466 3950\n",
            " 6125 8370 6142 6131  995 5548 1721 5067 4015 7092 3156   25 1749 5554\n",
            " 4878 8248 6836 5310 2698 6960 2339 1650 6227 4104 5130 7089 6993 2400\n",
            " 1289 4305 5585 7933 6284 7057 3807 5790 8347 5280 2841 7220 1384 7693\n",
            " 8090 4171 5260  133 7451 3957 1719 7379 4743 4746 5435 6216 4858 4417\n",
            " 8355 4492 2648 5104 5278  743 7308  158 7543 2168 5913 7111 3414 7592\n",
            "   39 7157 5647 4555 3956 3617 4630 4384 6596   96 4539 6749 7202 2413\n",
            " 4749 5122 5052 5929  252 5565  336 6008 4564 5580 7237 1466 1568 5958\n",
            " 7657 4327 5091 6918 6487 4082 2989 7448 4715 3382  467 3784 4624 4419\n",
            " 1117 7954 7122 6392 3174 1534 4164 3659 7526  498 5944 4572 2307 2929\n",
            " 7480 4412  731 4563 7692 1869 2472 6966 4238 6549 1594 2276 7269 2545\n",
            " 2216  605 6587 6315 5297 5282 1619 5504 6399 3917 6342 5754 5721 7385\n",
            " 7909 6126 7382 8382 4383 5502 7130  485 4936 7674 5372 3893 7109 4605\n",
            " 5211 5462 8065   50 4080 3334 8167 8378 5595 1873 8207 7076 4106 3719\n",
            " 4627  395  231 2405 6973 3480 3246 5414 5931 8208 4825 6203 3019 5822\n",
            "   63 5854 6543 5787 4764 6701 6562 3822 4861 3051 1315 8318 1089 8169\n",
            " 8218 7549  707 1284 1076 2465 3815 4636   80 7025 7127 4026 2777 4961\n",
            " 7531  744 6640 6684 4297 7939 7384  446 3287 6675 7752 5279 8196 3020\n",
            " 1203 4192 4253 2887 2556 5729 1898 3208 5973 7033 1431 5915 8312 5799\n",
            " 7105  511 6404 2852 1561 4150 1178]\n",
            "Thu Jun  2 19:07:23 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   60C    P0    39W / 250W |  15855MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "677/677 [==============================] - 423s 599ms/step - loss: 7.2254 - accuracy: 0.6002 - val_loss: 6.4075 - val_accuracy: 0.6924\n",
            "Epoch 2/5\n",
            "677/677 [==============================] - 405s 598ms/step - loss: 5.9186 - accuracy: 0.7588 - val_loss: 5.5894 - val_accuracy: 0.7176\n",
            "Epoch 3/5\n",
            "677/677 [==============================] - 405s 598ms/step - loss: 5.0132 - accuracy: 0.8283 - val_loss: 5.0572 - val_accuracy: 0.7261\n",
            "Epoch 4/5\n",
            "677/677 [==============================] - 405s 598ms/step - loss: 4.2244 - accuracy: 0.8860 - val_loss: 4.3987 - val_accuracy: 0.7513\n",
            "Epoch 5/5\n",
            "677/677 [==============================] - 401s 592ms/step - loss: 3.5209 - accuracy: 0.9287 - val_loss: 4.1191 - val_accuracy: 0.7227\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_3[0][0]',                \n",
            "                                thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_bert_model[1][0]']          \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 15)           7695        ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.7232779097387173\n",
            "Weighted F1: 0.7278638144329493\n",
            "Micro F1: 0.7232779097387173\n",
            "Weighted Precision: 0.7420162622785776\n",
            "Micro Precision: 0.7232779097387173\n",
            "Weighted Recall: 0.7232779097387173\n",
            "Micro Recall: 0.7232779097387173\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_1[0][0]',                \n",
            "                                thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_bert_model[0][0]']          \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 15)           7695        ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 8  1  8 ...  2  1 10]\n",
            "[   1    2    3 ... 8413 8414 8417]\n",
            "[6622   35 6713 4989 8233 7444 5061 3942 8344 5464  317 4515 3001 6991\n",
            " 2875 1125 3885 6647 4857 4504  214 8340 2969 3847 4609 1346 5993 7358\n",
            " 2752 7651 8188 7073 4782 6069 2346 6483 1663 4952 5072 1967 4071 7590\n",
            "  860 6418 5030 7297  626 5463 6029 6302 7256 5667 1235 3993 1323 4263\n",
            " 3835 2624 5801 4293 8129 5866 5777 6059 4837 3843 1418 5409 5274 6205\n",
            " 2356 5997 6685 3444 3310 7721 5928 2360 7197 4252 8343  909 4354 4752\n",
            "  649 1916 3488 2496 5765 6612 4004 4052 1403 6697 5268 6293 6380 4602\n",
            " 6057 1851 4577 4236 7978 5083 6552  361 7706 4186 4452 4396 8351 3834\n",
            "  812 5108  463 6600 8264 8064 7121 8279 5556 5119 7947 3350 7152 5199\n",
            " 4282 1092 5025 5051  933 1927 3615 1221 8062 1489 7442 5459 2031 7351\n",
            "  234 8015 7416 7921 6574 6257 8396  941 4520 3300 5640  240 7810 7425\n",
            " 1321 6580  928 1170 5404 8273 6742   94    9 7861 7975 1018 2140 1415\n",
            " 7061 7767 4990 8124 5927 6051 3269 6916 5959 2846 6818 7052 7775 7582\n",
            " 8116 4240 4767 5248 1812 2175 6055 8331  998 1564 7326 1559  292 3065\n",
            " 6764 6854 4915 5552 3583 5902 4229 2144 7565 3712 7224 4502 1406 1651\n",
            " 6256 3827 6525 6318 2686 3616 5369 8235 1704 1011 3351 8089 8021  102\n",
            "  224 5671 8294 7707 2985 6624 5146 7381 3709 6824 6308 2281 6837 5436\n",
            " 1741 3520 2649 4886  966 5476 1199 8387 5449 1028 2226 6166 4387  350\n",
            " 6123 5236 5041  734 3412 6085  587  489 5760 4982 5115 5575 3839 6743\n",
            " 7173 5724 7689 7787 8302 3150 2247 8128 6660 3505 4610 2866 4161 6905\n",
            " 7096  768 1062 4411 6087 2096 4903 7546 6630 2928 4640 7125 1792 2063\n",
            " 1700 7928 8095  737 5849  922 2359 6015 4489 4361 6081 6346 5014 5167\n",
            " 1023 8009 3335  268 4777 5221 4323 6168  711 5702 6129 7181 1860 7192\n",
            " 1347 5467 6579   67 2693 6134 4841 4270 4611 2943 6746 7696 3466 3950\n",
            " 6125 8370 6142 6131  995 5548 1721 5067 4015 7092 3156   25 1749 5554\n",
            " 4878 8248 6836 5310 2698 6960 2339 1650 6227 4104 5130 7089 6993 2400\n",
            " 1289 4305 5585 7933 6284 7057 3807 5790 8347 5280 2841 7220 1384 7693\n",
            " 8090 4171 5260  133 7451 3957 1719 7379 4743 4746 5435 6216 4858 4417\n",
            " 8355 4492 2648 5104 5278  743 7308  158 7543 2168 5913 7111 3414 7592\n",
            "   39 7157 5647 4555 3956 3617 4630 4384 6596   96 4539 6749 7202 2413\n",
            " 4749 5122 5052 5929  252 5565  336 6008 4564 5580 7237 1466 1568 5958\n",
            " 7657 4327 5091 6918 6487 4082 2989 7448 4715 3382  467 3784 4624 4419\n",
            " 1117 7954 7122 6392 3174 1534 4164 3659 7526  498 5944 4572 2307 2929\n",
            " 7480 4412  731 4563 7692 1869 2472 6966 4238 6549 1594 2276 7269 2545\n",
            " 2216  605 6587 6315 5297 5282 1619 5504 6399 3917 6342 5754 5721 7385\n",
            " 7909 6126 7382 8382 4383 5502 7130  485 4936 7674 5372 3893 7109 4605\n",
            " 5211 5462 8065   50 4080 3334 8167 8378 5595 1873 8207 7076 4106 3719\n",
            " 4627  395  231 2405 6973 3480 3246 5414 5931 8208 4825 6203 3019 5822\n",
            "   63 5854 6543 5787 4764 6701 6562 3822 4861 3051 1315 8318 1089 8169\n",
            " 8218 7549  707 1284 1076 2465 3815 4636   80 7025 7127 4026 2777 4961\n",
            " 7531  744 6640 6684 4297 7939 7384  446 3287 6675 7752 5279 8196 3020\n",
            " 1203 4192 4253 2887 2556 5729 1898 3208 5973 7033 1431 5915 8312 5799\n",
            " 7105  511 6404 2852 1561 4150 1178]\n",
            "Thu Jun  2 19:44:26 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   59C    P0    39W / 250W |  15855MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "677/677 [==============================] - 423s 599ms/step - loss: 7.2575 - accuracy: 0.6015 - val_loss: 6.4293 - val_accuracy: 0.7025\n",
            "Epoch 2/5\n",
            "677/677 [==============================] - 404s 597ms/step - loss: 5.9002 - accuracy: 0.7603 - val_loss: 5.5801 - val_accuracy: 0.7210\n",
            "Epoch 3/5\n",
            "677/677 [==============================] - 405s 598ms/step - loss: 4.9600 - accuracy: 0.8255 - val_loss: 4.9927 - val_accuracy: 0.7429\n",
            "Epoch 4/5\n",
            "677/677 [==============================] - 404s 597ms/step - loss: 4.1584 - accuracy: 0.8834 - val_loss: 4.3226 - val_accuracy: 0.7479\n",
            "Epoch 5/5\n",
            "677/677 [==============================] - 401s 592ms/step - loss: 3.4613 - accuracy: 0.9227 - val_loss: 3.9220 - val_accuracy: 0.7294\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_3[0][0]',                \n",
            "                                thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_bert_model[1][0]']          \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 15)           7695        ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.7280285035629454\n",
            "Weighted F1: 0.7291045055690852\n",
            "Micro F1: 0.7280285035629454\n",
            "Weighted Precision: 0.7373937006856056\n",
            "Micro Precision: 0.7280285035629454\n",
            "Weighted Recall: 0.7280285035629454\n",
            "Micro Recall: 0.7280285035629454\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_1[0][0]',                \n",
            "                                thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_bert_model[0][0]']          \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 15)           7695        ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 8  1  8 ...  2  1 10]\n",
            "[   1    2    3 ... 8413 8414 8417]\n",
            "[6622   35 6713 4989 8233 7444 5061 3942 8344 5464  317 4515 3001 6991\n",
            " 2875 1125 3885 6647 4857 4504  214 8340 2969 3847 4609 1346 5993 7358\n",
            " 2752 7651 8188 7073 4782 6069 2346 6483 1663 4952 5072 1967 4071 7590\n",
            "  860 6418 5030 7297  626 5463 6029 6302 7256 5667 1235 3993 1323 4263\n",
            " 3835 2624 5801 4293 8129 5866 5777 6059 4837 3843 1418 5409 5274 6205\n",
            " 2356 5997 6685 3444 3310 7721 5928 2360 7197 4252 8343  909 4354 4752\n",
            "  649 1916 3488 2496 5765 6612 4004 4052 1403 6697 5268 6293 6380 4602\n",
            " 6057 1851 4577 4236 7978 5083 6552  361 7706 4186 4452 4396 8351 3834\n",
            "  812 5108  463 6600 8264 8064 7121 8279 5556 5119 7947 3350 7152 5199\n",
            " 4282 1092 5025 5051  933 1927 3615 1221 8062 1489 7442 5459 2031 7351\n",
            "  234 8015 7416 7921 6574 6257 8396  941 4520 3300 5640  240 7810 7425\n",
            " 1321 6580  928 1170 5404 8273 6742   94    9 7861 7975 1018 2140 1415\n",
            " 7061 7767 4990 8124 5927 6051 3269 6916 5959 2846 6818 7052 7775 7582\n",
            " 8116 4240 4767 5248 1812 2175 6055 8331  998 1564 7326 1559  292 3065\n",
            " 6764 6854 4915 5552 3583 5902 4229 2144 7565 3712 7224 4502 1406 1651\n",
            " 6256 3827 6525 6318 2686 3616 5369 8235 1704 1011 3351 8089 8021  102\n",
            "  224 5671 8294 7707 2985 6624 5146 7381 3709 6824 6308 2281 6837 5436\n",
            " 1741 3520 2649 4886  966 5476 1199 8387 5449 1028 2226 6166 4387  350\n",
            " 6123 5236 5041  734 3412 6085  587  489 5760 4982 5115 5575 3839 6743\n",
            " 7173 5724 7689 7787 8302 3150 2247 8128 6660 3505 4610 2866 4161 6905\n",
            " 7096  768 1062 4411 6087 2096 4903 7546 6630 2928 4640 7125 1792 2063\n",
            " 1700 7928 8095  737 5849  922 2359 6015 4489 4361 6081 6346 5014 5167\n",
            " 1023 8009 3335  268 4777 5221 4323 6168  711 5702 6129 7181 1860 7192\n",
            " 1347 5467 6579   67 2693 6134 4841 4270 4611 2943 6746 7696 3466 3950\n",
            " 6125 8370 6142 6131  995 5548 1721 5067 4015 7092 3156   25 1749 5554\n",
            " 4878 8248 6836 5310 2698 6960 2339 1650 6227 4104 5130 7089 6993 2400\n",
            " 1289 4305 5585 7933 6284 7057 3807 5790 8347 5280 2841 7220 1384 7693\n",
            " 8090 4171 5260  133 7451 3957 1719 7379 4743 4746 5435 6216 4858 4417\n",
            " 8355 4492 2648 5104 5278  743 7308  158 7543 2168 5913 7111 3414 7592\n",
            "   39 7157 5647 4555 3956 3617 4630 4384 6596   96 4539 6749 7202 2413\n",
            " 4749 5122 5052 5929  252 5565  336 6008 4564 5580 7237 1466 1568 5958\n",
            " 7657 4327 5091 6918 6487 4082 2989 7448 4715 3382  467 3784 4624 4419\n",
            " 1117 7954 7122 6392 3174 1534 4164 3659 7526  498 5944 4572 2307 2929\n",
            " 7480 4412  731 4563 7692 1869 2472 6966 4238 6549 1594 2276 7269 2545\n",
            " 2216  605 6587 6315 5297 5282 1619 5504 6399 3917 6342 5754 5721 7385\n",
            " 7909 6126 7382 8382 4383 5502 7130  485 4936 7674 5372 3893 7109 4605\n",
            " 5211 5462 8065   50 4080 3334 8167 8378 5595 1873 8207 7076 4106 3719\n",
            " 4627  395  231 2405 6973 3480 3246 5414 5931 8208 4825 6203 3019 5822\n",
            "   63 5854 6543 5787 4764 6701 6562 3822 4861 3051 1315 8318 1089 8169\n",
            " 8218 7549  707 1284 1076 2465 3815 4636   80 7025 7127 4026 2777 4961\n",
            " 7531  744 6640 6684 4297 7939 7384  446 3287 6675 7752 5279 8196 3020\n",
            " 1203 4192 4253 2887 2556 5729 1898 3208 5973 7033 1431 5915 8312 5799\n",
            " 7105  511 6404 2852 1561 4150 1178]\n",
            "Thu Jun  2 20:21:28 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   59C    P0    39W / 250W |  15855MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "677/677 [==============================] - 424s 600ms/step - loss: 7.3041 - accuracy: 0.5983 - val_loss: 6.4951 - val_accuracy: 0.7042\n",
            "Epoch 2/5\n",
            "677/677 [==============================] - 405s 598ms/step - loss: 5.9637 - accuracy: 0.7608 - val_loss: 5.6651 - val_accuracy: 0.7176\n",
            "Epoch 3/5\n",
            "677/677 [==============================] - 404s 597ms/step - loss: 5.0435 - accuracy: 0.8305 - val_loss: 5.0910 - val_accuracy: 0.7277\n",
            "Epoch 4/5\n",
            "677/677 [==============================] - 405s 598ms/step - loss: 4.2568 - accuracy: 0.8847 - val_loss: 4.3963 - val_accuracy: 0.7445\n",
            "Epoch 5/5\n",
            "677/677 [==============================] - 405s 598ms/step - loss: 3.5544 - accuracy: 0.9255 - val_loss: 4.0254 - val_accuracy: 0.7479\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_3[0][0]',                \n",
            "                                thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_bert_model[1][0]']          \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 15)           7695        ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.7244655581947743\n",
            "Weighted F1: 0.726004007214897\n",
            "Micro F1: 0.7244655581947743\n",
            "Weighted Precision: 0.7368297476026336\n",
            "Micro Precision: 0.7244655581947743\n",
            "Weighted Recall: 0.7244655581947743\n",
            "Micro Recall: 0.7244655581947743\n",
            "Average Accuracy: 0.7247030878859857\n",
            "Average Weighted F1: 0.7257302246252278\n",
            "Average Micro F1: 0.7247030878859857\n",
            "Average Weighted Precision: 0.7340760468449629\n",
            "Average Micro Precision: 0.7247030878859857\n",
            "Average Weighted Recall: 0.7247030878859857\n",
            "Average Micro Recall: 0.7247030878859857\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "total_accuracy=0\n",
        "total_weighted_f1=0\n",
        "total_micro_f1=0\n",
        "total_weighted_precision=0\n",
        "total_micro_precision=0\n",
        "total_weighted_recall=0\n",
        "total_micro_recall=0\n",
        "\n",
        "for f in range(5):\n",
        "  gc.collect()\n",
        "  tf.keras.backend.clear_session()\n",
        "  dbert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "  dbert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
        "  max_len=512\n",
        "  sentences=summarized_data['text']\n",
        "  labels=summarized_data['label']\n",
        "  filenumbers=summarized_data['filenum']\n",
        "  len(sentences),len(labels),len(filenumbers)\n",
        "  \n",
        "  model_3=create_model()\n",
        "  input_ids=[]\n",
        "  attention_masks=[]\n",
        "\n",
        "  for sent in sentences:\n",
        "    dbert_inps=dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n",
        "    input_ids.append(dbert_inps['input_ids'])\n",
        "    attention_masks.append(dbert_inps['attention_mask'])\n",
        "  input_ids=np.asarray(input_ids)\n",
        "\n",
        "  attention_masks=np.array(attention_masks)\n",
        "  labels=np.array(labels)\n",
        "  filenumbers=[int(i) for i in filenumbers]\n",
        "  filenumbers=np.array(filenumbers)\n",
        "  print(labels)\n",
        "  print(filenumbers)\n",
        "\n",
        "  #train_inp,val_inp,train_label,val_label,train_mask,val_mask,train_fnum,val_fnum=train_test_split(input_ids,labels,attention_masks,filenumbers,test_size=0.1,random_state=42)\n",
        "\n",
        "  new_train_inp_3=[]\n",
        "  new_val_inp_3=[]\n",
        "  new_train_label_3=[]\n",
        "  new_val_label_3=[]\n",
        "  new_train_mask_3=[]\n",
        "  new_train_fnum_3=[]\n",
        "  new_val_fnum_3=[]\n",
        "  new_val_mask_3=[]\n",
        "  for i in range(len(train_fnum)):\n",
        "    if train_fnum[i] in filenumbers:\n",
        "      new_train_inp_3.append(train_inp[i])\n",
        "      #new_val_inp.append(val_inp[i])\n",
        "      new_train_label_3.append(train_label[i])\n",
        "      #new_val_label.append(val_label[i])\n",
        "      new_train_mask_3.append(train_mask[i])\n",
        "      #new_val_mask.append(val_mask[i])\n",
        "      new_train_fnum_3.append(train_fnum[i])\n",
        "\n",
        "  for i in range(len(val_fnum)):\n",
        "    if val_fnum[i] in filenumbers:\n",
        "      #new_train_inp.append(train_inp[i])\n",
        "      new_val_inp_3.append(val_inp[i])\n",
        "      #new_train_label.append(train_label[i])\n",
        "      new_val_label_3.append(val_label[i])\n",
        "      #new_train_mask.append(train_mask[i])\n",
        "      new_val_mask_3.append(val_mask[i])\n",
        "      new_val_fnum_3.append(val_fnum[i])\n",
        "\n",
        "  # print(new_train_label)\n",
        "  # print(len(new_train_label))\n",
        "  # print(new_val_label)\n",
        "  # print(len(new_val_label))\n",
        "  new_train_inp_3=np.array(new_train_inp_3)\n",
        "  new_val_inp_3=np.array(new_val_inp_3)\n",
        "  new_train_label_3=np.array(new_train_label_3)\n",
        "  new_val_label_3=np.array(new_val_label_3)\n",
        "  new_train_mask_3=np.array(new_train_mask_3)\n",
        "  new_train_fnum_3=np.array(new_train_fnum_3)\n",
        "  new_val_fnum_3=np.array(new_val_fnum_3)\n",
        "  new_val_mask_3=np.array(new_val_mask_3)\n",
        "  \n",
        "  print(new_val_fnum_3)\n",
        "  \n",
        "  log_dir='dbert_model'\n",
        "\n",
        "  model_save_path='./drive/MyDrive/Ensemble/bert-ensemble-512-model_3-'+str(f)+'-15labels.h5'\n",
        "\n",
        "  loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "  accuracy = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
        "  callbacks= [tf.keras.callbacks.ModelCheckpoint(filepath=model_save_path,monitor='val_accuracy',mode='max',save_best_only=True,save_weights_only=True),keras.callbacks.TensorBoard(log_dir=log_dir)]\n",
        "  model_3.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])\n",
        "  gpu_info = !nvidia-smi\n",
        "  gpu_info = '\\n'.join(gpu_info)\n",
        "  if gpu_info.find('failed') >= 0:\n",
        "    print('Not connected to a GPU')\n",
        "  else:\n",
        "    print(gpu_info)\n",
        "  \n",
        "  history=model_3.fit([new_train_inp_3,new_train_mask_3],new_train_label_3,batch_size=8,epochs=5,validation_data=([new_val_inp_3,new_val_mask_3],new_val_label_3),callbacks=callbacks)\n",
        "\n",
        "  pred_labels=[]\n",
        "\n",
        "  model_saved_3= create_model()\n",
        "  model_saved_3.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])\n",
        "  model_saved_3.load_weights('./drive/MyDrive/Ensemble/bert-ensemble-512-model_3-'+str(f)+'-15labels.h5')\n",
        "\n",
        "  for i in range(0,len(val_inp)):\n",
        "    pred=model_saved_3.predict([val_inp[i].reshape(1,512),val_mask[i].reshape(1,512)])\n",
        "    pred_label = pred.argmax(axis=1)\n",
        "    pred_labels.append(pred_label)\n",
        "  accuracy=accuracy_score(val_label, pred_labels)\n",
        "  print(\"Accuracy: \"+str(accuracy))\n",
        "  total_accuracy=total_accuracy+accuracy\n",
        "  \n",
        "  weighted_f1=f1_score(val_label,pred_labels, average='weighted')\n",
        "  print(\"Weighted F1: \"+ str(weighted_f1))\n",
        "  total_weighted_f1=total_weighted_f1+weighted_f1\n",
        "  micro_f1=f1_score(val_label,pred_labels, average='micro')\n",
        "  print(\"Micro F1: \"+ str(micro_f1))\n",
        "  total_micro_f1=total_micro_f1+micro_f1\n",
        "\n",
        "  weighted_precision=precision_score(val_label, pred_labels, average='weighted')\n",
        "  print(\"Weighted Precision: \" + str(weighted_precision))\n",
        "  total_weighted_precision=total_weighted_precision+weighted_precision\n",
        "  micro_precision=precision_score(val_label, pred_labels, average='micro')\n",
        "  print(\"Micro Precision: \" + str(micro_precision))\n",
        "  total_micro_precision=total_micro_precision+micro_precision\n",
        "\n",
        "  weighted_recall=recall_score(val_label, pred_labels, average='weighted')\n",
        "  print(\"Weighted Recall: \" + str(weighted_recall))\n",
        "  total_weighted_recall=total_weighted_recall+weighted_recall\n",
        "  micro_recall=recall_score(val_label, pred_labels, average='micro')\n",
        "  print(\"Micro Recall: \" + str(micro_recall))\n",
        "  total_micro_recall=total_micro_recall+micro_recall\n",
        "\n",
        "\n",
        "print(\"Average Accuracy: \"+str(total_accuracy/5))\n",
        "print(\"Average Weighted F1: \"+str(total_weighted_f1/5))\n",
        "print(\"Average Micro F1: \"+str(total_micro_f1/5))\n",
        "print(\"Average Weighted Precision: \"+str(total_weighted_precision/5))\n",
        "print(\"Average Micro Precision: \"+str(total_micro_precision/5))\n",
        "print(\"Average Weighted Recall: \"+str(total_weighted_recall/5))\n",
        "print(\"Average Micro Recall: \"+str(total_micro_recall/5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "T2gkm41KOvo3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aef514bc-f9c5-4c2b-ba3d-cb595b2bcad7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5275\n",
            "5275\n",
            "Average Length 511.7759241706161\n",
            "['8', '1', '8', '2', '8', '8', '8', '9', '7', '8', '1', '2', '1', '1', '8', '4', '8', '8', '12', '3', '3', '7', '3', '12', '1', '8', '8', '8', '8', '8', '8', '8', '1', '9', '5', '9', '9', '9', '11', '8', '8', '8', '4', '8', '8', '8', '8', '1', '3', '9', '3', '7', '1', '2', '9', '9', '7', '8', '8', '1', '10', '7', '8', '8', '9', '8', '7', '9', '9', '12', '7', '2', '8', '1', '11', '11', '1', '7', '7', '12', '1', '9', '8', '10', '12', '7', '8', '2', '8', '9', '9', '1', '8', '9', '1', '7', '12', '10', '10', '10', '8', '3', '7', '9', '8', '9', '1', '8', '8', '2', '7', '2', '9', '9', '11', '8', '8', '12', '12', '7', '8', '12', '4', '9', '3', '1', '12', '1', '1', '8', '8', '3', '8', '8', '8', '8', '9', '1', '8', '8', '10', '1', '8', '2', '8', '8', '7', '3', '8', '2', '4', '4', '9', '8', '10', '12', '12', '1', '1', '9', '1', '1', '1', '8', '2', '2', '8', '1', '1', '2', '2', '1', '2', '8', '1', '9', '9', '8', '8', '4', '2', '9', '9', '8', '3', '4', '3', '1', '8', '8', '2', '1', '9', '7', '8', '8', '1', '12', '3', '8', '2', '4', '2', '9', '12', '1', '4', '1', '8', '8', '8', '2', '2', '8', '9', '8', '8', '8', '10', '9', '8', '7', '9', '1', '1', '9', '4', '2', '4', '2', '2', '1', '7', '8', '11', '11', '3', '9', '2', '4', '8', '9', '1', '8', '1', '1', '4', '9', '1', '1', '8', '8', '2', '1', '8', '4', '2', '8', '9', '8', '8', '2', '8', '8', '8', '7', '1', '1', '1', '2', '1', '1', '8', '7', '8', '8', '12', '2', '12', '12', '8', '10', '12', '8', '3', '3', '12', '10', '1', '8', '12', '1', '8', '8', '2', '8', '4', '7', '8', '7', '10', '8', '10', '9', '8', '12', '12', '1', '8', '8', '3', '8', '8', '8', '8', '8', '1', '9', '8', '11', '1', '1', '1', '9', '8', '1', '9', '2', '3', '11', '8', '9', '9', '9', '2', '1', '8', '8', '9', '7', '1', '4', '9', '4', '8', '8', '4', '8', '12', '9', '4', '8', '2', '10', '10', '10', '8', '9', '9', '8', '8', '12', '7', '1', '8', '8', '8', '4', '1', '1', '1', '1', '1', '1', '8', '1', '9', '8', '9', '9', '4', '8', '12', '9', '8', '8', '2', '8', '8', '8', '6', '9', '8', '3', '7', '8', '8', '4', '12', '8', '8', '9', '12', '12', '9', '8', '2', '9', '2', '3', '1', '12', '8', '10', '9', '9', '9', '10', '10', '3', '8', '12', '1', '4', '2', '1', '10', '8', '2', '8', '4', '8', '9', '1', '9', '9', '10', '10', '1', '4', '9', '2', '4', '9', '1', '1', '3', '10', '3', '3', '8', '7', '3', '8', '9', '9', '12', '4', '8', '12', '2', '2', '4', '1', '9', '9', '4', '1', '4', '2', '8', '12', '2', '3', '10', '10', '9', '8', '9', '9', '1', '12', '8', '8', '8', '12', '4', '1', '8', '8', '1', '9', '8', '8', '2', '1', '8', '9', '8', '3', '3', '3', '1', '8', '8', '9', '1', '10', '9', '9', '9', '9', '5', '9', '9', '8', '8', '8', '8', '8', '8', '8', '8', '11', '12', '8', '8', '1', '8', '9', '11', '2', '2', '2', '2', '3', '1', '2', '2', '8', '2', '4', '9', '1', '2', '9', '8', '2', '8', '9', '9', '3', '10', '9', '9', '2', '8', '9', '8', '12', '12', '1', '3', '8', '8', '8', '2', '7', '7', '7', '7', '3', '9', '1', '9', '8', '9', '9', '1', '1', '1', '2', '9', '9', '9', '11', '1', '8', '8', '9', '1', '9', '8', '8', '8', '1', '1', '8', '7', '1', '1', '8', '8', '9', '4', '4', '8', '2', '2', '8', '8', '8', '8', '8', '8', '11', '8', '2', '9', '4', '9', '3', '9', '9', '1', '3', '9', '3', '1', '12', '8', '9', '12', '1', '8', '4', '2', '1', '4', '8', '3', '3', '8', '2', '8', '9', '7', '8', '8', '8', '5', '8', '3', '9', '8', '8', '13', '12', '1', '1', '2', '8', '4', '1', '9', '9', '12', '8', '9', '12', '9', '1', '9', '9', '9', '9', '3', '2', '9', '9', '4', '8', '12', '2', '4', '9', '3', '1', '9', '7', '8', '9', '9', '8', '4', '8', '8', '7', '9', '10', '3', '8', '8', '8', '1', '1', '1', '1', '8', '8', '4', '1', '10', '1', '5', '7', '7', '1', '8', '9', '3', '7', '2', '7', '7', '2', '4', '8', '12', '7', '4', '2', '9', '9', '12', '6', '10', '8', '2', '4', '12', '9', '9', '3', '8', '8', '1', '2', '10', '9', '9', '8', '4', '12', '2', '1', '8', '8', '8', '12', '10', '10', '9', '3', '8', '8', '9', '2', '8', '10', '1', '1', '1', '1', '2', '1', '1', '1', '1', '9', '8', '12', '9', '4', '8', '8', '9', '1', '9', '3', '9', '8', '8', '1', '7', '7', '10', '1', '8', '8', '1', '9', '8', '10', '3', '1', '7', '1', '8', '8', '12', '8', '8', '1', '8', '7', '1', '7', '7', '8', '2', '1', '8', '8', '2', '10', '8', '8', '8', '8', '8', '10', '1', '8', '8', '12', '8', '3', '3', '2', '2', '2', '10', '8', '8', '8', '2', '9', '1', '8', '9', '3', '2', '8', '10', '8', '6', '1', '1', '8', '4', '1', '9', '10', '8', '1', '7', '1', '2', '8', '1', '1', '1', '12', '1', '9', '12', '8', '12', '12', '12', '8', '8', '12', '4', '8', '8', '8', '8', '9', '9', '1', '3', '3', '3', '3', '1', '12', '12', '9', '10', '8', '8', '1', '9', '2', '2', '13', '9', '8', '9', '2', '1', '9', '1', '8', '8', '8', '4', '8', '1', '1', '1', '12', '12', '7', '2', '2', '2', '8', '3', '8', '9', '2', '10', '7', '8', '9', '2', '1', '2', '12', '12', '8', '8', '9', '2', '2', '9', '11', '1', '8', '1', '10', '9', '2', '1', '4', '7', '7', '7', '7', '7', '12', '8', '8', '8', '1', '1', '10', '1', '12', '1', '8', '2', '1', '1', '12', '8', '7', '9', '12', '8', '9', '3', '9', '8', '8', '8', '8', '3', '11', '2', '2', '9', '8', '8', '10', '8', '2', '7', '3', '1', '4', '7', '8', '8', '1', '8', '3', '7', '12', '8', '10', '9', '9', '8', '8', '2', '8', '9', '9', '1', '2', '8', '8', '9', '8', '3', '8', '1', '8', '10', '9', '8', '9', '9', '12', '4', '4', '8', '9', '9', '8', '2', '10', '1', '2', '8', '9', '1', '9', '9', '9', '7', '12', '12', '8', '1', '1', '1', '1', '8', '3', '1', '1', '8', '1', '8', '8', '7', '8', '8', '8', '8', '3', '2', '2', '10', '10', '10', '7', '8', '1', '2', '12', '7', '9', '8', '7', '8', '12', '2', '8', '9', '2', '6', '6', '7', '9', '8', '1', '8', '9', '8', '1', '12', '1', '2', '8', '7', '7', '7', '8', '2', '2', '8', '1', '2', '2', '9', '9', '1', '8', '8', '4', '3', '3', '1', '6', '3', '3', '12', '3', '8', '9', '1', '4', '3', '1', '8', '3', '9', '2', '8', '2', '8', '8', '8', '1', '1', '1', '9', '9', '8', '1', '9', '8', '1', '1', '3', '10', '8', '1', '1', '3', '9', '1', '4', '4', '1', '8', '9', '9', '2', '0', '0', '1', '8', '3', '1', '8', '8', '9', '8', '8', '1', '1', '8', '9', '8', '8', '8', '7', '9', '8', '8', '8', '10', '9', '8', '1', '2', '6', '1', '9', '9', '8', '12', '12', '12', '8', '8', '2', '8', '1', '2', '2', '2', '1', '9', '8', '2', '12', '2', '8', '12', '8', '9', '8', '8', '9', '7', '1', '1', '1', '1', '1', '8', '8', '1', '8', '8', '1', '1', '3', '2', '8', '8', '9', '10', '10', '2', '2', '1', '9', '2', '9', '9', '4', '12', '12', '12', '10', '7', '3', '3', '4', '2', '2', '9', '2', '8', '4', '2', '4', '1', '10', '9', '7', '8', '7', '1', '1', '3', '3', '1', '1', '3', '3', '3', '1', '1', '1', '1', '8', '2', '3', '1', '1', '2', '8', '8', '12', '8', '8', '8', '8', '11', '9', '1', '8', '9', '2', '8', '8', '8', '3', '9', '1', '9', '2', '7', '2', '8', '2', '8', '10', '8', '1', '10', '1', '1', '9', '9', '8', '8', '1', '8', '8', '8', '12', '8', '8', '8', '1', '8', '8', '8', '1', '9', '1', '1', '8', '1', '8', '9', '8', '2', '12', '9', '9', '0', '1', '8', '8', '1', '8', '12', '8', '8', '10', '8', '8', '8', '7', '8', '1', '8', '7', '3', '10', '1', '8', '9', '1', '8', '8', '8', '10', '1', '10', '3', '9', '1', '8', '9', '2', '8', '3', '3', '9', '9', '7', '9', '1', '1', '9', '2', '1', '1', '1', '7', '1', '1', '8', '8', '1', '1', '8', '1', '8', '3', '12', '9', '3', '3', '8', '8', '8', '8', '3', '1', '3', '3', '1', '11', '0', '8', '8', '7', '8', '12', '1', '8', '9', '8', '9', '8', '8', '3', '8', '8', '1', '1', '1', '9', '2', '2', '2', '8', '7', '12', '8', '8', '9', '10', '10', '7', '8', '1', '9', '8', '7', '3', '1', '3', '8', '2', '2', '3', '9', '8', '4', '4', '8', '9', '2', '1', '1', '7', '8', '9', '9', '7', '8', '7', '7', '8', '2', '2', '8', '4', '9', '7', '10', '0', '9', '8', '3', '7', '8', '1', '1', '8', '9', '9', '2', '2', '10', '1', '9', '10', '10', '10', '8', '3', '2', '12', '9', '9', '10', '12', '9', '12', '12', '9', '1', '2', '4', '12', '12', '7', '8', '9', '7', '7', '7', '3', '9', '8', '9', '1', '12', '8', '9', '4', '1', '3', '12', '12', '12', '12', '8', '8', '2', '1', '1', '2', '1', '1', '1', '12', '12', '8', '12', '2', '2', '12', '3', '3', '12', '8', '2', '8', '8', '12', '2', '1', '10', '3', '2', '8', '7', '1', '8', '1', '3', '7', '8', '9', '8', '3', '1', '1', '7', '8', '8', '9', '8', '2', '9', '2', '2', '9', '8', '1', '8', '8', '1', '3', '3', '1', '1', '10', '1', '2', '8', '1', '1', '1', '1', '9', '1', '4', '1', '7', '7', '7', '7', '2', '2', '8', '8', '12', '1', '9', '1', '7', '3', '3', '1', '8', '8', '10', '8', '9', '2', '9', '1', '3', '8', '8', '3', '12', '2', '8', '12', '2', '9', '1', '3', '3', '3', '3', '2', '8', '7', '9', '8', '3', '3', '1', '7', '8', '3', '1', '1', '12', '8', '9', '1', '2', '3', '8', '1', '1', '3', '3', '9', '1', '1', '1', '12', '1', '7', '3', '3', '1', '8', '8', '8', '1', '2', '4', '8', '1', '10', '2', '5', '3', '3', '12', '10', '9', '9', '12', '9', '0', '2', '8', '8', '9', '9', '9', '8', '1', '3', '1', '1', '4', '8', '1', '10', '8', '7', '2', '8', '2', '8', '4', '7', '8', '1', '9', '1', '9', '8', '2', '8', '2', '7', '9', '2', '2', '9', '1', '8', '12', '1', '8', '1', '4', '1', '9', '9', '1', '10', '12', '4', '8', '1', '7', '3', '9', '2', '12', '7', '8', '8', '2', '1', '12', '9', '8', '1', '2', '2', '8', '10', '2', '1', '7', '7', '7', '12', '3', '3', '8', '3', '8', '8', '3', '9', '8', '9', '1', '1', '8', '7', '9', '3', '3', '8', '1', '0', '9', '9', '9', '1', '8', '9', '9', '10', '1', '8', '7', '8', '8', '8', '9', '9', '8', '9', '10', '4', '9', '3', '7', '12', '1', '9', '9', '8', '9', '1', '9', '3', '1', '8', '4', '12', '10', '9', '8', '7', '10', '8', '12', '12', '3', '10', '8', '12', '2', '1', '2', '3', '9', '8', '8', '7', '1', '1', '2', '2', '1', '2', '2', '7', '1', '3', '9', '9', '3', '8', '8', '8', '8', '8', '7', '8', '8', '10', '8', '1', '8', '2', '8', '2', '2', '2', '2', '2', '1', '7', '12', '10', '1', '2', '8', '1', '4', '7', '8', '1', '12', '8', '7', '9', '2', '2', '2', '8', '1', '8', '1', '1', '0', '2', '2', '6', '1', '8', '2', '1', '8', '1', '1', '2', '2', '3', '2', '2', '10', '10', '9', '1', '7', '7', '8', '8', '1', '12', '10', '12', '3', '8', '8', '8', '3', '8', '3', '10', '2', '2', '2', '1', '2', '2', '1', '0', '1', '8', '9', '1', '1', '1', '2', '2', '2', '2', '2', '1', '1', '1', '1', '12', '9', '9', '1', '10', '11', '12', '9', '1', '1', '8', '8', '1', '2', '12', '8', '3', '7', '2', '10', '9', '7', '8', '1', '2', '2', '7', '9', '12', '2', '2', '1', '8', '9', '2', '3', '2', '2', '2', '9', '9', '8', '9', '2', '3', '8', '8', '9', '7', '3', '10', '8', '8', '9', '1', '1', '2', '8', '1', '10', '8', '8', '9', '7', '1', '7', '12', '8', '1', '7', '2', '1', '3', '9', '2', '8', '8', '1', '2', '3', '9', '3', '8', '9', '7', '7', '1', '10', '2', '8', '8', '8', '1', '8', '2', '8', '8', '1', '2', '8', '10', '2', '1', '0', '8', '3', '8', '10', '10', '12', '3', '8', '9', '3', '10', '8', '8', '8', '1', '8', '8', '2', '2', '2', '2', '2', '2', '1', '1', '10', '1', '10', '8', '2', '1', '2', '2', '8', '3', '3', '2', '2', '1', '8', '1', '3', '1', '9', '2', '2', '1', '3', '2', '2', '3', '2', '2', '3', '2', '1', '1', '2', '1', '2', '7', '8', '2', '1', '12', '12', '3', '1', '9', '9', '7', '8', '7', '2', '7', '9', '2', '2', '2', '8', '9', '7', '9', '2', '8', '9', '9', '2', '9', '1', '1', '9', '8', '3', '3', '8', '2', '7', '8', '11', '2', '8', '9', '1', '8', '3', '1', '2', '1', '2', '2', '2', '2', '8', '3', '9', '2', '10', '9', '9', '2', '7', '7', '7', '1', '3', '8', '8', '1', '1', '8', '7', '1', '8', '8', '9', '3', '3', '3', '2', '4', '8', '12', '8', '1', '12', '9', '2', '8', '2', '12', '2', '1', '3', '8', '12', '12', '11', '8', '10', '9', '2', '9', '4', '10', '1', '12', '12', '9', '3', '8', '9', '12', '9', '9', '2', '2', '2', '3', '5', '1', '1', '1', '7', '7', '8', '2', '2', '8', '2', '1', '7', '3', '2', '9', '8', '9', '1', '1', '9', '8', '9', '3', '3', '2', '9', '4', '9', '8', '8', '1', '8', '10', '2', '0', '3', '8', '8', '8', '9', '12', '9', '8', '2', '2', '8', '10', '9', '8', '2', '2', '4', '1', '9', '9', '8', '8', '10', '3', '1', '1', '2', '8', '7', '8', '2', '1', '1', '8', '12', '2', '9', '4', '8', '3', '3', '3', '8', '2', '12', '12', '8', '9', '12', '8', '2', '12', '7', '9', '2', '2', '1', '3', '1', '3', '11', '9', '8', '1', '10', '2', '12', '3', '8', '4', '1', '2', '8', '8', '3', '9', '3', '8', '7', '1', '1', '3', '1', '8', '2', '9', '8', '12', '1', '1', '1', '9', '9', '1', '8', '8', '8', '2', '2', '12', '3', '1', '1', '1', '9', '9', '3', '2', '1', '1', '9', '9', '1', '9', '9', '1', '9', '3', '9', '1', '8', '1', '9', '8', '3', '8', '8', '9', '9', '8', '2', '8', '1', '2', '2', '10', '1', '2', '1', '1', '9', '1', '9', '3', '7', '7', '2', '10', '3', '3', '1', '1', '9', '1', '2', '1', '3', '2', '1', '1', '1', '1', '1', '1', '1', '1', '1', '2', '8', '8', '7', '1', '1', '8', '8', '2', '1', '12', '1', '8', '1', '9', '8', '8', '2', '1', '9', '9', '2', '2', '8', '1', '7', '7', '9', '8', '1', '6', '8', '8', '2', '2', '8', '3', '8', '3', '2', '3', '8', '1', '2', '2', '2', '8', '9', '9', '9', '12', '1', '9', '2', '1', '9', '2', '9', '9', '10', '8', '1', '1', '9', '2', '1', '7', '1', '3', '7', '1', '1', '1', '3', '8', '8', '8', '9', '9', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '1', '0', '1', '1', '1', '2', '2', '3', '1', '1', '2', '3', '3', '3', '7', '3', '1', '9', '3', '9', '1', '4', '1', '1', '9', '10', '9', '8', '2', '3', '10', '9', '3', '10', '12', '8', '9', '8', '2', '1', '9', '8', '1', '7', '8', '1', '9', '1', '2', '10', '7', '7', '8', '1', '3', '3', '9', '3', '3', '1', '1', '1', '9', '1', '3', '8', '8', '8', '1', '8', '1', '9', '7', '8', '8', '2', '1', '3', '8', '2', '1', '6', '1', '8', '8', '2', '8', '2', '2', '9', '8', '1', '1', '6', '8', '6', '10', '2', '1', '1', '8', '9', '3', '3', '3', '1', '3', '9', '6', '8', '1', '8', '3', '1', '2', '2', '12', '7', '1', '1', '1', '1', '12', '1', '8', '8', '3', '2', '1', '9', '2', '3', '3', '2', '2', '7', '2', '2', '2', '3', '1', '9', '8', '7', '1', '1', '3', '1', '9', '4', '3', '9', '2', '1', '1', '9', '8', '8', '7', '2', '1', '3', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '2', '10', '1', '1', '8', '2', '8', '1', '8', '1', '3', '1', '2', '1', '1', '3', '1', '2', '1', '1', '1', '1', '1', '1', '1', '2', '1', '8', '1', '9', '8', '8', '1', '10', '8', '8', '1', '3', '9', '8', '9', '8', '2', '1', '3', '4', '1', '12', '1', '9', '8', '2', '7', '2', '1', '2', '9', '2', '1', '3', '8', '1', '2', '3', '8', '2', '10', '10', '10', '9', '9', '9', '3', '8', '3', '1', '1', '1', '1', '1', '1', '2', '1', '9', '2', '7', '9', '8', '7', '9', '1', '1', '1', '1', '8', '2', '2', '3', '3', '2', '12', '8', '3', '1', '1', '12', '9', '8', '2', '2', '9', '1', '2', '2', '9', '1', '1', '2', '4', '8', '1', '8', '10', '2', '9', '9', '9', '1', '2', '1', '1', '2', '2', '2', '12', '1', '2', '8', '3', '9', '9', '3', '2', '9', '9', '7', '2', '8', '8', '9', '2', '9', '1', '1', '1', '1', '1', '2', '8', '2', '1', '2', '2', '8', '3', '1', '1', '9', '1', '3', '10', '7', '9', '8', '8', '2', '2', '9', '2', '2', '2', '2', '2', '2', '9', '1', '3', '6', '4', '2', '2', '9', '1', '9', '8', '1', '1', '1', '7', '3', '2', '9', '9', '1', '9', '7', '2', '8', '9', '9', '12', '10', '8', '10', '2', '3', '1', '2', '2', '12', '12', '3', '12', '1', '2', '1', '2', '1', '2', '8', '2', '3', '12', '12', '8', '2', '2', '9', '2', '9', '2', '3', '3', '1', '1', '1', '12', '3', '7', '1', '3', '1', '2', '9', '2', '7', '2', '1', '8', '3', '7', '8', '1', '1', '3', '8', '3', '2', '9', '3', '1', '1', '1', '1', '11', '1', '2', '6', '1', '2', '3', '8', '9', '9', '2', '9', '2', '1', '1', '3', '1', '12', '9', '1', '1', '9', '1', '9', '9', '8', '9', '8', '1', '2', '7', '1', '0', '9', '8', '2', '3', '2', '4', '1', '1', '1', '8', '1', '12', '1', '2', '3', '3', '9', '9', '9', '9', '9', '7', '9', '3', '9', '1', '7', '3', '3', '3', '3', '7', '8', '2', '2', '2', '8', '2', '3', '9', '9', '3', '1', '8', '9', '12', '8', '8', '3', '2', '1', '6', '1', '9', '1', '1', '1', '1', '1', '1', '2', '2', '1', '2', '2', '2', '2', '9', '2', '5', '3', '2', '2', '8', '2', '1', '8', '3', '3', '8', '3', '1', '3', '9', '2', '8', '9', '4', '4', '9', '8', '8', '7', '7', '3', '1', '8', '8', '3', '2', '2', '10', '3', '3', '2', '2', '2', '2', '12', '3', '1', '2', '7', '7', '12', '2', '1', '2', '1', '1', '2', '1', '3', '3', '3', '1', '3', '9', '4', '8', '8', '4', '2', '4', '2', '8', '9', '1', '1', '2', '2', '2', '8', '7', '2', '8', '2', '9', '1', '8', '1', '1', '2', '1', '1', '1', '2', '9', '9', '8', '1', '8', '1', '4', '8', '1', '2', '7', '9', '2', '1', '8', '9', '8', '9', '1', '1', '2', '3', '2', '12', '7', '1', '2', '4', '4', '4', '2', '2', '2', '9', '8', '8', '1', '12', '3', '3', '2', '2', '3', '12', '1', '5', '1', '1', '3', '2', '8', '8', '2', '2', '2', '3', '9', '8', '8', '9', '8', '8', '11', '8', '2', '9', '2', '3', '1', '7', '1', '8', '2', '8', '1', '1', '1', '8', '1', '1', '1', '7', '8', '8', '2', '3', '2', '1', '8', '8', '1', '8', '4', '8', '9', '8', '9', '8', '1', '2', '3', '2', '1', '2', '2', '3', '1', '9', '4', '8', '1', '1', '9', '1', '1', '8', '2', '2', '2', '1', '7', '3', '3', '1', '3', '3', '12', '3', '1', '1', '3', '1', '1', '4', '8', '4', '4', '3', '3', '3', '4', '8', '1', '9', '9', '9', '4', '1', '7', '4', '8', '4', '9', '8', '1', '1', '3', '8', '9', '1', '9', '7', '1', '1', '9', '1', '8', '8', '8', '1', '1', '8', '2', '2', '2', '9', '4', '4', '1', '2', '1', '2', '9', '1', '1', '8', '5', '5', '5', '9', '12', '4', '1', '2', '1', '8', '8', '1', '9', '9', '2', '12', '1', '1', '7', '8', '8', '4', '1', '2', '8', '11', '2', '3', '2', '2', '1', '11', '2', '2', '2', '2', '9', '9', '2', '2', '9', '3', '1', '9', '1', '1', '1', '7', '8', '1', '8', '9', '1', '2', '9', '8', '1', '2', '9', '8', '12', '2', '2', '10', '8', '2', '2', '8', '8', '1', '2', '6', '1', '1', '7', '7', '2', '3', '2', '1', '1', '8', '1', '10', '8', '12', '6', '4', '1', '4', '1', '2', '9', '8', '9', '4', '8', '8', '10', '8', '8', '8', '8', '8', '2', '2', '8', '4', '9', '3', '3', '3', '3', '3', '9', '2', '1', '1', '2', '3', '2', '1', '2', '3', '3', '3', '2', '2', '3', '3', '2', '1', '2', '3', '3', '3', '3', '12', '9', '3', '2', '2', '3', '1', '5', '1', '1', '3', '2', '2', '1', '2', '9', '3', '9', '8', '1', '8', '7', '7', '9', '1', '1', '7', '9', '10', '1', '7', '1', '4', '3', '8', '9', '4', '9', '9', '2', '9', '2', '8', '4', '2', '2', '9', '5', '2', '1', '10', '9', '3', '1', '1', '1', '2', '1', '2', '9', '1', '7', '9', '9', '8', '2', '2', '8', '9', '1', '9', '8', '2', '3', '2', '2', '1', '2', '2', '2', '1', '1', '8', '1', '9', '4', '12', '8', '2', '7', '2', '2', '1', '9', '3', '1', '8', '12', '1', '1', '2', '1', '10', '2', '6', '12', '12', '4', '9', '1', '9', '10', '1', '2', '8', '2', '1', '8', '6', '12', '9', '9', '2', '1', '7', '8', '9', '5', '8', '1', '8', '8', '3', '1', '8', '2', '8', '2', '2', '1', '2', '9', '2', '8', '1', '12', '8', '2', '7', '3', '3', '12', '2', '3', '3', '9', '9', '3', '3', '3', '3', '3', '2', '1', '4', '1', '4', '8', '8', '2', '1', '2', '9', '12', '1', '10', '12', '1', '2', '9', '4', '8', '10', '3', '2', '8', '7', '4', '8', '9', '2', '7', '1', '10', '2', '2', '2', '8', '4', '2', '2', '1', '1', '8', '7', '9', '2', '1', '9', '8', '9', '12', '4', '8', '2', '8', '7', '7', '1', '9', '2', '1', '1', '9', '1', '8', '2', '3', '9', '9', '1', '3', '2', '9', '2', '2', '8', '1', '1', '2', '1', '2', '1', '9', '8', '2', '2', '4', '8', '8', '9', '5', '5', '9', '8', '9', '6', '2', '1', '12', '2', '3', '8', '8', '9', '2', '2', '1', '3', '2', '10', '9', '10', '7', '2', '8', '4', '2', '8', '9', '8', '1', '3', '8', '8', '1', '8', '3', '8', '8', '1', '10', '3', '1', '5', '8', '8', '9', '2', '2', '9', '2', '1', '9', '1', '4', '1', '12', '8', '8', '2', '1', '1', '1', '1', '1', '9', '2', '9', '8', '9', '5', '9', '1', '8', '2', '4', '1', '1', '1', '9', '12', '4', '1', '1', '9', '8', '4', '1', '8', '9', '8', '8', '9', '9', '9', '9', '9', '9', '1', '3', '4', '2', '2', '8', '2', '9', '2', '3', '8', '3', '1', '7', '2', '1', '1', '4', '2', '9', '2', '1', '3', '2', '9', '2', '9', '3', '8', '1', '9', '1', '1', '2', '8', '8', '8', '2', '3', '9', '9', '8', '2', '4', '1', '5', '1', '1', '9', '9', '2', '1', '1', '1', '9', '9', '1', '3', '9', '2', '2', '9', '9', '8', '3', '9', '9', '8', '2', '2', '8', '9', '2', '2', '8', '9', '10', '10', '2', '8', '7', '9', '9', '4', '2', '4', '11', '2', '1', '7', '8', '8', '8', '9', '4', '9', '8', '8', '2', '1', '1', '9', '8', '3', '3', '8', '8', '7', '9', '1', '3', '4', '8', '10', '2', '4', '4', '8', '2', '8', '2', '8', '9', '2', '1', '2', '2', '3', '1', '9', '4', '5', '9', '9', '1', '1', '1', '1', '1', '1', '7', '1', '1', '1', '8', '1', '1', '1', '1', '1', '9', '1', '2', '1', '12', '9', '3', '9', '9', '4', '1', '9', '1', '1', '2', '11', '9', '3', '2', '2', '2', '4', '2', '3', '2', '7', '7', '8', '1', '10', '9', '5', '7', '1', '10', '8', '1', '2', '7', '1', '12', '5', '8', '4', '2', '9', '9', '8', '2', '9', '8', '8', '10', '2', '3', '2', '7', '2', '8', '10', '3', '2', '9', '9', '9', '1', '1', '1', '1', '8', '2', '10', '9', '8', '1', '2', '2', '2', '1', '2', '1', '3', '9', '12', '2', '2', '4', '8', '1', '3', '9', '4', '9', '1', '9', '9', '2', '1', '1', '9', '7', '10', '3', '10', '2', '9', '2', '9', '2', '8', '2', '1', '2', '3', '1', '1', '1', '8', '5', '8', '3', '1', '1', '4', '2', '2', '1', '3', '8', '2', '1', '1', '1', '2', '9', '4', '1', '8', '1', '2', '8', '2', '9', '2', '2', '5', '5', '2', '5', '2', '1', '8', '8', '1', '3', '12', '2', '4', '3', '2', '2', '2', '6', '2', '3', '3', '1', '8', '2', '2', '1', '1', '8', '8', '10', '1', '9', '2', '9', '12', '0', '2', '1', '3', '2', '2', '1', '2', '1', '9', '9', '2', '9', '8', '8', '1', '7', '8', '1', '2', '6', '9', '8', '10', '1', '12', '2', '2', '1', '8', '2', '1', '12', '7', '4', '2', '10', '2', '2', '1', '2', '1', '2', '1', '1', '2', '1', '9', '8', '10', '2', '10', '9', '9', '12', '5', '3', '2', '8', '2', '8', '3', '3', '1', '4', '9', '10', '2', '4', '8', '1', '2', '10', '12', '12', '3', '1', '1', '2', '9', '6', '6', '1', '9', '1', '8', '1', '8', '8', '8', '8', '2', '2', '2', '8', '8', '2', '1', '1', '1', '1', '1', '8', '8', '5', '8', '8', '1', '8', '9', '9', '1', '7', '12', '9', '9', '7', '2', '2', '7', '8', '9', '8', '2', '9', '6', '3', '4', '8', '4', '1', '2', '9', '8', '8', '8', '2', '1', '1', '8', '8', '3', '1', '9', '9', '7', '1', '2', '2', '2', '9', '4', '1', '2', '12', '1', '8', '2', '1', '9', '1', '8', '9', '9', '2', '5', '4', '2', '1', '2', '8', '12', '8', '10', '3', '8', '8', '8', '2', '2', '8', '2', '8', '2', '4', '8', '10', '2', '7', '2', '1', '8', '8', '9', '5', '8', '1', '12', '7', '10', '2', '2', '2', '1', '4', '8', '12', '1', '8', '2', '2', '9', '1', '8', '3', '1', '1', '5', '10', '2', '1', '2', '4', '8', '9', '3', '2', '8', '7', '4', '4', '9', '9', '2', '8', '1', '1', '2', '4', '7', '1', '9', '1', '1', '1', '8', '4', '1', '3', '1', '2', '2', '9', '1', '8', '8', '2', '2', '9', '9', '1', '8', '8', '1', '9', '8', '8', '2', '2', '2', '2', '1', '1', '1', '7', '4', '1', '1', '4', '2', '3', '3', '2', '3', '9', '2', '8', '2', '1', '5', '1', '2', '2', '1', '3', '5', '2', '9', '1', '1', '9', '8', '1', '4', '8', '1', '8', '2', '4', '8', '4', '7', '9', '8', '1', '2', '2', '4', '4', '11', '3', '11', '1', '3', '8', '1', '9', '6', '8', '9', '3', '9', '8', '9', '2', '3', '3', '7', '1', '7', '2', '9', '9', '1', '8', '8', '1', '5', '5', '8', '8', '1', '2', '1', '3', '9', '9', '5', '9', '8', '1', '4', '3', '8', '1', '1', '2', '1', '8', '9', '8', '2', '2', '2', '1', '2', '4', '10', '8', '1', '9', '1', '8', '1', '1', '9', '9', '8', '11', '1', '8', '2', '1', '2', '8', '2', '8', '8', '2', '9', '6', '10', '1', '8', '6', '4', '5', '11', '2', '2', '8', '1', '8', '1', '8', '1', '4', '8', '1', '1', '1', '8', '3', '7', '3', '3', '7', '1', '1', '4', '8', '1', '6', '4', '2', '2', '1', '1', '1', '1', '6', '2', '2', '8', '1', '8', '5', '5', '2', '1', '1', '2', '3', '7', '1', '1', '6', '2', '9', '3', '9', '8', '9', '2', '1', '4', '2', '9', '13', '9', '2', '8', '12', '4', '1', '1', '9', '5', '1', '1', '1', '1', '8', '2', '1', '1', '2', '9', '0', '12', '1', '1', '8', '9', '8', '3', '2', '2', '12', '8', '2', '2', '2', '1', '10', '1', '9', '8', '9', '5', '2', '2', '9', '2', '8', '4', '8', '3', '7', '9', '2', '2', '7', '2', '2', '8', '1', '1', '9', '10', '9', '8', '2', '9', '0', '7', '1', '1', '1', '9', '8', '2', '8', '12', '8', '3', '2', '8', '8', '7', '7', '8', '12', '2', '1', '3', '9', '2', '2', '8', '2', '8', '8', '12', '8', '8', '1', '8', '9', '2', '4', '4', '7', '2', '1', '1', '9', '3', '7', '1', '3', '9', '8', '2', '2', '3', '1', '3', '10', '2', '3', '7', '1', '8', '1', '1', '10', '3', '10', '8', '4', '1', '2', '4', '3', '10', '9', '3', '9', '5', '8', '7', '2', '9', '3', '3', '2', '11', '8', '1', '1', '7', '2', '7', '2', '9', '4', '8', '1', '12', '8', '7', '9', '1', '0', '2', '6', '4', '9', '3', '2', '4', '12', '9', '8', '5', '9', '2', '8', '4', '6', '8', '9', '3', '1', '8', '7', '3', '1', '1', '1', '8', '9', '1', '8', '7', '9', '8', '2', '1', '2', '2', '3', '2', '2', '1', '1', '4', '9', '7', '3', '2', '8', '8', '9', '1', '9', '7', '4', '2', '2', '8', '5', '9', '1', '5', '7', '1', '4', '6', '9', '10', '1', '8', '2', '9', '1', '9', '2', '9', '8', '2', '9', '9', '11', '2', '12', '2', '2', '9', '4', '8', '1', '9', '9', '2', '8', '9', '1', '2', '9', '3', '8', '8', '1', '7', '8', '8', '2', '3', '2', '2', '2', '2', '9', '8', '10', '2', '2', '1', '9', '11', '1', '8', '8', '2', '4', '2', '1', '2', '8', '8', '9', '2', '8', '9', '2', '3', '1', '2', '1', '3', '10', '8', '1', '2', '4', '1', '9', '8', '9', '3', '3', '9', '8', '2', '11', '3', '7', '1', '8', '7', '1', '8', '8', '8', '1', '4', '8', '8', '1', '9', '3', '1', '1', '2', '8', '8', '8', '2', '1', '8', '10', '2', '12', '8', '8', '9', '1', '8', '1', '3', '2', '2', '10', '7', '2', '2', '1', '1', '8', '2', '9', '1', '2', '9', '3', '3', '10', '1', '4', '7', '10', '12', '2', '4', '10', '8', '6', '2', '9', '1', '8', '3', '1', '2', '8', '8', '2', '12', '10', '2', '10', '9', '1', '2', '5', '8', '12', '8', '8', '7', '8', '8', '1', '8', '2', '2', '9', '1', '8', '10', '5', '5', '5', '8', '1', '9', '1', '13', '1', '2', '2', '1', '2', '4', '9', '2', '1', '9', '13', '11', '1', '9', '9', '3', '10', '8', '7', '8', '8', '4', '2', '1', '9', '4', '3', '1', '1', '10', '9', '2', '8', '6', '2', '1', '1', '3', '2', '8', '1', '1', '1', '1', '2', '4', '1', '1', '10', '1', '8', '1', '1', '9', '1', '7', '1', '1', '9', '9', '2', '12', '9', '1', '8', '1', '12', '1', '8', '3', '8', '1', '8', '1', '2', '10', '1', '1', '2', '2', '9', '3', '9', '12', '1', '9', '1', '2', '8', '7', '2', '1', '1', '3', '2', '8', '4', '4', '5', '12', '7', '8', '9', '6', '1', '8', '9', '2', '1', '9', '1', '4', '1', '8', '2', '1', '1', '8', '7', '4', '9', '1', '8', '4', '2', '7', '5', '1', '3', '9', '2', '8', '2', '1', '2', '9', '1', '1', '2', '3', '4', '3', '1', '2', '2', '10', '2', '2', '8', '1', '1', '2', '4', '9', '2', '11', '9', '8', '9', '2', '1', '10', '4', '1', '1', '10', '7', '2', '10', '8', '1', '10', '8', '8', '8', '8', '8', '8', '2', '2', '2', '8', '3', '4', '1', '1', '1', '2', '1', '8', '8', '2', '8', '3', '8', '3', '1', '2', '3', '1', '1', '10', '1', '4', '4', '3', '3', '1', '9', '1', '1', '2', '1', '1', '6', '1', '1', '1', '9', '1', '4', '1', '1', '1', '1', '1', '1', '2', '12', '8', '8', '1', '9', '12', '10', '8', '2', '12', '1', '1', '2', '1', '7', '10', '7', '1', '2', '1', '10', '8', '2', '2', '8', '2', '2', '1', '9', '8', '8', '8', '8', '3', '1', '4', '2', '8', '3', '10', '10', '1', '9', '1', '9', '1', '9', '1', '8', '3', '1', '1', '8', '8', '8', '6', '3', '4', '1', '1', '1', '9', '9', '5', '2', '10', '2', '6', '2', '2', '7', '1', '8', '2', '11', '1', '1', '1', '2', '1', '9', '4', '4', '2', '8', '4', '6', '8', '8', '10', '10', '2', '1', '2', '9', '1', '8', '8', '3', '4', '8', '8', '2', '8', '8', '1', '2', '1', '2', '9', '4', '1', '3', '3', '3', '8', '8', '8', '2', '6', '4', '3', '3', '12', '3', '4', '9', '6', '8', '7', '7', '8', '6', '4', '1', '10', '6', '3', '3', '8', '1', '1', '8', '1', '11', '8', '1', '1', '2', '3', '11', '7', '2', '1', '1', '1', '9', '1', '2', '1', '1', '9', '8', '1', '1', '8', '0', '4', '1', '1', '2', '1', '1', '2', '2', '9', '1', '10', '1', '3', '8', '9', '3', '3', '1', '10', '1', '9', '1', '1', '7', '4', '1', '8', '10', '1', '2', '10', '1', '1', '2', '3', '3', '9', '1', '8', '10', '1', '7', '9', '1', '9', '6', '3', '9', '2', '8', '4', '12', '12', '3', '1', '1', '9', '1', '9', '2', '1', '1', '1', '1', '1', '9', '2', '1', '2', '10', '9', '8', '8', '10', '8', '2', '3', '2', '1', '12', '8', '12', '2', '9', '1', '3', '2', '5', '2', '2', '8', '2', '2', '4', '10', '9', '10', '9', '4', '2', '4', '12', '8', '2', '1', '8', '3', '9', '2', '9', '4', '1', '1', '1', '1', '1', '9', '6', '2', '9', '2', '1', '3', '2', '2', '5', '8', '9', '1', '6', '3', '1', '2', '2', '2', '6', '1', '8', '10', '8', '3', '3', '13', '2', '9', '9', '10', '6', '12', '8', '2', '9', '2', '3', '8', '8', '1', '2', '2', '3', '2', '10', '1', '8', '9', '1', '8', '9', '8', '8', '2', '10', '2', '1', '1', '1', '9', '9', '12', '1', '1', '4', '8', '3', '2', '2', '4', '4', '2', '2', '1', '1', '1', '9', '2', '2', '4', '1', '1', '8', '8', '8', '9', '2', '13', '9', '9', '4', '1', '9', '7', '10', '10', '8', '1', '9', '1', '1', '1', '8', '12', '4', '1', '2', '1', '1', '1', '7', '8', '3', '3', '1', '2', '2', '7', '2', '2', '10', '1', '8', '4', '0', '1', '9', '4', '1', '1', '2', '10', '10', '7', '9', '3', '12', '11', '2', '8', '10', '8', '9', '4', '3', '4', '10', '1', '1', '9', '3', '10', '1', '5', '3', '3', '2', '2', '1', '1', '6', '1', '7', '1', '1', '12', '1', '1', '1', '10', '1', '8', '8', '1', '3', '8', '3', '1', '2', '8', '3', '4', '2', '8', '8', '6', '1', '1', '4', '1', '2', '9', '5', '1', '7', '8', '9', '2', '4', '9', '8', '5', '9', '9', '9', '1', '2', '1', '3', '8', '9', '2', '9', '8', '9', '1', '8', '13', '9', '3', '8', '1', '4', '1', '3', '1', '6', '4', '10', '2', '2', '7', '8', '8', '12', '8', '9', '9', '9', '10', '3', '12', '12', '2', '9', '10', '9', '7', '2', '3', '4', '10', '10', '2', '7', '7', '1', '7', '3', '9', '1', '9', '8', '2', '9', '1', '8', '1', '10', '8', '8', '2', '7', '8', '1', '8', '9', '8', '1', '4', '1', '8', '8', '8', '1', '1', '10', '3', '2', '2', '6', '8', '9', '8', '2', '1', '1', '3', '8', '8', '1', '4', '4', '3', '9', '10', '4', '2', '4', '2', '9', '2', '2', '9', '1', '1', '10', '1', '1', '2', '1', '1', '1', '9', '1', '8', '8', '8', '2', '3', '8', '1', '6', '7', '3', '1', '9', '1', '2', '1', '6', '1', '2', '10', '1', '4', '2', '3', '2', '9', '9', '4', '9', '9', '1', '2', '8', '4', '7', '8', '7', '13', '9', '1', '2', '1', '9', '3', '3', '6', '8', '9', '10', '9', '2', '2', '3', '12', '1', '1', '1', '13', '2', '1', '7', '10', '8', '9', '1', '2', '9', '1', '1', '1', '2', '1', '12', '5', '6', '9', '10', '3', '1', '10', '1', '3', '2', '8', '9', '2', '10', '8', '8', '9', '2', '8', '9', '2', '2', '2', '8', '8', '2', '3', '1', '4', '8', '1', '9', '9', '2', '8', '10', '9', '1', '2', '12', '8', '8', '0', '2', '1', '1', '9', '10', '1', '6', '2', '2', '10', '2', '2', '4', '2', '2', '12', '1', '6', '7', '8', '9', '1', '3', '5', '7', '3', '1', '3', '1', '1', '3', '2', '6', '3', '2', '1', '8', '3', '5', '2', '1', '1', '8', '1', '1', '0', '2', '3', '5', '3', '1', '1', '9', '8', '8', '8', '8', '4', '7', '2', '9', '12', '5', '9', '3', '12', '3', '9', '1', '9', '1', '7', '3', '9', '3', '10', '1', '2', '1', '4', '9', '8', '2', '9', '11', '2', '8', '9', '4', '1', '1', '1', '1', '1', '1', '1', '9', '1', '9', '1', '1', '1', '12', '3', '10', '3', '6', '1', '7', '10', '8', '3', '7', '1', '1', '2', '6', '1', '3', '9', '2', '8', '1', '8', '8', '1', '8', '10', '12', '1', '10', '12', '10', '1', '8', '8', '1', '4', '7', '9', '2', '1', '9', '3', '8', '12', '10', '6', '1', '6', '8', '3', '1', '3', '1', '10', '10', '9', '8', '1', '2', '9', '8', '8', '1', '9', '8', '8', '3', '1', '3', '9', '12', '1', '8', '1', '5', '8', '11', '5', '5', '1', '2', '1', '3', '1', '1', '1', '9', '8', '8', '1', '9', '10', '8', '4', '1', '1', '8', '10', '8', '9', '1', '10', '1', '7', '2', '1', '1', '9', '6', '2', '9', '8', '1', '1', '1', '9', '8', '8', '1', '9', '9', '1', '1', '2', '7', '9', '2', '9', '9', '8', '7', '6', '8', '7', '8', '2', '9', '2', '1', '8', '8', '8', '2', '6', '3', '1', '8', '12', '12', '8', '7', '1', '2', '2', '7', '1', '9', '8', '1', '4', '1', '1', '8', '5', '8', '1', '8', '8', '8', '8', '1', '2', '11', '1', '1', '1', '2', '7', '1', '9', '2', '2', '2', '9', '8', '9', '2', '9', '1', '8', '1', '7', '11', '13', '1', '9', '8', '2', '2', '1', '2', '3', '10', '3', '10', '4', '3', '9', '1', '8', '1', '1', '13', '1', '1', '3', '8', '1', '2', '2', '2', '9', '4', '1', '8', '1', '9', '3', '6', '8', '5', '2', '8', '2', '8', '1', '9', '2', '2', '8', '1', '1', '4', '8', '9', '2', '2', '7', '1', '8', '12', '2', '12', '8', '2', '9', '9', '3', '8', '1', '1', '9', '9', '9', '2', '8', '1', '9', '1', '8', '9', '8', '2', '2', '4', '1', '9', '10', '10', '8', '8', '1', '1', '2', '1', '4', '1', '2', '1', '1', '3', '12', '8', '8', '8', '8', '10', '2', '8', '12', '1', '9', '9', '8', '9', '1', '2', '1', '8', '8', '8', '2', '8', '8', '3', '10', '8', '9', '1', '1', '1', '3', '4', '8', '10', '6', '3', '1', '3', '2', '8', '9', '3', '5', '4', '1', '5', '2', '9', '9', '9', '1', '1', '1', '9', '9', '6', '10', '9', '8', '12', '2', '9', '1', '2', '1', '1', '1', '8', '1', '12', '1', '12', '1', '9', '1', '8', '1', '2', '1', '2', '8', '8', '10', '1', '9', '1', '2', '2', '8', '8', '3', '12', '1', '11', '2', '10', '12', '11', '2', '1', '10', '1', '1', '9', '3', '10', '2', '7', '1', '8', '1', '9', '9', '2', '1', '3', '12', '5', '9', '9', '8', '1', '8', '8', '8', '1', '1', '3', '9', '8', '9', '8', '3', '10', '3', '7', '8', '9', '2', '3', '1', '9', '10', '9', '2', '8', '2', '6', '4', '1', '1', '3', '8', '2', '3', '9', '1', '2', '1', '8', '1', '2', '2', '2', '2', '9', '9', '4', '8', '9', '1', '1', '2', '9', '1', '5', '2', '9', '8', '10', '1', '2', '10', '8', '5', '6', '4', '8', '1', '1', '1', '8', '9', '2', '8', '8', '8', '12', '2', '2', '1', '8', '1', '2', '1', '9', '8', '8', '9', '1', '1', '8', '7', '1', '8', '0', '2', '3', '8', '1', '1', '8', '1', '1', '12', '3', '2', '10', '8', '10', '6', '1', '8', '8', '10', '8', '8', '1', '4', '8', '1', '2', '9', '8', '1', '1', '3', '3', '5', '1', '1', '2', '1', '2', '1', '9', '8', '3', '9', '8', '2', '10', '9', '1', '8', '1', '10', '9', '10', '1', '2', '3', '8', '1', '3', '1', '9', '9', '8', '1', '9', '8', '8', '9', '1', '9', '2', '8', '8', '2', '13', '8', '10', '8', '9', '3', '2', '1', '3', '1', '12', '10', '10', '1', '11', '1', '2', '4', '2', '10', '1', '8', '11', '1', '1', '1', '2', '2', '8', '2', '9', '8', '9', '9', '12', '8', '1', '9', '2', '4', '1', '8', '3', '9', '1', '6', '5', '8', '9', '3', '3', '2', '1', '1', '1', '8', '11', '1', '8', '7', '1', '11', '9', '1', '7', '9', '9', '8', '8', '8', '12', '8', '1', '2', '9', '8', '8', '2', '10', '9', '8', '8', '4', '10', '9', '8', '2', '10', '10', '2', '1', '2', '1', '2', '1', '4', '1', '7', '1', '1', '3', '12', '9', '8', '5', '9', '10', '1', '9', '9', '2', '7', '4', '1', '1', '8', '12', '8', '2', '2', '9', '4', '1', '1', '5', '9', '1', '8', '12', '8', '1', '1', '4', '9', '10', '2', '3', '1', '3', '3', '3', '8', '1', '2', '1', '2', '1', '2', '9', '1', '2', '8', '9', '4', '8', '8', '10', '2', '12', '5', '5', '1', '8', '1', '7', '8', '1', '1', '8', '9', '12', '4', '9', '3', '1', '2', '5', '9', '2', '1', '2', '2', '1', '2', '8', '1', '8', '8', '2', '2', '1', '8', '4', '1', '2', '10', '9', '10', '8', '1', '10', '4', '8', '8', '10', '2', '8', '1', '1', '3', '10', '1', '4', '2', '8', '3', '3', '2', '9', '8', '5', '5', '9', '3', '10', '8', '1', '2', '1', '2', '9', '1', '1', '2', '9', '9', '8', '8', '11', '9', '4', '1', '8', '2', '7', '1', '2', '8', '2', '9', '9', '8', '2', '9', '9', '8', '8', '1', '2', '8', '1', '1', '2', '9', '8', '1', '4', '9', '1', '8', '12', '1', '12', '2', '8', '2', '9', '1', '12', '1', '2', '9', '1', '1', '9', '3', '2', '9', '9', '2', '11', '8', '7', '9', '9', '9', '8', '8', '1', '2', '8', '1', '1', '4', '1', '2', '10', '1', '9', '2', '1', '1', '1', '9', '1', '13', '4', '3', '2', '1', '1', '2', '2', '7', '8', '7', '1', '1', '2', '8', '8', '3', '4', '8', '2', '10', '13', '8', '8', '9', '2', '2', '1', '1', '10', '4', '2', '7', '9', '8', '2', '1', '9', '8', '1', '1', '9', '8', '9', '8', '2', '8', '2', '2', '9', '2', '1', '9', '1', '2', '4', '8', '2', '1', '1', '8', '1', '4', '1', '9', '3', '9', '2', '7', '1', '9', '6', '1', '10', '2', '2', '2', '2', '2', '10', '10', '10', '9', '9', '9', '3', '12', '10', '1', '1', '10', '1', '1', '2', '2', '1', '2', '3', '12', '8', '8', '1', '2', '9', '9', '1', '1', '10', '8', '9', '8', '3', '1', '1', '3', '1', '10', '1', '1', '1', '4', '1', '1', '1', '7', '10', '2', '1', '8', '1', '9', '3', '1', '10', '8', '8', '1', '5', '9', '1', '2', '1', '8', '8', '1', '3', '1', '10', '2', '1', '1', '2', '8', '2', '5', '3', '5', '2', '1', '1', '9', '1', '7', '9', '9', '2', '8', '1', '12', '4', '1', '4', '9', '9', '4', '8', '1', '10', '10', '8', '8', '8', '9', '2', '3', '5', '1', '8', '1', '1', '1', '7', '10', '1', '4', '12', '3', '2', '2', '9', '1', '1', '1', '2', '8', '4', '1', '7', '3', '12', '8', '6', '2', '2', '7', '9', '11', '1', '10', '9', '1', '8', '12', '2', '11', '1', '2', '3', '1', '1', '1', '8', '9', '2', '2', '2', '2', '3', '3', '8', '10', '4', '1', '2', '9', '8', '2', '2', '9', '1', '8', '11', '4', '2', '8', '7', '1', '1', '7', '3', '9', '9', '4', '5', '8', '10', '2', '9', '9', '9', '8', '1', '9', '2', '2', '2', '1', '2', '9', '4', '3', '12', '4', '3', '2', '3', '8', '3', '10', '1', '10', '1', '1', '8', '10', '6', '8', '9', '9', '1', '2', '9', '2', '9', '3', '8', '1', '1', '12', '1', '5', '1', '10', '8', '10', '2', '7', '1', '1', '1', '1', '3', '2', '3', '5', '9', '1', '2', '1', '9', '8', '10', '9', '8', '1', '8', '1', '8', '2', '8', '1', '2', '8', '1', '2', '5', '8', '12', '2', '1', '1', '1', '1', '1', '8', '8', '2', '1', '4', '2', '8', '8', '8', '2', '9', '9', '11', '1', '2', '9', '3', '1', '1', '8', '9', '2', '4', '10', '1', '9', '8', '4', '9', '8', '10', '8', '8', '9', '2', '8', '9', '3', '3', '3', '4', '3', '2', '2', '10', '8', '2', '1', '5', '1', '9', '1', '1', '2', '1', '2', '3', '1', '1', '8', '8', '1', '10', '8', '8', '1', '6', '4', '8', '2', '5', '8', '1', '3', '8', '2', '8', '9', '1', '1', '8', '12', '10', '1', '5', '8', '2', '8', '10', '9', '9', '1', '6', '9', '1', '10', '8', '2', '9', '10', '1', '2', '1', '1', '8', '8', '3', '9', '8', '1', '10', '2', '8', '1', '10', '1', '5', '1', '1', '1', '5', '1', '1', '1', '4', '1', '1', '1', '8', '2', '8', '1', '8', '3', '8', '9', '1', '8', '1', '8', '1', '1', '2', '2', '1', '12', '9', '9', '1', '8', '2', '1', '2', '8', '1', '9', '1', '1', '2', '1', '2', '2', '2', '1', '9', '9', '8', '8', '1', '1', '9', '10', '8', '4', '3', '3', '4', '9', '1', '3', '3', '10', '2', '2', '1', '8', '4', '1', '9', '4', '1', '1', '9', '8', '10', '4', '9', '9', '8', '2', '1', '3', '4', '9', '3', '8', '9', '1', '1', '2', '1', '7', '8', '2', '9', '1', '9', '2', '6', '9', '10', '8', '1', '1', '5', '9', '5', '1', '9', '10', '9', '3', '3', '8', '8', '2', '9', '2', '8', '2', '1', '8', '5', '8', '3', '10', '1', '1', '2', '1', '8', '1', '4', '3', '2', '9', '4', '9', '8', '8', '8', '1', '3', '9', '1', '9', '9', '1', '2', '1', '9', '8', '9', '8', '1', '1', '1', '4', '2', '2', '9', '9', '1', '1', '1', '3', '6', '1', '2', '3', '1', '4', '2', '9', '1', '9', '9', '1', '8', '2', '1', '9', '1', '8', '1', '8', '8', '2', '1', '9', '9', '6', '8', '8', '9', '8', '10', '8', '9', '5', '1', '1', '1', '1', '8', '1', '8', '12', '8', '1', '12', '9', '2', '8', '1', '1', '2', '1', '8', '6', '9', '9', '8', '1', '8', '9', '7', '7', '8', '9', '9', '1', '8', '3', '8', '1', '3', '3', '8', '9', '8', '2', '8', '1', '2', '9', '8', '1', '1', '1', '1', '9', '1', '9', '8', '12', '3', '8', '8', '1', '10', '10', '10', '2', '2', '1', '10', '2', '2', '10', '8', '12', '8', '1', '1', '1', '2', '1', '1', '3', '8', '1', '2', '2', '2', '1', '1', '6', '2', '8', '8', '8', '1', '1', '1', '9', '9', '2', '8', '10', '2', '8', '2', '1', '2', '9', '9', '2', '1', '1', '8', '8', '1', '3', '1', '8', '9', '1', '10', '9', '1', '1', '1', '1', '7', '2', '2', '1', '2', '8', '8', '1', '2', '1', '3', '2', '1', '8', '3', '9', '2', '10', '2', '10', '1', '2', '1', '1', '13', '9', '10', '8', '2', '2', '1', '1', '8', '2', '2', '1', '9', '1', '1', '8', '9', '9', '2', '9', '2', '2', '1', '1', '1', '8', '9', '4', '9', '9', '1', '8', '2', '1', '1', '8', '2', '2', '2', '8', '1', '1', '8', '2', '10', '2', '9', '9', '7', '1', '1', '2', '2', '8', '3', '1', '2', '1', '1', '8', '1', '1', '1', '9', '1', '1', '8', '8', '1', '9', '6', '1', '9', '1', '8', '1', '8', '8', '8', '1', '3', '8', '8', '2', '6', '3', '2', '1', '10', '1', '8', '2', '1', '6', '1', '1', '1', '8', '1', '10', '1', '1', '8', '9', '2', '6', '9', '2', '9', '4', '1', '8', '1', '3', '7', '8', '8', '5', '8', '7', '1', '1', '1', '13', '8', '3', '1', '1', '1', '1', '2', '8', '2', '12', '8', '1', '2', '5', '1', '8', '2', '10', '8', '2', '10', '1', '5', '2', '1', '1', '5', '2', '2', '7', '8', '2', '9', '9', '1', '10', '1', '8', '2', '5', '8', '1', '5', '1', '10', '1', '1', '9', '8', '6', '1', '8', '9', '8', '1', '8', '2', '2', '8', '3', '9', '1', '9', '3', '8', '4', '4', '3', '3', '1', '1', '1', '8', '9', '1', '1', '8', '3', '1', '2', '8', '2', '1', '1', '10', '1', '2', '2', '1', '1', '8', '2', '8', '2', '10', '2', '2', '10', '8', '8', '8', '13', '2', '2', '9', '2', '5', '9', '2', '1', '8', '8', '8', '1', '12', '12', '2', '9', '2', '1', '8', '2', '1', '8', '1', '9', '9', '2', '7', '1', '1', '7', '1', '4', '3', '1', '10', '3', '10', '9', '4', '6', '7', '5', '2', '1', '8', '8', '1', '4', '9', '2', '1', '8', '1', '1', '1', '1', '1', '8', '9', '8', '8', '8', '8', '9', '8', '10', '1', '8', '2', '7', '8', '9', '1', '2', '5', '9', '10', '8', '9', '12', '1', '6', '1', '2', '10', '9', '4', '1', '8', '10', '10', '1', '1', '1', '1', '1', '2', '3', '8', '1', '2', '10', '2', '1', '2', '4', '2', '2', '9', '1', '4', '2', '1', '1', '12', '12', '9', '9', '7', '1', '8', '4', '9', '9', '8', '1', '7', '3', '2', '1', '1', '4', '8', '1', '5', '8', '2', '8', '1', '14', '9', '12', '1', '10', '2', '1', '1', '8', '8', '6', '6', '3', '1', '2', '8', '1', '3', '2', '1', '8', '8', '1', '10', '8', '2', '8', '2', '5', '9', '1', '8', '8', '1', '1', '8', '8', '13', '1', '3', '8', '4', '1', '9', '7', '8', '8', '8', '3', '8', '1', '4', '2', '8', '8', '8', '7', '1', '1', '2', '8', '8', '9', '8', '8', '2', '3', '9', '9', '2', '1', '2', '2', '1', '8', '2', '6', '2', '1', '1', '9', '10', '3', '8', '2', '3', '2', '8', '8', '1', '8', '8', '8', '9', '1', '4', '1', '9', '2', '1', '8', '0', '1', '8', '1', '1', '8', '9', '9', '1', '3', '1', '2', '8', '8', '7', '1', '1', '2', '9', '2', '10', '2', '8', '2', '2', '2', '7', '1', '9', '8', '1', '3', '9', '2', '1', '10', '8', '1', '4', '1', '8', '5', '9', '8', '8', '1', '2', '2', '1', '8', '8', '6', '8', '1', '1', '8', '3', '2', '2', '1', '8', '1', '7', '6', '8', '1', '1', '1', '9', '1', '1', '1', '2', '1', '11', '10', '10', '11', '9', '']\n",
            "['8', '1', '8', '2', '8', '8', '8', '9', '7', '8', '1', '2', '1', '1', '8', '4', '8', '8', '12', '3', '3', '7', '3', '12', '1', '8', '8', '8', '8', '8', '8', '8', '1', '9', '5', '9', '9', '9', '11', '8', '8', '8', '4', '8', '8', '8', '8', '1', '3', '9', '3', '7', '1', '2', '9', '9', '7', '8', '8', '1', '10', '7', '8', '8', '9', '8', '7', '9', '9', '12', '7', '2', '8', '1', '11', '11', '1', '7', '7', '12', '1', '9', '8', '10', '12', '7', '8', '2', '8', '9', '9', '1', '8', '9', '1', '7', '12', '10', '10', '10', '8', '3', '7', '9', '8', '9', '1', '8', '8', '2', '7', '2', '9', '9', '11', '8', '8', '12', '12', '7', '8', '12', '4', '9', '3', '1', '12', '1', '1', '8', '8', '3', '8', '8', '8', '8', '9', '1', '8', '8', '10', '1', '8', '2', '8', '8', '7', '3', '8', '2', '4', '4', '9', '8', '10', '12', '12', '1', '1', '9', '1', '1', '1', '8', '2', '2', '8', '1', '1', '2', '2', '1', '2', '8', '1', '9', '9', '8', '8', '4', '2', '9', '9', '8', '3', '4', '3', '1', '8', '8', '2', '1', '9', '7', '8', '8', '1', '12', '3', '8', '2', '4', '2', '9', '12', '1', '4', '1', '8', '8', '8', '2', '2', '8', '9', '8', '8', '8', '10', '9', '8', '7', '9', '1', '1', '9', '4', '2', '4', '2', '2', '1', '7', '8', '11', '11', '3', '9', '2', '4', '8', '9', '1', '8', '1', '1', '4', '9', '1', '1', '8', '8', '2', '1', '8', '4', '2', '8', '9', '8', '8', '2', '8', '8', '8', '7', '1', '1', '1', '2', '1', '1', '8', '7', '8', '8', '12', '2', '12', '12', '8', '10', '12', '8', '3', '3', '12', '10', '1', '8', '12', '1', '8', '8', '2', '8', '4', '7', '8', '7', '10', '8', '10', '9', '8', '12', '12', '1', '8', '8', '3', '8', '8', '8', '8', '8', '1', '9', '8', '11', '1', '1', '1', '9', '8', '1', '9', '2', '3', '11', '8', '9', '9', '9', '2', '1', '8', '8', '9', '7', '1', '4', '9', '4', '8', '8', '4', '8', '12', '9', '4', '8', '2', '10', '10', '10', '8', '9', '9', '8', '8', '12', '7', '1', '8', '8', '8', '4', '1', '1', '1', '1', '1', '1', '8', '1', '9', '8', '9', '9', '4', '8', '12', '9', '8', '8', '2', '8', '8', '8', '6', '9', '8', '3', '7', '8', '8', '4', '12', '8', '8', '9', '12', '12', '9', '8', '2', '9', '2', '3', '1', '12', '8', '10', '9', '9', '9', '10', '10', '3', '8', '12', '1', '4', '2', '1', '10', '8', '2', '8', '4', '8', '9', '1', '9', '9', '10', '10', '1', '4', '9', '2', '4', '9', '1', '1', '3', '10', '3', '3', '8', '7', '3', '8', '9', '9', '12', '4', '8', '12', '2', '2', '4', '1', '9', '9', '4', '1', '4', '2', '8', '12', '2', '3', '10', '10', '9', '8', '9', '9', '1', '12', '8', '8', '8', '12', '4', '1', '8', '8', '1', '9', '8', '8', '2', '1', '8', '9', '8', '3', '3', '3', '1', '8', '8', '9', '1', '10', '9', '9', '9', '9', '5', '9', '9', '8', '8', '8', '8', '8', '8', '8', '8', '11', '12', '8', '8', '1', '8', '9', '11', '2', '2', '2', '2', '3', '1', '2', '2', '8', '2', '4', '9', '1', '2', '9', '8', '2', '8', '9', '9', '3', '10', '9', '9', '2', '8', '9', '8', '12', '12', '1', '3', '8', '8', '8', '2', '7', '7', '7', '7', '3', '9', '1', '9', '8', '9', '9', '1', '1', '1', '2', '9', '9', '9', '11', '1', '8', '8', '9', '1', '9', '8', '8', '8', '1', '1', '8', '7', '1', '1', '8', '8', '9', '4', '4', '8', '2', '2', '8', '8', '8', '8', '8', '8', '11', '8', '2', '9', '4', '9', '3', '9', '9', '1', '3', '9', '3', '1', '12', '8', '9', '12', '1', '8', '4', '2', '1', '4', '8', '3', '3', '8', '2', '8', '9', '7', '8', '8', '8', '5', '8', '3', '9', '8', '8', '13', '12', '1', '1', '2', '8', '4', '1', '9', '9', '12', '8', '9', '12', '9', '1', '9', '9', '9', '9', '3', '2', '9', '9', '4', '8', '12', '2', '4', '9', '3', '1', '9', '7', '8', '9', '9', '8', '4', '8', '8', '7', '9', '10', '3', '8', '8', '8', '1', '1', '1', '1', '8', '8', '4', '1', '10', '1', '5', '7', '7', '1', '8', '9', '3', '7', '2', '7', '7', '2', '4', '8', '12', '7', '4', '2', '9', '9', '12', '6', '10', '8', '2', '4', '12', '9', '9', '3', '8', '8', '1', '2', '10', '9', '9', '8', '4', '12', '2', '1', '8', '8', '8', '12', '10', '10', '9', '3', '8', '8', '9', '2', '8', '10', '1', '1', '1', '1', '2', '1', '1', '1', '1', '9', '8', '12', '9', '4', '8', '8', '9', '1', '9', '3', '9', '8', '8', '1', '7', '7', '10', '1', '8', '8', '1', '9', '8', '10', '3', '1', '7', '1', '8', '8', '12', '8', '8', '1', '8', '7', '1', '7', '7', '8', '2', '1', '8', '8', '2', '10', '8', '8', '8', '8', '8', '10', '1', '8', '8', '12', '8', '3', '3', '2', '2', '2', '10', '8', '8', '8', '2', '9', '1', '8', '9', '3', '2', '8', '10', '8', '6', '1', '1', '8', '4', '1', '9', '10', '8', '1', '7', '1', '2', '8', '1', '1', '1', '12', '1', '9', '12', '8', '12', '12', '12', '8', '8', '12', '4', '8', '8', '8', '8', '9', '9', '1', '3', '3', '3', '3', '1', '12', '12', '9', '10', '8', '8', '1', '9', '2', '2', '13', '9', '8', '9', '2', '1', '9', '1', '8', '8', '8', '4', '8', '1', '1', '1', '12', '12', '7', '2', '2', '2', '8', '3', '8', '9', '2', '10', '7', '8', '9', '2', '1', '2', '12', '12', '8', '8', '9', '2', '2', '9', '11', '1', '8', '1', '10', '9', '2', '1', '4', '7', '7', '7', '7', '7', '12', '8', '8', '8', '1', '1', '10', '1', '12', '1', '8', '2', '1', '1', '12', '8', '7', '9', '12', '8', '9', '3', '9', '8', '8', '8', '8', '3', '11', '2', '2', '9', '8', '8', '10', '8', '2', '7', '3', '1', '4', '7', '8', '8', '1', '8', '3', '7', '12', '8', '10', '9', '9', '8', '8', '2', '8', '9', '9', '1', '2', '8', '8', '9', '8', '3', '8', '1', '8', '10', '9', '8', '9', '9', '12', '4', '4', '8', '9', '9', '8', '2', '10', '1', '2', '8', '9', '1', '9', '9', '9', '7', '12', '12', '8', '1', '1', '1', '1', '8', '3', '1', '1', '8', '1', '8', '8', '7', '8', '8', '8', '8', '3', '2', '2', '10', '10', '10', '7', '8', '1', '2', '12', '7', '9', '8', '7', '8', '12', '2', '8', '9', '2', '6', '6', '7', '9', '8', '1', '8', '9', '8', '1', '12', '1', '2', '8', '7', '7', '7', '8', '2', '2', '8', '1', '2', '2', '9', '9', '1', '8', '8', '4', '3', '3', '1', '6', '3', '3', '12', '3', '8', '9', '1', '4', '3', '1', '8', '3', '9', '2', '8', '2', '8', '8', '8', '1', '1', '1', '9', '9', '8', '1', '9', '8', '1', '1', '3', '10', '8', '1', '1', '3', '9', '1', '4', '4', '1', '8', '9', '9', '2', '0', '0', '1', '8', '3', '1', '8', '8', '9', '8', '8', '1', '1', '8', '9', '8', '8', '8', '7', '9', '8', '8', '8', '10', '9', '8', '1', '2', '6', '1', '9', '9', '8', '12', '12', '12', '8', '8', '2', '8', '1', '2', '2', '2', '1', '9', '8', '2', '12', '2', '8', '12', '8', '9', '8', '8', '9', '7', '1', '1', '1', '1', '1', '8', '8', '1', '8', '8', '1', '1', '3', '2', '8', '8', '9', '10', '10', '2', '2', '1', '9', '2', '9', '9', '4', '12', '12', '12', '10', '7', '3', '3', '4', '2', '2', '9', '2', '8', '4', '2', '4', '1', '10', '9', '7', '8', '7', '1', '1', '3', '3', '1', '1', '3', '3', '3', '1', '1', '1', '1', '8', '2', '3', '1', '1', '2', '8', '8', '12', '8', '8', '8', '8', '11', '9', '1', '8', '9', '2', '8', '8', '8', '3', '9', '1', '9', '2', '7', '2', '8', '2', '8', '10', '8', '1', '10', '1', '1', '9', '9', '8', '8', '1', '8', '8', '8', '12', '8', '8', '8', '1', '8', '8', '8', '1', '9', '1', '1', '8', '1', '8', '9', '8', '2', '12', '9', '9', '0', '1', '8', '8', '1', '8', '12', '8', '8', '10', '8', '8', '8', '7', '8', '1', '8', '7', '3', '10', '1', '8', '9', '1', '8', '8', '8', '10', '1', '10', '3', '9', '1', '8', '9', '2', '8', '3', '3', '9', '9', '7', '9', '1', '1', '9', '2', '1', '1', '1', '7', '1', '1', '8', '8', '1', '1', '8', '1', '8', '3', '12', '9', '3', '3', '8', '8', '8', '8', '3', '1', '3', '3', '1', '11', '0', '8', '8', '7', '8', '12', '1', '8', '9', '8', '9', '8', '8', '3', '8', '8', '1', '1', '1', '9', '2', '2', '2', '8', '7', '12', '8', '8', '9', '10', '10', '7', '8', '1', '9', '8', '7', '3', '1', '3', '8', '2', '2', '3', '9', '8', '4', '4', '8', '9', '2', '1', '1', '7', '8', '9', '9', '7', '8', '7', '7', '8', '2', '2', '8', '4', '9', '7', '10', '0', '9', '8', '3', '7', '8', '1', '1', '8', '9', '9', '2', '2', '10', '1', '9', '10', '10', '10', '8', '3', '2', '12', '9', '9', '10', '12', '9', '12', '12', '9', '1', '2', '4', '12', '12', '7', '8', '9', '7', '7', '7', '3', '9', '8', '9', '1', '12', '8', '9', '4', '1', '3', '12', '12', '12', '12', '8', '8', '2', '1', '1', '2', '1', '1', '1', '12', '12', '8', '12', '2', '2', '12', '3', '3', '12', '8', '2', '8', '8', '12', '2', '1', '10', '3', '2', '8', '7', '1', '8', '1', '3', '7', '8', '9', '8', '3', '1', '1', '7', '8', '8', '9', '8', '2', '9', '2', '2', '9', '8', '1', '8', '8', '1', '3', '3', '1', '1', '10', '1', '2', '8', '1', '1', '1', '1', '9', '1', '4', '1', '7', '7', '7', '7', '2', '2', '8', '8', '12', '1', '9', '1', '7', '3', '3', '1', '8', '8', '10', '8', '9', '2', '9', '1', '3', '8', '8', '3', '12', '2', '8', '12', '2', '9', '1', '3', '3', '3', '3', '2', '8', '7', '9', '8', '3', '3', '1', '7', '8', '3', '1', '1', '12', '8', '9', '1', '2', '3', '8', '1', '1', '3', '3', '9', '1', '1', '1', '12', '1', '7', '3', '3', '1', '8', '8', '8', '1', '2', '4', '8', '1', '10', '2', '5', '3', '3', '12', '10', '9', '9', '12', '9', '0', '2', '8', '8', '9', '9', '9', '8', '1', '3', '1', '1', '4', '8', '1', '10', '8', '7', '2', '8', '2', '8', '4', '7', '8', '1', '9', '1', '9', '8', '2', '8', '2', '7', '9', '2', '2', '9', '1', '8', '12', '1', '8', '1', '4', '1', '9', '9', '1', '10', '12', '4', '8', '1', '7', '3', '9', '2', '12', '7', '8', '8', '2', '1', '12', '9', '8', '1', '2', '2', '8', '10', '2', '1', '7', '7', '7', '12', '3', '3', '8', '3', '8', '8', '3', '9', '8', '9', '1', '1', '8', '7', '9', '3', '3', '8', '1', '0', '9', '9', '9', '1', '8', '9', '9', '10', '1', '8', '7', '8', '8', '8', '9', '9', '8', '9', '10', '4', '9', '3', '7', '12', '1', '9', '9', '8', '9', '1', '9', '3', '1', '8', '4', '12', '10', '9', '8', '7', '10', '8', '12', '12', '3', '10', '8', '12', '2', '1', '2', '3', '9', '8', '8', '7', '1', '1', '2', '2', '1', '2', '2', '7', '1', '3', '9', '9', '3', '8', '8', '8', '8', '8', '7', '8', '8', '10', '8', '1', '8', '2', '8', '2', '2', '2', '2', '2', '1', '7', '12', '10', '1', '2', '8', '1', '4', '7', '8', '1', '12', '8', '7', '9', '2', '2', '2', '8', '1', '8', '1', '1', '0', '2', '2', '6', '1', '8', '2', '1', '8', '1', '1', '2', '2', '3', '2', '2', '10', '10', '9', '1', '7', '7', '8', '8', '1', '12', '10', '12', '3', '8', '8', '8', '3', '8', '3', '10', '2', '2', '2', '1', '2', '2', '1', '0', '1', '8', '9', '1', '1', '1', '2', '2', '2', '2', '2', '1', '1', '1', '1', '12', '9', '9', '1', '10', '11', '12', '9', '1', '1', '8', '8', '1', '2', '12', '8', '3', '7', '2', '10', '9', '7', '8', '1', '2', '2', '7', '9', '12', '2', '2', '1', '8', '9', '2', '3', '2', '2', '2', '9', '9', '8', '9', '2', '3', '8', '8', '9', '7', '3', '10', '8', '8', '9', '1', '1', '2', '8', '1', '10', '8', '8', '9', '7', '1', '7', '12', '8', '1', '7', '2', '1', '3', '9', '2', '8', '8', '1', '2', '3', '9', '3', '8', '9', '7', '7', '1', '10', '2', '8', '8', '8', '1', '8', '2', '8', '8', '1', '2', '8', '10', '2', '1', '0', '8', '3', '8', '10', '10', '12', '3', '8', '9', '3', '10', '8', '8', '8', '1', '8', '8', '2', '2', '2', '2', '2', '2', '1', '1', '10', '1', '10', '8', '2', '1', '2', '2', '8', '3', '3', '2', '2', '1', '8', '1', '3', '1', '9', '2', '2', '1', '3', '2', '2', '3', '2', '2', '3', '2', '1', '1', '2', '1', '2', '7', '8', '2', '1', '12', '12', '3', '1', '9', '9', '7', '8', '7', '2', '7', '9', '2', '2', '2', '8', '9', '7', '9', '2', '8', '9', '9', '2', '9', '1', '1', '9', '8', '3', '3', '8', '2', '7', '8', '11', '2', '8', '9', '1', '8', '3', '1', '2', '1', '2', '2', '2', '2', '8', '3', '9', '2', '10', '9', '9', '2', '7', '7', '7', '1', '3', '8', '8', '1', '1', '8', '7', '1', '8', '8', '9', '3', '3', '3', '2', '4', '8', '12', '8', '1', '12', '9', '2', '8', '2', '12', '2', '1', '3', '8', '12', '12', '11', '8', '10', '9', '2', '9', '4', '10', '1', '12', '12', '9', '3', '8', '9', '12', '9', '9', '2', '2', '2', '3', '5', '1', '1', '1', '7', '7', '8', '2', '2', '8', '2', '1', '7', '3', '2', '9', '8', '9', '1', '1', '9', '8', '9', '3', '3', '2', '9', '4', '9', '8', '8', '1', '8', '10', '2', '0', '3', '8', '8', '8', '9', '12', '9', '8', '2', '2', '8', '10', '9', '8', '2', '2', '4', '1', '9', '9', '8', '8', '10', '3', '1', '1', '2', '8', '7', '8', '2', '1', '1', '8', '12', '2', '9', '4', '8', '3', '3', '3', '8', '2', '12', '12', '8', '9', '12', '8', '2', '12', '7', '9', '2', '2', '1', '3', '1', '3', '11', '9', '8', '1', '10', '2', '12', '3', '8', '4', '1', '2', '8', '8', '3', '9', '3', '8', '7', '1', '1', '3', '1', '8', '2', '9', '8', '12', '1', '1', '1', '9', '9', '1', '8', '8', '8', '2', '2', '12', '3', '1', '1', '1', '9', '9', '3', '2', '1', '1', '9', '9', '1', '9', '9', '1', '9', '3', '9', '1', '8', '1', '9', '8', '3', '8', '8', '9', '9', '8', '2', '8', '1', '2', '2', '10', '1', '2', '1', '1', '9', '1', '9', '3', '7', '7', '2', '10', '3', '3', '1', '1', '9', '1', '2', '1', '3', '2', '1', '1', '1', '1', '1', '1', '1', '1', '1', '2', '8', '8', '7', '1', '1', '8', '8', '2', '1', '12', '1', '8', '1', '9', '8', '8', '2', '1', '9', '9', '2', '2', '8', '1', '7', '7', '9', '8', '1', '6', '8', '8', '2', '2', '8', '3', '8', '3', '2', '3', '8', '1', '2', '2', '2', '8', '9', '9', '9', '12', '1', '9', '2', '1', '9', '2', '9', '9', '10', '8', '1', '1', '9', '2', '1', '7', '1', '3', '7', '1', '1', '1', '3', '8', '8', '8', '9', '9', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '1', '0', '1', '1', '1', '2', '2', '3', '1', '1', '2', '3', '3', '3', '7', '3', '1', '9', '3', '9', '1', '4', '1', '1', '9', '10', '9', '8', '2', '3', '10', '9', '3', '10', '12', '8', '9', '8', '2', '1', '9', '8', '1', '7', '8', '1', '9', '1', '2', '10', '7', '7', '8', '1', '3', '3', '9', '3', '3', '1', '1', '1', '9', '1', '3', '8', '8', '8', '1', '8', '1', '9', '7', '8', '8', '2', '1', '3', '8', '2', '1', '6', '1', '8', '8', '2', '8', '2', '2', '9', '8', '1', '1', '6', '8', '6', '10', '2', '1', '1', '8', '9', '3', '3', '3', '1', '3', '9', '6', '8', '1', '8', '3', '1', '2', '2', '12', '7', '1', '1', '1', '1', '12', '1', '8', '8', '3', '2', '1', '9', '2', '3', '3', '2', '2', '7', '2', '2', '2', '3', '1', '9', '8', '7', '1', '1', '3', '1', '9', '4', '3', '9', '2', '1', '1', '9', '8', '8', '7', '2', '1', '3', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '2', '10', '1', '1', '8', '2', '8', '1', '8', '1', '3', '1', '2', '1', '1', '3', '1', '2', '1', '1', '1', '1', '1', '1', '1', '2', '1', '8', '1', '9', '8', '8', '1', '10', '8', '8', '1', '3', '9', '8', '9', '8', '2', '1', '3', '4', '1', '12', '1', '9', '8', '2', '7', '2', '1', '2', '9', '2', '1', '3', '8', '1', '2', '3', '8', '2', '10', '10', '10', '9', '9', '9', '3', '8', '3', '1', '1', '1', '1', '1', '1', '2', '1', '9', '2', '7', '9', '8', '7', '9', '1', '1', '1', '1', '8', '2', '2', '3', '3', '2', '12', '8', '3', '1', '1', '12', '9', '8', '2', '2', '9', '1', '2', '2', '9', '1', '1', '2', '4', '8', '1', '8', '10', '2', '9', '9', '9', '1', '2', '1', '1', '2', '2', '2', '12', '1', '2', '8', '3', '9', '9', '3', '2', '9', '9', '7', '2', '8', '8', '9', '2', '9', '1', '1', '1', '1', '1', '2', '8', '2', '1', '2', '2', '8', '3', '1', '1', '9', '1', '3', '10', '7', '9', '8', '8', '2', '2', '9', '2', '2', '2', '2', '2', '2', '9', '1', '3', '6', '4', '2', '2', '9', '1', '9', '8', '1', '1', '1', '7', '3', '2', '9', '9', '1', '9', '7', '2', '8', '9', '9', '12', '10', '8', '10', '2', '3', '1', '2', '2', '12', '12', '3', '12', '1', '2', '1', '2', '1', '2', '8', '2', '3', '12', '12', '8', '2', '2', '9', '2', '9', '2', '3', '3', '1', '1', '1', '12', '3', '7', '1', '3', '1', '2', '9', '2', '7', '2', '1', '8', '3', '7', '8', '1', '1', '3', '8', '3', '2', '9', '3', '1', '1', '1', '1', '11', '1', '2', '6', '1', '2', '3', '8', '9', '9', '2', '9', '2', '1', '1', '3', '1', '12', '9', '1', '1', '9', '1', '9', '9', '8', '9', '8', '1', '2', '7', '1', '0', '9', '8', '2', '3', '2', '4', '1', '1', '1', '8', '1', '12', '1', '2', '3', '3', '9', '9', '9', '9', '9', '7', '9', '3', '9', '1', '7', '3', '3', '3', '3', '7', '8', '2', '2', '2', '8', '2', '3', '9', '9', '3', '1', '8', '9', '12', '8', '8', '3', '2', '1', '6', '1', '9', '1', '1', '1', '1', '1', '1', '2', '2', '1', '2', '2', '2', '2', '9', '2', '5', '3', '2', '2', '8', '2', '1', '8', '3', '3', '8', '3', '1', '3', '9', '2', '8', '9', '4', '4', '9', '8', '8', '7', '7', '3', '1', '8', '8', '3', '2', '2', '10', '3', '3', '2', '2', '2', '2', '12', '3', '1', '2', '7', '7', '12', '2', '1', '2', '1', '1', '2', '1', '3', '3', '3', '1', '3', '9', '4', '8', '8', '4', '2', '4', '2', '8', '9', '1', '1', '2', '2', '2', '8', '7', '2', '8', '2', '9', '1', '8', '1', '1', '2', '1', '1', '1', '2', '9', '9', '8', '1', '8', '1', '4', '8', '1', '2', '7', '9', '2', '1', '8', '9', '8', '9', '1', '1', '2', '3', '2', '12', '7', '1', '2', '4', '4', '4', '2', '2', '2', '9', '8', '8', '1', '12', '3', '3', '2', '2', '3', '12', '1', '5', '1', '1', '3', '2', '8', '8', '2', '2', '2', '3', '9', '8', '8', '9', '8', '8', '11', '8', '2', '9', '2', '3', '1', '7', '1', '8', '2', '8', '1', '1', '1', '8', '1', '1', '1', '7', '8', '8', '2', '3', '2', '1', '8', '8', '1', '8', '4', '8', '9', '8', '9', '8', '1', '2', '3', '2', '1', '2', '2', '3', '1', '9', '4', '8', '1', '1', '9', '1', '1', '8', '2', '2', '2', '1', '7', '3', '3', '1', '3', '3', '12', '3', '1', '1', '3', '1', '1', '4', '8', '4', '4', '3', '3', '3', '4', '8', '1', '9', '9', '9', '4', '1', '7', '4', '8', '4', '9', '8', '1', '1', '3', '8', '9', '1', '9', '7', '1', '1', '9', '1', '8', '8', '8', '1', '1', '8', '2', '2', '2', '9', '4', '4', '1', '2', '1', '2', '9', '1', '1', '8', '5', '5', '5', '9', '12', '4', '1', '2', '1', '8', '8', '1', '9', '9', '2', '12', '1', '1', '7', '8', '8', '4', '1', '2', '8', '11', '2', '3', '2', '2', '1', '11', '2', '2', '2', '2', '9', '9', '2', '2', '9', '3', '1', '9', '1', '1', '1', '7', '8', '1', '8', '9', '1', '2', '9', '8', '1', '2', '9', '8', '12', '2', '2', '10', '8', '2', '2', '8', '8', '1', '2', '6', '1', '1', '7', '7', '2', '3', '2', '1', '1', '8', '1', '10', '8', '12', '6', '4', '1', '4', '1', '2', '9', '8', '9', '4', '8', '8', '10', '8', '8', '8', '8', '8', '2', '2', '8', '4', '9', '3', '3', '3', '3', '3', '9', '2', '1', '1', '2', '3', '2', '1', '2', '3', '3', '3', '2', '2', '3', '3', '2', '1', '2', '3', '3', '3', '3', '12', '9', '3', '2', '2', '3', '1', '5', '1', '1', '3', '2', '2', '1', '2', '9', '3', '9', '8', '1', '8', '7', '7', '9', '1', '1', '7', '9', '10', '1', '7', '1', '4', '3', '8', '9', '4', '9', '9', '2', '9', '2', '8', '4', '2', '2', '9', '5', '2', '1', '10', '9', '3', '1', '1', '1', '2', '1', '2', '9', '1', '7', '9', '9', '8', '2', '2', '8', '9', '1', '9', '8', '2', '3', '2', '2', '1', '2', '2', '2', '1', '1', '8', '1', '9', '4', '12', '8', '2', '7', '2', '2', '1', '9', '3', '1', '8', '12', '1', '1', '2', '1', '10', '2', '6', '12', '12', '4', '9', '1', '9', '10', '1', '2', '8', '2', '1', '8', '6', '12', '9', '9', '2', '1', '7', '8', '9', '5', '8', '1', '8', '8', '3', '1', '8', '2', '8', '2', '2', '1', '2', '9', '2', '8', '1', '12', '8', '2', '7', '3', '3', '12', '2', '3', '3', '9', '9', '3', '3', '3', '3', '3', '2', '1', '4', '1', '4', '8', '8', '2', '1', '2', '9', '12', '1', '10', '12', '1', '2', '9', '4', '8', '10', '3', '2', '8', '7', '4', '8', '9', '2', '7', '1', '10', '2', '2', '2', '8', '4', '2', '2', '1', '1', '8', '7', '9', '2', '1', '9', '8', '9', '12', '4', '8', '2', '8', '7', '7', '1', '9', '2', '1', '1', '9', '1', '8', '2', '3', '9', '9', '1', '3', '2', '9', '2', '2', '8', '1', '1', '2', '1', '2', '1', '9', '8', '2', '2', '4', '8', '8', '9', '5', '5', '9', '8', '9', '6', '2', '1', '12', '2', '3', '8', '8', '9', '2', '2', '1', '3', '2', '10', '9', '10', '7', '2', '8', '4', '2', '8', '9', '8', '1', '3', '8', '8', '1', '8', '3', '8', '8', '1', '10', '3', '1', '5', '8', '8', '9', '2', '2', '9', '2', '1', '9', '1', '4', '1', '12', '8', '8', '2', '1', '1', '1', '1', '1', '9', '2', '9', '8', '9', '5', '9', '1', '8', '2', '4', '1', '1', '1', '9', '12', '4', '1', '1', '9', '8', '4', '1', '8', '9', '8', '8', '9', '9', '9', '9', '9', '9', '1', '3', '4', '2', '2', '8', '2', '9', '2', '3', '8', '3', '1', '7', '2', '1', '1', '4', '2', '9', '2', '1', '3', '2', '9', '2', '9', '3', '8', '1', '9', '1', '1', '2', '8', '8', '8', '2', '3', '9', '9', '8', '2', '4', '1', '5', '1', '1', '9', '9', '2', '1', '1', '1', '9', '9', '1', '3', '9', '2', '2', '9', '9', '8', '3', '9', '9', '8', '2', '2', '8', '9', '2', '2', '8', '9', '10', '10', '2', '8', '7', '9', '9', '4', '2', '4', '11', '2', '1', '7', '8', '8', '8', '9', '4', '9', '8', '8', '2', '1', '1', '9', '8', '3', '3', '8', '8', '7', '9', '1', '3', '4', '8', '10', '2', '4', '4', '8', '2', '8', '2', '8', '9', '2', '1', '2', '2', '3', '1', '9', '4', '5', '9', '9', '1', '1', '1', '1', '1', '1', '7', '1', '1', '1', '8', '1', '1', '1', '1', '1', '9', '1', '2', '1', '12', '9', '3', '9', '9', '4', '1', '9', '1', '1', '2', '11', '9', '3', '2', '2', '2', '4', '2', '3', '2', '7', '7', '8', '1', '10', '9', '5', '7', '1', '10', '8', '1', '2', '7', '1', '12', '5', '8', '4', '2', '9', '9', '8', '2', '9', '8', '8', '10', '2', '3', '2', '7', '2', '8', '10', '3', '2', '9', '9', '9', '1', '1', '1', '1', '8', '2', '10', '9', '8', '1', '2', '2', '2', '1', '2', '1', '3', '9', '12', '2', '2', '4', '8', '1', '3', '9', '4', '9', '1', '9', '9', '2', '1', '1', '9', '7', '10', '3', '10', '2', '9', '2', '9', '2', '8', '2', '1', '2', '3', '1', '1', '1', '8', '5', '8', '3', '1', '1', '4', '2', '2', '1', '3', '8', '2', '1', '1', '1', '2', '9', '4', '1', '8', '1', '2', '8', '2', '9', '2', '2', '5', '5', '2', '5', '2', '1', '8', '8', '1', '3', '12', '2', '4', '3', '2', '2', '2', '6', '2', '3', '3', '1', '8', '2', '2', '1', '1', '8', '8', '10', '1', '9', '2', '9', '12', '0', '2', '1', '3', '2', '2', '1', '2', '1', '9', '9', '2', '9', '8', '8', '1', '7', '8', '1', '2', '6', '9', '8', '10', '1', '12', '2', '2', '1', '8', '2', '1', '12', '7', '4', '2', '10', '2', '2', '1', '2', '1', '2', '1', '1', '2', '1', '9', '8', '10', '2', '10', '9', '9', '12', '5', '3', '2', '8', '2', '8', '3', '3', '1', '4', '9', '10', '2', '4', '8', '1', '2', '10', '12', '12', '3', '1', '1', '2', '9', '6', '6', '1', '9', '1', '8', '1', '8', '8', '8', '8', '2', '2', '2', '8', '8', '2', '1', '1', '1', '1', '1', '8', '8', '5', '8', '8', '1', '8', '9', '9', '1', '7', '12', '9', '9', '7', '2', '2', '7', '8', '9', '8', '2', '9', '6', '3', '4', '8', '4', '1', '2', '9', '8', '8', '8', '2', '1', '1', '8', '8', '3', '1', '9', '9', '7', '1', '2', '2', '2', '9', '4', '1', '2', '12', '1', '8', '2', '1', '9', '1', '8', '9', '9', '2', '5', '4', '2', '1', '2', '8', '12', '8', '10', '3', '8', '8', '8', '2', '2', '8', '2', '8', '2', '4', '8', '10', '2', '7', '2', '1', '8', '8', '9', '5', '8', '1', '12', '7', '10', '2', '2', '2', '1', '4', '8', '12', '1', '8', '2', '2', '9', '1', '8', '3', '1', '1', '5', '10', '2', '1', '2', '4', '8', '9', '3', '2', '8', '7', '4', '4', '9', '9', '2', '8', '1', '1', '2', '4', '7', '1', '9', '1', '1', '1', '8', '4', '1', '3', '1', '2', '2', '9', '1', '8', '8', '2', '2', '9', '9', '1', '8', '8', '1', '9', '8', '8', '2', '2', '2', '2', '1', '1', '1', '7', '4', '1', '1', '4', '2', '3', '3', '2', '3', '9', '2', '8', '2', '1', '5', '1', '2', '2', '1', '3', '5', '2', '9', '1', '1', '9', '8', '1', '4', '8', '1', '8', '2', '4', '8', '4', '7', '9', '8', '1', '2', '2', '4', '4', '11', '3', '11', '1', '3', '8', '1', '9', '6', '8', '9', '3', '9', '8', '9', '2', '3', '3', '7', '1', '7', '2', '9', '9', '1', '8', '8', '1', '5', '5', '8', '8', '1', '2', '1', '3', '9', '9', '5', '9', '8', '1', '4', '3', '8', '1', '1', '2', '1', '8', '9', '8', '2', '2', '2', '1', '2', '4', '10', '8', '1', '9', '1', '8', '1', '1', '9', '9', '8', '11', '1', '8', '2', '1', '2', '8', '2', '8', '8', '2', '9', '6', '10', '1', '8', '6', '4', '5', '11', '2', '2', '8', '1', '8', '1', '8', '1', '4', '8', '1', '1', '1', '8', '3', '7', '3', '3', '7', '1', '1', '4', '8', '1', '6', '4', '2', '2', '1', '1', '1', '1', '6', '2', '2', '8', '1', '8', '5', '5', '2', '1', '1', '2', '3', '7', '1', '1', '6', '2', '9', '3', '9', '8', '9', '2', '1', '4', '2', '9', '13', '9', '2', '8', '12', '4', '1', '1', '9', '5', '1', '1', '1', '1', '8', '2', '1', '1', '2', '9', '0', '12', '1', '1', '8', '9', '8', '3', '2', '2', '12', '8', '2', '2', '2', '1', '10', '1', '9', '8', '9', '5', '2', '2', '9', '2', '8', '4', '8', '3', '7', '9', '2', '2', '7', '2', '2', '8', '1', '1', '9', '10', '9', '8', '2', '9', '0', '7', '1', '1', '1', '9', '8', '2', '8', '12', '8', '3', '2', '8', '8', '7', '7', '8', '12', '2', '1', '3', '9', '2', '2', '8', '2', '8', '8', '12', '8', '8', '1', '8', '9', '2', '4', '4', '7', '2', '1', '1', '9', '3', '7', '1', '3', '9', '8', '2', '2', '3', '1', '3', '10', '2', '3', '7', '1', '8', '1', '1', '10', '3', '10', '8', '4', '1', '2', '4', '3', '10', '9', '3', '9', '5', '8', '7', '2', '9', '3', '3', '2', '11', '8', '1', '1', '7', '2', '7', '2', '9', '4', '8', '1', '12', '8', '7', '9', '1', '0', '2', '6', '4', '9', '3', '2', '4', '12', '9', '8', '5', '9', '2', '8', '4', '6', '8', '9', '3', '1', '8', '7', '3', '1', '1', '1', '8', '9', '1', '8', '7', '9', '8', '2', '1', '2', '2', '3', '2', '2', '1', '1', '4', '9', '7', '3', '2', '8', '8', '9', '1', '9', '7', '4', '2', '2', '8', '5', '9', '1', '5', '7', '1', '4', '6', '9', '10', '1', '8', '2', '9', '1', '9', '2', '9', '8', '2', '9', '9', '11', '2', '12', '2', '2', '9', '4', '8', '1', '9', '9', '2', '8', '9', '1', '2', '9', '3', '8', '8', '1', '7', '8', '8', '2', '3', '2', '2', '2', '2', '9', '8', '10', '2', '2', '1', '9', '11', '1', '8', '8', '2', '4', '2', '1', '2', '8', '8', '9', '2', '8', '9', '2', '3', '1', '2', '1', '3', '10', '8', '1', '2', '4', '1', '9', '8', '9', '3', '3', '9', '8', '2', '11', '3', '7', '1', '8', '7', '1', '8', '8', '8', '1', '4', '8', '8', '1', '9', '3', '1', '1', '2', '8', '8', '8', '2', '1', '8', '10', '2', '12', '8', '8', '9', '1', '8', '1', '3', '2', '2', '10', '7', '2', '2', '1', '1', '8', '2', '9', '1', '2', '9', '3', '3', '10', '1', '4', '7', '10', '12', '2', '4', '10', '8', '6', '2', '9', '1', '8', '3', '1', '2', '8', '8', '2', '12', '10', '2', '10', '9', '1', '2', '5', '8', '12', '8', '8', '7', '8', '8', '1', '8', '2', '2', '9', '1', '8', '10', '5', '5', '5', '8', '1', '9', '1', '13', '1', '2', '2', '1', '2', '4', '9', '2', '1', '9', '13', '11', '1', '9', '9', '3', '10', '8', '7', '8', '8', '4', '2', '1', '9', '4', '3', '1', '1', '10', '9', '2', '8', '6', '2', '1', '1', '3', '2', '8', '1', '1', '1', '1', '2', '4', '1', '1', '10', '1', '8', '1', '1', '9', '1', '7', '1', '1', '9', '9', '2', '12', '9', '1', '8', '1', '12', '1', '8', '3', '8', '1', '8', '1', '2', '10', '1', '1', '2', '2', '9', '3', '9', '12', '1', '9', '1', '2', '8', '7', '2', '1', '1', '3', '2', '8', '4', '4', '5', '12', '7', '8', '9', '6', '1', '8', '9', '2', '1', '9', '1', '4', '1', '8', '2', '1', '1', '8', '7', '4', '9', '1', '8', '4', '2', '7', '5', '1', '3', '9', '2', '8', '2', '1', '2', '9', '1', '1', '2', '3', '4', '3', '1', '2', '2', '10', '2', '2', '8', '1', '1', '2', '4', '9', '2', '11', '9', '8', '9', '2', '1', '10', '4', '1', '1', '10', '7', '2', '10', '8', '1', '10', '8', '8', '8', '8', '8', '8', '2', '2', '2', '8', '3', '4', '1', '1', '1', '2', '1', '8', '8', '2', '8', '3', '8', '3', '1', '2', '3', '1', '1', '10', '1', '4', '4', '3', '3', '1', '9', '1', '1', '2', '1', '1', '6', '1', '1', '1', '9', '1', '4', '1', '1', '1', '1', '1', '1', '2', '12', '8', '8', '1', '9', '12', '10', '8', '2', '12', '1', '1', '2', '1', '7', '10', '7', '1', '2', '1', '10', '8', '2', '2', '8', '2', '2', '1', '9', '8', '8', '8', '8', '3', '1', '4', '2', '8', '3', '10', '10', '1', '9', '1', '9', '1', '9', '1', '8', '3', '1', '1', '8', '8', '8', '6', '3', '4', '1', '1', '1', '9', '9', '5', '2', '10', '2', '6', '2', '2', '7', '1', '8', '2', '11', '1', '1', '1', '2', '1', '9', '4', '4', '2', '8', '4', '6', '8', '8', '10', '10', '2', '1', '2', '9', '1', '8', '8', '3', '4', '8', '8', '2', '8', '8', '1', '2', '1', '2', '9', '4', '1', '3', '3', '3', '8', '8', '8', '2', '6', '4', '3', '3', '12', '3', '4', '9', '6', '8', '7', '7', '8', '6', '4', '1', '10', '6', '3', '3', '8', '1', '1', '8', '1', '11', '8', '1', '1', '2', '3', '11', '7', '2', '1', '1', '1', '9', '1', '2', '1', '1', '9', '8', '1', '1', '8', '0', '4', '1', '1', '2', '1', '1', '2', '2', '9', '1', '10', '1', '3', '8', '9', '3', '3', '1', '10', '1', '9', '1', '1', '7', '4', '1', '8', '10', '1', '2', '10', '1', '1', '2', '3', '3', '9', '1', '8', '10', '1', '7', '9', '1', '9', '6', '3', '9', '2', '8', '4', '12', '12', '3', '1', '1', '9', '1', '9', '2', '1', '1', '1', '1', '1', '9', '2', '1', '2', '10', '9', '8', '8', '10', '8', '2', '3', '2', '1', '12', '8', '12', '2', '9', '1', '3', '2', '5', '2', '2', '8', '2', '2', '4', '10', '9', '10', '9', '4', '2', '4', '12', '8', '2', '1', '8', '3', '9', '2', '9', '4', '1', '1', '1', '1', '1', '9', '6', '2', '9', '2', '1', '3', '2', '2', '5', '8', '9', '1', '6', '3', '1', '2', '2', '2', '6', '1', '8', '10', '8', '3', '3', '13', '2', '9', '9', '10', '6', '12', '8', '2', '9', '2', '3', '8', '8', '1', '2', '2', '3', '2', '10', '1', '8', '9', '1', '8', '9', '8', '8', '2', '10', '2', '1', '1', '1', '9', '9', '12', '1', '1', '4', '8', '3', '2', '2', '4', '4', '2', '2', '1', '1', '1', '9', '2', '2', '4', '1', '1', '8', '8', '8', '9', '2', '13', '9', '9', '4', '1', '9', '7', '10', '10', '8', '1', '9', '1', '1', '1', '8', '12', '4', '1', '2', '1', '1', '1', '7', '8', '3', '3', '1', '2', '2', '7', '2', '2', '10', '1', '8', '4', '0', '1', '9', '4', '1', '1', '2', '10', '10', '7', '9', '3', '12', '11', '2', '8', '10', '8', '9', '4', '3', '4', '10', '1', '1', '9', '3', '10', '1', '5', '3', '3', '2', '2', '1', '1', '6', '1', '7', '1', '1', '12', '1', '1', '1', '10', '1', '8', '8', '1', '3', '8', '3', '1', '2', '8', '3', '4', '2', '8', '8', '6', '1', '1', '4', '1', '2', '9', '5', '1', '7', '8', '9', '2', '4', '9', '8', '5', '9', '9', '9', '1', '2', '1', '3', '8', '9', '2', '9', '8', '9', '1', '8', '13', '9', '3', '8', '1', '4', '1', '3', '1', '6', '4', '10', '2', '2', '7', '8', '8', '12', '8', '9', '9', '9', '10', '3', '12', '12', '2', '9', '10', '9', '7', '2', '3', '4', '10', '10', '2', '7', '7', '1', '7', '3', '9', '1', '9', '8', '2', '9', '1', '8', '1', '10', '8', '8', '2', '7', '8', '1', '8', '9', '8', '1', '4', '1', '8', '8', '8', '1', '1', '10', '3', '2', '2', '6', '8', '9', '8', '2', '1', '1', '3', '8', '8', '1', '4', '4', '3', '9', '10', '4', '2', '4', '2', '9', '2', '2', '9', '1', '1', '10', '1', '1', '2', '1', '1', '1', '9', '1', '8', '8', '8', '2', '3', '8', '1', '6', '7', '3', '1', '9', '1', '2', '1', '6', '1', '2', '10', '1', '4', '2', '3', '2', '9', '9', '4', '9', '9', '1', '2', '8', '4', '7', '8', '7', '13', '9', '1', '2', '1', '9', '3', '3', '6', '8', '9', '10', '9', '2', '2', '3', '12', '1', '1', '1', '13', '2', '1', '7', '10', '8', '9', '1', '2', '9', '1', '1', '1', '2', '1', '12', '5', '6', '9', '10', '3', '1', '10', '1', '3', '2', '8', '9', '2', '10', '8', '8', '9', '2', '8', '9', '2', '2', '2', '8', '8', '2', '3', '1', '4', '8', '1', '9', '9', '2', '8', '10', '9', '1', '2', '12', '8', '8', '0', '2', '1', '1', '9', '10', '1', '6', '2', '2', '10', '2', '2', '4', '2', '2', '12', '1', '6', '7', '8', '9', '1', '3', '5', '7', '3', '1', '3', '1', '1', '3', '2', '6', '3', '2', '1', '8', '3', '5', '2', '1', '1', '8', '1', '1', '0', '2', '3', '5', '3', '1', '1', '9', '8', '8', '8', '8', '4', '7', '2', '9', '12', '5', '9', '3', '12', '3', '9', '1', '9', '1', '7', '3', '9', '3', '10', '1', '2', '1', '4', '9', '8', '2', '9', '11', '2', '8', '9', '4', '1', '1', '1', '1', '1', '1', '1', '9', '1', '9', '1', '1', '1', '12', '3', '10', '3', '6', '1', '7', '10', '8', '3', '7', '1', '1', '2', '6', '1', '3', '9', '2', '8', '1', '8', '8', '1', '8', '10', '12', '1', '10', '12', '10', '1', '8', '8', '1', '4', '7', '9', '2', '1', '9', '3', '8', '12', '10', '6', '1', '6', '8', '3', '1', '3', '1', '10', '10', '9', '8', '1', '2', '9', '8', '8', '1', '9', '8', '8', '3', '1', '3', '9', '12', '1', '8', '1', '5', '8', '11', '5', '5', '1', '2', '1', '3', '1', '1', '1', '9', '8', '8', '1', '9', '10', '8', '4', '1', '1', '8', '10', '8', '9', '1', '10', '1', '7', '2', '1', '1', '9', '6', '2', '9', '8', '1', '1', '1', '9', '8', '8', '1', '9', '9', '1', '1', '2', '7', '9', '2', '9', '9', '8', '7', '6', '8', '7', '8', '2', '9', '2', '1', '8', '8', '8', '2', '6', '3', '1', '8', '12', '12', '8', '7', '1', '2', '2', '7', '1', '9', '8', '1', '4', '1', '1', '8', '5', '8', '1', '8', '8', '8', '8', '1', '2', '11', '1', '1', '1', '2', '7', '1', '9', '2', '2', '2', '9', '8', '9', '2', '9', '1', '8', '1', '7', '11', '13', '1', '9', '8', '2', '2', '1', '2', '3', '10', '3', '10', '4', '3', '9', '1', '8', '1', '1', '13', '1', '1', '3', '8', '1', '2', '2', '2', '9', '4', '1', '8', '1', '9', '3', '6', '8', '5', '2', '8', '2', '8', '1', '9', '2', '2', '8', '1', '1', '4', '8', '9', '2', '2', '7', '1', '8', '12', '2', '12', '8', '2', '9', '9', '3', '8', '1', '1', '9', '9', '9', '2', '8', '1', '9', '1', '8', '9', '8', '2', '2', '4', '1', '9', '10', '10', '8', '8', '1', '1', '2', '1', '4', '1', '2', '1', '1', '3', '12', '8', '8', '8', '8', '10', '2', '8', '12', '1', '9', '9', '8', '9', '1', '2', '1', '8', '8', '8', '2', '8', '8', '3', '10', '8', '9', '1', '1', '1', '3', '4', '8', '10', '6', '3', '1', '3', '2', '8', '9', '3', '5', '4', '1', '5', '2', '9', '9', '9', '1', '1', '1', '9', '9', '6', '10', '9', '8', '12', '2', '9', '1', '2', '1', '1', '1', '8', '1', '12', '1', '12', '1', '9', '1', '8', '1', '2', '1', '2', '8', '8', '10', '1', '9', '1', '2', '2', '8', '8', '3', '12', '1', '11', '2', '10', '12', '11', '2', '1', '10', '1', '1', '9', '3', '10', '2', '7', '1', '8', '1', '9', '9', '2', '1', '3', '12', '5', '9', '9', '8', '1', '8', '8', '8', '1', '1', '3', '9', '8', '9', '8', '3', '10', '3', '7', '8', '9', '2', '3', '1', '9', '10', '9', '2', '8', '2', '6', '4', '1', '1', '3', '8', '2', '3', '9', '1', '2', '1', '8', '1', '2', '2', '2', '2', '9', '9', '4', '8', '9', '1', '1', '2', '9', '1', '5', '2', '9', '8', '10', '1', '2', '10', '8', '5', '6', '4', '8', '1', '1', '1', '8', '9', '2', '8', '8', '8', '12', '2', '2', '1', '8', '1', '2', '1', '9', '8', '8', '9', '1', '1', '8', '7', '1', '8', '0', '2', '3', '8', '1', '1', '8', '1', '1', '12', '3', '2', '10', '8', '10', '6', '1', '8', '8', '10', '8', '8', '1', '4', '8', '1', '2', '9', '8', '1', '1', '3', '3', '5', '1', '1', '2', '1', '2', '1', '9', '8', '3', '9', '8', '2', '10', '9', '1', '8', '1', '10', '9', '10', '1', '2', '3', '8', '1', '3', '1', '9', '9', '8', '1', '9', '8', '8', '9', '1', '9', '2', '8', '8', '2', '13', '8', '10', '8', '9', '3', '2', '1', '3', '1', '12', '10', '10', '1', '11', '1', '2', '4', '2', '10', '1', '8', '11', '1', '1', '1', '2', '2', '8', '2', '9', '8', '9', '9', '12', '8', '1', '9', '2', '4', '1', '8', '3', '9', '1', '6', '5', '8', '9', '3', '3', '2', '1', '1', '1', '8', '11', '1', '8', '7', '1', '11', '9', '1', '7', '9', '9', '8', '8', '8', '12', '8', '1', '2', '9', '8', '8', '2', '10', '9', '8', '8', '4', '10', '9', '8', '2', '10', '10', '2', '1', '2', '1', '2', '1', '4', '1', '7', '1', '1', '3', '12', '9', '8', '5', '9', '10', '1', '9', '9', '2', '7', '4', '1', '1', '8', '12', '8', '2', '2', '9', '4', '1', '1', '5', '9', '1', '8', '12', '8', '1', '1', '4', '9', '10', '2', '3', '1', '3', '3', '3', '8', '1', '2', '1', '2', '1', '2', '9', '1', '2', '8', '9', '4', '8', '8', '10', '2', '12', '5', '5', '1', '8', '1', '7', '8', '1', '1', '8', '9', '12', '4', '9', '3', '1', '2', '5', '9', '2', '1', '2', '2', '1', '2', '8', '1', '8', '8', '2', '2', '1', '8', '4', '1', '2', '10', '9', '10', '8', '1', '10', '4', '8', '8', '10', '2', '8', '1', '1', '3', '10', '1', '4', '2', '8', '3', '3', '2', '9', '8', '5', '5', '9', '3', '10', '8', '1', '2', '1', '2', '9', '1', '1', '2', '9', '9', '8', '8', '11', '9', '4', '1', '8', '2', '7', '1', '2', '8', '2', '9', '9', '8', '2', '9', '9', '8', '8', '1', '2', '8', '1', '1', '2', '9', '8', '1', '4', '9', '1', '8', '12', '1', '12', '2', '8', '2', '9', '1', '12', '1', '2', '9', '1', '1', '9', '3', '2', '9', '9', '2', '11', '8', '7', '9', '9', '9', '8', '8', '1', '2', '8', '1', '1', '4', '1', '2', '10', '1', '9', '2', '1', '1', '1', '9', '1', '13', '4', '3', '2', '1', '1', '2', '2', '7', '8', '7', '1', '1', '2', '8', '8', '3', '4', '8', '2', '10', '13', '8', '8', '9', '2', '2', '1', '1', '10', '4', '2', '7', '9', '8', '2', '1', '9', '8', '1', '1', '9', '8', '9', '8', '2', '8', '2', '2', '9', '2', '1', '9', '1', '2', '4', '8', '2', '1', '1', '8', '1', '4', '1', '9', '3', '9', '2', '7', '1', '9', '6', '1', '10', '2', '2', '2', '2', '2', '10', '10', '10', '9', '9', '9', '3', '12', '10', '1', '1', '10', '1', '1', '2', '2', '1', '2', '3', '12', '8', '8', '1', '2', '9', '9', '1', '1', '10', '8', '9', '8', '3', '1', '1', '3', '1', '10', '1', '1', '1', '4', '1', '1', '1', '7', '10', '2', '1', '8', '1', '9', '3', '1', '10', '8', '8', '1', '5', '9', '1', '2', '1', '8', '8', '1', '3', '1', '10', '2', '1', '1', '2', '8', '2', '5', '3', '5', '2', '1', '1', '9', '1', '7', '9', '9', '2', '8', '1', '12', '4', '1', '4', '9', '9', '4', '8', '1', '10', '10', '8', '8', '8', '9', '2', '3', '5', '1', '8', '1', '1', '1', '7', '10', '1', '4', '12', '3', '2', '2', '9', '1', '1', '1', '2', '8', '4', '1', '7', '3', '12', '8', '6', '2', '2', '7', '9', '11', '1', '10', '9', '1', '8', '12', '2', '11', '1', '2', '3', '1', '1', '1', '8', '9', '2', '2', '2', '2', '3', '3', '8', '10', '4', '1', '2', '9', '8', '2', '2', '9', '1', '8', '11', '4', '2', '8', '7', '1', '1', '7', '3', '9', '9', '4', '5', '8', '10', '2', '9', '9', '9', '8', '1', '9', '2', '2', '2', '1', '2', '9', '4', '3', '12', '4', '3', '2', '3', '8', '3', '10', '1', '10', '1', '1', '8', '10', '6', '8', '9', '9', '1', '2', '9', '2', '9', '3', '8', '1', '1', '12', '1', '5', '1', '10', '8', '10', '2', '7', '1', '1', '1', '1', '3', '2', '3', '5', '9', '1', '2', '1', '9', '8', '10', '9', '8', '1', '8', '1', '8', '2', '8', '1', '2', '8', '1', '2', '5', '8', '12', '2', '1', '1', '1', '1', '1', '8', '8', '2', '1', '4', '2', '8', '8', '8', '2', '9', '9', '11', '1', '2', '9', '3', '1', '1', '8', '9', '2', '4', '10', '1', '9', '8', '4', '9', '8', '10', '8', '8', '9', '2', '8', '9', '3', '3', '3', '4', '3', '2', '2', '10', '8', '2', '1', '5', '1', '9', '1', '1', '2', '1', '2', '3', '1', '1', '8', '8', '1', '10', '8', '8', '1', '6', '4', '8', '2', '5', '8', '1', '3', '8', '2', '8', '9', '1', '1', '8', '12', '10', '1', '5', '8', '2', '8', '10', '9', '9', '1', '6', '9', '1', '10', '8', '2', '9', '10', '1', '2', '1', '1', '8', '8', '3', '9', '8', '1', '10', '2', '8', '1', '10', '1', '5', '1', '1', '1', '5', '1', '1', '1', '4', '1', '1', '1', '8', '2', '8', '1', '8', '3', '8', '9', '1', '8', '1', '8', '1', '1', '2', '2', '1', '12', '9', '9', '1', '8', '2', '1', '2', '8', '1', '9', '1', '1', '2', '1', '2', '2', '2', '1', '9', '9', '8', '8', '1', '1', '9', '10', '8', '4', '3', '3', '4', '9', '1', '3', '3', '10', '2', '2', '1', '8', '4', '1', '9', '4', '1', '1', '9', '8', '10', '4', '9', '9', '8', '2', '1', '3', '4', '9', '3', '8', '9', '1', '1', '2', '1', '7', '8', '2', '9', '1', '9', '2', '6', '9', '10', '8', '1', '1', '5', '9', '5', '1', '9', '10', '9', '3', '3', '8', '8', '2', '9', '2', '8', '2', '1', '8', '5', '8', '3', '10', '1', '1', '2', '1', '8', '1', '4', '3', '2', '9', '4', '9', '8', '8', '8', '1', '3', '9', '1', '9', '9', '1', '2', '1', '9', '8', '9', '8', '1', '1', '1', '4', '2', '2', '9', '9', '1', '1', '1', '3', '6', '1', '2', '3', '1', '4', '2', '9', '1', '9', '9', '1', '8', '2', '1', '9', '1', '8', '1', '8', '8', '2', '1', '9', '9', '6', '8', '8', '9', '8', '10', '8', '9', '5', '1', '1', '1', '1', '8', '1', '8', '12', '8', '1', '12', '9', '2', '8', '1', '1', '2', '1', '8', '6', '9', '9', '8', '1', '8', '9', '7', '7', '8', '9', '9', '1', '8', '3', '8', '1', '3', '3', '8', '9', '8', '2', '8', '1', '2', '9', '8', '1', '1', '1', '1', '9', '1', '9', '8', '12', '3', '8', '8', '1', '10', '10', '10', '2', '2', '1', '10', '2', '2', '10', '8', '12', '8', '1', '1', '1', '2', '1', '1', '3', '8', '1', '2', '2', '2', '1', '1', '6', '2', '8', '8', '8', '1', '1', '1', '9', '9', '2', '8', '10', '2', '8', '2', '1', '2', '9', '9', '2', '1', '1', '8', '8', '1', '3', '1', '8', '9', '1', '10', '9', '1', '1', '1', '1', '7', '2', '2', '1', '2', '8', '8', '1', '2', '1', '3', '2', '1', '8', '3', '9', '2', '10', '2', '10', '1', '2', '1', '1', '13', '9', '10', '8', '2', '2', '1', '1', '8', '2', '2', '1', '9', '1', '1', '8', '9', '9', '2', '9', '2', '2', '1', '1', '1', '8', '9', '4', '9', '9', '1', '8', '2', '1', '1', '8', '2', '2', '2', '8', '1', '1', '8', '2', '10', '2', '9', '9', '7', '1', '1', '2', '2', '8', '3', '1', '2', '1', '1', '8', '1', '1', '1', '9', '1', '1', '8', '8', '1', '9', '6', '1', '9', '1', '8', '1', '8', '8', '8', '1', '3', '8', '8', '2', '6', '3', '2', '1', '10', '1', '8', '2', '1', '6', '1', '1', '1', '8', '1', '10', '1', '1', '8', '9', '2', '6', '9', '2', '9', '4', '1', '8', '1', '3', '7', '8', '8', '5', '8', '7', '1', '1', '1', '13', '8', '3', '1', '1', '1', '1', '2', '8', '2', '12', '8', '1', '2', '5', '1', '8', '2', '10', '8', '2', '10', '1', '5', '2', '1', '1', '5', '2', '2', '7', '8', '2', '9', '9', '1', '10', '1', '8', '2', '5', '8', '1', '5', '1', '10', '1', '1', '9', '8', '6', '1', '8', '9', '8', '1', '8', '2', '2', '8', '3', '9', '1', '9', '3', '8', '4', '4', '3', '3', '1', '1', '1', '8', '9', '1', '1', '8', '3', '1', '2', '8', '2', '1', '1', '10', '1', '2', '2', '1', '1', '8', '2', '8', '2', '10', '2', '2', '10', '8', '8', '8', '13', '2', '2', '9', '2', '5', '9', '2', '1', '8', '8', '8', '1', '12', '12', '2', '9', '2', '1', '8', '2', '1', '8', '1', '9', '9', '2', '7', '1', '1', '7', '1', '4', '3', '1', '10', '3', '10', '9', '4', '6', '7', '5', '2', '1', '8', '8', '1', '4', '9', '2', '1', '8', '1', '1', '1', '1', '1', '8', '9', '8', '8', '8', '8', '9', '8', '10', '1', '8', '2', '7', '8', '9', '1', '2', '5', '9', '10', '8', '9', '12', '1', '6', '1', '2', '10', '9', '4', '1', '8', '10', '10', '1', '1', '1', '1', '1', '2', '3', '8', '1', '2', '10', '2', '1', '2', '4', '2', '2', '9', '1', '4', '2', '1', '1', '12', '12', '9', '9', '7', '1', '8', '4', '9', '9', '8', '1', '7', '3', '2', '1', '1', '4', '8', '1', '5', '8', '2', '8', '1', '14', '9', '12', '1', '10', '2', '1', '1', '8', '8', '6', '6', '3', '1', '2', '8', '1', '3', '2', '1', '8', '8', '1', '10', '8', '2', '8', '2', '5', '9', '1', '8', '8', '1', '1', '8', '8', '13', '1', '3', '8', '4', '1', '9', '7', '8', '8', '8', '3', '8', '1', '4', '2', '8', '8', '8', '7', '1', '1', '2', '8', '8', '9', '8', '8', '2', '3', '9', '9', '2', '1', '2', '2', '1', '8', '2', '6', '2', '1', '1', '9', '10', '3', '8', '2', '3', '2', '8', '8', '1', '8', '8', '8', '9', '1', '4', '1', '9', '2', '1', '8', '0', '1', '8', '1', '1', '8', '9', '9', '1', '3', '1', '2', '8', '8', '7', '1', '1', '2', '9', '2', '10', '2', '8', '2', '2', '2', '7', '1', '9', '8', '1', '3', '9', '2', '1', '10', '8', '1', '4', '1', '8', '5', '9', '8', '8', '1', '2', '2', '1', '8', '8', '6', '8', '1', '1', '8', '3', '2', '2', '1', '8', '1', '7', '6', '8', '1', '1', '1', '9', '1', '1', '1', '2', '1', '11', '10', '10', '11', '9']\n",
            "     filenum                                               text  label\n",
            "0          1  scope of what is meant by the equivalent of an...      8\n",
            "1          2  time during which the silence has endured, can...      1\n",
            "2          4  on original Indian title were held to be outsi...      2\n",
            "3          6  who framed it. Every word appears to have been...      8\n",
            "4          7  in of the Act, the general policy declarations...      8\n",
            "...      ...                                                ...    ...\n",
            "5270    8411  of life without the possibility of parole. In ...      1\n",
            "5271    8412  habeas relief. We reverse. In Ylst, we said th...      1\n",
            "5272    8413  existence. Novelty in procedural requirements ...      2\n",
            "5273    8414  circumstances marked 'yes' in Section II outwe...      1\n",
            "5274    8417  it, the States' claims of ownership prior to t...     10\n",
            "\n",
            "[5275 rows x 3 columns]\n"
          ]
        }
      ],
      "source": [
        "fh = open('sc_model_4.txt')\n",
        "filenums=[]\n",
        "records=[]\n",
        "for record in fh:\n",
        "    record=record.split(\"---\")\n",
        "    filenum=record[0]\n",
        "    modelnum=record[1]\n",
        "    text=record[2]\n",
        "    filenums.append(filenum)\n",
        "    records.append(text)\n",
        "    \n",
        "print(len(filenums))\n",
        "print(len(records))\n",
        "\n",
        "summarized_data = pd.DataFrame(list(zip(filenums, records)),columns =['filenum','text'])\n",
        "len_list = [len(ele.split()) for ele in records]\n",
        "res = 0 if len(len_list) == 0 else (float(sum(len_list)) / len(len_list))\n",
        "print(\"Average Length %s\" % res)\n",
        "\n",
        "temp_file = open(\"labels_sc.txt\", \"r\")\n",
        "#temp_file = open(\"labels_sc_279.txt\", \"r\")\n",
        "\n",
        "data = temp_file.read()\n",
        "\n",
        "label_list = data.split(\"\\n\")\n",
        "print(label_list)\n",
        "label_list = label_list[0:-1]\n",
        "print(label_list)\n",
        "label_list = [int(i) for i in label_list]\n",
        "temp_file.close()\n",
        "\n",
        "\n",
        "new_label_list=[]\n",
        "for num in filenums:\n",
        "  new_label_list.append(label_list[int(num)-1])\n",
        "summarized_data['label'] = new_label_list\n",
        "\n",
        "print(summarized_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kenM-hJwR4q",
        "outputId": "90bb5fd3-85d1-4c10-e7ef-51ce44a47e7c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_1[0][0]',                \n",
            "                                thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_bert_model[0][0]']          \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 15)           7695        ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 8  1  2 ...  2  1 10]\n",
            "[   1    2    4 ... 8413 8414 8417]\n",
            "[6622   35 6713 4989 8233 7444 5061 8344 5464  317 4515 3001 6991 2875\n",
            " 3885 6647 4857 4504  214 8340 2969 3847 4609 1346 5993 7358 2752 7651\n",
            " 8188 7073 6069 2346 6483 4952 5072 1967 4071 7590 5030 7297  626 5463\n",
            " 6029 6302 7256 3993 4263 3835 5801 4293 8129 5866 5777 4837 3843 1418\n",
            " 5274 6205 2356 5997 6685 3310 7721 5928 2360 7197 4252 8343  909 4354\n",
            " 4752  649 3488 2496 5765 6612 4004 4052 1403 6697 5268 6293 6380 4602\n",
            " 6057 4577 4236 7978 5083 6552  361 7706 4186 8351  812 5108 6600 8264\n",
            " 8064 7121 8279 5556 5119 7947 3350 7152 5199 4282 5025 5051 1927 3615\n",
            " 1221 8062 1489 2031 7351  234 8015 7416 7921 6574 6257 8396  941 4520\n",
            " 3300 5640  240 7810 7425 1321 6580 5404 8273 6742   94    9 7861 7975\n",
            " 1018 2140 1415 7061 7767 4990 8124 5927 6051 3269 6916 2846 6818 7052\n",
            " 7775 7582 8116 4767 5248 1812 6055 8331  998 1564 7326  292 3065 6764\n",
            " 6854 4915 3583 5902 4229 2144 7565 3712 7224 4502 1406 1651 3827 6525\n",
            " 6318 2686 5369 8235 1704 1011 3351 8021  102  224 8294 7707 2985 5146\n",
            " 3709 6824 2281 6837 5436 2649 4886  966 5476 1199 8387 5449 1028 2226\n",
            " 6166 4387  350 6123 5236 5041  734 3412 6085  587  489 4982 5115 5575\n",
            " 3839 6743 7173 5724 7689 7787 8302 3150 2247 8128 6660 3505 4610 2866\n",
            " 4161 6905 7096  768 1062 4411 6087 2096 4903 7546 6630 2928 4640 7125\n",
            " 1792 2063 1700 7928 8095  737 5849  922 2359 6015 4489 4361 6081 6346\n",
            " 5014 5167 1023 8009  268 4777 5221 4323 6168 5702 6129 7181 1860 7192\n",
            " 5467 6579   67 2693 6134 4270 4611 6746 7696 3466 3950 6125 8370 6142\n",
            " 6131  995 5548 1721 5067 4015 7092 3156   25 1749 4878 8248 6836 5310\n",
            " 6960 2339 1650 6227 4104 5130 7089 6993 1289 4305 5585 7933 6284 7057\n",
            " 3807 5790 8347 5280 2841 7693 8090 4171 5260  133 7451 3957 1719 4743\n",
            " 4746 5435 6216 4858 4417 8355 4492 2648 5104 5278 7308  158 7543 2168\n",
            " 5913 7111 3414 7592   39 7157 5647 4555 3956 3617 4630 4384 6596 4539\n",
            " 6749 7202 2413 4749 5122 5929  252 5565  336 6008 4564 5580 7237 5958\n",
            " 7657 4327 5091 6918 6487 2989 7448 4715 3382  467 3784 4419 1117 7954\n",
            " 7122 3174 1534 4164 7526  498 5944 4572 2307 2929 7480 4412 4563 7692\n",
            " 1869 2472 6966 6549 1594 2276 7269 2545 2216  605 6587 6315 5297 5282\n",
            " 1619 5504 6399 3917 6342 5721 7909 6126 8382 4383 5502 7130  485 4936\n",
            " 7674 5372 7109 4605 5211 5462 8065   50 4080 3334 8167 8378 5595 1873\n",
            " 8207 7076 4106 3719 4627  231 2405 6973 3480 3246 5931 8208 4825 6203\n",
            " 3019 5822   63 5854 6543 5787 4764 6701 6562 3822 4861 1315 8318 8169\n",
            " 8218 7549  707 1076 2465 4636   80 7025 7127 4026 4961 7531  744 6684\n",
            " 4297 7939 7384  446 6675 7752 5279 8196 3020 4192 4253 2887 2556 5729\n",
            " 1898 3208 5973 7033 1431 5915 8312 5799 7105  511 6404 2852 1561]\n"
          ]
        }
      ],
      "source": [
        "gc.collect()\n",
        "tf.keras.backend.clear_session()\n",
        "dbert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "dbert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
        "max_len=512\n",
        "sentences=summarized_data['text']\n",
        "labels=summarized_data['label']\n",
        "filenumbers=summarized_data['filenum']\n",
        "len(sentences),len(labels),len(filenumbers)\n",
        "\n",
        "model_4=create_model()\n",
        "input_ids=[]\n",
        "attention_masks=[]\n",
        "\n",
        "for sent in sentences:\n",
        "  dbert_inps=dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n",
        "  input_ids.append(dbert_inps['input_ids'])\n",
        "  attention_masks.append(dbert_inps['attention_mask'])\n",
        "input_ids=np.asarray(input_ids)\n",
        "\n",
        "attention_masks=np.array(attention_masks)\n",
        "labels=np.array(labels)\n",
        "filenumbers=[int(i) for i in filenumbers]\n",
        "filenumbers=np.array(filenumbers)\n",
        "print(labels)\n",
        "print(filenumbers)\n",
        "\n",
        "#train_inp,val_inp,train_label,val_label,train_mask,val_mask,train_fnum,val_fnum=train_test_split(input_ids,labels,attention_masks,filenumbers,test_size=0.1,random_state=42)\n",
        "\n",
        "new_train_inp_4=[]\n",
        "new_val_inp_4=[]\n",
        "new_train_label_4=[]\n",
        "new_val_label_4=[]\n",
        "new_train_mask_4=[]\n",
        "new_train_fnum_4=[]\n",
        "new_val_fnum_4=[]\n",
        "new_val_mask_4=[]\n",
        "for i in range(len(train_fnum)):\n",
        "  if train_fnum[i] in filenumbers:\n",
        "    new_train_inp_4.append(train_inp[i])\n",
        "    #new_val_inp.append(val_inp[i])\n",
        "    new_train_label_4.append(train_label[i])\n",
        "    #new_val_label.append(val_label[i])\n",
        "    new_train_mask_4.append(train_mask[i])\n",
        "    #new_val_mask.append(val_mask[i])\n",
        "    new_train_fnum_4.append(train_fnum[i])\n",
        "\n",
        "for i in range(len(val_fnum)):\n",
        "  if val_fnum[i] in filenumbers:\n",
        "    #new_train_inp.append(train_inp[i])\n",
        "    new_val_inp_4.append(val_inp[i])\n",
        "    #new_train_label.append(train_label[i])\n",
        "    new_val_label_4.append(val_label[i])\n",
        "    #new_train_mask.append(train_mask[i])\n",
        "    new_val_mask_4.append(val_mask[i])\n",
        "    new_val_fnum_4.append(val_fnum[i])\n",
        "\n",
        "# print(new_train_label)\n",
        "# print(len(new_train_label))\n",
        "# print(new_val_label)\n",
        "# print(len(new_val_label))\n",
        "new_train_inp_4=np.array(new_train_inp_4)\n",
        "new_val_inp_4=np.array(new_val_inp_4)\n",
        "new_train_label_4=np.array(new_train_label_4)\n",
        "new_val_label_4=np.array(new_val_label_4)\n",
        "new_train_mask_4=np.array(new_train_mask_4)\n",
        "new_train_fnum_4=np.array(new_train_fnum_4)\n",
        "new_val_fnum_4=np.array(new_val_fnum_4)\n",
        "new_val_mask_4=np.array(new_val_mask_4)\n",
        "\n",
        "print(new_val_fnum_4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "qAa4P7OtO0QK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bc9be9d-2e9b-4761-fbf6-29e3451af9fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_1[0][0]',                \n",
            "                                thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_bert_model[0][0]']          \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 15)           7695        ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 8  1  2 ...  2  1 10]\n",
            "[   1    2    4 ... 8413 8414 8417]\n",
            "[6622   35 6713 4989 8233 7444 5061 8344 5464  317 4515 3001 6991 2875\n",
            " 3885 6647 4857 4504  214 8340 2969 3847 4609 1346 5993 7358 2752 7651\n",
            " 8188 7073 6069 2346 6483 4952 5072 1967 4071 7590 5030 7297  626 5463\n",
            " 6029 6302 7256 3993 4263 3835 5801 4293 8129 5866 5777 4837 3843 1418\n",
            " 5274 6205 2356 5997 6685 3310 7721 5928 2360 7197 4252 8343  909 4354\n",
            " 4752  649 3488 2496 5765 6612 4004 4052 1403 6697 5268 6293 6380 4602\n",
            " 6057 4577 4236 7978 5083 6552  361 7706 4186 8351  812 5108 6600 8264\n",
            " 8064 7121 8279 5556 5119 7947 3350 7152 5199 4282 5025 5051 1927 3615\n",
            " 1221 8062 1489 2031 7351  234 8015 7416 7921 6574 6257 8396  941 4520\n",
            " 3300 5640  240 7810 7425 1321 6580 5404 8273 6742   94    9 7861 7975\n",
            " 1018 2140 1415 7061 7767 4990 8124 5927 6051 3269 6916 2846 6818 7052\n",
            " 7775 7582 8116 4767 5248 1812 6055 8331  998 1564 7326  292 3065 6764\n",
            " 6854 4915 3583 5902 4229 2144 7565 3712 7224 4502 1406 1651 3827 6525\n",
            " 6318 2686 5369 8235 1704 1011 3351 8021  102  224 8294 7707 2985 5146\n",
            " 3709 6824 2281 6837 5436 2649 4886  966 5476 1199 8387 5449 1028 2226\n",
            " 6166 4387  350 6123 5236 5041  734 3412 6085  587  489 4982 5115 5575\n",
            " 3839 6743 7173 5724 7689 7787 8302 3150 2247 8128 6660 3505 4610 2866\n",
            " 4161 6905 7096  768 1062 4411 6087 2096 4903 7546 6630 2928 4640 7125\n",
            " 1792 2063 1700 7928 8095  737 5849  922 2359 6015 4489 4361 6081 6346\n",
            " 5014 5167 1023 8009  268 4777 5221 4323 6168 5702 6129 7181 1860 7192\n",
            " 5467 6579   67 2693 6134 4270 4611 6746 7696 3466 3950 6125 8370 6142\n",
            " 6131  995 5548 1721 5067 4015 7092 3156   25 1749 4878 8248 6836 5310\n",
            " 6960 2339 1650 6227 4104 5130 7089 6993 1289 4305 5585 7933 6284 7057\n",
            " 3807 5790 8347 5280 2841 7693 8090 4171 5260  133 7451 3957 1719 4743\n",
            " 4746 5435 6216 4858 4417 8355 4492 2648 5104 5278 7308  158 7543 2168\n",
            " 5913 7111 3414 7592   39 7157 5647 4555 3956 3617 4630 4384 6596 4539\n",
            " 6749 7202 2413 4749 5122 5929  252 5565  336 6008 4564 5580 7237 5958\n",
            " 7657 4327 5091 6918 6487 2989 7448 4715 3382  467 3784 4419 1117 7954\n",
            " 7122 3174 1534 4164 7526  498 5944 4572 2307 2929 7480 4412 4563 7692\n",
            " 1869 2472 6966 6549 1594 2276 7269 2545 2216  605 6587 6315 5297 5282\n",
            " 1619 5504 6399 3917 6342 5721 7909 6126 8382 4383 5502 7130  485 4936\n",
            " 7674 5372 7109 4605 5211 5462 8065   50 4080 3334 8167 8378 5595 1873\n",
            " 8207 7076 4106 3719 4627  231 2405 6973 3480 3246 5931 8208 4825 6203\n",
            " 3019 5822   63 5854 6543 5787 4764 6701 6562 3822 4861 1315 8318 8169\n",
            " 8218 7549  707 1076 2465 4636   80 7025 7127 4026 4961 7531  744 6684\n",
            " 4297 7939 7384  446 6675 7752 5279 8196 3020 4192 4253 2887 2556 5729\n",
            " 1898 3208 5973 7033 1431 5915 8312 5799 7105  511 6404 2852 1561]\n",
            "Fri Jun  3 00:12:40 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    38W / 300W |  15237MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "595/595 [==============================] - 221s 345ms/step - loss: 7.3976 - accuracy: 0.5734 - val_loss: 6.6554 - val_accuracy: 0.6596\n",
            "Epoch 2/5\n",
            "595/595 [==============================] - 205s 345ms/step - loss: 6.1346 - accuracy: 0.7423 - val_loss: 5.8625 - val_accuracy: 0.7485\n",
            "Epoch 3/5\n",
            "595/595 [==============================] - 202s 339ms/step - loss: 5.2607 - accuracy: 0.8169 - val_loss: 5.2879 - val_accuracy: 0.7157\n",
            "Epoch 4/5\n",
            "595/595 [==============================] - 201s 339ms/step - loss: 4.5177 - accuracy: 0.8779 - val_loss: 4.7440 - val_accuracy: 0.7273\n",
            "Epoch 5/5\n",
            "595/595 [==============================] - 201s 338ms/step - loss: 3.8639 - accuracy: 0.9189 - val_loss: 4.4215 - val_accuracy: 0.7311\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_3[0][0]',                \n",
            "                                thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_bert_model[1][0]']          \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 15)           7695        ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.6971496437054632\n",
            "Weighted F1: 0.6932476413312391\n",
            "Micro F1: 0.6971496437054632\n",
            "Weighted Precision: 0.7103826401940982\n",
            "Micro Precision: 0.6971496437054632\n",
            "Weighted Recall: 0.6971496437054632\n",
            "Micro Recall: 0.6971496437054632\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_1[0][0]',                \n",
            "                                thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_bert_model[0][0]']          \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 15)           7695        ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 8  1  2 ...  2  1 10]\n",
            "[   1    2    4 ... 8413 8414 8417]\n",
            "[6622   35 6713 4989 8233 7444 5061 8344 5464  317 4515 3001 6991 2875\n",
            " 3885 6647 4857 4504  214 8340 2969 3847 4609 1346 5993 7358 2752 7651\n",
            " 8188 7073 6069 2346 6483 4952 5072 1967 4071 7590 5030 7297  626 5463\n",
            " 6029 6302 7256 3993 4263 3835 5801 4293 8129 5866 5777 4837 3843 1418\n",
            " 5274 6205 2356 5997 6685 3310 7721 5928 2360 7197 4252 8343  909 4354\n",
            " 4752  649 3488 2496 5765 6612 4004 4052 1403 6697 5268 6293 6380 4602\n",
            " 6057 4577 4236 7978 5083 6552  361 7706 4186 8351  812 5108 6600 8264\n",
            " 8064 7121 8279 5556 5119 7947 3350 7152 5199 4282 5025 5051 1927 3615\n",
            " 1221 8062 1489 2031 7351  234 8015 7416 7921 6574 6257 8396  941 4520\n",
            " 3300 5640  240 7810 7425 1321 6580 5404 8273 6742   94    9 7861 7975\n",
            " 1018 2140 1415 7061 7767 4990 8124 5927 6051 3269 6916 2846 6818 7052\n",
            " 7775 7582 8116 4767 5248 1812 6055 8331  998 1564 7326  292 3065 6764\n",
            " 6854 4915 3583 5902 4229 2144 7565 3712 7224 4502 1406 1651 3827 6525\n",
            " 6318 2686 5369 8235 1704 1011 3351 8021  102  224 8294 7707 2985 5146\n",
            " 3709 6824 2281 6837 5436 2649 4886  966 5476 1199 8387 5449 1028 2226\n",
            " 6166 4387  350 6123 5236 5041  734 3412 6085  587  489 4982 5115 5575\n",
            " 3839 6743 7173 5724 7689 7787 8302 3150 2247 8128 6660 3505 4610 2866\n",
            " 4161 6905 7096  768 1062 4411 6087 2096 4903 7546 6630 2928 4640 7125\n",
            " 1792 2063 1700 7928 8095  737 5849  922 2359 6015 4489 4361 6081 6346\n",
            " 5014 5167 1023 8009  268 4777 5221 4323 6168 5702 6129 7181 1860 7192\n",
            " 5467 6579   67 2693 6134 4270 4611 6746 7696 3466 3950 6125 8370 6142\n",
            " 6131  995 5548 1721 5067 4015 7092 3156   25 1749 4878 8248 6836 5310\n",
            " 6960 2339 1650 6227 4104 5130 7089 6993 1289 4305 5585 7933 6284 7057\n",
            " 3807 5790 8347 5280 2841 7693 8090 4171 5260  133 7451 3957 1719 4743\n",
            " 4746 5435 6216 4858 4417 8355 4492 2648 5104 5278 7308  158 7543 2168\n",
            " 5913 7111 3414 7592   39 7157 5647 4555 3956 3617 4630 4384 6596 4539\n",
            " 6749 7202 2413 4749 5122 5929  252 5565  336 6008 4564 5580 7237 5958\n",
            " 7657 4327 5091 6918 6487 2989 7448 4715 3382  467 3784 4419 1117 7954\n",
            " 7122 3174 1534 4164 7526  498 5944 4572 2307 2929 7480 4412 4563 7692\n",
            " 1869 2472 6966 6549 1594 2276 7269 2545 2216  605 6587 6315 5297 5282\n",
            " 1619 5504 6399 3917 6342 5721 7909 6126 8382 4383 5502 7130  485 4936\n",
            " 7674 5372 7109 4605 5211 5462 8065   50 4080 3334 8167 8378 5595 1873\n",
            " 8207 7076 4106 3719 4627  231 2405 6973 3480 3246 5931 8208 4825 6203\n",
            " 3019 5822   63 5854 6543 5787 4764 6701 6562 3822 4861 1315 8318 8169\n",
            " 8218 7549  707 1076 2465 4636   80 7025 7127 4026 4961 7531  744 6684\n",
            " 4297 7939 7384  446 6675 7752 5279 8196 3020 4192 4253 2887 2556 5729\n",
            " 1898 3208 5973 7033 1431 5915 8312 5799 7105  511 6404 2852 1561]\n",
            "Fri Jun  3 00:32:20 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    38W / 300W |  15237MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "595/595 [==============================] - 221s 345ms/step - loss: 7.3241 - accuracy: 0.6007 - val_loss: 6.6566 - val_accuracy: 0.6441\n",
            "Epoch 2/5\n",
            "595/595 [==============================] - 204s 344ms/step - loss: 6.0207 - accuracy: 0.7461 - val_loss: 5.7179 - val_accuracy: 0.7427\n",
            "Epoch 3/5\n",
            "595/595 [==============================] - 201s 338ms/step - loss: 5.1429 - accuracy: 0.8220 - val_loss: 5.2488 - val_accuracy: 0.6963\n",
            "Epoch 4/5\n",
            "595/595 [==============================] - 201s 338ms/step - loss: 4.3967 - accuracy: 0.8789 - val_loss: 4.7922 - val_accuracy: 0.7137\n",
            "Epoch 5/5\n",
            "595/595 [==============================] - 201s 338ms/step - loss: 3.7457 - accuracy: 0.9180 - val_loss: 4.3420 - val_accuracy: 0.7118\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_3[0][0]',                \n",
            "                                thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_bert_model[1][0]']          \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 15)           7695        ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.7066508313539193\n",
            "Weighted F1: 0.7048062491297006\n",
            "Micro F1: 0.7066508313539193\n",
            "Weighted Precision: 0.7123360578802019\n",
            "Micro Precision: 0.7066508313539193\n",
            "Weighted Recall: 0.7066508313539193\n",
            "Micro Recall: 0.7066508313539193\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_1[0][0]',                \n",
            "                                thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_bert_model[0][0]']          \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 15)           7695        ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 8  1  2 ...  2  1 10]\n",
            "[   1    2    4 ... 8413 8414 8417]\n",
            "[6622   35 6713 4989 8233 7444 5061 8344 5464  317 4515 3001 6991 2875\n",
            " 3885 6647 4857 4504  214 8340 2969 3847 4609 1346 5993 7358 2752 7651\n",
            " 8188 7073 6069 2346 6483 4952 5072 1967 4071 7590 5030 7297  626 5463\n",
            " 6029 6302 7256 3993 4263 3835 5801 4293 8129 5866 5777 4837 3843 1418\n",
            " 5274 6205 2356 5997 6685 3310 7721 5928 2360 7197 4252 8343  909 4354\n",
            " 4752  649 3488 2496 5765 6612 4004 4052 1403 6697 5268 6293 6380 4602\n",
            " 6057 4577 4236 7978 5083 6552  361 7706 4186 8351  812 5108 6600 8264\n",
            " 8064 7121 8279 5556 5119 7947 3350 7152 5199 4282 5025 5051 1927 3615\n",
            " 1221 8062 1489 2031 7351  234 8015 7416 7921 6574 6257 8396  941 4520\n",
            " 3300 5640  240 7810 7425 1321 6580 5404 8273 6742   94    9 7861 7975\n",
            " 1018 2140 1415 7061 7767 4990 8124 5927 6051 3269 6916 2846 6818 7052\n",
            " 7775 7582 8116 4767 5248 1812 6055 8331  998 1564 7326  292 3065 6764\n",
            " 6854 4915 3583 5902 4229 2144 7565 3712 7224 4502 1406 1651 3827 6525\n",
            " 6318 2686 5369 8235 1704 1011 3351 8021  102  224 8294 7707 2985 5146\n",
            " 3709 6824 2281 6837 5436 2649 4886  966 5476 1199 8387 5449 1028 2226\n",
            " 6166 4387  350 6123 5236 5041  734 3412 6085  587  489 4982 5115 5575\n",
            " 3839 6743 7173 5724 7689 7787 8302 3150 2247 8128 6660 3505 4610 2866\n",
            " 4161 6905 7096  768 1062 4411 6087 2096 4903 7546 6630 2928 4640 7125\n",
            " 1792 2063 1700 7928 8095  737 5849  922 2359 6015 4489 4361 6081 6346\n",
            " 5014 5167 1023 8009  268 4777 5221 4323 6168 5702 6129 7181 1860 7192\n",
            " 5467 6579   67 2693 6134 4270 4611 6746 7696 3466 3950 6125 8370 6142\n",
            " 6131  995 5548 1721 5067 4015 7092 3156   25 1749 4878 8248 6836 5310\n",
            " 6960 2339 1650 6227 4104 5130 7089 6993 1289 4305 5585 7933 6284 7057\n",
            " 3807 5790 8347 5280 2841 7693 8090 4171 5260  133 7451 3957 1719 4743\n",
            " 4746 5435 6216 4858 4417 8355 4492 2648 5104 5278 7308  158 7543 2168\n",
            " 5913 7111 3414 7592   39 7157 5647 4555 3956 3617 4630 4384 6596 4539\n",
            " 6749 7202 2413 4749 5122 5929  252 5565  336 6008 4564 5580 7237 5958\n",
            " 7657 4327 5091 6918 6487 2989 7448 4715 3382  467 3784 4419 1117 7954\n",
            " 7122 3174 1534 4164 7526  498 5944 4572 2307 2929 7480 4412 4563 7692\n",
            " 1869 2472 6966 6549 1594 2276 7269 2545 2216  605 6587 6315 5297 5282\n",
            " 1619 5504 6399 3917 6342 5721 7909 6126 8382 4383 5502 7130  485 4936\n",
            " 7674 5372 7109 4605 5211 5462 8065   50 4080 3334 8167 8378 5595 1873\n",
            " 8207 7076 4106 3719 4627  231 2405 6973 3480 3246 5931 8208 4825 6203\n",
            " 3019 5822   63 5854 6543 5787 4764 6701 6562 3822 4861 1315 8318 8169\n",
            " 8218 7549  707 1076 2465 4636   80 7025 7127 4026 4961 7531  744 6684\n",
            " 4297 7939 7384  446 6675 7752 5279 8196 3020 4192 4253 2887 2556 5729\n",
            " 1898 3208 5973 7033 1431 5915 8312 5799 7105  511 6404 2852 1561]\n",
            "Fri Jun  3 00:52:00 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P0    38W / 300W |  15237MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "595/595 [==============================] - 221s 345ms/step - loss: 7.3261 - accuracy: 0.5704 - val_loss: 6.4989 - val_accuracy: 0.6905\n",
            "Epoch 2/5\n",
            "595/595 [==============================] - 205s 344ms/step - loss: 5.9825 - accuracy: 0.7499 - val_loss: 5.6867 - val_accuracy: 0.7447\n",
            "Epoch 3/5\n",
            "595/595 [==============================] - 201s 338ms/step - loss: 5.1139 - accuracy: 0.8306 - val_loss: 5.2367 - val_accuracy: 0.7079\n",
            "Epoch 4/5\n",
            "595/595 [==============================] - 201s 338ms/step - loss: 4.3743 - accuracy: 0.8783 - val_loss: 4.5896 - val_accuracy: 0.7331\n",
            "Epoch 5/5\n",
            "595/595 [==============================] - 201s 338ms/step - loss: 3.7281 - accuracy: 0.9264 - val_loss: 4.3262 - val_accuracy: 0.7253\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_3[0][0]',                \n",
            "                                thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_bert_model[1][0]']          \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 15)           7695        ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.7114014251781473\n",
            "Weighted F1: 0.7086249167504493\n",
            "Micro F1: 0.7114014251781473\n",
            "Weighted Precision: 0.7192096510117685\n",
            "Micro Precision: 0.7114014251781473\n",
            "Weighted Recall: 0.7114014251781473\n",
            "Micro Recall: 0.7114014251781473\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_1[0][0]',                \n",
            "                                thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_bert_model[0][0]']          \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 15)           7695        ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 8  1  2 ...  2  1 10]\n",
            "[   1    2    4 ... 8413 8414 8417]\n",
            "[6622   35 6713 4989 8233 7444 5061 8344 5464  317 4515 3001 6991 2875\n",
            " 3885 6647 4857 4504  214 8340 2969 3847 4609 1346 5993 7358 2752 7651\n",
            " 8188 7073 6069 2346 6483 4952 5072 1967 4071 7590 5030 7297  626 5463\n",
            " 6029 6302 7256 3993 4263 3835 5801 4293 8129 5866 5777 4837 3843 1418\n",
            " 5274 6205 2356 5997 6685 3310 7721 5928 2360 7197 4252 8343  909 4354\n",
            " 4752  649 3488 2496 5765 6612 4004 4052 1403 6697 5268 6293 6380 4602\n",
            " 6057 4577 4236 7978 5083 6552  361 7706 4186 8351  812 5108 6600 8264\n",
            " 8064 7121 8279 5556 5119 7947 3350 7152 5199 4282 5025 5051 1927 3615\n",
            " 1221 8062 1489 2031 7351  234 8015 7416 7921 6574 6257 8396  941 4520\n",
            " 3300 5640  240 7810 7425 1321 6580 5404 8273 6742   94    9 7861 7975\n",
            " 1018 2140 1415 7061 7767 4990 8124 5927 6051 3269 6916 2846 6818 7052\n",
            " 7775 7582 8116 4767 5248 1812 6055 8331  998 1564 7326  292 3065 6764\n",
            " 6854 4915 3583 5902 4229 2144 7565 3712 7224 4502 1406 1651 3827 6525\n",
            " 6318 2686 5369 8235 1704 1011 3351 8021  102  224 8294 7707 2985 5146\n",
            " 3709 6824 2281 6837 5436 2649 4886  966 5476 1199 8387 5449 1028 2226\n",
            " 6166 4387  350 6123 5236 5041  734 3412 6085  587  489 4982 5115 5575\n",
            " 3839 6743 7173 5724 7689 7787 8302 3150 2247 8128 6660 3505 4610 2866\n",
            " 4161 6905 7096  768 1062 4411 6087 2096 4903 7546 6630 2928 4640 7125\n",
            " 1792 2063 1700 7928 8095  737 5849  922 2359 6015 4489 4361 6081 6346\n",
            " 5014 5167 1023 8009  268 4777 5221 4323 6168 5702 6129 7181 1860 7192\n",
            " 5467 6579   67 2693 6134 4270 4611 6746 7696 3466 3950 6125 8370 6142\n",
            " 6131  995 5548 1721 5067 4015 7092 3156   25 1749 4878 8248 6836 5310\n",
            " 6960 2339 1650 6227 4104 5130 7089 6993 1289 4305 5585 7933 6284 7057\n",
            " 3807 5790 8347 5280 2841 7693 8090 4171 5260  133 7451 3957 1719 4743\n",
            " 4746 5435 6216 4858 4417 8355 4492 2648 5104 5278 7308  158 7543 2168\n",
            " 5913 7111 3414 7592   39 7157 5647 4555 3956 3617 4630 4384 6596 4539\n",
            " 6749 7202 2413 4749 5122 5929  252 5565  336 6008 4564 5580 7237 5958\n",
            " 7657 4327 5091 6918 6487 2989 7448 4715 3382  467 3784 4419 1117 7954\n",
            " 7122 3174 1534 4164 7526  498 5944 4572 2307 2929 7480 4412 4563 7692\n",
            " 1869 2472 6966 6549 1594 2276 7269 2545 2216  605 6587 6315 5297 5282\n",
            " 1619 5504 6399 3917 6342 5721 7909 6126 8382 4383 5502 7130  485 4936\n",
            " 7674 5372 7109 4605 5211 5462 8065   50 4080 3334 8167 8378 5595 1873\n",
            " 8207 7076 4106 3719 4627  231 2405 6973 3480 3246 5931 8208 4825 6203\n",
            " 3019 5822   63 5854 6543 5787 4764 6701 6562 3822 4861 1315 8318 8169\n",
            " 8218 7549  707 1076 2465 4636   80 7025 7127 4026 4961 7531  744 6684\n",
            " 4297 7939 7384  446 6675 7752 5279 8196 3020 4192 4253 2887 2556 5729\n",
            " 1898 3208 5973 7033 1431 5915 8312 5799 7105  511 6404 2852 1561]\n",
            "Fri Jun  3 01:11:39 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P0    38W / 300W |  15237MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "595/595 [==============================] - 221s 344ms/step - loss: 7.2906 - accuracy: 0.5944 - val_loss: 6.5616 - val_accuracy: 0.6750\n",
            "Epoch 2/5\n",
            "595/595 [==============================] - 204s 343ms/step - loss: 6.0034 - accuracy: 0.7549 - val_loss: 5.7100 - val_accuracy: 0.7505\n",
            "Epoch 3/5\n",
            "595/595 [==============================] - 201s 338ms/step - loss: 5.1194 - accuracy: 0.8306 - val_loss: 5.0910 - val_accuracy: 0.7408\n",
            "Epoch 4/5\n",
            "595/595 [==============================] - 201s 337ms/step - loss: 4.3748 - accuracy: 0.8796 - val_loss: 4.6218 - val_accuracy: 0.7447\n",
            "Epoch 5/5\n",
            "595/595 [==============================] - 201s 337ms/step - loss: 3.7268 - accuracy: 0.9220 - val_loss: 4.4257 - val_accuracy: 0.7041\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_3[0][0]',                \n",
            "                                thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_bert_model[1][0]']          \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 15)           7695        ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.7054631828978623\n",
            "Weighted F1: 0.6983031489431419\n",
            "Micro F1: 0.7054631828978623\n",
            "Weighted Precision: 0.7122943537549282\n",
            "Micro Precision: 0.7054631828978623\n",
            "Weighted Recall: 0.7054631828978623\n",
            "Micro Recall: 0.7054631828978623\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_1[0][0]',                \n",
            "                                thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_bert_model[0][0]']          \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 15)           7695        ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 8  1  2 ...  2  1 10]\n",
            "[   1    2    4 ... 8413 8414 8417]\n",
            "[6622   35 6713 4989 8233 7444 5061 8344 5464  317 4515 3001 6991 2875\n",
            " 3885 6647 4857 4504  214 8340 2969 3847 4609 1346 5993 7358 2752 7651\n",
            " 8188 7073 6069 2346 6483 4952 5072 1967 4071 7590 5030 7297  626 5463\n",
            " 6029 6302 7256 3993 4263 3835 5801 4293 8129 5866 5777 4837 3843 1418\n",
            " 5274 6205 2356 5997 6685 3310 7721 5928 2360 7197 4252 8343  909 4354\n",
            " 4752  649 3488 2496 5765 6612 4004 4052 1403 6697 5268 6293 6380 4602\n",
            " 6057 4577 4236 7978 5083 6552  361 7706 4186 8351  812 5108 6600 8264\n",
            " 8064 7121 8279 5556 5119 7947 3350 7152 5199 4282 5025 5051 1927 3615\n",
            " 1221 8062 1489 2031 7351  234 8015 7416 7921 6574 6257 8396  941 4520\n",
            " 3300 5640  240 7810 7425 1321 6580 5404 8273 6742   94    9 7861 7975\n",
            " 1018 2140 1415 7061 7767 4990 8124 5927 6051 3269 6916 2846 6818 7052\n",
            " 7775 7582 8116 4767 5248 1812 6055 8331  998 1564 7326  292 3065 6764\n",
            " 6854 4915 3583 5902 4229 2144 7565 3712 7224 4502 1406 1651 3827 6525\n",
            " 6318 2686 5369 8235 1704 1011 3351 8021  102  224 8294 7707 2985 5146\n",
            " 3709 6824 2281 6837 5436 2649 4886  966 5476 1199 8387 5449 1028 2226\n",
            " 6166 4387  350 6123 5236 5041  734 3412 6085  587  489 4982 5115 5575\n",
            " 3839 6743 7173 5724 7689 7787 8302 3150 2247 8128 6660 3505 4610 2866\n",
            " 4161 6905 7096  768 1062 4411 6087 2096 4903 7546 6630 2928 4640 7125\n",
            " 1792 2063 1700 7928 8095  737 5849  922 2359 6015 4489 4361 6081 6346\n",
            " 5014 5167 1023 8009  268 4777 5221 4323 6168 5702 6129 7181 1860 7192\n",
            " 5467 6579   67 2693 6134 4270 4611 6746 7696 3466 3950 6125 8370 6142\n",
            " 6131  995 5548 1721 5067 4015 7092 3156   25 1749 4878 8248 6836 5310\n",
            " 6960 2339 1650 6227 4104 5130 7089 6993 1289 4305 5585 7933 6284 7057\n",
            " 3807 5790 8347 5280 2841 7693 8090 4171 5260  133 7451 3957 1719 4743\n",
            " 4746 5435 6216 4858 4417 8355 4492 2648 5104 5278 7308  158 7543 2168\n",
            " 5913 7111 3414 7592   39 7157 5647 4555 3956 3617 4630 4384 6596 4539\n",
            " 6749 7202 2413 4749 5122 5929  252 5565  336 6008 4564 5580 7237 5958\n",
            " 7657 4327 5091 6918 6487 2989 7448 4715 3382  467 3784 4419 1117 7954\n",
            " 7122 3174 1534 4164 7526  498 5944 4572 2307 2929 7480 4412 4563 7692\n",
            " 1869 2472 6966 6549 1594 2276 7269 2545 2216  605 6587 6315 5297 5282\n",
            " 1619 5504 6399 3917 6342 5721 7909 6126 8382 4383 5502 7130  485 4936\n",
            " 7674 5372 7109 4605 5211 5462 8065   50 4080 3334 8167 8378 5595 1873\n",
            " 8207 7076 4106 3719 4627  231 2405 6973 3480 3246 5931 8208 4825 6203\n",
            " 3019 5822   63 5854 6543 5787 4764 6701 6562 3822 4861 1315 8318 8169\n",
            " 8218 7549  707 1076 2465 4636   80 7025 7127 4026 4961 7531  744 6684\n",
            " 4297 7939 7384  446 6675 7752 5279 8196 3020 4192 4253 2887 2556 5729\n",
            " 1898 3208 5973 7033 1431 5915 8312 5799 7105  511 6404 2852 1561]\n",
            "Fri Jun  3 01:31:16 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P0    38W / 300W |  15237MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "595/595 [==============================] - 220s 343ms/step - loss: 7.4392 - accuracy: 0.5902 - val_loss: 6.6760 - val_accuracy: 0.6750\n",
            "Epoch 2/5\n",
            "595/595 [==============================] - 204s 343ms/step - loss: 6.1653 - accuracy: 0.7478 - val_loss: 5.8813 - val_accuracy: 0.7253\n",
            "Epoch 3/5\n",
            "595/595 [==============================] - 201s 337ms/step - loss: 5.3038 - accuracy: 0.8230 - val_loss: 5.3978 - val_accuracy: 0.7118\n",
            "Epoch 4/5\n",
            "595/595 [==============================] - 201s 337ms/step - loss: 4.5488 - accuracy: 0.8758 - val_loss: 4.8276 - val_accuracy: 0.7195\n",
            "Epoch 5/5\n",
            "595/595 [==============================] - 204s 343ms/step - loss: 3.8745 - accuracy: 0.9271 - val_loss: 4.4154 - val_accuracy: 0.7350\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_3[0][0]',                \n",
            "                                thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_bert_model[1][0]']          \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 15)           7695        ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.7114014251781473\n",
            "Weighted F1: 0.701453113689681\n",
            "Micro F1: 0.7114014251781473\n",
            "Weighted Precision: 0.7086913680992655\n",
            "Micro Precision: 0.7114014251781473\n",
            "Weighted Recall: 0.7114014251781473\n",
            "Micro Recall: 0.7114014251781473\n",
            "Average Accuracy: 0.7064133016627079\n",
            "Average Weighted F1: 0.7012870139688424\n",
            "Average Micro F1: 0.7064133016627079\n",
            "Average Weighted Precision: 0.7125828141880526\n",
            "Average Micro Precision: 0.7064133016627079\n",
            "Average Weighted Recall: 0.7064133016627079\n",
            "Average Micro Recall: 0.7064133016627079\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "total_accuracy=0\n",
        "total_weighted_f1=0\n",
        "total_micro_f1=0\n",
        "total_weighted_precision=0\n",
        "total_micro_precision=0\n",
        "total_weighted_recall=0\n",
        "total_micro_recall=0\n",
        "\n",
        "for f in range(5):\n",
        "  gc.collect()\n",
        "  tf.keras.backend.clear_session()\n",
        "  dbert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "  dbert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
        "  max_len=512\n",
        "  sentences=summarized_data['text']\n",
        "  labels=summarized_data['label']\n",
        "  filenumbers=summarized_data['filenum']\n",
        "  len(sentences),len(labels),len(filenumbers)\n",
        "  \n",
        "  model_4=create_model()\n",
        "  input_ids=[]\n",
        "  attention_masks=[]\n",
        "\n",
        "  for sent in sentences:\n",
        "    dbert_inps=dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n",
        "    input_ids.append(dbert_inps['input_ids'])\n",
        "    attention_masks.append(dbert_inps['attention_mask'])\n",
        "  input_ids=np.asarray(input_ids)\n",
        "\n",
        "  attention_masks=np.array(attention_masks)\n",
        "  labels=np.array(labels)\n",
        "  filenumbers=[int(i) for i in filenumbers]\n",
        "  filenumbers=np.array(filenumbers)\n",
        "  print(labels)\n",
        "  print(filenumbers)\n",
        "\n",
        "  #train_inp,val_inp,train_label,val_label,train_mask,val_mask,train_fnum,val_fnum=train_test_split(input_ids,labels,attention_masks,filenumbers,test_size=0.1,random_state=42)\n",
        "\n",
        "  new_train_inp_4=[]\n",
        "  new_val_inp_4=[]\n",
        "  new_train_label_4=[]\n",
        "  new_val_label_4=[]\n",
        "  new_train_mask_4=[]\n",
        "  new_train_fnum_4=[]\n",
        "  new_val_fnum_4=[]\n",
        "  new_val_mask_4=[]\n",
        "  for i in range(len(train_fnum)):\n",
        "    if train_fnum[i] in filenumbers:\n",
        "      new_train_inp_4.append(train_inp[i])\n",
        "      #new_val_inp.append(val_inp[i])\n",
        "      new_train_label_4.append(train_label[i])\n",
        "      #new_val_label.append(val_label[i])\n",
        "      new_train_mask_4.append(train_mask[i])\n",
        "      #new_val_mask.append(val_mask[i])\n",
        "      new_train_fnum_4.append(train_fnum[i])\n",
        "\n",
        "  for i in range(len(val_fnum)):\n",
        "    if val_fnum[i] in filenumbers:\n",
        "      #new_train_inp.append(train_inp[i])\n",
        "      new_val_inp_4.append(val_inp[i])\n",
        "      #new_train_label.append(train_label[i])\n",
        "      new_val_label_4.append(val_label[i])\n",
        "      #new_train_mask.append(train_mask[i])\n",
        "      new_val_mask_4.append(val_mask[i])\n",
        "      new_val_fnum_4.append(val_fnum[i])\n",
        "\n",
        "  # print(new_train_label)\n",
        "  # print(len(new_train_label))\n",
        "  # print(new_val_label)\n",
        "  # print(len(new_val_label))\n",
        "  new_train_inp_4=np.array(new_train_inp_4)\n",
        "  new_val_inp_4=np.array(new_val_inp_4)\n",
        "  new_train_label_4=np.array(new_train_label_4)\n",
        "  new_val_label_4=np.array(new_val_label_4)\n",
        "  new_train_mask_4=np.array(new_train_mask_4)\n",
        "  new_train_fnum_4=np.array(new_train_fnum_4)\n",
        "  new_val_fnum_4=np.array(new_val_fnum_4)\n",
        "  new_val_mask_4=np.array(new_val_mask_4)\n",
        "\n",
        "  print(new_val_fnum_4)\n",
        "  \n",
        "  log_dir='dbert_model'\n",
        "\n",
        "  model_save_path='./drive/MyDrive/Ensemble/bert-ensemble-512-model_4-'+str(f)+'-15labels.h5'\n",
        "\n",
        "  loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "  accuracy = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
        "  callbacks= [tf.keras.callbacks.ModelCheckpoint(filepath=model_save_path,monitor='val_accuracy',mode='max',save_best_only=True,save_weights_only=True),keras.callbacks.TensorBoard(log_dir=log_dir)]\n",
        "  model_4.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])\n",
        "  gpu_info = !nvidia-smi\n",
        "  gpu_info = '\\n'.join(gpu_info)\n",
        "  if gpu_info.find('failed') >= 0:\n",
        "    print('Not connected to a GPU')\n",
        "  else:\n",
        "    print(gpu_info)\n",
        "  \n",
        "  history=model_4.fit([new_train_inp_4,new_train_mask_4],new_train_label_4,batch_size=8,epochs=5,validation_data=([new_val_inp_4,new_val_mask_4],new_val_label_4),callbacks=callbacks)\n",
        "\n",
        "  pred_labels=[]\n",
        "\n",
        "  model_saved_4= create_model()\n",
        "  model_saved_4.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])\n",
        "  model_saved_4.load_weights('./drive/MyDrive/Ensemble/bert-ensemble-512-model_4-'+str(f)+'-15labels.h5')\n",
        "\n",
        "  for i in range(0,len(val_inp)):\n",
        "    pred=model_saved_4.predict([val_inp[i].reshape(1,512),val_mask[i].reshape(1,512)])\n",
        "    pred_label = pred.argmax(axis=1)\n",
        "    pred_labels.append(pred_label)\n",
        "  accuracy=accuracy_score(val_label, pred_labels)\n",
        "  print(\"Accuracy: \"+str(accuracy))\n",
        "  total_accuracy=total_accuracy+accuracy\n",
        "  \n",
        "  weighted_f1=f1_score(val_label,pred_labels, average='weighted')\n",
        "  print(\"Weighted F1: \"+ str(weighted_f1))\n",
        "  total_weighted_f1=total_weighted_f1+weighted_f1\n",
        "  micro_f1=f1_score(val_label,pred_labels, average='micro')\n",
        "  print(\"Micro F1: \"+ str(micro_f1))\n",
        "  total_micro_f1=total_micro_f1+micro_f1\n",
        "\n",
        "  weighted_precision=precision_score(val_label, pred_labels, average='weighted')\n",
        "  print(\"Weighted Precision: \" + str(weighted_precision))\n",
        "  total_weighted_precision=total_weighted_precision+weighted_precision\n",
        "  micro_precision=precision_score(val_label, pred_labels, average='micro')\n",
        "  print(\"Micro Precision: \" + str(micro_precision))\n",
        "  total_micro_precision=total_micro_precision+micro_precision\n",
        "\n",
        "  weighted_recall=recall_score(val_label, pred_labels, average='weighted')\n",
        "  print(\"Weighted Recall: \" + str(weighted_recall))\n",
        "  total_weighted_recall=total_weighted_recall+weighted_recall\n",
        "  micro_recall=recall_score(val_label, pred_labels, average='micro')\n",
        "  print(\"Micro Recall: \" + str(micro_recall))\n",
        "  total_micro_recall=total_micro_recall+micro_recall\n",
        "\n",
        "\n",
        "print(\"Average Accuracy: \"+str(total_accuracy/5))\n",
        "print(\"Average Weighted F1: \"+str(total_weighted_f1/5))\n",
        "print(\"Average Micro F1: \"+str(total_micro_f1/5))\n",
        "print(\"Average Weighted Precision: \"+str(total_weighted_precision/5))\n",
        "print(\"Average Micro Precision: \"+str(total_micro_precision/5))\n",
        "print(\"Average Weighted Recall: \"+str(total_weighted_recall/5))\n",
        "print(\"Average Micro Recall: \"+str(total_micro_recall/5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "maHyXaSvPmP9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b1f2439-b4f5-4e0c-d42d-5eba00536545"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4504\n",
            "4504\n",
            "Average Length 511.5863676731794\n",
            "['8', '1', '8', '2', '8', '8', '8', '9', '7', '8', '1', '2', '1', '1', '8', '4', '8', '8', '12', '3', '3', '7', '3', '12', '1', '8', '8', '8', '8', '8', '8', '8', '1', '9', '5', '9', '9', '9', '11', '8', '8', '8', '4', '8', '8', '8', '8', '1', '3', '9', '3', '7', '1', '2', '9', '9', '7', '8', '8', '1', '10', '7', '8', '8', '9', '8', '7', '9', '9', '12', '7', '2', '8', '1', '11', '11', '1', '7', '7', '12', '1', '9', '8', '10', '12', '7', '8', '2', '8', '9', '9', '1', '8', '9', '1', '7', '12', '10', '10', '10', '8', '3', '7', '9', '8', '9', '1', '8', '8', '2', '7', '2', '9', '9', '11', '8', '8', '12', '12', '7', '8', '12', '4', '9', '3', '1', '12', '1', '1', '8', '8', '3', '8', '8', '8', '8', '9', '1', '8', '8', '10', '1', '8', '2', '8', '8', '7', '3', '8', '2', '4', '4', '9', '8', '10', '12', '12', '1', '1', '9', '1', '1', '1', '8', '2', '2', '8', '1', '1', '2', '2', '1', '2', '8', '1', '9', '9', '8', '8', '4', '2', '9', '9', '8', '3', '4', '3', '1', '8', '8', '2', '1', '9', '7', '8', '8', '1', '12', '3', '8', '2', '4', '2', '9', '12', '1', '4', '1', '8', '8', '8', '2', '2', '8', '9', '8', '8', '8', '10', '9', '8', '7', '9', '1', '1', '9', '4', '2', '4', '2', '2', '1', '7', '8', '11', '11', '3', '9', '2', '4', '8', '9', '1', '8', '1', '1', '4', '9', '1', '1', '8', '8', '2', '1', '8', '4', '2', '8', '9', '8', '8', '2', '8', '8', '8', '7', '1', '1', '1', '2', '1', '1', '8', '7', '8', '8', '12', '2', '12', '12', '8', '10', '12', '8', '3', '3', '12', '10', '1', '8', '12', '1', '8', '8', '2', '8', '4', '7', '8', '7', '10', '8', '10', '9', '8', '12', '12', '1', '8', '8', '3', '8', '8', '8', '8', '8', '1', '9', '8', '11', '1', '1', '1', '9', '8', '1', '9', '2', '3', '11', '8', '9', '9', '9', '2', '1', '8', '8', '9', '7', '1', '4', '9', '4', '8', '8', '4', '8', '12', '9', '4', '8', '2', '10', '10', '10', '8', '9', '9', '8', '8', '12', '7', '1', '8', '8', '8', '4', '1', '1', '1', '1', '1', '1', '8', '1', '9', '8', '9', '9', '4', '8', '12', '9', '8', '8', '2', '8', '8', '8', '6', '9', '8', '3', '7', '8', '8', '4', '12', '8', '8', '9', '12', '12', '9', '8', '2', '9', '2', '3', '1', '12', '8', '10', '9', '9', '9', '10', '10', '3', '8', '12', '1', '4', '2', '1', '10', '8', '2', '8', '4', '8', '9', '1', '9', '9', '10', '10', '1', '4', '9', '2', '4', '9', '1', '1', '3', '10', '3', '3', '8', '7', '3', '8', '9', '9', '12', '4', '8', '12', '2', '2', '4', '1', '9', '9', '4', '1', '4', '2', '8', '12', '2', '3', '10', '10', '9', '8', '9', '9', '1', '12', '8', '8', '8', '12', '4', '1', '8', '8', '1', '9', '8', '8', '2', '1', '8', '9', '8', '3', '3', '3', '1', '8', '8', '9', '1', '10', '9', '9', '9', '9', '5', '9', '9', '8', '8', '8', '8', '8', '8', '8', '8', '11', '12', '8', '8', '1', '8', '9', '11', '2', '2', '2', '2', '3', '1', '2', '2', '8', '2', '4', '9', '1', '2', '9', '8', '2', '8', '9', '9', '3', '10', '9', '9', '2', '8', '9', '8', '12', '12', '1', '3', '8', '8', '8', '2', '7', '7', '7', '7', '3', '9', '1', '9', '8', '9', '9', '1', '1', '1', '2', '9', '9', '9', '11', '1', '8', '8', '9', '1', '9', '8', '8', '8', '1', '1', '8', '7', '1', '1', '8', '8', '9', '4', '4', '8', '2', '2', '8', '8', '8', '8', '8', '8', '11', '8', '2', '9', '4', '9', '3', '9', '9', '1', '3', '9', '3', '1', '12', '8', '9', '12', '1', '8', '4', '2', '1', '4', '8', '3', '3', '8', '2', '8', '9', '7', '8', '8', '8', '5', '8', '3', '9', '8', '8', '13', '12', '1', '1', '2', '8', '4', '1', '9', '9', '12', '8', '9', '12', '9', '1', '9', '9', '9', '9', '3', '2', '9', '9', '4', '8', '12', '2', '4', '9', '3', '1', '9', '7', '8', '9', '9', '8', '4', '8', '8', '7', '9', '10', '3', '8', '8', '8', '1', '1', '1', '1', '8', '8', '4', '1', '10', '1', '5', '7', '7', '1', '8', '9', '3', '7', '2', '7', '7', '2', '4', '8', '12', '7', '4', '2', '9', '9', '12', '6', '10', '8', '2', '4', '12', '9', '9', '3', '8', '8', '1', '2', '10', '9', '9', '8', '4', '12', '2', '1', '8', '8', '8', '12', '10', '10', '9', '3', '8', '8', '9', '2', '8', '10', '1', '1', '1', '1', '2', '1', '1', '1', '1', '9', '8', '12', '9', '4', '8', '8', '9', '1', '9', '3', '9', '8', '8', '1', '7', '7', '10', '1', '8', '8', '1', '9', '8', '10', '3', '1', '7', '1', '8', '8', '12', '8', '8', '1', '8', '7', '1', '7', '7', '8', '2', '1', '8', '8', '2', '10', '8', '8', '8', '8', '8', '10', '1', '8', '8', '12', '8', '3', '3', '2', '2', '2', '10', '8', '8', '8', '2', '9', '1', '8', '9', '3', '2', '8', '10', '8', '6', '1', '1', '8', '4', '1', '9', '10', '8', '1', '7', '1', '2', '8', '1', '1', '1', '12', '1', '9', '12', '8', '12', '12', '12', '8', '8', '12', '4', '8', '8', '8', '8', '9', '9', '1', '3', '3', '3', '3', '1', '12', '12', '9', '10', '8', '8', '1', '9', '2', '2', '13', '9', '8', '9', '2', '1', '9', '1', '8', '8', '8', '4', '8', '1', '1', '1', '12', '12', '7', '2', '2', '2', '8', '3', '8', '9', '2', '10', '7', '8', '9', '2', '1', '2', '12', '12', '8', '8', '9', '2', '2', '9', '11', '1', '8', '1', '10', '9', '2', '1', '4', '7', '7', '7', '7', '7', '12', '8', '8', '8', '1', '1', '10', '1', '12', '1', '8', '2', '1', '1', '12', '8', '7', '9', '12', '8', '9', '3', '9', '8', '8', '8', '8', '3', '11', '2', '2', '9', '8', '8', '10', '8', '2', '7', '3', '1', '4', '7', '8', '8', '1', '8', '3', '7', '12', '8', '10', '9', '9', '8', '8', '2', '8', '9', '9', '1', '2', '8', '8', '9', '8', '3', '8', '1', '8', '10', '9', '8', '9', '9', '12', '4', '4', '8', '9', '9', '8', '2', '10', '1', '2', '8', '9', '1', '9', '9', '9', '7', '12', '12', '8', '1', '1', '1', '1', '8', '3', '1', '1', '8', '1', '8', '8', '7', '8', '8', '8', '8', '3', '2', '2', '10', '10', '10', '7', '8', '1', '2', '12', '7', '9', '8', '7', '8', '12', '2', '8', '9', '2', '6', '6', '7', '9', '8', '1', '8', '9', '8', '1', '12', '1', '2', '8', '7', '7', '7', '8', '2', '2', '8', '1', '2', '2', '9', '9', '1', '8', '8', '4', '3', '3', '1', '6', '3', '3', '12', '3', '8', '9', '1', '4', '3', '1', '8', '3', '9', '2', '8', '2', '8', '8', '8', '1', '1', '1', '9', '9', '8', '1', '9', '8', '1', '1', '3', '10', '8', '1', '1', '3', '9', '1', '4', '4', '1', '8', '9', '9', '2', '0', '0', '1', '8', '3', '1', '8', '8', '9', '8', '8', '1', '1', '8', '9', '8', '8', '8', '7', '9', '8', '8', '8', '10', '9', '8', '1', '2', '6', '1', '9', '9', '8', '12', '12', '12', '8', '8', '2', '8', '1', '2', '2', '2', '1', '9', '8', '2', '12', '2', '8', '12', '8', '9', '8', '8', '9', '7', '1', '1', '1', '1', '1', '8', '8', '1', '8', '8', '1', '1', '3', '2', '8', '8', '9', '10', '10', '2', '2', '1', '9', '2', '9', '9', '4', '12', '12', '12', '10', '7', '3', '3', '4', '2', '2', '9', '2', '8', '4', '2', '4', '1', '10', '9', '7', '8', '7', '1', '1', '3', '3', '1', '1', '3', '3', '3', '1', '1', '1', '1', '8', '2', '3', '1', '1', '2', '8', '8', '12', '8', '8', '8', '8', '11', '9', '1', '8', '9', '2', '8', '8', '8', '3', '9', '1', '9', '2', '7', '2', '8', '2', '8', '10', '8', '1', '10', '1', '1', '9', '9', '8', '8', '1', '8', '8', '8', '12', '8', '8', '8', '1', '8', '8', '8', '1', '9', '1', '1', '8', '1', '8', '9', '8', '2', '12', '9', '9', '0', '1', '8', '8', '1', '8', '12', '8', '8', '10', '8', '8', '8', '7', '8', '1', '8', '7', '3', '10', '1', '8', '9', '1', '8', '8', '8', '10', '1', '10', '3', '9', '1', '8', '9', '2', '8', '3', '3', '9', '9', '7', '9', '1', '1', '9', '2', '1', '1', '1', '7', '1', '1', '8', '8', '1', '1', '8', '1', '8', '3', '12', '9', '3', '3', '8', '8', '8', '8', '3', '1', '3', '3', '1', '11', '0', '8', '8', '7', '8', '12', '1', '8', '9', '8', '9', '8', '8', '3', '8', '8', '1', '1', '1', '9', '2', '2', '2', '8', '7', '12', '8', '8', '9', '10', '10', '7', '8', '1', '9', '8', '7', '3', '1', '3', '8', '2', '2', '3', '9', '8', '4', '4', '8', '9', '2', '1', '1', '7', '8', '9', '9', '7', '8', '7', '7', '8', '2', '2', '8', '4', '9', '7', '10', '0', '9', '8', '3', '7', '8', '1', '1', '8', '9', '9', '2', '2', '10', '1', '9', '10', '10', '10', '8', '3', '2', '12', '9', '9', '10', '12', '9', '12', '12', '9', '1', '2', '4', '12', '12', '7', '8', '9', '7', '7', '7', '3', '9', '8', '9', '1', '12', '8', '9', '4', '1', '3', '12', '12', '12', '12', '8', '8', '2', '1', '1', '2', '1', '1', '1', '12', '12', '8', '12', '2', '2', '12', '3', '3', '12', '8', '2', '8', '8', '12', '2', '1', '10', '3', '2', '8', '7', '1', '8', '1', '3', '7', '8', '9', '8', '3', '1', '1', '7', '8', '8', '9', '8', '2', '9', '2', '2', '9', '8', '1', '8', '8', '1', '3', '3', '1', '1', '10', '1', '2', '8', '1', '1', '1', '1', '9', '1', '4', '1', '7', '7', '7', '7', '2', '2', '8', '8', '12', '1', '9', '1', '7', '3', '3', '1', '8', '8', '10', '8', '9', '2', '9', '1', '3', '8', '8', '3', '12', '2', '8', '12', '2', '9', '1', '3', '3', '3', '3', '2', '8', '7', '9', '8', '3', '3', '1', '7', '8', '3', '1', '1', '12', '8', '9', '1', '2', '3', '8', '1', '1', '3', '3', '9', '1', '1', '1', '12', '1', '7', '3', '3', '1', '8', '8', '8', '1', '2', '4', '8', '1', '10', '2', '5', '3', '3', '12', '10', '9', '9', '12', '9', '0', '2', '8', '8', '9', '9', '9', '8', '1', '3', '1', '1', '4', '8', '1', '10', '8', '7', '2', '8', '2', '8', '4', '7', '8', '1', '9', '1', '9', '8', '2', '8', '2', '7', '9', '2', '2', '9', '1', '8', '12', '1', '8', '1', '4', '1', '9', '9', '1', '10', '12', '4', '8', '1', '7', '3', '9', '2', '12', '7', '8', '8', '2', '1', '12', '9', '8', '1', '2', '2', '8', '10', '2', '1', '7', '7', '7', '12', '3', '3', '8', '3', '8', '8', '3', '9', '8', '9', '1', '1', '8', '7', '9', '3', '3', '8', '1', '0', '9', '9', '9', '1', '8', '9', '9', '10', '1', '8', '7', '8', '8', '8', '9', '9', '8', '9', '10', '4', '9', '3', '7', '12', '1', '9', '9', '8', '9', '1', '9', '3', '1', '8', '4', '12', '10', '9', '8', '7', '10', '8', '12', '12', '3', '10', '8', '12', '2', '1', '2', '3', '9', '8', '8', '7', '1', '1', '2', '2', '1', '2', '2', '7', '1', '3', '9', '9', '3', '8', '8', '8', '8', '8', '7', '8', '8', '10', '8', '1', '8', '2', '8', '2', '2', '2', '2', '2', '1', '7', '12', '10', '1', '2', '8', '1', '4', '7', '8', '1', '12', '8', '7', '9', '2', '2', '2', '8', '1', '8', '1', '1', '0', '2', '2', '6', '1', '8', '2', '1', '8', '1', '1', '2', '2', '3', '2', '2', '10', '10', '9', '1', '7', '7', '8', '8', '1', '12', '10', '12', '3', '8', '8', '8', '3', '8', '3', '10', '2', '2', '2', '1', '2', '2', '1', '0', '1', '8', '9', '1', '1', '1', '2', '2', '2', '2', '2', '1', '1', '1', '1', '12', '9', '9', '1', '10', '11', '12', '9', '1', '1', '8', '8', '1', '2', '12', '8', '3', '7', '2', '10', '9', '7', '8', '1', '2', '2', '7', '9', '12', '2', '2', '1', '8', '9', '2', '3', '2', '2', '2', '9', '9', '8', '9', '2', '3', '8', '8', '9', '7', '3', '10', '8', '8', '9', '1', '1', '2', '8', '1', '10', '8', '8', '9', '7', '1', '7', '12', '8', '1', '7', '2', '1', '3', '9', '2', '8', '8', '1', '2', '3', '9', '3', '8', '9', '7', '7', '1', '10', '2', '8', '8', '8', '1', '8', '2', '8', '8', '1', '2', '8', '10', '2', '1', '0', '8', '3', '8', '10', '10', '12', '3', '8', '9', '3', '10', '8', '8', '8', '1', '8', '8', '2', '2', '2', '2', '2', '2', '1', '1', '10', '1', '10', '8', '2', '1', '2', '2', '8', '3', '3', '2', '2', '1', '8', '1', '3', '1', '9', '2', '2', '1', '3', '2', '2', '3', '2', '2', '3', '2', '1', '1', '2', '1', '2', '7', '8', '2', '1', '12', '12', '3', '1', '9', '9', '7', '8', '7', '2', '7', '9', '2', '2', '2', '8', '9', '7', '9', '2', '8', '9', '9', '2', '9', '1', '1', '9', '8', '3', '3', '8', '2', '7', '8', '11', '2', '8', '9', '1', '8', '3', '1', '2', '1', '2', '2', '2', '2', '8', '3', '9', '2', '10', '9', '9', '2', '7', '7', '7', '1', '3', '8', '8', '1', '1', '8', '7', '1', '8', '8', '9', '3', '3', '3', '2', '4', '8', '12', '8', '1', '12', '9', '2', '8', '2', '12', '2', '1', '3', '8', '12', '12', '11', '8', '10', '9', '2', '9', '4', '10', '1', '12', '12', '9', '3', '8', '9', '12', '9', '9', '2', '2', '2', '3', '5', '1', '1', '1', '7', '7', '8', '2', '2', '8', '2', '1', '7', '3', '2', '9', '8', '9', '1', '1', '9', '8', '9', '3', '3', '2', '9', '4', '9', '8', '8', '1', '8', '10', '2', '0', '3', '8', '8', '8', '9', '12', '9', '8', '2', '2', '8', '10', '9', '8', '2', '2', '4', '1', '9', '9', '8', '8', '10', '3', '1', '1', '2', '8', '7', '8', '2', '1', '1', '8', '12', '2', '9', '4', '8', '3', '3', '3', '8', '2', '12', '12', '8', '9', '12', '8', '2', '12', '7', '9', '2', '2', '1', '3', '1', '3', '11', '9', '8', '1', '10', '2', '12', '3', '8', '4', '1', '2', '8', '8', '3', '9', '3', '8', '7', '1', '1', '3', '1', '8', '2', '9', '8', '12', '1', '1', '1', '9', '9', '1', '8', '8', '8', '2', '2', '12', '3', '1', '1', '1', '9', '9', '3', '2', '1', '1', '9', '9', '1', '9', '9', '1', '9', '3', '9', '1', '8', '1', '9', '8', '3', '8', '8', '9', '9', '8', '2', '8', '1', '2', '2', '10', '1', '2', '1', '1', '9', '1', '9', '3', '7', '7', '2', '10', '3', '3', '1', '1', '9', '1', '2', '1', '3', '2', '1', '1', '1', '1', '1', '1', '1', '1', '1', '2', '8', '8', '7', '1', '1', '8', '8', '2', '1', '12', '1', '8', '1', '9', '8', '8', '2', '1', '9', '9', '2', '2', '8', '1', '7', '7', '9', '8', '1', '6', '8', '8', '2', '2', '8', '3', '8', '3', '2', '3', '8', '1', '2', '2', '2', '8', '9', '9', '9', '12', '1', '9', '2', '1', '9', '2', '9', '9', '10', '8', '1', '1', '9', '2', '1', '7', '1', '3', '7', '1', '1', '1', '3', '8', '8', '8', '9', '9', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '1', '0', '1', '1', '1', '2', '2', '3', '1', '1', '2', '3', '3', '3', '7', '3', '1', '9', '3', '9', '1', '4', '1', '1', '9', '10', '9', '8', '2', '3', '10', '9', '3', '10', '12', '8', '9', '8', '2', '1', '9', '8', '1', '7', '8', '1', '9', '1', '2', '10', '7', '7', '8', '1', '3', '3', '9', '3', '3', '1', '1', '1', '9', '1', '3', '8', '8', '8', '1', '8', '1', '9', '7', '8', '8', '2', '1', '3', '8', '2', '1', '6', '1', '8', '8', '2', '8', '2', '2', '9', '8', '1', '1', '6', '8', '6', '10', '2', '1', '1', '8', '9', '3', '3', '3', '1', '3', '9', '6', '8', '1', '8', '3', '1', '2', '2', '12', '7', '1', '1', '1', '1', '12', '1', '8', '8', '3', '2', '1', '9', '2', '3', '3', '2', '2', '7', '2', '2', '2', '3', '1', '9', '8', '7', '1', '1', '3', '1', '9', '4', '3', '9', '2', '1', '1', '9', '8', '8', '7', '2', '1', '3', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '2', '10', '1', '1', '8', '2', '8', '1', '8', '1', '3', '1', '2', '1', '1', '3', '1', '2', '1', '1', '1', '1', '1', '1', '1', '2', '1', '8', '1', '9', '8', '8', '1', '10', '8', '8', '1', '3', '9', '8', '9', '8', '2', '1', '3', '4', '1', '12', '1', '9', '8', '2', '7', '2', '1', '2', '9', '2', '1', '3', '8', '1', '2', '3', '8', '2', '10', '10', '10', '9', '9', '9', '3', '8', '3', '1', '1', '1', '1', '1', '1', '2', '1', '9', '2', '7', '9', '8', '7', '9', '1', '1', '1', '1', '8', '2', '2', '3', '3', '2', '12', '8', '3', '1', '1', '12', '9', '8', '2', '2', '9', '1', '2', '2', '9', '1', '1', '2', '4', '8', '1', '8', '10', '2', '9', '9', '9', '1', '2', '1', '1', '2', '2', '2', '12', '1', '2', '8', '3', '9', '9', '3', '2', '9', '9', '7', '2', '8', '8', '9', '2', '9', '1', '1', '1', '1', '1', '2', '8', '2', '1', '2', '2', '8', '3', '1', '1', '9', '1', '3', '10', '7', '9', '8', '8', '2', '2', '9', '2', '2', '2', '2', '2', '2', '9', '1', '3', '6', '4', '2', '2', '9', '1', '9', '8', '1', '1', '1', '7', '3', '2', '9', '9', '1', '9', '7', '2', '8', '9', '9', '12', '10', '8', '10', '2', '3', '1', '2', '2', '12', '12', '3', '12', '1', '2', '1', '2', '1', '2', '8', '2', '3', '12', '12', '8', '2', '2', '9', '2', '9', '2', '3', '3', '1', '1', '1', '12', '3', '7', '1', '3', '1', '2', '9', '2', '7', '2', '1', '8', '3', '7', '8', '1', '1', '3', '8', '3', '2', '9', '3', '1', '1', '1', '1', '11', '1', '2', '6', '1', '2', '3', '8', '9', '9', '2', '9', '2', '1', '1', '3', '1', '12', '9', '1', '1', '9', '1', '9', '9', '8', '9', '8', '1', '2', '7', '1', '0', '9', '8', '2', '3', '2', '4', '1', '1', '1', '8', '1', '12', '1', '2', '3', '3', '9', '9', '9', '9', '9', '7', '9', '3', '9', '1', '7', '3', '3', '3', '3', '7', '8', '2', '2', '2', '8', '2', '3', '9', '9', '3', '1', '8', '9', '12', '8', '8', '3', '2', '1', '6', '1', '9', '1', '1', '1', '1', '1', '1', '2', '2', '1', '2', '2', '2', '2', '9', '2', '5', '3', '2', '2', '8', '2', '1', '8', '3', '3', '8', '3', '1', '3', '9', '2', '8', '9', '4', '4', '9', '8', '8', '7', '7', '3', '1', '8', '8', '3', '2', '2', '10', '3', '3', '2', '2', '2', '2', '12', '3', '1', '2', '7', '7', '12', '2', '1', '2', '1', '1', '2', '1', '3', '3', '3', '1', '3', '9', '4', '8', '8', '4', '2', '4', '2', '8', '9', '1', '1', '2', '2', '2', '8', '7', '2', '8', '2', '9', '1', '8', '1', '1', '2', '1', '1', '1', '2', '9', '9', '8', '1', '8', '1', '4', '8', '1', '2', '7', '9', '2', '1', '8', '9', '8', '9', '1', '1', '2', '3', '2', '12', '7', '1', '2', '4', '4', '4', '2', '2', '2', '9', '8', '8', '1', '12', '3', '3', '2', '2', '3', '12', '1', '5', '1', '1', '3', '2', '8', '8', '2', '2', '2', '3', '9', '8', '8', '9', '8', '8', '11', '8', '2', '9', '2', '3', '1', '7', '1', '8', '2', '8', '1', '1', '1', '8', '1', '1', '1', '7', '8', '8', '2', '3', '2', '1', '8', '8', '1', '8', '4', '8', '9', '8', '9', '8', '1', '2', '3', '2', '1', '2', '2', '3', '1', '9', '4', '8', '1', '1', '9', '1', '1', '8', '2', '2', '2', '1', '7', '3', '3', '1', '3', '3', '12', '3', '1', '1', '3', '1', '1', '4', '8', '4', '4', '3', '3', '3', '4', '8', '1', '9', '9', '9', '4', '1', '7', '4', '8', '4', '9', '8', '1', '1', '3', '8', '9', '1', '9', '7', '1', '1', '9', '1', '8', '8', '8', '1', '1', '8', '2', '2', '2', '9', '4', '4', '1', '2', '1', '2', '9', '1', '1', '8', '5', '5', '5', '9', '12', '4', '1', '2', '1', '8', '8', '1', '9', '9', '2', '12', '1', '1', '7', '8', '8', '4', '1', '2', '8', '11', '2', '3', '2', '2', '1', '11', '2', '2', '2', '2', '9', '9', '2', '2', '9', '3', '1', '9', '1', '1', '1', '7', '8', '1', '8', '9', '1', '2', '9', '8', '1', '2', '9', '8', '12', '2', '2', '10', '8', '2', '2', '8', '8', '1', '2', '6', '1', '1', '7', '7', '2', '3', '2', '1', '1', '8', '1', '10', '8', '12', '6', '4', '1', '4', '1', '2', '9', '8', '9', '4', '8', '8', '10', '8', '8', '8', '8', '8', '2', '2', '8', '4', '9', '3', '3', '3', '3', '3', '9', '2', '1', '1', '2', '3', '2', '1', '2', '3', '3', '3', '2', '2', '3', '3', '2', '1', '2', '3', '3', '3', '3', '12', '9', '3', '2', '2', '3', '1', '5', '1', '1', '3', '2', '2', '1', '2', '9', '3', '9', '8', '1', '8', '7', '7', '9', '1', '1', '7', '9', '10', '1', '7', '1', '4', '3', '8', '9', '4', '9', '9', '2', '9', '2', '8', '4', '2', '2', '9', '5', '2', '1', '10', '9', '3', '1', '1', '1', '2', '1', '2', '9', '1', '7', '9', '9', '8', '2', '2', '8', '9', '1', '9', '8', '2', '3', '2', '2', '1', '2', '2', '2', '1', '1', '8', '1', '9', '4', '12', '8', '2', '7', '2', '2', '1', '9', '3', '1', '8', '12', '1', '1', '2', '1', '10', '2', '6', '12', '12', '4', '9', '1', '9', '10', '1', '2', '8', '2', '1', '8', '6', '12', '9', '9', '2', '1', '7', '8', '9', '5', '8', '1', '8', '8', '3', '1', '8', '2', '8', '2', '2', '1', '2', '9', '2', '8', '1', '12', '8', '2', '7', '3', '3', '12', '2', '3', '3', '9', '9', '3', '3', '3', '3', '3', '2', '1', '4', '1', '4', '8', '8', '2', '1', '2', '9', '12', '1', '10', '12', '1', '2', '9', '4', '8', '10', '3', '2', '8', '7', '4', '8', '9', '2', '7', '1', '10', '2', '2', '2', '8', '4', '2', '2', '1', '1', '8', '7', '9', '2', '1', '9', '8', '9', '12', '4', '8', '2', '8', '7', '7', '1', '9', '2', '1', '1', '9', '1', '8', '2', '3', '9', '9', '1', '3', '2', '9', '2', '2', '8', '1', '1', '2', '1', '2', '1', '9', '8', '2', '2', '4', '8', '8', '9', '5', '5', '9', '8', '9', '6', '2', '1', '12', '2', '3', '8', '8', '9', '2', '2', '1', '3', '2', '10', '9', '10', '7', '2', '8', '4', '2', '8', '9', '8', '1', '3', '8', '8', '1', '8', '3', '8', '8', '1', '10', '3', '1', '5', '8', '8', '9', '2', '2', '9', '2', '1', '9', '1', '4', '1', '12', '8', '8', '2', '1', '1', '1', '1', '1', '9', '2', '9', '8', '9', '5', '9', '1', '8', '2', '4', '1', '1', '1', '9', '12', '4', '1', '1', '9', '8', '4', '1', '8', '9', '8', '8', '9', '9', '9', '9', '9', '9', '1', '3', '4', '2', '2', '8', '2', '9', '2', '3', '8', '3', '1', '7', '2', '1', '1', '4', '2', '9', '2', '1', '3', '2', '9', '2', '9', '3', '8', '1', '9', '1', '1', '2', '8', '8', '8', '2', '3', '9', '9', '8', '2', '4', '1', '5', '1', '1', '9', '9', '2', '1', '1', '1', '9', '9', '1', '3', '9', '2', '2', '9', '9', '8', '3', '9', '9', '8', '2', '2', '8', '9', '2', '2', '8', '9', '10', '10', '2', '8', '7', '9', '9', '4', '2', '4', '11', '2', '1', '7', '8', '8', '8', '9', '4', '9', '8', '8', '2', '1', '1', '9', '8', '3', '3', '8', '8', '7', '9', '1', '3', '4', '8', '10', '2', '4', '4', '8', '2', '8', '2', '8', '9', '2', '1', '2', '2', '3', '1', '9', '4', '5', '9', '9', '1', '1', '1', '1', '1', '1', '7', '1', '1', '1', '8', '1', '1', '1', '1', '1', '9', '1', '2', '1', '12', '9', '3', '9', '9', '4', '1', '9', '1', '1', '2', '11', '9', '3', '2', '2', '2', '4', '2', '3', '2', '7', '7', '8', '1', '10', '9', '5', '7', '1', '10', '8', '1', '2', '7', '1', '12', '5', '8', '4', '2', '9', '9', '8', '2', '9', '8', '8', '10', '2', '3', '2', '7', '2', '8', '10', '3', '2', '9', '9', '9', '1', '1', '1', '1', '8', '2', '10', '9', '8', '1', '2', '2', '2', '1', '2', '1', '3', '9', '12', '2', '2', '4', '8', '1', '3', '9', '4', '9', '1', '9', '9', '2', '1', '1', '9', '7', '10', '3', '10', '2', '9', '2', '9', '2', '8', '2', '1', '2', '3', '1', '1', '1', '8', '5', '8', '3', '1', '1', '4', '2', '2', '1', '3', '8', '2', '1', '1', '1', '2', '9', '4', '1', '8', '1', '2', '8', '2', '9', '2', '2', '5', '5', '2', '5', '2', '1', '8', '8', '1', '3', '12', '2', '4', '3', '2', '2', '2', '6', '2', '3', '3', '1', '8', '2', '2', '1', '1', '8', '8', '10', '1', '9', '2', '9', '12', '0', '2', '1', '3', '2', '2', '1', '2', '1', '9', '9', '2', '9', '8', '8', '1', '7', '8', '1', '2', '6', '9', '8', '10', '1', '12', '2', '2', '1', '8', '2', '1', '12', '7', '4', '2', '10', '2', '2', '1', '2', '1', '2', '1', '1', '2', '1', '9', '8', '10', '2', '10', '9', '9', '12', '5', '3', '2', '8', '2', '8', '3', '3', '1', '4', '9', '10', '2', '4', '8', '1', '2', '10', '12', '12', '3', '1', '1', '2', '9', '6', '6', '1', '9', '1', '8', '1', '8', '8', '8', '8', '2', '2', '2', '8', '8', '2', '1', '1', '1', '1', '1', '8', '8', '5', '8', '8', '1', '8', '9', '9', '1', '7', '12', '9', '9', '7', '2', '2', '7', '8', '9', '8', '2', '9', '6', '3', '4', '8', '4', '1', '2', '9', '8', '8', '8', '2', '1', '1', '8', '8', '3', '1', '9', '9', '7', '1', '2', '2', '2', '9', '4', '1', '2', '12', '1', '8', '2', '1', '9', '1', '8', '9', '9', '2', '5', '4', '2', '1', '2', '8', '12', '8', '10', '3', '8', '8', '8', '2', '2', '8', '2', '8', '2', '4', '8', '10', '2', '7', '2', '1', '8', '8', '9', '5', '8', '1', '12', '7', '10', '2', '2', '2', '1', '4', '8', '12', '1', '8', '2', '2', '9', '1', '8', '3', '1', '1', '5', '10', '2', '1', '2', '4', '8', '9', '3', '2', '8', '7', '4', '4', '9', '9', '2', '8', '1', '1', '2', '4', '7', '1', '9', '1', '1', '1', '8', '4', '1', '3', '1', '2', '2', '9', '1', '8', '8', '2', '2', '9', '9', '1', '8', '8', '1', '9', '8', '8', '2', '2', '2', '2', '1', '1', '1', '7', '4', '1', '1', '4', '2', '3', '3', '2', '3', '9', '2', '8', '2', '1', '5', '1', '2', '2', '1', '3', '5', '2', '9', '1', '1', '9', '8', '1', '4', '8', '1', '8', '2', '4', '8', '4', '7', '9', '8', '1', '2', '2', '4', '4', '11', '3', '11', '1', '3', '8', '1', '9', '6', '8', '9', '3', '9', '8', '9', '2', '3', '3', '7', '1', '7', '2', '9', '9', '1', '8', '8', '1', '5', '5', '8', '8', '1', '2', '1', '3', '9', '9', '5', '9', '8', '1', '4', '3', '8', '1', '1', '2', '1', '8', '9', '8', '2', '2', '2', '1', '2', '4', '10', '8', '1', '9', '1', '8', '1', '1', '9', '9', '8', '11', '1', '8', '2', '1', '2', '8', '2', '8', '8', '2', '9', '6', '10', '1', '8', '6', '4', '5', '11', '2', '2', '8', '1', '8', '1', '8', '1', '4', '8', '1', '1', '1', '8', '3', '7', '3', '3', '7', '1', '1', '4', '8', '1', '6', '4', '2', '2', '1', '1', '1', '1', '6', '2', '2', '8', '1', '8', '5', '5', '2', '1', '1', '2', '3', '7', '1', '1', '6', '2', '9', '3', '9', '8', '9', '2', '1', '4', '2', '9', '13', '9', '2', '8', '12', '4', '1', '1', '9', '5', '1', '1', '1', '1', '8', '2', '1', '1', '2', '9', '0', '12', '1', '1', '8', '9', '8', '3', '2', '2', '12', '8', '2', '2', '2', '1', '10', '1', '9', '8', '9', '5', '2', '2', '9', '2', '8', '4', '8', '3', '7', '9', '2', '2', '7', '2', '2', '8', '1', '1', '9', '10', '9', '8', '2', '9', '0', '7', '1', '1', '1', '9', '8', '2', '8', '12', '8', '3', '2', '8', '8', '7', '7', '8', '12', '2', '1', '3', '9', '2', '2', '8', '2', '8', '8', '12', '8', '8', '1', '8', '9', '2', '4', '4', '7', '2', '1', '1', '9', '3', '7', '1', '3', '9', '8', '2', '2', '3', '1', '3', '10', '2', '3', '7', '1', '8', '1', '1', '10', '3', '10', '8', '4', '1', '2', '4', '3', '10', '9', '3', '9', '5', '8', '7', '2', '9', '3', '3', '2', '11', '8', '1', '1', '7', '2', '7', '2', '9', '4', '8', '1', '12', '8', '7', '9', '1', '0', '2', '6', '4', '9', '3', '2', '4', '12', '9', '8', '5', '9', '2', '8', '4', '6', '8', '9', '3', '1', '8', '7', '3', '1', '1', '1', '8', '9', '1', '8', '7', '9', '8', '2', '1', '2', '2', '3', '2', '2', '1', '1', '4', '9', '7', '3', '2', '8', '8', '9', '1', '9', '7', '4', '2', '2', '8', '5', '9', '1', '5', '7', '1', '4', '6', '9', '10', '1', '8', '2', '9', '1', '9', '2', '9', '8', '2', '9', '9', '11', '2', '12', '2', '2', '9', '4', '8', '1', '9', '9', '2', '8', '9', '1', '2', '9', '3', '8', '8', '1', '7', '8', '8', '2', '3', '2', '2', '2', '2', '9', '8', '10', '2', '2', '1', '9', '11', '1', '8', '8', '2', '4', '2', '1', '2', '8', '8', '9', '2', '8', '9', '2', '3', '1', '2', '1', '3', '10', '8', '1', '2', '4', '1', '9', '8', '9', '3', '3', '9', '8', '2', '11', '3', '7', '1', '8', '7', '1', '8', '8', '8', '1', '4', '8', '8', '1', '9', '3', '1', '1', '2', '8', '8', '8', '2', '1', '8', '10', '2', '12', '8', '8', '9', '1', '8', '1', '3', '2', '2', '10', '7', '2', '2', '1', '1', '8', '2', '9', '1', '2', '9', '3', '3', '10', '1', '4', '7', '10', '12', '2', '4', '10', '8', '6', '2', '9', '1', '8', '3', '1', '2', '8', '8', '2', '12', '10', '2', '10', '9', '1', '2', '5', '8', '12', '8', '8', '7', '8', '8', '1', '8', '2', '2', '9', '1', '8', '10', '5', '5', '5', '8', '1', '9', '1', '13', '1', '2', '2', '1', '2', '4', '9', '2', '1', '9', '13', '11', '1', '9', '9', '3', '10', '8', '7', '8', '8', '4', '2', '1', '9', '4', '3', '1', '1', '10', '9', '2', '8', '6', '2', '1', '1', '3', '2', '8', '1', '1', '1', '1', '2', '4', '1', '1', '10', '1', '8', '1', '1', '9', '1', '7', '1', '1', '9', '9', '2', '12', '9', '1', '8', '1', '12', '1', '8', '3', '8', '1', '8', '1', '2', '10', '1', '1', '2', '2', '9', '3', '9', '12', '1', '9', '1', '2', '8', '7', '2', '1', '1', '3', '2', '8', '4', '4', '5', '12', '7', '8', '9', '6', '1', '8', '9', '2', '1', '9', '1', '4', '1', '8', '2', '1', '1', '8', '7', '4', '9', '1', '8', '4', '2', '7', '5', '1', '3', '9', '2', '8', '2', '1', '2', '9', '1', '1', '2', '3', '4', '3', '1', '2', '2', '10', '2', '2', '8', '1', '1', '2', '4', '9', '2', '11', '9', '8', '9', '2', '1', '10', '4', '1', '1', '10', '7', '2', '10', '8', '1', '10', '8', '8', '8', '8', '8', '8', '2', '2', '2', '8', '3', '4', '1', '1', '1', '2', '1', '8', '8', '2', '8', '3', '8', '3', '1', '2', '3', '1', '1', '10', '1', '4', '4', '3', '3', '1', '9', '1', '1', '2', '1', '1', '6', '1', '1', '1', '9', '1', '4', '1', '1', '1', '1', '1', '1', '2', '12', '8', '8', '1', '9', '12', '10', '8', '2', '12', '1', '1', '2', '1', '7', '10', '7', '1', '2', '1', '10', '8', '2', '2', '8', '2', '2', '1', '9', '8', '8', '8', '8', '3', '1', '4', '2', '8', '3', '10', '10', '1', '9', '1', '9', '1', '9', '1', '8', '3', '1', '1', '8', '8', '8', '6', '3', '4', '1', '1', '1', '9', '9', '5', '2', '10', '2', '6', '2', '2', '7', '1', '8', '2', '11', '1', '1', '1', '2', '1', '9', '4', '4', '2', '8', '4', '6', '8', '8', '10', '10', '2', '1', '2', '9', '1', '8', '8', '3', '4', '8', '8', '2', '8', '8', '1', '2', '1', '2', '9', '4', '1', '3', '3', '3', '8', '8', '8', '2', '6', '4', '3', '3', '12', '3', '4', '9', '6', '8', '7', '7', '8', '6', '4', '1', '10', '6', '3', '3', '8', '1', '1', '8', '1', '11', '8', '1', '1', '2', '3', '11', '7', '2', '1', '1', '1', '9', '1', '2', '1', '1', '9', '8', '1', '1', '8', '0', '4', '1', '1', '2', '1', '1', '2', '2', '9', '1', '10', '1', '3', '8', '9', '3', '3', '1', '10', '1', '9', '1', '1', '7', '4', '1', '8', '10', '1', '2', '10', '1', '1', '2', '3', '3', '9', '1', '8', '10', '1', '7', '9', '1', '9', '6', '3', '9', '2', '8', '4', '12', '12', '3', '1', '1', '9', '1', '9', '2', '1', '1', '1', '1', '1', '9', '2', '1', '2', '10', '9', '8', '8', '10', '8', '2', '3', '2', '1', '12', '8', '12', '2', '9', '1', '3', '2', '5', '2', '2', '8', '2', '2', '4', '10', '9', '10', '9', '4', '2', '4', '12', '8', '2', '1', '8', '3', '9', '2', '9', '4', '1', '1', '1', '1', '1', '9', '6', '2', '9', '2', '1', '3', '2', '2', '5', '8', '9', '1', '6', '3', '1', '2', '2', '2', '6', '1', '8', '10', '8', '3', '3', '13', '2', '9', '9', '10', '6', '12', '8', '2', '9', '2', '3', '8', '8', '1', '2', '2', '3', '2', '10', '1', '8', '9', '1', '8', '9', '8', '8', '2', '10', '2', '1', '1', '1', '9', '9', '12', '1', '1', '4', '8', '3', '2', '2', '4', '4', '2', '2', '1', '1', '1', '9', '2', '2', '4', '1', '1', '8', '8', '8', '9', '2', '13', '9', '9', '4', '1', '9', '7', '10', '10', '8', '1', '9', '1', '1', '1', '8', '12', '4', '1', '2', '1', '1', '1', '7', '8', '3', '3', '1', '2', '2', '7', '2', '2', '10', '1', '8', '4', '0', '1', '9', '4', '1', '1', '2', '10', '10', '7', '9', '3', '12', '11', '2', '8', '10', '8', '9', '4', '3', '4', '10', '1', '1', '9', '3', '10', '1', '5', '3', '3', '2', '2', '1', '1', '6', '1', '7', '1', '1', '12', '1', '1', '1', '10', '1', '8', '8', '1', '3', '8', '3', '1', '2', '8', '3', '4', '2', '8', '8', '6', '1', '1', '4', '1', '2', '9', '5', '1', '7', '8', '9', '2', '4', '9', '8', '5', '9', '9', '9', '1', '2', '1', '3', '8', '9', '2', '9', '8', '9', '1', '8', '13', '9', '3', '8', '1', '4', '1', '3', '1', '6', '4', '10', '2', '2', '7', '8', '8', '12', '8', '9', '9', '9', '10', '3', '12', '12', '2', '9', '10', '9', '7', '2', '3', '4', '10', '10', '2', '7', '7', '1', '7', '3', '9', '1', '9', '8', '2', '9', '1', '8', '1', '10', '8', '8', '2', '7', '8', '1', '8', '9', '8', '1', '4', '1', '8', '8', '8', '1', '1', '10', '3', '2', '2', '6', '8', '9', '8', '2', '1', '1', '3', '8', '8', '1', '4', '4', '3', '9', '10', '4', '2', '4', '2', '9', '2', '2', '9', '1', '1', '10', '1', '1', '2', '1', '1', '1', '9', '1', '8', '8', '8', '2', '3', '8', '1', '6', '7', '3', '1', '9', '1', '2', '1', '6', '1', '2', '10', '1', '4', '2', '3', '2', '9', '9', '4', '9', '9', '1', '2', '8', '4', '7', '8', '7', '13', '9', '1', '2', '1', '9', '3', '3', '6', '8', '9', '10', '9', '2', '2', '3', '12', '1', '1', '1', '13', '2', '1', '7', '10', '8', '9', '1', '2', '9', '1', '1', '1', '2', '1', '12', '5', '6', '9', '10', '3', '1', '10', '1', '3', '2', '8', '9', '2', '10', '8', '8', '9', '2', '8', '9', '2', '2', '2', '8', '8', '2', '3', '1', '4', '8', '1', '9', '9', '2', '8', '10', '9', '1', '2', '12', '8', '8', '0', '2', '1', '1', '9', '10', '1', '6', '2', '2', '10', '2', '2', '4', '2', '2', '12', '1', '6', '7', '8', '9', '1', '3', '5', '7', '3', '1', '3', '1', '1', '3', '2', '6', '3', '2', '1', '8', '3', '5', '2', '1', '1', '8', '1', '1', '0', '2', '3', '5', '3', '1', '1', '9', '8', '8', '8', '8', '4', '7', '2', '9', '12', '5', '9', '3', '12', '3', '9', '1', '9', '1', '7', '3', '9', '3', '10', '1', '2', '1', '4', '9', '8', '2', '9', '11', '2', '8', '9', '4', '1', '1', '1', '1', '1', '1', '1', '9', '1', '9', '1', '1', '1', '12', '3', '10', '3', '6', '1', '7', '10', '8', '3', '7', '1', '1', '2', '6', '1', '3', '9', '2', '8', '1', '8', '8', '1', '8', '10', '12', '1', '10', '12', '10', '1', '8', '8', '1', '4', '7', '9', '2', '1', '9', '3', '8', '12', '10', '6', '1', '6', '8', '3', '1', '3', '1', '10', '10', '9', '8', '1', '2', '9', '8', '8', '1', '9', '8', '8', '3', '1', '3', '9', '12', '1', '8', '1', '5', '8', '11', '5', '5', '1', '2', '1', '3', '1', '1', '1', '9', '8', '8', '1', '9', '10', '8', '4', '1', '1', '8', '10', '8', '9', '1', '10', '1', '7', '2', '1', '1', '9', '6', '2', '9', '8', '1', '1', '1', '9', '8', '8', '1', '9', '9', '1', '1', '2', '7', '9', '2', '9', '9', '8', '7', '6', '8', '7', '8', '2', '9', '2', '1', '8', '8', '8', '2', '6', '3', '1', '8', '12', '12', '8', '7', '1', '2', '2', '7', '1', '9', '8', '1', '4', '1', '1', '8', '5', '8', '1', '8', '8', '8', '8', '1', '2', '11', '1', '1', '1', '2', '7', '1', '9', '2', '2', '2', '9', '8', '9', '2', '9', '1', '8', '1', '7', '11', '13', '1', '9', '8', '2', '2', '1', '2', '3', '10', '3', '10', '4', '3', '9', '1', '8', '1', '1', '13', '1', '1', '3', '8', '1', '2', '2', '2', '9', '4', '1', '8', '1', '9', '3', '6', '8', '5', '2', '8', '2', '8', '1', '9', '2', '2', '8', '1', '1', '4', '8', '9', '2', '2', '7', '1', '8', '12', '2', '12', '8', '2', '9', '9', '3', '8', '1', '1', '9', '9', '9', '2', '8', '1', '9', '1', '8', '9', '8', '2', '2', '4', '1', '9', '10', '10', '8', '8', '1', '1', '2', '1', '4', '1', '2', '1', '1', '3', '12', '8', '8', '8', '8', '10', '2', '8', '12', '1', '9', '9', '8', '9', '1', '2', '1', '8', '8', '8', '2', '8', '8', '3', '10', '8', '9', '1', '1', '1', '3', '4', '8', '10', '6', '3', '1', '3', '2', '8', '9', '3', '5', '4', '1', '5', '2', '9', '9', '9', '1', '1', '1', '9', '9', '6', '10', '9', '8', '12', '2', '9', '1', '2', '1', '1', '1', '8', '1', '12', '1', '12', '1', '9', '1', '8', '1', '2', '1', '2', '8', '8', '10', '1', '9', '1', '2', '2', '8', '8', '3', '12', '1', '11', '2', '10', '12', '11', '2', '1', '10', '1', '1', '9', '3', '10', '2', '7', '1', '8', '1', '9', '9', '2', '1', '3', '12', '5', '9', '9', '8', '1', '8', '8', '8', '1', '1', '3', '9', '8', '9', '8', '3', '10', '3', '7', '8', '9', '2', '3', '1', '9', '10', '9', '2', '8', '2', '6', '4', '1', '1', '3', '8', '2', '3', '9', '1', '2', '1', '8', '1', '2', '2', '2', '2', '9', '9', '4', '8', '9', '1', '1', '2', '9', '1', '5', '2', '9', '8', '10', '1', '2', '10', '8', '5', '6', '4', '8', '1', '1', '1', '8', '9', '2', '8', '8', '8', '12', '2', '2', '1', '8', '1', '2', '1', '9', '8', '8', '9', '1', '1', '8', '7', '1', '8', '0', '2', '3', '8', '1', '1', '8', '1', '1', '12', '3', '2', '10', '8', '10', '6', '1', '8', '8', '10', '8', '8', '1', '4', '8', '1', '2', '9', '8', '1', '1', '3', '3', '5', '1', '1', '2', '1', '2', '1', '9', '8', '3', '9', '8', '2', '10', '9', '1', '8', '1', '10', '9', '10', '1', '2', '3', '8', '1', '3', '1', '9', '9', '8', '1', '9', '8', '8', '9', '1', '9', '2', '8', '8', '2', '13', '8', '10', '8', '9', '3', '2', '1', '3', '1', '12', '10', '10', '1', '11', '1', '2', '4', '2', '10', '1', '8', '11', '1', '1', '1', '2', '2', '8', '2', '9', '8', '9', '9', '12', '8', '1', '9', '2', '4', '1', '8', '3', '9', '1', '6', '5', '8', '9', '3', '3', '2', '1', '1', '1', '8', '11', '1', '8', '7', '1', '11', '9', '1', '7', '9', '9', '8', '8', '8', '12', '8', '1', '2', '9', '8', '8', '2', '10', '9', '8', '8', '4', '10', '9', '8', '2', '10', '10', '2', '1', '2', '1', '2', '1', '4', '1', '7', '1', '1', '3', '12', '9', '8', '5', '9', '10', '1', '9', '9', '2', '7', '4', '1', '1', '8', '12', '8', '2', '2', '9', '4', '1', '1', '5', '9', '1', '8', '12', '8', '1', '1', '4', '9', '10', '2', '3', '1', '3', '3', '3', '8', '1', '2', '1', '2', '1', '2', '9', '1', '2', '8', '9', '4', '8', '8', '10', '2', '12', '5', '5', '1', '8', '1', '7', '8', '1', '1', '8', '9', '12', '4', '9', '3', '1', '2', '5', '9', '2', '1', '2', '2', '1', '2', '8', '1', '8', '8', '2', '2', '1', '8', '4', '1', '2', '10', '9', '10', '8', '1', '10', '4', '8', '8', '10', '2', '8', '1', '1', '3', '10', '1', '4', '2', '8', '3', '3', '2', '9', '8', '5', '5', '9', '3', '10', '8', '1', '2', '1', '2', '9', '1', '1', '2', '9', '9', '8', '8', '11', '9', '4', '1', '8', '2', '7', '1', '2', '8', '2', '9', '9', '8', '2', '9', '9', '8', '8', '1', '2', '8', '1', '1', '2', '9', '8', '1', '4', '9', '1', '8', '12', '1', '12', '2', '8', '2', '9', '1', '12', '1', '2', '9', '1', '1', '9', '3', '2', '9', '9', '2', '11', '8', '7', '9', '9', '9', '8', '8', '1', '2', '8', '1', '1', '4', '1', '2', '10', '1', '9', '2', '1', '1', '1', '9', '1', '13', '4', '3', '2', '1', '1', '2', '2', '7', '8', '7', '1', '1', '2', '8', '8', '3', '4', '8', '2', '10', '13', '8', '8', '9', '2', '2', '1', '1', '10', '4', '2', '7', '9', '8', '2', '1', '9', '8', '1', '1', '9', '8', '9', '8', '2', '8', '2', '2', '9', '2', '1', '9', '1', '2', '4', '8', '2', '1', '1', '8', '1', '4', '1', '9', '3', '9', '2', '7', '1', '9', '6', '1', '10', '2', '2', '2', '2', '2', '10', '10', '10', '9', '9', '9', '3', '12', '10', '1', '1', '10', '1', '1', '2', '2', '1', '2', '3', '12', '8', '8', '1', '2', '9', '9', '1', '1', '10', '8', '9', '8', '3', '1', '1', '3', '1', '10', '1', '1', '1', '4', '1', '1', '1', '7', '10', '2', '1', '8', '1', '9', '3', '1', '10', '8', '8', '1', '5', '9', '1', '2', '1', '8', '8', '1', '3', '1', '10', '2', '1', '1', '2', '8', '2', '5', '3', '5', '2', '1', '1', '9', '1', '7', '9', '9', '2', '8', '1', '12', '4', '1', '4', '9', '9', '4', '8', '1', '10', '10', '8', '8', '8', '9', '2', '3', '5', '1', '8', '1', '1', '1', '7', '10', '1', '4', '12', '3', '2', '2', '9', '1', '1', '1', '2', '8', '4', '1', '7', '3', '12', '8', '6', '2', '2', '7', '9', '11', '1', '10', '9', '1', '8', '12', '2', '11', '1', '2', '3', '1', '1', '1', '8', '9', '2', '2', '2', '2', '3', '3', '8', '10', '4', '1', '2', '9', '8', '2', '2', '9', '1', '8', '11', '4', '2', '8', '7', '1', '1', '7', '3', '9', '9', '4', '5', '8', '10', '2', '9', '9', '9', '8', '1', '9', '2', '2', '2', '1', '2', '9', '4', '3', '12', '4', '3', '2', '3', '8', '3', '10', '1', '10', '1', '1', '8', '10', '6', '8', '9', '9', '1', '2', '9', '2', '9', '3', '8', '1', '1', '12', '1', '5', '1', '10', '8', '10', '2', '7', '1', '1', '1', '1', '3', '2', '3', '5', '9', '1', '2', '1', '9', '8', '10', '9', '8', '1', '8', '1', '8', '2', '8', '1', '2', '8', '1', '2', '5', '8', '12', '2', '1', '1', '1', '1', '1', '8', '8', '2', '1', '4', '2', '8', '8', '8', '2', '9', '9', '11', '1', '2', '9', '3', '1', '1', '8', '9', '2', '4', '10', '1', '9', '8', '4', '9', '8', '10', '8', '8', '9', '2', '8', '9', '3', '3', '3', '4', '3', '2', '2', '10', '8', '2', '1', '5', '1', '9', '1', '1', '2', '1', '2', '3', '1', '1', '8', '8', '1', '10', '8', '8', '1', '6', '4', '8', '2', '5', '8', '1', '3', '8', '2', '8', '9', '1', '1', '8', '12', '10', '1', '5', '8', '2', '8', '10', '9', '9', '1', '6', '9', '1', '10', '8', '2', '9', '10', '1', '2', '1', '1', '8', '8', '3', '9', '8', '1', '10', '2', '8', '1', '10', '1', '5', '1', '1', '1', '5', '1', '1', '1', '4', '1', '1', '1', '8', '2', '8', '1', '8', '3', '8', '9', '1', '8', '1', '8', '1', '1', '2', '2', '1', '12', '9', '9', '1', '8', '2', '1', '2', '8', '1', '9', '1', '1', '2', '1', '2', '2', '2', '1', '9', '9', '8', '8', '1', '1', '9', '10', '8', '4', '3', '3', '4', '9', '1', '3', '3', '10', '2', '2', '1', '8', '4', '1', '9', '4', '1', '1', '9', '8', '10', '4', '9', '9', '8', '2', '1', '3', '4', '9', '3', '8', '9', '1', '1', '2', '1', '7', '8', '2', '9', '1', '9', '2', '6', '9', '10', '8', '1', '1', '5', '9', '5', '1', '9', '10', '9', '3', '3', '8', '8', '2', '9', '2', '8', '2', '1', '8', '5', '8', '3', '10', '1', '1', '2', '1', '8', '1', '4', '3', '2', '9', '4', '9', '8', '8', '8', '1', '3', '9', '1', '9', '9', '1', '2', '1', '9', '8', '9', '8', '1', '1', '1', '4', '2', '2', '9', '9', '1', '1', '1', '3', '6', '1', '2', '3', '1', '4', '2', '9', '1', '9', '9', '1', '8', '2', '1', '9', '1', '8', '1', '8', '8', '2', '1', '9', '9', '6', '8', '8', '9', '8', '10', '8', '9', '5', '1', '1', '1', '1', '8', '1', '8', '12', '8', '1', '12', '9', '2', '8', '1', '1', '2', '1', '8', '6', '9', '9', '8', '1', '8', '9', '7', '7', '8', '9', '9', '1', '8', '3', '8', '1', '3', '3', '8', '9', '8', '2', '8', '1', '2', '9', '8', '1', '1', '1', '1', '9', '1', '9', '8', '12', '3', '8', '8', '1', '10', '10', '10', '2', '2', '1', '10', '2', '2', '10', '8', '12', '8', '1', '1', '1', '2', '1', '1', '3', '8', '1', '2', '2', '2', '1', '1', '6', '2', '8', '8', '8', '1', '1', '1', '9', '9', '2', '8', '10', '2', '8', '2', '1', '2', '9', '9', '2', '1', '1', '8', '8', '1', '3', '1', '8', '9', '1', '10', '9', '1', '1', '1', '1', '7', '2', '2', '1', '2', '8', '8', '1', '2', '1', '3', '2', '1', '8', '3', '9', '2', '10', '2', '10', '1', '2', '1', '1', '13', '9', '10', '8', '2', '2', '1', '1', '8', '2', '2', '1', '9', '1', '1', '8', '9', '9', '2', '9', '2', '2', '1', '1', '1', '8', '9', '4', '9', '9', '1', '8', '2', '1', '1', '8', '2', '2', '2', '8', '1', '1', '8', '2', '10', '2', '9', '9', '7', '1', '1', '2', '2', '8', '3', '1', '2', '1', '1', '8', '1', '1', '1', '9', '1', '1', '8', '8', '1', '9', '6', '1', '9', '1', '8', '1', '8', '8', '8', '1', '3', '8', '8', '2', '6', '3', '2', '1', '10', '1', '8', '2', '1', '6', '1', '1', '1', '8', '1', '10', '1', '1', '8', '9', '2', '6', '9', '2', '9', '4', '1', '8', '1', '3', '7', '8', '8', '5', '8', '7', '1', '1', '1', '13', '8', '3', '1', '1', '1', '1', '2', '8', '2', '12', '8', '1', '2', '5', '1', '8', '2', '10', '8', '2', '10', '1', '5', '2', '1', '1', '5', '2', '2', '7', '8', '2', '9', '9', '1', '10', '1', '8', '2', '5', '8', '1', '5', '1', '10', '1', '1', '9', '8', '6', '1', '8', '9', '8', '1', '8', '2', '2', '8', '3', '9', '1', '9', '3', '8', '4', '4', '3', '3', '1', '1', '1', '8', '9', '1', '1', '8', '3', '1', '2', '8', '2', '1', '1', '10', '1', '2', '2', '1', '1', '8', '2', '8', '2', '10', '2', '2', '10', '8', '8', '8', '13', '2', '2', '9', '2', '5', '9', '2', '1', '8', '8', '8', '1', '12', '12', '2', '9', '2', '1', '8', '2', '1', '8', '1', '9', '9', '2', '7', '1', '1', '7', '1', '4', '3', '1', '10', '3', '10', '9', '4', '6', '7', '5', '2', '1', '8', '8', '1', '4', '9', '2', '1', '8', '1', '1', '1', '1', '1', '8', '9', '8', '8', '8', '8', '9', '8', '10', '1', '8', '2', '7', '8', '9', '1', '2', '5', '9', '10', '8', '9', '12', '1', '6', '1', '2', '10', '9', '4', '1', '8', '10', '10', '1', '1', '1', '1', '1', '2', '3', '8', '1', '2', '10', '2', '1', '2', '4', '2', '2', '9', '1', '4', '2', '1', '1', '12', '12', '9', '9', '7', '1', '8', '4', '9', '9', '8', '1', '7', '3', '2', '1', '1', '4', '8', '1', '5', '8', '2', '8', '1', '14', '9', '12', '1', '10', '2', '1', '1', '8', '8', '6', '6', '3', '1', '2', '8', '1', '3', '2', '1', '8', '8', '1', '10', '8', '2', '8', '2', '5', '9', '1', '8', '8', '1', '1', '8', '8', '13', '1', '3', '8', '4', '1', '9', '7', '8', '8', '8', '3', '8', '1', '4', '2', '8', '8', '8', '7', '1', '1', '2', '8', '8', '9', '8', '8', '2', '3', '9', '9', '2', '1', '2', '2', '1', '8', '2', '6', '2', '1', '1', '9', '10', '3', '8', '2', '3', '2', '8', '8', '1', '8', '8', '8', '9', '1', '4', '1', '9', '2', '1', '8', '0', '1', '8', '1', '1', '8', '9', '9', '1', '3', '1', '2', '8', '8', '7', '1', '1', '2', '9', '2', '10', '2', '8', '2', '2', '2', '7', '1', '9', '8', '1', '3', '9', '2', '1', '10', '8', '1', '4', '1', '8', '5', '9', '8', '8', '1', '2', '2', '1', '8', '8', '6', '8', '1', '1', '8', '3', '2', '2', '1', '8', '1', '7', '6', '8', '1', '1', '1', '9', '1', '1', '1', '2', '1', '11', '10', '10', '11', '9', '']\n",
            "['8', '1', '8', '2', '8', '8', '8', '9', '7', '8', '1', '2', '1', '1', '8', '4', '8', '8', '12', '3', '3', '7', '3', '12', '1', '8', '8', '8', '8', '8', '8', '8', '1', '9', '5', '9', '9', '9', '11', '8', '8', '8', '4', '8', '8', '8', '8', '1', '3', '9', '3', '7', '1', '2', '9', '9', '7', '8', '8', '1', '10', '7', '8', '8', '9', '8', '7', '9', '9', '12', '7', '2', '8', '1', '11', '11', '1', '7', '7', '12', '1', '9', '8', '10', '12', '7', '8', '2', '8', '9', '9', '1', '8', '9', '1', '7', '12', '10', '10', '10', '8', '3', '7', '9', '8', '9', '1', '8', '8', '2', '7', '2', '9', '9', '11', '8', '8', '12', '12', '7', '8', '12', '4', '9', '3', '1', '12', '1', '1', '8', '8', '3', '8', '8', '8', '8', '9', '1', '8', '8', '10', '1', '8', '2', '8', '8', '7', '3', '8', '2', '4', '4', '9', '8', '10', '12', '12', '1', '1', '9', '1', '1', '1', '8', '2', '2', '8', '1', '1', '2', '2', '1', '2', '8', '1', '9', '9', '8', '8', '4', '2', '9', '9', '8', '3', '4', '3', '1', '8', '8', '2', '1', '9', '7', '8', '8', '1', '12', '3', '8', '2', '4', '2', '9', '12', '1', '4', '1', '8', '8', '8', '2', '2', '8', '9', '8', '8', '8', '10', '9', '8', '7', '9', '1', '1', '9', '4', '2', '4', '2', '2', '1', '7', '8', '11', '11', '3', '9', '2', '4', '8', '9', '1', '8', '1', '1', '4', '9', '1', '1', '8', '8', '2', '1', '8', '4', '2', '8', '9', '8', '8', '2', '8', '8', '8', '7', '1', '1', '1', '2', '1', '1', '8', '7', '8', '8', '12', '2', '12', '12', '8', '10', '12', '8', '3', '3', '12', '10', '1', '8', '12', '1', '8', '8', '2', '8', '4', '7', '8', '7', '10', '8', '10', '9', '8', '12', '12', '1', '8', '8', '3', '8', '8', '8', '8', '8', '1', '9', '8', '11', '1', '1', '1', '9', '8', '1', '9', '2', '3', '11', '8', '9', '9', '9', '2', '1', '8', '8', '9', '7', '1', '4', '9', '4', '8', '8', '4', '8', '12', '9', '4', '8', '2', '10', '10', '10', '8', '9', '9', '8', '8', '12', '7', '1', '8', '8', '8', '4', '1', '1', '1', '1', '1', '1', '8', '1', '9', '8', '9', '9', '4', '8', '12', '9', '8', '8', '2', '8', '8', '8', '6', '9', '8', '3', '7', '8', '8', '4', '12', '8', '8', '9', '12', '12', '9', '8', '2', '9', '2', '3', '1', '12', '8', '10', '9', '9', '9', '10', '10', '3', '8', '12', '1', '4', '2', '1', '10', '8', '2', '8', '4', '8', '9', '1', '9', '9', '10', '10', '1', '4', '9', '2', '4', '9', '1', '1', '3', '10', '3', '3', '8', '7', '3', '8', '9', '9', '12', '4', '8', '12', '2', '2', '4', '1', '9', '9', '4', '1', '4', '2', '8', '12', '2', '3', '10', '10', '9', '8', '9', '9', '1', '12', '8', '8', '8', '12', '4', '1', '8', '8', '1', '9', '8', '8', '2', '1', '8', '9', '8', '3', '3', '3', '1', '8', '8', '9', '1', '10', '9', '9', '9', '9', '5', '9', '9', '8', '8', '8', '8', '8', '8', '8', '8', '11', '12', '8', '8', '1', '8', '9', '11', '2', '2', '2', '2', '3', '1', '2', '2', '8', '2', '4', '9', '1', '2', '9', '8', '2', '8', '9', '9', '3', '10', '9', '9', '2', '8', '9', '8', '12', '12', '1', '3', '8', '8', '8', '2', '7', '7', '7', '7', '3', '9', '1', '9', '8', '9', '9', '1', '1', '1', '2', '9', '9', '9', '11', '1', '8', '8', '9', '1', '9', '8', '8', '8', '1', '1', '8', '7', '1', '1', '8', '8', '9', '4', '4', '8', '2', '2', '8', '8', '8', '8', '8', '8', '11', '8', '2', '9', '4', '9', '3', '9', '9', '1', '3', '9', '3', '1', '12', '8', '9', '12', '1', '8', '4', '2', '1', '4', '8', '3', '3', '8', '2', '8', '9', '7', '8', '8', '8', '5', '8', '3', '9', '8', '8', '13', '12', '1', '1', '2', '8', '4', '1', '9', '9', '12', '8', '9', '12', '9', '1', '9', '9', '9', '9', '3', '2', '9', '9', '4', '8', '12', '2', '4', '9', '3', '1', '9', '7', '8', '9', '9', '8', '4', '8', '8', '7', '9', '10', '3', '8', '8', '8', '1', '1', '1', '1', '8', '8', '4', '1', '10', '1', '5', '7', '7', '1', '8', '9', '3', '7', '2', '7', '7', '2', '4', '8', '12', '7', '4', '2', '9', '9', '12', '6', '10', '8', '2', '4', '12', '9', '9', '3', '8', '8', '1', '2', '10', '9', '9', '8', '4', '12', '2', '1', '8', '8', '8', '12', '10', '10', '9', '3', '8', '8', '9', '2', '8', '10', '1', '1', '1', '1', '2', '1', '1', '1', '1', '9', '8', '12', '9', '4', '8', '8', '9', '1', '9', '3', '9', '8', '8', '1', '7', '7', '10', '1', '8', '8', '1', '9', '8', '10', '3', '1', '7', '1', '8', '8', '12', '8', '8', '1', '8', '7', '1', '7', '7', '8', '2', '1', '8', '8', '2', '10', '8', '8', '8', '8', '8', '10', '1', '8', '8', '12', '8', '3', '3', '2', '2', '2', '10', '8', '8', '8', '2', '9', '1', '8', '9', '3', '2', '8', '10', '8', '6', '1', '1', '8', '4', '1', '9', '10', '8', '1', '7', '1', '2', '8', '1', '1', '1', '12', '1', '9', '12', '8', '12', '12', '12', '8', '8', '12', '4', '8', '8', '8', '8', '9', '9', '1', '3', '3', '3', '3', '1', '12', '12', '9', '10', '8', '8', '1', '9', '2', '2', '13', '9', '8', '9', '2', '1', '9', '1', '8', '8', '8', '4', '8', '1', '1', '1', '12', '12', '7', '2', '2', '2', '8', '3', '8', '9', '2', '10', '7', '8', '9', '2', '1', '2', '12', '12', '8', '8', '9', '2', '2', '9', '11', '1', '8', '1', '10', '9', '2', '1', '4', '7', '7', '7', '7', '7', '12', '8', '8', '8', '1', '1', '10', '1', '12', '1', '8', '2', '1', '1', '12', '8', '7', '9', '12', '8', '9', '3', '9', '8', '8', '8', '8', '3', '11', '2', '2', '9', '8', '8', '10', '8', '2', '7', '3', '1', '4', '7', '8', '8', '1', '8', '3', '7', '12', '8', '10', '9', '9', '8', '8', '2', '8', '9', '9', '1', '2', '8', '8', '9', '8', '3', '8', '1', '8', '10', '9', '8', '9', '9', '12', '4', '4', '8', '9', '9', '8', '2', '10', '1', '2', '8', '9', '1', '9', '9', '9', '7', '12', '12', '8', '1', '1', '1', '1', '8', '3', '1', '1', '8', '1', '8', '8', '7', '8', '8', '8', '8', '3', '2', '2', '10', '10', '10', '7', '8', '1', '2', '12', '7', '9', '8', '7', '8', '12', '2', '8', '9', '2', '6', '6', '7', '9', '8', '1', '8', '9', '8', '1', '12', '1', '2', '8', '7', '7', '7', '8', '2', '2', '8', '1', '2', '2', '9', '9', '1', '8', '8', '4', '3', '3', '1', '6', '3', '3', '12', '3', '8', '9', '1', '4', '3', '1', '8', '3', '9', '2', '8', '2', '8', '8', '8', '1', '1', '1', '9', '9', '8', '1', '9', '8', '1', '1', '3', '10', '8', '1', '1', '3', '9', '1', '4', '4', '1', '8', '9', '9', '2', '0', '0', '1', '8', '3', '1', '8', '8', '9', '8', '8', '1', '1', '8', '9', '8', '8', '8', '7', '9', '8', '8', '8', '10', '9', '8', '1', '2', '6', '1', '9', '9', '8', '12', '12', '12', '8', '8', '2', '8', '1', '2', '2', '2', '1', '9', '8', '2', '12', '2', '8', '12', '8', '9', '8', '8', '9', '7', '1', '1', '1', '1', '1', '8', '8', '1', '8', '8', '1', '1', '3', '2', '8', '8', '9', '10', '10', '2', '2', '1', '9', '2', '9', '9', '4', '12', '12', '12', '10', '7', '3', '3', '4', '2', '2', '9', '2', '8', '4', '2', '4', '1', '10', '9', '7', '8', '7', '1', '1', '3', '3', '1', '1', '3', '3', '3', '1', '1', '1', '1', '8', '2', '3', '1', '1', '2', '8', '8', '12', '8', '8', '8', '8', '11', '9', '1', '8', '9', '2', '8', '8', '8', '3', '9', '1', '9', '2', '7', '2', '8', '2', '8', '10', '8', '1', '10', '1', '1', '9', '9', '8', '8', '1', '8', '8', '8', '12', '8', '8', '8', '1', '8', '8', '8', '1', '9', '1', '1', '8', '1', '8', '9', '8', '2', '12', '9', '9', '0', '1', '8', '8', '1', '8', '12', '8', '8', '10', '8', '8', '8', '7', '8', '1', '8', '7', '3', '10', '1', '8', '9', '1', '8', '8', '8', '10', '1', '10', '3', '9', '1', '8', '9', '2', '8', '3', '3', '9', '9', '7', '9', '1', '1', '9', '2', '1', '1', '1', '7', '1', '1', '8', '8', '1', '1', '8', '1', '8', '3', '12', '9', '3', '3', '8', '8', '8', '8', '3', '1', '3', '3', '1', '11', '0', '8', '8', '7', '8', '12', '1', '8', '9', '8', '9', '8', '8', '3', '8', '8', '1', '1', '1', '9', '2', '2', '2', '8', '7', '12', '8', '8', '9', '10', '10', '7', '8', '1', '9', '8', '7', '3', '1', '3', '8', '2', '2', '3', '9', '8', '4', '4', '8', '9', '2', '1', '1', '7', '8', '9', '9', '7', '8', '7', '7', '8', '2', '2', '8', '4', '9', '7', '10', '0', '9', '8', '3', '7', '8', '1', '1', '8', '9', '9', '2', '2', '10', '1', '9', '10', '10', '10', '8', '3', '2', '12', '9', '9', '10', '12', '9', '12', '12', '9', '1', '2', '4', '12', '12', '7', '8', '9', '7', '7', '7', '3', '9', '8', '9', '1', '12', '8', '9', '4', '1', '3', '12', '12', '12', '12', '8', '8', '2', '1', '1', '2', '1', '1', '1', '12', '12', '8', '12', '2', '2', '12', '3', '3', '12', '8', '2', '8', '8', '12', '2', '1', '10', '3', '2', '8', '7', '1', '8', '1', '3', '7', '8', '9', '8', '3', '1', '1', '7', '8', '8', '9', '8', '2', '9', '2', '2', '9', '8', '1', '8', '8', '1', '3', '3', '1', '1', '10', '1', '2', '8', '1', '1', '1', '1', '9', '1', '4', '1', '7', '7', '7', '7', '2', '2', '8', '8', '12', '1', '9', '1', '7', '3', '3', '1', '8', '8', '10', '8', '9', '2', '9', '1', '3', '8', '8', '3', '12', '2', '8', '12', '2', '9', '1', '3', '3', '3', '3', '2', '8', '7', '9', '8', '3', '3', '1', '7', '8', '3', '1', '1', '12', '8', '9', '1', '2', '3', '8', '1', '1', '3', '3', '9', '1', '1', '1', '12', '1', '7', '3', '3', '1', '8', '8', '8', '1', '2', '4', '8', '1', '10', '2', '5', '3', '3', '12', '10', '9', '9', '12', '9', '0', '2', '8', '8', '9', '9', '9', '8', '1', '3', '1', '1', '4', '8', '1', '10', '8', '7', '2', '8', '2', '8', '4', '7', '8', '1', '9', '1', '9', '8', '2', '8', '2', '7', '9', '2', '2', '9', '1', '8', '12', '1', '8', '1', '4', '1', '9', '9', '1', '10', '12', '4', '8', '1', '7', '3', '9', '2', '12', '7', '8', '8', '2', '1', '12', '9', '8', '1', '2', '2', '8', '10', '2', '1', '7', '7', '7', '12', '3', '3', '8', '3', '8', '8', '3', '9', '8', '9', '1', '1', '8', '7', '9', '3', '3', '8', '1', '0', '9', '9', '9', '1', '8', '9', '9', '10', '1', '8', '7', '8', '8', '8', '9', '9', '8', '9', '10', '4', '9', '3', '7', '12', '1', '9', '9', '8', '9', '1', '9', '3', '1', '8', '4', '12', '10', '9', '8', '7', '10', '8', '12', '12', '3', '10', '8', '12', '2', '1', '2', '3', '9', '8', '8', '7', '1', '1', '2', '2', '1', '2', '2', '7', '1', '3', '9', '9', '3', '8', '8', '8', '8', '8', '7', '8', '8', '10', '8', '1', '8', '2', '8', '2', '2', '2', '2', '2', '1', '7', '12', '10', '1', '2', '8', '1', '4', '7', '8', '1', '12', '8', '7', '9', '2', '2', '2', '8', '1', '8', '1', '1', '0', '2', '2', '6', '1', '8', '2', '1', '8', '1', '1', '2', '2', '3', '2', '2', '10', '10', '9', '1', '7', '7', '8', '8', '1', '12', '10', '12', '3', '8', '8', '8', '3', '8', '3', '10', '2', '2', '2', '1', '2', '2', '1', '0', '1', '8', '9', '1', '1', '1', '2', '2', '2', '2', '2', '1', '1', '1', '1', '12', '9', '9', '1', '10', '11', '12', '9', '1', '1', '8', '8', '1', '2', '12', '8', '3', '7', '2', '10', '9', '7', '8', '1', '2', '2', '7', '9', '12', '2', '2', '1', '8', '9', '2', '3', '2', '2', '2', '9', '9', '8', '9', '2', '3', '8', '8', '9', '7', '3', '10', '8', '8', '9', '1', '1', '2', '8', '1', '10', '8', '8', '9', '7', '1', '7', '12', '8', '1', '7', '2', '1', '3', '9', '2', '8', '8', '1', '2', '3', '9', '3', '8', '9', '7', '7', '1', '10', '2', '8', '8', '8', '1', '8', '2', '8', '8', '1', '2', '8', '10', '2', '1', '0', '8', '3', '8', '10', '10', '12', '3', '8', '9', '3', '10', '8', '8', '8', '1', '8', '8', '2', '2', '2', '2', '2', '2', '1', '1', '10', '1', '10', '8', '2', '1', '2', '2', '8', '3', '3', '2', '2', '1', '8', '1', '3', '1', '9', '2', '2', '1', '3', '2', '2', '3', '2', '2', '3', '2', '1', '1', '2', '1', '2', '7', '8', '2', '1', '12', '12', '3', '1', '9', '9', '7', '8', '7', '2', '7', '9', '2', '2', '2', '8', '9', '7', '9', '2', '8', '9', '9', '2', '9', '1', '1', '9', '8', '3', '3', '8', '2', '7', '8', '11', '2', '8', '9', '1', '8', '3', '1', '2', '1', '2', '2', '2', '2', '8', '3', '9', '2', '10', '9', '9', '2', '7', '7', '7', '1', '3', '8', '8', '1', '1', '8', '7', '1', '8', '8', '9', '3', '3', '3', '2', '4', '8', '12', '8', '1', '12', '9', '2', '8', '2', '12', '2', '1', '3', '8', '12', '12', '11', '8', '10', '9', '2', '9', '4', '10', '1', '12', '12', '9', '3', '8', '9', '12', '9', '9', '2', '2', '2', '3', '5', '1', '1', '1', '7', '7', '8', '2', '2', '8', '2', '1', '7', '3', '2', '9', '8', '9', '1', '1', '9', '8', '9', '3', '3', '2', '9', '4', '9', '8', '8', '1', '8', '10', '2', '0', '3', '8', '8', '8', '9', '12', '9', '8', '2', '2', '8', '10', '9', '8', '2', '2', '4', '1', '9', '9', '8', '8', '10', '3', '1', '1', '2', '8', '7', '8', '2', '1', '1', '8', '12', '2', '9', '4', '8', '3', '3', '3', '8', '2', '12', '12', '8', '9', '12', '8', '2', '12', '7', '9', '2', '2', '1', '3', '1', '3', '11', '9', '8', '1', '10', '2', '12', '3', '8', '4', '1', '2', '8', '8', '3', '9', '3', '8', '7', '1', '1', '3', '1', '8', '2', '9', '8', '12', '1', '1', '1', '9', '9', '1', '8', '8', '8', '2', '2', '12', '3', '1', '1', '1', '9', '9', '3', '2', '1', '1', '9', '9', '1', '9', '9', '1', '9', '3', '9', '1', '8', '1', '9', '8', '3', '8', '8', '9', '9', '8', '2', '8', '1', '2', '2', '10', '1', '2', '1', '1', '9', '1', '9', '3', '7', '7', '2', '10', '3', '3', '1', '1', '9', '1', '2', '1', '3', '2', '1', '1', '1', '1', '1', '1', '1', '1', '1', '2', '8', '8', '7', '1', '1', '8', '8', '2', '1', '12', '1', '8', '1', '9', '8', '8', '2', '1', '9', '9', '2', '2', '8', '1', '7', '7', '9', '8', '1', '6', '8', '8', '2', '2', '8', '3', '8', '3', '2', '3', '8', '1', '2', '2', '2', '8', '9', '9', '9', '12', '1', '9', '2', '1', '9', '2', '9', '9', '10', '8', '1', '1', '9', '2', '1', '7', '1', '3', '7', '1', '1', '1', '3', '8', '8', '8', '9', '9', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '1', '0', '1', '1', '1', '2', '2', '3', '1', '1', '2', '3', '3', '3', '7', '3', '1', '9', '3', '9', '1', '4', '1', '1', '9', '10', '9', '8', '2', '3', '10', '9', '3', '10', '12', '8', '9', '8', '2', '1', '9', '8', '1', '7', '8', '1', '9', '1', '2', '10', '7', '7', '8', '1', '3', '3', '9', '3', '3', '1', '1', '1', '9', '1', '3', '8', '8', '8', '1', '8', '1', '9', '7', '8', '8', '2', '1', '3', '8', '2', '1', '6', '1', '8', '8', '2', '8', '2', '2', '9', '8', '1', '1', '6', '8', '6', '10', '2', '1', '1', '8', '9', '3', '3', '3', '1', '3', '9', '6', '8', '1', '8', '3', '1', '2', '2', '12', '7', '1', '1', '1', '1', '12', '1', '8', '8', '3', '2', '1', '9', '2', '3', '3', '2', '2', '7', '2', '2', '2', '3', '1', '9', '8', '7', '1', '1', '3', '1', '9', '4', '3', '9', '2', '1', '1', '9', '8', '8', '7', '2', '1', '3', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '2', '10', '1', '1', '8', '2', '8', '1', '8', '1', '3', '1', '2', '1', '1', '3', '1', '2', '1', '1', '1', '1', '1', '1', '1', '2', '1', '8', '1', '9', '8', '8', '1', '10', '8', '8', '1', '3', '9', '8', '9', '8', '2', '1', '3', '4', '1', '12', '1', '9', '8', '2', '7', '2', '1', '2', '9', '2', '1', '3', '8', '1', '2', '3', '8', '2', '10', '10', '10', '9', '9', '9', '3', '8', '3', '1', '1', '1', '1', '1', '1', '2', '1', '9', '2', '7', '9', '8', '7', '9', '1', '1', '1', '1', '8', '2', '2', '3', '3', '2', '12', '8', '3', '1', '1', '12', '9', '8', '2', '2', '9', '1', '2', '2', '9', '1', '1', '2', '4', '8', '1', '8', '10', '2', '9', '9', '9', '1', '2', '1', '1', '2', '2', '2', '12', '1', '2', '8', '3', '9', '9', '3', '2', '9', '9', '7', '2', '8', '8', '9', '2', '9', '1', '1', '1', '1', '1', '2', '8', '2', '1', '2', '2', '8', '3', '1', '1', '9', '1', '3', '10', '7', '9', '8', '8', '2', '2', '9', '2', '2', '2', '2', '2', '2', '9', '1', '3', '6', '4', '2', '2', '9', '1', '9', '8', '1', '1', '1', '7', '3', '2', '9', '9', '1', '9', '7', '2', '8', '9', '9', '12', '10', '8', '10', '2', '3', '1', '2', '2', '12', '12', '3', '12', '1', '2', '1', '2', '1', '2', '8', '2', '3', '12', '12', '8', '2', '2', '9', '2', '9', '2', '3', '3', '1', '1', '1', '12', '3', '7', '1', '3', '1', '2', '9', '2', '7', '2', '1', '8', '3', '7', '8', '1', '1', '3', '8', '3', '2', '9', '3', '1', '1', '1', '1', '11', '1', '2', '6', '1', '2', '3', '8', '9', '9', '2', '9', '2', '1', '1', '3', '1', '12', '9', '1', '1', '9', '1', '9', '9', '8', '9', '8', '1', '2', '7', '1', '0', '9', '8', '2', '3', '2', '4', '1', '1', '1', '8', '1', '12', '1', '2', '3', '3', '9', '9', '9', '9', '9', '7', '9', '3', '9', '1', '7', '3', '3', '3', '3', '7', '8', '2', '2', '2', '8', '2', '3', '9', '9', '3', '1', '8', '9', '12', '8', '8', '3', '2', '1', '6', '1', '9', '1', '1', '1', '1', '1', '1', '2', '2', '1', '2', '2', '2', '2', '9', '2', '5', '3', '2', '2', '8', '2', '1', '8', '3', '3', '8', '3', '1', '3', '9', '2', '8', '9', '4', '4', '9', '8', '8', '7', '7', '3', '1', '8', '8', '3', '2', '2', '10', '3', '3', '2', '2', '2', '2', '12', '3', '1', '2', '7', '7', '12', '2', '1', '2', '1', '1', '2', '1', '3', '3', '3', '1', '3', '9', '4', '8', '8', '4', '2', '4', '2', '8', '9', '1', '1', '2', '2', '2', '8', '7', '2', '8', '2', '9', '1', '8', '1', '1', '2', '1', '1', '1', '2', '9', '9', '8', '1', '8', '1', '4', '8', '1', '2', '7', '9', '2', '1', '8', '9', '8', '9', '1', '1', '2', '3', '2', '12', '7', '1', '2', '4', '4', '4', '2', '2', '2', '9', '8', '8', '1', '12', '3', '3', '2', '2', '3', '12', '1', '5', '1', '1', '3', '2', '8', '8', '2', '2', '2', '3', '9', '8', '8', '9', '8', '8', '11', '8', '2', '9', '2', '3', '1', '7', '1', '8', '2', '8', '1', '1', '1', '8', '1', '1', '1', '7', '8', '8', '2', '3', '2', '1', '8', '8', '1', '8', '4', '8', '9', '8', '9', '8', '1', '2', '3', '2', '1', '2', '2', '3', '1', '9', '4', '8', '1', '1', '9', '1', '1', '8', '2', '2', '2', '1', '7', '3', '3', '1', '3', '3', '12', '3', '1', '1', '3', '1', '1', '4', '8', '4', '4', '3', '3', '3', '4', '8', '1', '9', '9', '9', '4', '1', '7', '4', '8', '4', '9', '8', '1', '1', '3', '8', '9', '1', '9', '7', '1', '1', '9', '1', '8', '8', '8', '1', '1', '8', '2', '2', '2', '9', '4', '4', '1', '2', '1', '2', '9', '1', '1', '8', '5', '5', '5', '9', '12', '4', '1', '2', '1', '8', '8', '1', '9', '9', '2', '12', '1', '1', '7', '8', '8', '4', '1', '2', '8', '11', '2', '3', '2', '2', '1', '11', '2', '2', '2', '2', '9', '9', '2', '2', '9', '3', '1', '9', '1', '1', '1', '7', '8', '1', '8', '9', '1', '2', '9', '8', '1', '2', '9', '8', '12', '2', '2', '10', '8', '2', '2', '8', '8', '1', '2', '6', '1', '1', '7', '7', '2', '3', '2', '1', '1', '8', '1', '10', '8', '12', '6', '4', '1', '4', '1', '2', '9', '8', '9', '4', '8', '8', '10', '8', '8', '8', '8', '8', '2', '2', '8', '4', '9', '3', '3', '3', '3', '3', '9', '2', '1', '1', '2', '3', '2', '1', '2', '3', '3', '3', '2', '2', '3', '3', '2', '1', '2', '3', '3', '3', '3', '12', '9', '3', '2', '2', '3', '1', '5', '1', '1', '3', '2', '2', '1', '2', '9', '3', '9', '8', '1', '8', '7', '7', '9', '1', '1', '7', '9', '10', '1', '7', '1', '4', '3', '8', '9', '4', '9', '9', '2', '9', '2', '8', '4', '2', '2', '9', '5', '2', '1', '10', '9', '3', '1', '1', '1', '2', '1', '2', '9', '1', '7', '9', '9', '8', '2', '2', '8', '9', '1', '9', '8', '2', '3', '2', '2', '1', '2', '2', '2', '1', '1', '8', '1', '9', '4', '12', '8', '2', '7', '2', '2', '1', '9', '3', '1', '8', '12', '1', '1', '2', '1', '10', '2', '6', '12', '12', '4', '9', '1', '9', '10', '1', '2', '8', '2', '1', '8', '6', '12', '9', '9', '2', '1', '7', '8', '9', '5', '8', '1', '8', '8', '3', '1', '8', '2', '8', '2', '2', '1', '2', '9', '2', '8', '1', '12', '8', '2', '7', '3', '3', '12', '2', '3', '3', '9', '9', '3', '3', '3', '3', '3', '2', '1', '4', '1', '4', '8', '8', '2', '1', '2', '9', '12', '1', '10', '12', '1', '2', '9', '4', '8', '10', '3', '2', '8', '7', '4', '8', '9', '2', '7', '1', '10', '2', '2', '2', '8', '4', '2', '2', '1', '1', '8', '7', '9', '2', '1', '9', '8', '9', '12', '4', '8', '2', '8', '7', '7', '1', '9', '2', '1', '1', '9', '1', '8', '2', '3', '9', '9', '1', '3', '2', '9', '2', '2', '8', '1', '1', '2', '1', '2', '1', '9', '8', '2', '2', '4', '8', '8', '9', '5', '5', '9', '8', '9', '6', '2', '1', '12', '2', '3', '8', '8', '9', '2', '2', '1', '3', '2', '10', '9', '10', '7', '2', '8', '4', '2', '8', '9', '8', '1', '3', '8', '8', '1', '8', '3', '8', '8', '1', '10', '3', '1', '5', '8', '8', '9', '2', '2', '9', '2', '1', '9', '1', '4', '1', '12', '8', '8', '2', '1', '1', '1', '1', '1', '9', '2', '9', '8', '9', '5', '9', '1', '8', '2', '4', '1', '1', '1', '9', '12', '4', '1', '1', '9', '8', '4', '1', '8', '9', '8', '8', '9', '9', '9', '9', '9', '9', '1', '3', '4', '2', '2', '8', '2', '9', '2', '3', '8', '3', '1', '7', '2', '1', '1', '4', '2', '9', '2', '1', '3', '2', '9', '2', '9', '3', '8', '1', '9', '1', '1', '2', '8', '8', '8', '2', '3', '9', '9', '8', '2', '4', '1', '5', '1', '1', '9', '9', '2', '1', '1', '1', '9', '9', '1', '3', '9', '2', '2', '9', '9', '8', '3', '9', '9', '8', '2', '2', '8', '9', '2', '2', '8', '9', '10', '10', '2', '8', '7', '9', '9', '4', '2', '4', '11', '2', '1', '7', '8', '8', '8', '9', '4', '9', '8', '8', '2', '1', '1', '9', '8', '3', '3', '8', '8', '7', '9', '1', '3', '4', '8', '10', '2', '4', '4', '8', '2', '8', '2', '8', '9', '2', '1', '2', '2', '3', '1', '9', '4', '5', '9', '9', '1', '1', '1', '1', '1', '1', '7', '1', '1', '1', '8', '1', '1', '1', '1', '1', '9', '1', '2', '1', '12', '9', '3', '9', '9', '4', '1', '9', '1', '1', '2', '11', '9', '3', '2', '2', '2', '4', '2', '3', '2', '7', '7', '8', '1', '10', '9', '5', '7', '1', '10', '8', '1', '2', '7', '1', '12', '5', '8', '4', '2', '9', '9', '8', '2', '9', '8', '8', '10', '2', '3', '2', '7', '2', '8', '10', '3', '2', '9', '9', '9', '1', '1', '1', '1', '8', '2', '10', '9', '8', '1', '2', '2', '2', '1', '2', '1', '3', '9', '12', '2', '2', '4', '8', '1', '3', '9', '4', '9', '1', '9', '9', '2', '1', '1', '9', '7', '10', '3', '10', '2', '9', '2', '9', '2', '8', '2', '1', '2', '3', '1', '1', '1', '8', '5', '8', '3', '1', '1', '4', '2', '2', '1', '3', '8', '2', '1', '1', '1', '2', '9', '4', '1', '8', '1', '2', '8', '2', '9', '2', '2', '5', '5', '2', '5', '2', '1', '8', '8', '1', '3', '12', '2', '4', '3', '2', '2', '2', '6', '2', '3', '3', '1', '8', '2', '2', '1', '1', '8', '8', '10', '1', '9', '2', '9', '12', '0', '2', '1', '3', '2', '2', '1', '2', '1', '9', '9', '2', '9', '8', '8', '1', '7', '8', '1', '2', '6', '9', '8', '10', '1', '12', '2', '2', '1', '8', '2', '1', '12', '7', '4', '2', '10', '2', '2', '1', '2', '1', '2', '1', '1', '2', '1', '9', '8', '10', '2', '10', '9', '9', '12', '5', '3', '2', '8', '2', '8', '3', '3', '1', '4', '9', '10', '2', '4', '8', '1', '2', '10', '12', '12', '3', '1', '1', '2', '9', '6', '6', '1', '9', '1', '8', '1', '8', '8', '8', '8', '2', '2', '2', '8', '8', '2', '1', '1', '1', '1', '1', '8', '8', '5', '8', '8', '1', '8', '9', '9', '1', '7', '12', '9', '9', '7', '2', '2', '7', '8', '9', '8', '2', '9', '6', '3', '4', '8', '4', '1', '2', '9', '8', '8', '8', '2', '1', '1', '8', '8', '3', '1', '9', '9', '7', '1', '2', '2', '2', '9', '4', '1', '2', '12', '1', '8', '2', '1', '9', '1', '8', '9', '9', '2', '5', '4', '2', '1', '2', '8', '12', '8', '10', '3', '8', '8', '8', '2', '2', '8', '2', '8', '2', '4', '8', '10', '2', '7', '2', '1', '8', '8', '9', '5', '8', '1', '12', '7', '10', '2', '2', '2', '1', '4', '8', '12', '1', '8', '2', '2', '9', '1', '8', '3', '1', '1', '5', '10', '2', '1', '2', '4', '8', '9', '3', '2', '8', '7', '4', '4', '9', '9', '2', '8', '1', '1', '2', '4', '7', '1', '9', '1', '1', '1', '8', '4', '1', '3', '1', '2', '2', '9', '1', '8', '8', '2', '2', '9', '9', '1', '8', '8', '1', '9', '8', '8', '2', '2', '2', '2', '1', '1', '1', '7', '4', '1', '1', '4', '2', '3', '3', '2', '3', '9', '2', '8', '2', '1', '5', '1', '2', '2', '1', '3', '5', '2', '9', '1', '1', '9', '8', '1', '4', '8', '1', '8', '2', '4', '8', '4', '7', '9', '8', '1', '2', '2', '4', '4', '11', '3', '11', '1', '3', '8', '1', '9', '6', '8', '9', '3', '9', '8', '9', '2', '3', '3', '7', '1', '7', '2', '9', '9', '1', '8', '8', '1', '5', '5', '8', '8', '1', '2', '1', '3', '9', '9', '5', '9', '8', '1', '4', '3', '8', '1', '1', '2', '1', '8', '9', '8', '2', '2', '2', '1', '2', '4', '10', '8', '1', '9', '1', '8', '1', '1', '9', '9', '8', '11', '1', '8', '2', '1', '2', '8', '2', '8', '8', '2', '9', '6', '10', '1', '8', '6', '4', '5', '11', '2', '2', '8', '1', '8', '1', '8', '1', '4', '8', '1', '1', '1', '8', '3', '7', '3', '3', '7', '1', '1', '4', '8', '1', '6', '4', '2', '2', '1', '1', '1', '1', '6', '2', '2', '8', '1', '8', '5', '5', '2', '1', '1', '2', '3', '7', '1', '1', '6', '2', '9', '3', '9', '8', '9', '2', '1', '4', '2', '9', '13', '9', '2', '8', '12', '4', '1', '1', '9', '5', '1', '1', '1', '1', '8', '2', '1', '1', '2', '9', '0', '12', '1', '1', '8', '9', '8', '3', '2', '2', '12', '8', '2', '2', '2', '1', '10', '1', '9', '8', '9', '5', '2', '2', '9', '2', '8', '4', '8', '3', '7', '9', '2', '2', '7', '2', '2', '8', '1', '1', '9', '10', '9', '8', '2', '9', '0', '7', '1', '1', '1', '9', '8', '2', '8', '12', '8', '3', '2', '8', '8', '7', '7', '8', '12', '2', '1', '3', '9', '2', '2', '8', '2', '8', '8', '12', '8', '8', '1', '8', '9', '2', '4', '4', '7', '2', '1', '1', '9', '3', '7', '1', '3', '9', '8', '2', '2', '3', '1', '3', '10', '2', '3', '7', '1', '8', '1', '1', '10', '3', '10', '8', '4', '1', '2', '4', '3', '10', '9', '3', '9', '5', '8', '7', '2', '9', '3', '3', '2', '11', '8', '1', '1', '7', '2', '7', '2', '9', '4', '8', '1', '12', '8', '7', '9', '1', '0', '2', '6', '4', '9', '3', '2', '4', '12', '9', '8', '5', '9', '2', '8', '4', '6', '8', '9', '3', '1', '8', '7', '3', '1', '1', '1', '8', '9', '1', '8', '7', '9', '8', '2', '1', '2', '2', '3', '2', '2', '1', '1', '4', '9', '7', '3', '2', '8', '8', '9', '1', '9', '7', '4', '2', '2', '8', '5', '9', '1', '5', '7', '1', '4', '6', '9', '10', '1', '8', '2', '9', '1', '9', '2', '9', '8', '2', '9', '9', '11', '2', '12', '2', '2', '9', '4', '8', '1', '9', '9', '2', '8', '9', '1', '2', '9', '3', '8', '8', '1', '7', '8', '8', '2', '3', '2', '2', '2', '2', '9', '8', '10', '2', '2', '1', '9', '11', '1', '8', '8', '2', '4', '2', '1', '2', '8', '8', '9', '2', '8', '9', '2', '3', '1', '2', '1', '3', '10', '8', '1', '2', '4', '1', '9', '8', '9', '3', '3', '9', '8', '2', '11', '3', '7', '1', '8', '7', '1', '8', '8', '8', '1', '4', '8', '8', '1', '9', '3', '1', '1', '2', '8', '8', '8', '2', '1', '8', '10', '2', '12', '8', '8', '9', '1', '8', '1', '3', '2', '2', '10', '7', '2', '2', '1', '1', '8', '2', '9', '1', '2', '9', '3', '3', '10', '1', '4', '7', '10', '12', '2', '4', '10', '8', '6', '2', '9', '1', '8', '3', '1', '2', '8', '8', '2', '12', '10', '2', '10', '9', '1', '2', '5', '8', '12', '8', '8', '7', '8', '8', '1', '8', '2', '2', '9', '1', '8', '10', '5', '5', '5', '8', '1', '9', '1', '13', '1', '2', '2', '1', '2', '4', '9', '2', '1', '9', '13', '11', '1', '9', '9', '3', '10', '8', '7', '8', '8', '4', '2', '1', '9', '4', '3', '1', '1', '10', '9', '2', '8', '6', '2', '1', '1', '3', '2', '8', '1', '1', '1', '1', '2', '4', '1', '1', '10', '1', '8', '1', '1', '9', '1', '7', '1', '1', '9', '9', '2', '12', '9', '1', '8', '1', '12', '1', '8', '3', '8', '1', '8', '1', '2', '10', '1', '1', '2', '2', '9', '3', '9', '12', '1', '9', '1', '2', '8', '7', '2', '1', '1', '3', '2', '8', '4', '4', '5', '12', '7', '8', '9', '6', '1', '8', '9', '2', '1', '9', '1', '4', '1', '8', '2', '1', '1', '8', '7', '4', '9', '1', '8', '4', '2', '7', '5', '1', '3', '9', '2', '8', '2', '1', '2', '9', '1', '1', '2', '3', '4', '3', '1', '2', '2', '10', '2', '2', '8', '1', '1', '2', '4', '9', '2', '11', '9', '8', '9', '2', '1', '10', '4', '1', '1', '10', '7', '2', '10', '8', '1', '10', '8', '8', '8', '8', '8', '8', '2', '2', '2', '8', '3', '4', '1', '1', '1', '2', '1', '8', '8', '2', '8', '3', '8', '3', '1', '2', '3', '1', '1', '10', '1', '4', '4', '3', '3', '1', '9', '1', '1', '2', '1', '1', '6', '1', '1', '1', '9', '1', '4', '1', '1', '1', '1', '1', '1', '2', '12', '8', '8', '1', '9', '12', '10', '8', '2', '12', '1', '1', '2', '1', '7', '10', '7', '1', '2', '1', '10', '8', '2', '2', '8', '2', '2', '1', '9', '8', '8', '8', '8', '3', '1', '4', '2', '8', '3', '10', '10', '1', '9', '1', '9', '1', '9', '1', '8', '3', '1', '1', '8', '8', '8', '6', '3', '4', '1', '1', '1', '9', '9', '5', '2', '10', '2', '6', '2', '2', '7', '1', '8', '2', '11', '1', '1', '1', '2', '1', '9', '4', '4', '2', '8', '4', '6', '8', '8', '10', '10', '2', '1', '2', '9', '1', '8', '8', '3', '4', '8', '8', '2', '8', '8', '1', '2', '1', '2', '9', '4', '1', '3', '3', '3', '8', '8', '8', '2', '6', '4', '3', '3', '12', '3', '4', '9', '6', '8', '7', '7', '8', '6', '4', '1', '10', '6', '3', '3', '8', '1', '1', '8', '1', '11', '8', '1', '1', '2', '3', '11', '7', '2', '1', '1', '1', '9', '1', '2', '1', '1', '9', '8', '1', '1', '8', '0', '4', '1', '1', '2', '1', '1', '2', '2', '9', '1', '10', '1', '3', '8', '9', '3', '3', '1', '10', '1', '9', '1', '1', '7', '4', '1', '8', '10', '1', '2', '10', '1', '1', '2', '3', '3', '9', '1', '8', '10', '1', '7', '9', '1', '9', '6', '3', '9', '2', '8', '4', '12', '12', '3', '1', '1', '9', '1', '9', '2', '1', '1', '1', '1', '1', '9', '2', '1', '2', '10', '9', '8', '8', '10', '8', '2', '3', '2', '1', '12', '8', '12', '2', '9', '1', '3', '2', '5', '2', '2', '8', '2', '2', '4', '10', '9', '10', '9', '4', '2', '4', '12', '8', '2', '1', '8', '3', '9', '2', '9', '4', '1', '1', '1', '1', '1', '9', '6', '2', '9', '2', '1', '3', '2', '2', '5', '8', '9', '1', '6', '3', '1', '2', '2', '2', '6', '1', '8', '10', '8', '3', '3', '13', '2', '9', '9', '10', '6', '12', '8', '2', '9', '2', '3', '8', '8', '1', '2', '2', '3', '2', '10', '1', '8', '9', '1', '8', '9', '8', '8', '2', '10', '2', '1', '1', '1', '9', '9', '12', '1', '1', '4', '8', '3', '2', '2', '4', '4', '2', '2', '1', '1', '1', '9', '2', '2', '4', '1', '1', '8', '8', '8', '9', '2', '13', '9', '9', '4', '1', '9', '7', '10', '10', '8', '1', '9', '1', '1', '1', '8', '12', '4', '1', '2', '1', '1', '1', '7', '8', '3', '3', '1', '2', '2', '7', '2', '2', '10', '1', '8', '4', '0', '1', '9', '4', '1', '1', '2', '10', '10', '7', '9', '3', '12', '11', '2', '8', '10', '8', '9', '4', '3', '4', '10', '1', '1', '9', '3', '10', '1', '5', '3', '3', '2', '2', '1', '1', '6', '1', '7', '1', '1', '12', '1', '1', '1', '10', '1', '8', '8', '1', '3', '8', '3', '1', '2', '8', '3', '4', '2', '8', '8', '6', '1', '1', '4', '1', '2', '9', '5', '1', '7', '8', '9', '2', '4', '9', '8', '5', '9', '9', '9', '1', '2', '1', '3', '8', '9', '2', '9', '8', '9', '1', '8', '13', '9', '3', '8', '1', '4', '1', '3', '1', '6', '4', '10', '2', '2', '7', '8', '8', '12', '8', '9', '9', '9', '10', '3', '12', '12', '2', '9', '10', '9', '7', '2', '3', '4', '10', '10', '2', '7', '7', '1', '7', '3', '9', '1', '9', '8', '2', '9', '1', '8', '1', '10', '8', '8', '2', '7', '8', '1', '8', '9', '8', '1', '4', '1', '8', '8', '8', '1', '1', '10', '3', '2', '2', '6', '8', '9', '8', '2', '1', '1', '3', '8', '8', '1', '4', '4', '3', '9', '10', '4', '2', '4', '2', '9', '2', '2', '9', '1', '1', '10', '1', '1', '2', '1', '1', '1', '9', '1', '8', '8', '8', '2', '3', '8', '1', '6', '7', '3', '1', '9', '1', '2', '1', '6', '1', '2', '10', '1', '4', '2', '3', '2', '9', '9', '4', '9', '9', '1', '2', '8', '4', '7', '8', '7', '13', '9', '1', '2', '1', '9', '3', '3', '6', '8', '9', '10', '9', '2', '2', '3', '12', '1', '1', '1', '13', '2', '1', '7', '10', '8', '9', '1', '2', '9', '1', '1', '1', '2', '1', '12', '5', '6', '9', '10', '3', '1', '10', '1', '3', '2', '8', '9', '2', '10', '8', '8', '9', '2', '8', '9', '2', '2', '2', '8', '8', '2', '3', '1', '4', '8', '1', '9', '9', '2', '8', '10', '9', '1', '2', '12', '8', '8', '0', '2', '1', '1', '9', '10', '1', '6', '2', '2', '10', '2', '2', '4', '2', '2', '12', '1', '6', '7', '8', '9', '1', '3', '5', '7', '3', '1', '3', '1', '1', '3', '2', '6', '3', '2', '1', '8', '3', '5', '2', '1', '1', '8', '1', '1', '0', '2', '3', '5', '3', '1', '1', '9', '8', '8', '8', '8', '4', '7', '2', '9', '12', '5', '9', '3', '12', '3', '9', '1', '9', '1', '7', '3', '9', '3', '10', '1', '2', '1', '4', '9', '8', '2', '9', '11', '2', '8', '9', '4', '1', '1', '1', '1', '1', '1', '1', '9', '1', '9', '1', '1', '1', '12', '3', '10', '3', '6', '1', '7', '10', '8', '3', '7', '1', '1', '2', '6', '1', '3', '9', '2', '8', '1', '8', '8', '1', '8', '10', '12', '1', '10', '12', '10', '1', '8', '8', '1', '4', '7', '9', '2', '1', '9', '3', '8', '12', '10', '6', '1', '6', '8', '3', '1', '3', '1', '10', '10', '9', '8', '1', '2', '9', '8', '8', '1', '9', '8', '8', '3', '1', '3', '9', '12', '1', '8', '1', '5', '8', '11', '5', '5', '1', '2', '1', '3', '1', '1', '1', '9', '8', '8', '1', '9', '10', '8', '4', '1', '1', '8', '10', '8', '9', '1', '10', '1', '7', '2', '1', '1', '9', '6', '2', '9', '8', '1', '1', '1', '9', '8', '8', '1', '9', '9', '1', '1', '2', '7', '9', '2', '9', '9', '8', '7', '6', '8', '7', '8', '2', '9', '2', '1', '8', '8', '8', '2', '6', '3', '1', '8', '12', '12', '8', '7', '1', '2', '2', '7', '1', '9', '8', '1', '4', '1', '1', '8', '5', '8', '1', '8', '8', '8', '8', '1', '2', '11', '1', '1', '1', '2', '7', '1', '9', '2', '2', '2', '9', '8', '9', '2', '9', '1', '8', '1', '7', '11', '13', '1', '9', '8', '2', '2', '1', '2', '3', '10', '3', '10', '4', '3', '9', '1', '8', '1', '1', '13', '1', '1', '3', '8', '1', '2', '2', '2', '9', '4', '1', '8', '1', '9', '3', '6', '8', '5', '2', '8', '2', '8', '1', '9', '2', '2', '8', '1', '1', '4', '8', '9', '2', '2', '7', '1', '8', '12', '2', '12', '8', '2', '9', '9', '3', '8', '1', '1', '9', '9', '9', '2', '8', '1', '9', '1', '8', '9', '8', '2', '2', '4', '1', '9', '10', '10', '8', '8', '1', '1', '2', '1', '4', '1', '2', '1', '1', '3', '12', '8', '8', '8', '8', '10', '2', '8', '12', '1', '9', '9', '8', '9', '1', '2', '1', '8', '8', '8', '2', '8', '8', '3', '10', '8', '9', '1', '1', '1', '3', '4', '8', '10', '6', '3', '1', '3', '2', '8', '9', '3', '5', '4', '1', '5', '2', '9', '9', '9', '1', '1', '1', '9', '9', '6', '10', '9', '8', '12', '2', '9', '1', '2', '1', '1', '1', '8', '1', '12', '1', '12', '1', '9', '1', '8', '1', '2', '1', '2', '8', '8', '10', '1', '9', '1', '2', '2', '8', '8', '3', '12', '1', '11', '2', '10', '12', '11', '2', '1', '10', '1', '1', '9', '3', '10', '2', '7', '1', '8', '1', '9', '9', '2', '1', '3', '12', '5', '9', '9', '8', '1', '8', '8', '8', '1', '1', '3', '9', '8', '9', '8', '3', '10', '3', '7', '8', '9', '2', '3', '1', '9', '10', '9', '2', '8', '2', '6', '4', '1', '1', '3', '8', '2', '3', '9', '1', '2', '1', '8', '1', '2', '2', '2', '2', '9', '9', '4', '8', '9', '1', '1', '2', '9', '1', '5', '2', '9', '8', '10', '1', '2', '10', '8', '5', '6', '4', '8', '1', '1', '1', '8', '9', '2', '8', '8', '8', '12', '2', '2', '1', '8', '1', '2', '1', '9', '8', '8', '9', '1', '1', '8', '7', '1', '8', '0', '2', '3', '8', '1', '1', '8', '1', '1', '12', '3', '2', '10', '8', '10', '6', '1', '8', '8', '10', '8', '8', '1', '4', '8', '1', '2', '9', '8', '1', '1', '3', '3', '5', '1', '1', '2', '1', '2', '1', '9', '8', '3', '9', '8', '2', '10', '9', '1', '8', '1', '10', '9', '10', '1', '2', '3', '8', '1', '3', '1', '9', '9', '8', '1', '9', '8', '8', '9', '1', '9', '2', '8', '8', '2', '13', '8', '10', '8', '9', '3', '2', '1', '3', '1', '12', '10', '10', '1', '11', '1', '2', '4', '2', '10', '1', '8', '11', '1', '1', '1', '2', '2', '8', '2', '9', '8', '9', '9', '12', '8', '1', '9', '2', '4', '1', '8', '3', '9', '1', '6', '5', '8', '9', '3', '3', '2', '1', '1', '1', '8', '11', '1', '8', '7', '1', '11', '9', '1', '7', '9', '9', '8', '8', '8', '12', '8', '1', '2', '9', '8', '8', '2', '10', '9', '8', '8', '4', '10', '9', '8', '2', '10', '10', '2', '1', '2', '1', '2', '1', '4', '1', '7', '1', '1', '3', '12', '9', '8', '5', '9', '10', '1', '9', '9', '2', '7', '4', '1', '1', '8', '12', '8', '2', '2', '9', '4', '1', '1', '5', '9', '1', '8', '12', '8', '1', '1', '4', '9', '10', '2', '3', '1', '3', '3', '3', '8', '1', '2', '1', '2', '1', '2', '9', '1', '2', '8', '9', '4', '8', '8', '10', '2', '12', '5', '5', '1', '8', '1', '7', '8', '1', '1', '8', '9', '12', '4', '9', '3', '1', '2', '5', '9', '2', '1', '2', '2', '1', '2', '8', '1', '8', '8', '2', '2', '1', '8', '4', '1', '2', '10', '9', '10', '8', '1', '10', '4', '8', '8', '10', '2', '8', '1', '1', '3', '10', '1', '4', '2', '8', '3', '3', '2', '9', '8', '5', '5', '9', '3', '10', '8', '1', '2', '1', '2', '9', '1', '1', '2', '9', '9', '8', '8', '11', '9', '4', '1', '8', '2', '7', '1', '2', '8', '2', '9', '9', '8', '2', '9', '9', '8', '8', '1', '2', '8', '1', '1', '2', '9', '8', '1', '4', '9', '1', '8', '12', '1', '12', '2', '8', '2', '9', '1', '12', '1', '2', '9', '1', '1', '9', '3', '2', '9', '9', '2', '11', '8', '7', '9', '9', '9', '8', '8', '1', '2', '8', '1', '1', '4', '1', '2', '10', '1', '9', '2', '1', '1', '1', '9', '1', '13', '4', '3', '2', '1', '1', '2', '2', '7', '8', '7', '1', '1', '2', '8', '8', '3', '4', '8', '2', '10', '13', '8', '8', '9', '2', '2', '1', '1', '10', '4', '2', '7', '9', '8', '2', '1', '9', '8', '1', '1', '9', '8', '9', '8', '2', '8', '2', '2', '9', '2', '1', '9', '1', '2', '4', '8', '2', '1', '1', '8', '1', '4', '1', '9', '3', '9', '2', '7', '1', '9', '6', '1', '10', '2', '2', '2', '2', '2', '10', '10', '10', '9', '9', '9', '3', '12', '10', '1', '1', '10', '1', '1', '2', '2', '1', '2', '3', '12', '8', '8', '1', '2', '9', '9', '1', '1', '10', '8', '9', '8', '3', '1', '1', '3', '1', '10', '1', '1', '1', '4', '1', '1', '1', '7', '10', '2', '1', '8', '1', '9', '3', '1', '10', '8', '8', '1', '5', '9', '1', '2', '1', '8', '8', '1', '3', '1', '10', '2', '1', '1', '2', '8', '2', '5', '3', '5', '2', '1', '1', '9', '1', '7', '9', '9', '2', '8', '1', '12', '4', '1', '4', '9', '9', '4', '8', '1', '10', '10', '8', '8', '8', '9', '2', '3', '5', '1', '8', '1', '1', '1', '7', '10', '1', '4', '12', '3', '2', '2', '9', '1', '1', '1', '2', '8', '4', '1', '7', '3', '12', '8', '6', '2', '2', '7', '9', '11', '1', '10', '9', '1', '8', '12', '2', '11', '1', '2', '3', '1', '1', '1', '8', '9', '2', '2', '2', '2', '3', '3', '8', '10', '4', '1', '2', '9', '8', '2', '2', '9', '1', '8', '11', '4', '2', '8', '7', '1', '1', '7', '3', '9', '9', '4', '5', '8', '10', '2', '9', '9', '9', '8', '1', '9', '2', '2', '2', '1', '2', '9', '4', '3', '12', '4', '3', '2', '3', '8', '3', '10', '1', '10', '1', '1', '8', '10', '6', '8', '9', '9', '1', '2', '9', '2', '9', '3', '8', '1', '1', '12', '1', '5', '1', '10', '8', '10', '2', '7', '1', '1', '1', '1', '3', '2', '3', '5', '9', '1', '2', '1', '9', '8', '10', '9', '8', '1', '8', '1', '8', '2', '8', '1', '2', '8', '1', '2', '5', '8', '12', '2', '1', '1', '1', '1', '1', '8', '8', '2', '1', '4', '2', '8', '8', '8', '2', '9', '9', '11', '1', '2', '9', '3', '1', '1', '8', '9', '2', '4', '10', '1', '9', '8', '4', '9', '8', '10', '8', '8', '9', '2', '8', '9', '3', '3', '3', '4', '3', '2', '2', '10', '8', '2', '1', '5', '1', '9', '1', '1', '2', '1', '2', '3', '1', '1', '8', '8', '1', '10', '8', '8', '1', '6', '4', '8', '2', '5', '8', '1', '3', '8', '2', '8', '9', '1', '1', '8', '12', '10', '1', '5', '8', '2', '8', '10', '9', '9', '1', '6', '9', '1', '10', '8', '2', '9', '10', '1', '2', '1', '1', '8', '8', '3', '9', '8', '1', '10', '2', '8', '1', '10', '1', '5', '1', '1', '1', '5', '1', '1', '1', '4', '1', '1', '1', '8', '2', '8', '1', '8', '3', '8', '9', '1', '8', '1', '8', '1', '1', '2', '2', '1', '12', '9', '9', '1', '8', '2', '1', '2', '8', '1', '9', '1', '1', '2', '1', '2', '2', '2', '1', '9', '9', '8', '8', '1', '1', '9', '10', '8', '4', '3', '3', '4', '9', '1', '3', '3', '10', '2', '2', '1', '8', '4', '1', '9', '4', '1', '1', '9', '8', '10', '4', '9', '9', '8', '2', '1', '3', '4', '9', '3', '8', '9', '1', '1', '2', '1', '7', '8', '2', '9', '1', '9', '2', '6', '9', '10', '8', '1', '1', '5', '9', '5', '1', '9', '10', '9', '3', '3', '8', '8', '2', '9', '2', '8', '2', '1', '8', '5', '8', '3', '10', '1', '1', '2', '1', '8', '1', '4', '3', '2', '9', '4', '9', '8', '8', '8', '1', '3', '9', '1', '9', '9', '1', '2', '1', '9', '8', '9', '8', '1', '1', '1', '4', '2', '2', '9', '9', '1', '1', '1', '3', '6', '1', '2', '3', '1', '4', '2', '9', '1', '9', '9', '1', '8', '2', '1', '9', '1', '8', '1', '8', '8', '2', '1', '9', '9', '6', '8', '8', '9', '8', '10', '8', '9', '5', '1', '1', '1', '1', '8', '1', '8', '12', '8', '1', '12', '9', '2', '8', '1', '1', '2', '1', '8', '6', '9', '9', '8', '1', '8', '9', '7', '7', '8', '9', '9', '1', '8', '3', '8', '1', '3', '3', '8', '9', '8', '2', '8', '1', '2', '9', '8', '1', '1', '1', '1', '9', '1', '9', '8', '12', '3', '8', '8', '1', '10', '10', '10', '2', '2', '1', '10', '2', '2', '10', '8', '12', '8', '1', '1', '1', '2', '1', '1', '3', '8', '1', '2', '2', '2', '1', '1', '6', '2', '8', '8', '8', '1', '1', '1', '9', '9', '2', '8', '10', '2', '8', '2', '1', '2', '9', '9', '2', '1', '1', '8', '8', '1', '3', '1', '8', '9', '1', '10', '9', '1', '1', '1', '1', '7', '2', '2', '1', '2', '8', '8', '1', '2', '1', '3', '2', '1', '8', '3', '9', '2', '10', '2', '10', '1', '2', '1', '1', '13', '9', '10', '8', '2', '2', '1', '1', '8', '2', '2', '1', '9', '1', '1', '8', '9', '9', '2', '9', '2', '2', '1', '1', '1', '8', '9', '4', '9', '9', '1', '8', '2', '1', '1', '8', '2', '2', '2', '8', '1', '1', '8', '2', '10', '2', '9', '9', '7', '1', '1', '2', '2', '8', '3', '1', '2', '1', '1', '8', '1', '1', '1', '9', '1', '1', '8', '8', '1', '9', '6', '1', '9', '1', '8', '1', '8', '8', '8', '1', '3', '8', '8', '2', '6', '3', '2', '1', '10', '1', '8', '2', '1', '6', '1', '1', '1', '8', '1', '10', '1', '1', '8', '9', '2', '6', '9', '2', '9', '4', '1', '8', '1', '3', '7', '8', '8', '5', '8', '7', '1', '1', '1', '13', '8', '3', '1', '1', '1', '1', '2', '8', '2', '12', '8', '1', '2', '5', '1', '8', '2', '10', '8', '2', '10', '1', '5', '2', '1', '1', '5', '2', '2', '7', '8', '2', '9', '9', '1', '10', '1', '8', '2', '5', '8', '1', '5', '1', '10', '1', '1', '9', '8', '6', '1', '8', '9', '8', '1', '8', '2', '2', '8', '3', '9', '1', '9', '3', '8', '4', '4', '3', '3', '1', '1', '1', '8', '9', '1', '1', '8', '3', '1', '2', '8', '2', '1', '1', '10', '1', '2', '2', '1', '1', '8', '2', '8', '2', '10', '2', '2', '10', '8', '8', '8', '13', '2', '2', '9', '2', '5', '9', '2', '1', '8', '8', '8', '1', '12', '12', '2', '9', '2', '1', '8', '2', '1', '8', '1', '9', '9', '2', '7', '1', '1', '7', '1', '4', '3', '1', '10', '3', '10', '9', '4', '6', '7', '5', '2', '1', '8', '8', '1', '4', '9', '2', '1', '8', '1', '1', '1', '1', '1', '8', '9', '8', '8', '8', '8', '9', '8', '10', '1', '8', '2', '7', '8', '9', '1', '2', '5', '9', '10', '8', '9', '12', '1', '6', '1', '2', '10', '9', '4', '1', '8', '10', '10', '1', '1', '1', '1', '1', '2', '3', '8', '1', '2', '10', '2', '1', '2', '4', '2', '2', '9', '1', '4', '2', '1', '1', '12', '12', '9', '9', '7', '1', '8', '4', '9', '9', '8', '1', '7', '3', '2', '1', '1', '4', '8', '1', '5', '8', '2', '8', '1', '14', '9', '12', '1', '10', '2', '1', '1', '8', '8', '6', '6', '3', '1', '2', '8', '1', '3', '2', '1', '8', '8', '1', '10', '8', '2', '8', '2', '5', '9', '1', '8', '8', '1', '1', '8', '8', '13', '1', '3', '8', '4', '1', '9', '7', '8', '8', '8', '3', '8', '1', '4', '2', '8', '8', '8', '7', '1', '1', '2', '8', '8', '9', '8', '8', '2', '3', '9', '9', '2', '1', '2', '2', '1', '8', '2', '6', '2', '1', '1', '9', '10', '3', '8', '2', '3', '2', '8', '8', '1', '8', '8', '8', '9', '1', '4', '1', '9', '2', '1', '8', '0', '1', '8', '1', '1', '8', '9', '9', '1', '3', '1', '2', '8', '8', '7', '1', '1', '2', '9', '2', '10', '2', '8', '2', '2', '2', '7', '1', '9', '8', '1', '3', '9', '2', '1', '10', '8', '1', '4', '1', '8', '5', '9', '8', '8', '1', '2', '2', '1', '8', '8', '6', '8', '1', '1', '8', '3', '2', '2', '1', '8', '1', '7', '6', '8', '1', '1', '1', '9', '1', '1', '1', '2', '1', '11', '10', '10', '11', '9']\n",
            "     filenum                                               text  label\n",
            "0          1  ent waves which Lehr and Wyatt recorded on the...      8\n",
            "1          2  many times by the writers of the Old Testament...      1\n",
            "2          4  sued, recovery may be had for an involuntary, ...      2\n",
            "3          6  of the manufacturer.' The same result was reac...      8\n",
            "4          7  in the interest of either the consumer or the ...      8\n",
            "...      ...                                                ...    ...\n",
            "4499    8411  Dixon, supra, at P. at And the California Supr...      1\n",
            "4500    8412  improper venue because it was filed in the wro...      1\n",
            "4501    8414  hearing and the arguments of counsel, you find...      1\n",
            "4502    8417  re-examination, they should be reaffirmed in a...     10\n",
            "4503    8406  (emphasis added). And those listed locations a...      1\n",
            "\n",
            "[4504 rows x 3 columns]\n"
          ]
        }
      ],
      "source": [
        "fh = open('sc_model_5.txt')\n",
        "filenums=[]\n",
        "records=[]\n",
        "for record in fh:\n",
        "    record=record.split(\"---\")\n",
        "    filenum=record[0]\n",
        "    modelnum=record[1]\n",
        "    text=record[2]\n",
        "    filenums.append(filenum)\n",
        "    records.append(text)\n",
        "    \n",
        "print(len(filenums))\n",
        "print(len(records))\n",
        "\n",
        "summarized_data = pd.DataFrame(list(zip(filenums, records)),columns =['filenum','text'])\n",
        "len_list = [len(ele.split()) for ele in records]\n",
        "res = 0 if len(len_list) == 0 else (float(sum(len_list)) / len(len_list))\n",
        "print(\"Average Length %s\" % res)\n",
        "\n",
        "temp_file = open(\"labels_sc.txt\", \"r\")\n",
        "#temp_file = open(\"labels_sc_279.txt\", \"r\")\n",
        "\n",
        "data = temp_file.read()\n",
        "\n",
        "label_list = data.split(\"\\n\")\n",
        "print(label_list)\n",
        "label_list = label_list[0:-1]\n",
        "print(label_list)\n",
        "label_list = [int(i) for i in label_list]\n",
        "temp_file.close()\n",
        "\n",
        "\n",
        "new_label_list=[]\n",
        "for num in filenums:\n",
        "  new_label_list.append(label_list[int(num)-1])\n",
        "summarized_data['label'] = new_label_list\n",
        "\n",
        "print(summarized_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FL0rlksFwxTx",
        "outputId": "756b35b3-d306-4ee7-aa8f-8a9c5085228f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_1[0][0]',                \n",
            "                                thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_bert_model[0][0]']          \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 15)           7695        ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 8  1  2 ...  1 10  1]\n",
            "[   1    2    4 ... 8414 8417 8406]\n"
          ]
        }
      ],
      "source": [
        "gc.collect()\n",
        "tf.keras.backend.clear_session()\n",
        "dbert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "dbert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
        "max_len=512\n",
        "sentences=summarized_data['text']\n",
        "labels=summarized_data['label']\n",
        "filenumbers=summarized_data['filenum']\n",
        "len(sentences),len(labels),len(filenumbers)\n",
        "\n",
        "model_5=create_model()\n",
        "input_ids=[]\n",
        "attention_masks=[]\n",
        "\n",
        "for sent in sentences:\n",
        "  dbert_inps=dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n",
        "  input_ids.append(dbert_inps['input_ids'])\n",
        "  attention_masks.append(dbert_inps['attention_mask'])\n",
        "input_ids=np.asarray(input_ids)\n",
        "\n",
        "attention_masks=np.array(attention_masks)\n",
        "labels=np.array(labels)\n",
        "filenumbers=[int(i) for i in filenumbers]\n",
        "filenumbers=np.array(filenumbers)\n",
        "print(labels)\n",
        "print(filenumbers)\n",
        "\n",
        "train_inp,val_inp,train_label,val_label,train_mask,val_mask,train_fnum,val_fnum=train_test_split(input_ids,labels,attention_masks,filenumbers,test_size=0.1,random_state=42)\n",
        "\n",
        "new_train_inp_5=[]\n",
        "new_val_inp_5=[]\n",
        "new_train_label_5=[]\n",
        "new_val_label_5=[]\n",
        "new_train_mask_5=[]\n",
        "new_train_fnum_5=[]\n",
        "new_val_fnum_5=[]\n",
        "new_val_mask_5=[]\n",
        "for i in range(len(train_fnum)):\n",
        "  if train_fnum[i] in filenumbers:\n",
        "    new_train_inp_5.append(train_inp[i])\n",
        "    #new_val_inp.append(val_inp[i])\n",
        "    new_train_label_5.append(train_label[i])\n",
        "    #new_val_label.append(val_label[i])\n",
        "    new_train_mask_5.append(train_mask[i])\n",
        "    #new_val_mask.append(val_mask[i])\n",
        "    new_train_fnum_5.append(train_fnum[i])\n",
        "\n",
        "for i in range(len(val_fnum)):\n",
        "  if val_fnum[i] in filenumbers:\n",
        "    #new_train_inp.append(train_inp[i])\n",
        "    new_val_inp_5.append(val_inp[i])\n",
        "    #new_train_label.append(train_label[i])\n",
        "    new_val_label_5.append(val_label[i])\n",
        "    #new_train_mask.append(train_mask[i])\n",
        "    new_val_mask_5.append(val_mask[i])\n",
        "    new_val_fnum_5.append(val_fnum[i])\n",
        "\n",
        "# print(new_train_label)\n",
        "# print(len(new_train_label))\n",
        "# print(new_val_label)\n",
        "# print(len(new_val_label))\n",
        "new_train_inp_5=np.array(new_train_inp_5)\n",
        "new_val_inp_5=np.array(new_val_inp_5)\n",
        "new_train_label_5=np.array(new_train_label_5)\n",
        "new_val_label_5=np.array(new_val_label_5)\n",
        "new_train_mask_5=np.array(new_train_mask_5)\n",
        "new_train_fnum_5=np.array(new_train_fnum_5)\n",
        "new_val_fnum_5=np.array(new_val_fnum_5)\n",
        "new_val_mask_5=np.array(new_val_mask_5)\n",
        "\n",
        "print(new_val_fnum_5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "C9x3SEyjQiEC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a47eedcb-0675-4b41-eeec-3a9707c8a336"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_1[0][0]',                \n",
            "                                thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_bert_model[0][0]']          \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 15)           7695        ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 8  1  2 ...  1 10  1]\n",
            "[   1    2    4 ... 8414 8417 8406]\n",
            "[6622   35 6713 4989 8233 7444 5061 8344 5464  317 4515 3001 6991 2875\n",
            " 3885 6647 4857 4504  214 8340 2969 3847 4609 7358 2752 7651 8188 7073\n",
            " 6483 4952 5072 1967 4071 7590 5030 7297  626 6029 6302 7256 3993 4263\n",
            " 3835 4293 8129 4837 3843 1418 5274 6205 5997 6685 3310 7721 2360 7197\n",
            " 4252 8343  909 4354  649 3488 2496 5765 4004 4052 1403 6697 6293 6380\n",
            " 4602 6057 4236 7978 5083 6552  361 7706 4186 8351  812 6600 8264 8064\n",
            " 7121 8279 5556 5119 7947 3350 7152 4282 5025 5051 3615 8062 1489 7351\n",
            "  234 7416 7921 6574 6257 8396 4520 3300 5640  240 7810 1321 6580 5404\n",
            " 8273 6742 7861 7975 1018 2140 7061 7767 4990 8124 5927 6051 6916 2846\n",
            " 6818 7052 7775 7582 8116 4767 5248 6055 8331 1564 7326 3065 6764 6854\n",
            " 4915 3583 5902 4229 7565 3712 7224 4502 3827 6525 6318 2686 5369 8235\n",
            " 3351 8021  102  224 8294 7707 2985 5146 3709 6824 2281 6837 5436 2649\n",
            " 4886  966 5476 8387 6166 4387  350 6123 5236 5041  734 3412 6085  489\n",
            " 4982 5115 6743 7173 5724 7689 7787 8302 3150 2247 8128 6660 4610 2866\n",
            " 6905 7096  768 4411 6087 2096 4903 7546 6630 2928 4640 7125 1792 1700\n",
            " 7928 8095  737 5849 2359 6015 4361 6081 5014 5167 1023 8009  268 4777\n",
            " 5702 6129 7181 1860 7192 5467   67 6134 4270 4611 6746 7696 3466 3950\n",
            " 6125 8370 6142 6131 5548 1721 5067 4015 7092   25 1749 4878 8248 6836\n",
            " 5310 6960 2339 1650 6227 4104 5130 7089 6993 1289 4305 5585 7933 6284\n",
            " 7057 3807 8347 5280 2841 7693 8090 4171 5260  133 7451 3957 1719 5435\n",
            " 6216 4858 4417 8355 4492 2648 5278 7308  158 7543 2168 5913 7111 3414\n",
            " 7592   39 7157 5647 4555 3956 3617 4630 4384 6596 4539 6749 7202 2413\n",
            " 4749 5122 5929  252 5565  336 6008 4564 5580 7237 5958 6918 6487 2989\n",
            " 7448 3382  467 3784 4419 1117 7954 7122 3174 1534 4164 7526  498 5944\n",
            " 4572 2307 2929 7480 4412 4563 7692 1869 2472 6966 6549 1594 2276 7269\n",
            " 2545 2216 6587 6315 5297 5282 6399 3917 6342 5721 7909 8382 4383 7130\n",
            "  485 4936 7674 5372 7109 4605 5211 5462 8065   50 4080 3334 8167 8378\n",
            " 5595 1873 8207 7076 3719 4627  231 2405 6973 3246 5931 8208 4825 6203\n",
            " 3019 5822   63 5854 5787 4764 6701 3822 8318 8169 8218 7549  707 1076\n",
            " 2465 4636 7025 7127 4026 4961 7531  744 6684 4297 7939 7384  446 6675\n",
            " 7752 5279 8196 3020 4192 4253 2887 1898 3208 5973 7033 1431 5915 8312\n",
            " 5799 7105  511 6404 1561]\n",
            "Fri Jun  3 02:08:26 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    38W / 300W |  15237MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "509/509 [==============================] - 191s 344ms/step - loss: 7.4179 - accuracy: 0.5720 - val_loss: 6.6787 - val_accuracy: 0.6902\n",
            "Epoch 2/5\n",
            "509/509 [==============================] - 175s 343ms/step - loss: 6.2045 - accuracy: 0.7442 - val_loss: 5.9761 - val_accuracy: 0.7244\n",
            "Epoch 3/5\n",
            "509/509 [==============================] - 175s 343ms/step - loss: 5.3668 - accuracy: 0.8219 - val_loss: 5.4772 - val_accuracy: 0.7289\n",
            "Epoch 4/5\n",
            "509/509 [==============================] - 171s 337ms/step - loss: 4.7014 - accuracy: 0.8694 - val_loss: 5.0542 - val_accuracy: 0.7289\n",
            "Epoch 5/5\n",
            "509/509 [==============================] - 172s 337ms/step - loss: 4.0527 - accuracy: 0.9299 - val_loss: 4.7068 - val_accuracy: 0.7107\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_3[0][0]',                \n",
            "                                thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_bert_model[1][0]']          \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 15)           7695        ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.6971496437054632\n",
            "Weighted F1: 0.6932476413312391\n",
            "Micro F1: 0.6971496437054632\n",
            "Weighted Precision: 0.7103826401940982\n",
            "Micro Precision: 0.6971496437054632\n",
            "Weighted Recall: 0.6971496437054632\n",
            "Micro Recall: 0.6971496437054632\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_1[0][0]',                \n",
            "                                thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_bert_model[0][0]']          \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 15)           7695        ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 8  1  2 ...  1 10  1]\n",
            "[   1    2    4 ... 8414 8417 8406]\n",
            "[6622   35 6713 4989 8233 7444 5061 8344 5464  317 4515 3001 6991 2875\n",
            " 3885 6647 4857 4504  214 8340 2969 3847 4609 7358 2752 7651 8188 7073\n",
            " 6483 4952 5072 1967 4071 7590 5030 7297  626 6029 6302 7256 3993 4263\n",
            " 3835 4293 8129 4837 3843 1418 5274 6205 5997 6685 3310 7721 2360 7197\n",
            " 4252 8343  909 4354  649 3488 2496 5765 4004 4052 1403 6697 6293 6380\n",
            " 4602 6057 4236 7978 5083 6552  361 7706 4186 8351  812 6600 8264 8064\n",
            " 7121 8279 5556 5119 7947 3350 7152 4282 5025 5051 3615 8062 1489 7351\n",
            "  234 7416 7921 6574 6257 8396 4520 3300 5640  240 7810 1321 6580 5404\n",
            " 8273 6742 7861 7975 1018 2140 7061 7767 4990 8124 5927 6051 6916 2846\n",
            " 6818 7052 7775 7582 8116 4767 5248 6055 8331 1564 7326 3065 6764 6854\n",
            " 4915 3583 5902 4229 7565 3712 7224 4502 3827 6525 6318 2686 5369 8235\n",
            " 3351 8021  102  224 8294 7707 2985 5146 3709 6824 2281 6837 5436 2649\n",
            " 4886  966 5476 8387 6166 4387  350 6123 5236 5041  734 3412 6085  489\n",
            " 4982 5115 6743 7173 5724 7689 7787 8302 3150 2247 8128 6660 4610 2866\n",
            " 6905 7096  768 4411 6087 2096 4903 7546 6630 2928 4640 7125 1792 1700\n",
            " 7928 8095  737 5849 2359 6015 4361 6081 5014 5167 1023 8009  268 4777\n",
            " 5702 6129 7181 1860 7192 5467   67 6134 4270 4611 6746 7696 3466 3950\n",
            " 6125 8370 6142 6131 5548 1721 5067 4015 7092   25 1749 4878 8248 6836\n",
            " 5310 6960 2339 1650 6227 4104 5130 7089 6993 1289 4305 5585 7933 6284\n",
            " 7057 3807 8347 5280 2841 7693 8090 4171 5260  133 7451 3957 1719 5435\n",
            " 6216 4858 4417 8355 4492 2648 5278 7308  158 7543 2168 5913 7111 3414\n",
            " 7592   39 7157 5647 4555 3956 3617 4630 4384 6596 4539 6749 7202 2413\n",
            " 4749 5122 5929  252 5565  336 6008 4564 5580 7237 5958 6918 6487 2989\n",
            " 7448 3382  467 3784 4419 1117 7954 7122 3174 1534 4164 7526  498 5944\n",
            " 4572 2307 2929 7480 4412 4563 7692 1869 2472 6966 6549 1594 2276 7269\n",
            " 2545 2216 6587 6315 5297 5282 6399 3917 6342 5721 7909 8382 4383 7130\n",
            "  485 4936 7674 5372 7109 4605 5211 5462 8065   50 4080 3334 8167 8378\n",
            " 5595 1873 8207 7076 3719 4627  231 2405 6973 3246 5931 8208 4825 6203\n",
            " 3019 5822   63 5854 5787 4764 6701 3822 8318 8169 8218 7549  707 1076\n",
            " 2465 4636 7025 7127 4026 4961 7531  744 6684 4297 7939 7384  446 6675\n",
            " 7752 5279 8196 3020 4192 4253 2887 1898 3208 5973 7033 1431 5915 8312\n",
            " 5799 7105  511 6404 1561]\n",
            "Fri Jun  3 02:25:26 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P0    38W / 300W |  15237MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "509/509 [==============================] - 191s 346ms/step - loss: 7.4451 - accuracy: 0.5840 - val_loss: 6.7373 - val_accuracy: 0.6743\n",
            "Epoch 2/5\n",
            "509/509 [==============================] - 175s 344ms/step - loss: 6.2518 - accuracy: 0.7508 - val_loss: 6.1023 - val_accuracy: 0.6879\n",
            "Epoch 3/5\n",
            "509/509 [==============================] - 175s 345ms/step - loss: 5.4629 - accuracy: 0.8165 - val_loss: 5.6325 - val_accuracy: 0.7335\n",
            "Epoch 4/5\n",
            "509/509 [==============================] - 171s 337ms/step - loss: 4.7868 - accuracy: 0.8716 - val_loss: 5.0543 - val_accuracy: 0.7267\n",
            "Epoch 5/5\n",
            "509/509 [==============================] - 175s 344ms/step - loss: 4.1730 - accuracy: 0.9149 - val_loss: 4.6837 - val_accuracy: 0.7380\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_3[0][0]',                \n",
            "                                thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_bert_model[1][0]']          \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 15)           7695        ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.7066508313539193\n",
            "Weighted F1: 0.7048062491297006\n",
            "Micro F1: 0.7066508313539193\n",
            "Weighted Precision: 0.7123360578802019\n",
            "Micro Precision: 0.7066508313539193\n",
            "Weighted Recall: 0.7066508313539193\n",
            "Micro Recall: 0.7066508313539193\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_1[0][0]',                \n",
            "                                thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_bert_model[0][0]']          \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 15)           7695        ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 8  1  2 ...  1 10  1]\n",
            "[   1    2    4 ... 8414 8417 8406]\n",
            "[6622   35 6713 4989 8233 7444 5061 8344 5464  317 4515 3001 6991 2875\n",
            " 3885 6647 4857 4504  214 8340 2969 3847 4609 7358 2752 7651 8188 7073\n",
            " 6483 4952 5072 1967 4071 7590 5030 7297  626 6029 6302 7256 3993 4263\n",
            " 3835 4293 8129 4837 3843 1418 5274 6205 5997 6685 3310 7721 2360 7197\n",
            " 4252 8343  909 4354  649 3488 2496 5765 4004 4052 1403 6697 6293 6380\n",
            " 4602 6057 4236 7978 5083 6552  361 7706 4186 8351  812 6600 8264 8064\n",
            " 7121 8279 5556 5119 7947 3350 7152 4282 5025 5051 3615 8062 1489 7351\n",
            "  234 7416 7921 6574 6257 8396 4520 3300 5640  240 7810 1321 6580 5404\n",
            " 8273 6742 7861 7975 1018 2140 7061 7767 4990 8124 5927 6051 6916 2846\n",
            " 6818 7052 7775 7582 8116 4767 5248 6055 8331 1564 7326 3065 6764 6854\n",
            " 4915 3583 5902 4229 7565 3712 7224 4502 3827 6525 6318 2686 5369 8235\n",
            " 3351 8021  102  224 8294 7707 2985 5146 3709 6824 2281 6837 5436 2649\n",
            " 4886  966 5476 8387 6166 4387  350 6123 5236 5041  734 3412 6085  489\n",
            " 4982 5115 6743 7173 5724 7689 7787 8302 3150 2247 8128 6660 4610 2866\n",
            " 6905 7096  768 4411 6087 2096 4903 7546 6630 2928 4640 7125 1792 1700\n",
            " 7928 8095  737 5849 2359 6015 4361 6081 5014 5167 1023 8009  268 4777\n",
            " 5702 6129 7181 1860 7192 5467   67 6134 4270 4611 6746 7696 3466 3950\n",
            " 6125 8370 6142 6131 5548 1721 5067 4015 7092   25 1749 4878 8248 6836\n",
            " 5310 6960 2339 1650 6227 4104 5130 7089 6993 1289 4305 5585 7933 6284\n",
            " 7057 3807 8347 5280 2841 7693 8090 4171 5260  133 7451 3957 1719 5435\n",
            " 6216 4858 4417 8355 4492 2648 5278 7308  158 7543 2168 5913 7111 3414\n",
            " 7592   39 7157 5647 4555 3956 3617 4630 4384 6596 4539 6749 7202 2413\n",
            " 4749 5122 5929  252 5565  336 6008 4564 5580 7237 5958 6918 6487 2989\n",
            " 7448 3382  467 3784 4419 1117 7954 7122 3174 1534 4164 7526  498 5944\n",
            " 4572 2307 2929 7480 4412 4563 7692 1869 2472 6966 6549 1594 2276 7269\n",
            " 2545 2216 6587 6315 5297 5282 6399 3917 6342 5721 7909 8382 4383 7130\n",
            "  485 4936 7674 5372 7109 4605 5211 5462 8065   50 4080 3334 8167 8378\n",
            " 5595 1873 8207 7076 3719 4627  231 2405 6973 3246 5931 8208 4825 6203\n",
            " 3019 5822   63 5854 5787 4764 6701 3822 8318 8169 8218 7549  707 1076\n",
            " 2465 4636 7025 7127 4026 4961 7531  744 6684 4297 7939 7384  446 6675\n",
            " 7752 5279 8196 3020 4192 4253 2887 1898 3208 5973 7033 1431 5915 8312\n",
            " 5799 7105  511 6404 1561]\n",
            "Fri Jun  3 02:42:27 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P0    38W / 300W |  15237MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "509/509 [==============================] - 190s 344ms/step - loss: 7.4508 - accuracy: 0.5651 - val_loss: 6.7034 - val_accuracy: 0.6765\n",
            "Epoch 2/5\n",
            "509/509 [==============================] - 175s 344ms/step - loss: 6.1644 - accuracy: 0.7321 - val_loss: 5.9351 - val_accuracy: 0.6970\n",
            "Epoch 3/5\n",
            "509/509 [==============================] - 175s 343ms/step - loss: 5.3506 - accuracy: 0.8091 - val_loss: 5.3838 - val_accuracy: 0.7130\n",
            "Epoch 4/5\n",
            "509/509 [==============================] - 175s 343ms/step - loss: 4.6557 - accuracy: 0.8647 - val_loss: 4.9952 - val_accuracy: 0.7153\n",
            "Epoch 5/5\n",
            "509/509 [==============================] - 175s 344ms/step - loss: 4.0330 - accuracy: 0.9173 - val_loss: 4.6220 - val_accuracy: 0.7175\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_3[0][0]',                \n",
            "                                thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_bert_model[1][0]']          \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 15)           7695        ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.7114014251781473\n",
            "Weighted F1: 0.7086249167504493\n",
            "Micro F1: 0.7114014251781473\n",
            "Weighted Precision: 0.7192096510117685\n",
            "Micro Precision: 0.7114014251781473\n",
            "Weighted Recall: 0.7114014251781473\n",
            "Micro Recall: 0.7114014251781473\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_1[0][0]',                \n",
            "                                thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_bert_model[0][0]']          \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 15)           7695        ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 8  1  2 ...  1 10  1]\n",
            "[   1    2    4 ... 8414 8417 8406]\n",
            "[6622   35 6713 4989 8233 7444 5061 8344 5464  317 4515 3001 6991 2875\n",
            " 3885 6647 4857 4504  214 8340 2969 3847 4609 7358 2752 7651 8188 7073\n",
            " 6483 4952 5072 1967 4071 7590 5030 7297  626 6029 6302 7256 3993 4263\n",
            " 3835 4293 8129 4837 3843 1418 5274 6205 5997 6685 3310 7721 2360 7197\n",
            " 4252 8343  909 4354  649 3488 2496 5765 4004 4052 1403 6697 6293 6380\n",
            " 4602 6057 4236 7978 5083 6552  361 7706 4186 8351  812 6600 8264 8064\n",
            " 7121 8279 5556 5119 7947 3350 7152 4282 5025 5051 3615 8062 1489 7351\n",
            "  234 7416 7921 6574 6257 8396 4520 3300 5640  240 7810 1321 6580 5404\n",
            " 8273 6742 7861 7975 1018 2140 7061 7767 4990 8124 5927 6051 6916 2846\n",
            " 6818 7052 7775 7582 8116 4767 5248 6055 8331 1564 7326 3065 6764 6854\n",
            " 4915 3583 5902 4229 7565 3712 7224 4502 3827 6525 6318 2686 5369 8235\n",
            " 3351 8021  102  224 8294 7707 2985 5146 3709 6824 2281 6837 5436 2649\n",
            " 4886  966 5476 8387 6166 4387  350 6123 5236 5041  734 3412 6085  489\n",
            " 4982 5115 6743 7173 5724 7689 7787 8302 3150 2247 8128 6660 4610 2866\n",
            " 6905 7096  768 4411 6087 2096 4903 7546 6630 2928 4640 7125 1792 1700\n",
            " 7928 8095  737 5849 2359 6015 4361 6081 5014 5167 1023 8009  268 4777\n",
            " 5702 6129 7181 1860 7192 5467   67 6134 4270 4611 6746 7696 3466 3950\n",
            " 6125 8370 6142 6131 5548 1721 5067 4015 7092   25 1749 4878 8248 6836\n",
            " 5310 6960 2339 1650 6227 4104 5130 7089 6993 1289 4305 5585 7933 6284\n",
            " 7057 3807 8347 5280 2841 7693 8090 4171 5260  133 7451 3957 1719 5435\n",
            " 6216 4858 4417 8355 4492 2648 5278 7308  158 7543 2168 5913 7111 3414\n",
            " 7592   39 7157 5647 4555 3956 3617 4630 4384 6596 4539 6749 7202 2413\n",
            " 4749 5122 5929  252 5565  336 6008 4564 5580 7237 5958 6918 6487 2989\n",
            " 7448 3382  467 3784 4419 1117 7954 7122 3174 1534 4164 7526  498 5944\n",
            " 4572 2307 2929 7480 4412 4563 7692 1869 2472 6966 6549 1594 2276 7269\n",
            " 2545 2216 6587 6315 5297 5282 6399 3917 6342 5721 7909 8382 4383 7130\n",
            "  485 4936 7674 5372 7109 4605 5211 5462 8065   50 4080 3334 8167 8378\n",
            " 5595 1873 8207 7076 3719 4627  231 2405 6973 3246 5931 8208 4825 6203\n",
            " 3019 5822   63 5854 5787 4764 6701 3822 8318 8169 8218 7549  707 1076\n",
            " 2465 4636 7025 7127 4026 4961 7531  744 6684 4297 7939 7384  446 6675\n",
            " 7752 5279 8196 3020 4192 4253 2887 1898 3208 5973 7033 1431 5915 8312\n",
            " 5799 7105  511 6404 1561]\n",
            "Fri Jun  3 02:59:32 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P0    38W / 300W |  15237MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "509/509 [==============================] - 190s 344ms/step - loss: 7.4363 - accuracy: 0.5754 - val_loss: 6.7151 - val_accuracy: 0.6879\n",
            "Epoch 2/5\n",
            "509/509 [==============================] - 175s 343ms/step - loss: 6.2709 - accuracy: 0.7392 - val_loss: 6.0132 - val_accuracy: 0.7312\n",
            "Epoch 3/5\n",
            "509/509 [==============================] - 172s 337ms/step - loss: 5.4308 - accuracy: 0.8330 - val_loss: 5.5647 - val_accuracy: 0.7221\n",
            "Epoch 4/5\n",
            "509/509 [==============================] - 171s 337ms/step - loss: 4.7479 - accuracy: 0.8839 - val_loss: 5.2183 - val_accuracy: 0.6925\n",
            "Epoch 5/5\n",
            "509/509 [==============================] - 171s 337ms/step - loss: 4.1594 - accuracy: 0.9117 - val_loss: 4.8115 - val_accuracy: 0.7221\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_3[0][0]',                \n",
            "                                thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_bert_model[1][0]']          \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 15)           7695        ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.7054631828978623\n",
            "Weighted F1: 0.6983031489431419\n",
            "Micro F1: 0.7054631828978623\n",
            "Weighted Precision: 0.7122943537549282\n",
            "Micro Precision: 0.7054631828978623\n",
            "Weighted Recall: 0.7054631828978623\n",
            "Micro Recall: 0.7054631828978623\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_1[0][0]',                \n",
            "                                thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_bert_model[0][0]']          \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 15)           7695        ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 8  1  2 ...  1 10  1]\n",
            "[   1    2    4 ... 8414 8417 8406]\n",
            "[6622   35 6713 4989 8233 7444 5061 8344 5464  317 4515 3001 6991 2875\n",
            " 3885 6647 4857 4504  214 8340 2969 3847 4609 7358 2752 7651 8188 7073\n",
            " 6483 4952 5072 1967 4071 7590 5030 7297  626 6029 6302 7256 3993 4263\n",
            " 3835 4293 8129 4837 3843 1418 5274 6205 5997 6685 3310 7721 2360 7197\n",
            " 4252 8343  909 4354  649 3488 2496 5765 4004 4052 1403 6697 6293 6380\n",
            " 4602 6057 4236 7978 5083 6552  361 7706 4186 8351  812 6600 8264 8064\n",
            " 7121 8279 5556 5119 7947 3350 7152 4282 5025 5051 3615 8062 1489 7351\n",
            "  234 7416 7921 6574 6257 8396 4520 3300 5640  240 7810 1321 6580 5404\n",
            " 8273 6742 7861 7975 1018 2140 7061 7767 4990 8124 5927 6051 6916 2846\n",
            " 6818 7052 7775 7582 8116 4767 5248 6055 8331 1564 7326 3065 6764 6854\n",
            " 4915 3583 5902 4229 7565 3712 7224 4502 3827 6525 6318 2686 5369 8235\n",
            " 3351 8021  102  224 8294 7707 2985 5146 3709 6824 2281 6837 5436 2649\n",
            " 4886  966 5476 8387 6166 4387  350 6123 5236 5041  734 3412 6085  489\n",
            " 4982 5115 6743 7173 5724 7689 7787 8302 3150 2247 8128 6660 4610 2866\n",
            " 6905 7096  768 4411 6087 2096 4903 7546 6630 2928 4640 7125 1792 1700\n",
            " 7928 8095  737 5849 2359 6015 4361 6081 5014 5167 1023 8009  268 4777\n",
            " 5702 6129 7181 1860 7192 5467   67 6134 4270 4611 6746 7696 3466 3950\n",
            " 6125 8370 6142 6131 5548 1721 5067 4015 7092   25 1749 4878 8248 6836\n",
            " 5310 6960 2339 1650 6227 4104 5130 7089 6993 1289 4305 5585 7933 6284\n",
            " 7057 3807 8347 5280 2841 7693 8090 4171 5260  133 7451 3957 1719 5435\n",
            " 6216 4858 4417 8355 4492 2648 5278 7308  158 7543 2168 5913 7111 3414\n",
            " 7592   39 7157 5647 4555 3956 3617 4630 4384 6596 4539 6749 7202 2413\n",
            " 4749 5122 5929  252 5565  336 6008 4564 5580 7237 5958 6918 6487 2989\n",
            " 7448 3382  467 3784 4419 1117 7954 7122 3174 1534 4164 7526  498 5944\n",
            " 4572 2307 2929 7480 4412 4563 7692 1869 2472 6966 6549 1594 2276 7269\n",
            " 2545 2216 6587 6315 5297 5282 6399 3917 6342 5721 7909 8382 4383 7130\n",
            "  485 4936 7674 5372 7109 4605 5211 5462 8065   50 4080 3334 8167 8378\n",
            " 5595 1873 8207 7076 3719 4627  231 2405 6973 3246 5931 8208 4825 6203\n",
            " 3019 5822   63 5854 5787 4764 6701 3822 8318 8169 8218 7549  707 1076\n",
            " 2465 4636 7025 7127 4026 4961 7531  744 6684 4297 7939 7384  446 6675\n",
            " 7752 5279 8196 3020 4192 4253 2887 1898 3208 5973 7033 1431 5915 8312\n",
            " 5799 7105  511 6404 1561]\n",
            "Fri Jun  3 03:16:25 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P0    38W / 300W |  15237MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "509/509 [==============================] - 192s 346ms/step - loss: 7.4501 - accuracy: 0.5692 - val_loss: 6.7195 - val_accuracy: 0.6629\n",
            "Epoch 2/5\n",
            "509/509 [==============================] - 175s 344ms/step - loss: 6.2219 - accuracy: 0.7373 - val_loss: 5.9835 - val_accuracy: 0.7358\n",
            "Epoch 3/5\n",
            "509/509 [==============================] - 171s 337ms/step - loss: 5.3837 - accuracy: 0.8185 - val_loss: 5.5617 - val_accuracy: 0.7267\n",
            "Epoch 4/5\n",
            "509/509 [==============================] - 175s 343ms/step - loss: 4.7315 - accuracy: 0.8713 - val_loss: 4.9374 - val_accuracy: 0.7380\n",
            "Epoch 5/5\n",
            "509/509 [==============================] - 171s 336ms/step - loss: 4.1174 - accuracy: 0.9107 - val_loss: 4.7355 - val_accuracy: 0.7039\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_3[0][0]',                \n",
            "                                thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_bert_model[1][0]']          \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 15)           7695        ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.7114014251781473\n",
            "Weighted F1: 0.701453113689681\n",
            "Micro F1: 0.7114014251781473\n",
            "Weighted Precision: 0.7086913680992655\n",
            "Micro Precision: 0.7114014251781473\n",
            "Weighted Recall: 0.7114014251781473\n",
            "Micro Recall: 0.7114014251781473\n",
            "Average Accuracy: 0.7064133016627079\n",
            "Average Weighted F1: 0.7012870139688424\n",
            "Average Micro F1: 0.7064133016627079\n",
            "Average Weighted Precision: 0.7125828141880526\n",
            "Average Micro Precision: 0.7064133016627079\n",
            "Average Weighted Recall: 0.7064133016627079\n",
            "Average Micro Recall: 0.7064133016627079\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "total_accuracy=0\n",
        "total_weighted_f1=0\n",
        "total_micro_f1=0\n",
        "total_weighted_precision=0\n",
        "total_micro_precision=0\n",
        "total_weighted_recall=0\n",
        "total_micro_recall=0\n",
        "\n",
        "for f in range(5):\n",
        "  gc.collect()\n",
        "  tf.keras.backend.clear_session()\n",
        "  dbert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "  dbert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
        "  max_len=512\n",
        "  sentences=summarized_data['text']\n",
        "  labels=summarized_data['label']\n",
        "  filenumbers=summarized_data['filenum']\n",
        "  len(sentences),len(labels),len(filenumbers)\n",
        "  \n",
        "  model_5=create_model()\n",
        "  input_ids=[]\n",
        "  attention_masks=[]\n",
        "\n",
        "  for sent in sentences:\n",
        "    dbert_inps=dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n",
        "    input_ids.append(dbert_inps['input_ids'])\n",
        "    attention_masks.append(dbert_inps['attention_mask'])\n",
        "  input_ids=np.asarray(input_ids)\n",
        "\n",
        "  attention_masks=np.array(attention_masks)\n",
        "  labels=np.array(labels)\n",
        "  filenumbers=[int(i) for i in filenumbers]\n",
        "  filenumbers=np.array(filenumbers)\n",
        "  print(labels)\n",
        "  print(filenumbers)\n",
        "\n",
        "  #train_inp,val_inp,train_label,val_label,train_mask,val_mask,train_fnum,val_fnum=train_test_split(input_ids,labels,attention_masks,filenumbers,test_size=0.1,random_state=42)\n",
        "\n",
        "  new_train_inp_5=[]\n",
        "  new_val_inp_5=[]\n",
        "  new_train_label_5=[]\n",
        "  new_val_label_5=[]\n",
        "  new_train_mask_5=[]\n",
        "  new_train_fnum_5=[]\n",
        "  new_val_fnum_5=[]\n",
        "  new_val_mask_5=[]\n",
        "  for i in range(len(train_fnum)):\n",
        "    if train_fnum[i] in filenumbers:\n",
        "      new_train_inp_5.append(train_inp[i])\n",
        "      #new_val_inp.append(val_inp[i])\n",
        "      new_train_label_5.append(train_label[i])\n",
        "      #new_val_label.append(val_label[i])\n",
        "      new_train_mask_5.append(train_mask[i])\n",
        "      #new_val_mask.append(val_mask[i])\n",
        "      new_train_fnum_5.append(train_fnum[i])\n",
        "\n",
        "  for i in range(len(val_fnum)):\n",
        "    if val_fnum[i] in filenumbers:\n",
        "      #new_train_inp.append(train_inp[i])\n",
        "      new_val_inp_5.append(val_inp[i])\n",
        "      #new_train_label.append(train_label[i])\n",
        "      new_val_label_5.append(val_label[i])\n",
        "      #new_train_mask.append(train_mask[i])\n",
        "      new_val_mask_5.append(val_mask[i])\n",
        "      new_val_fnum_5.append(val_fnum[i])\n",
        "\n",
        "  # print(new_train_label)\n",
        "  # print(len(new_train_label))\n",
        "  # print(new_val_label)\n",
        "  # print(len(new_val_label))\n",
        "  new_train_inp_5=np.array(new_train_inp_5)\n",
        "  new_val_inp_5=np.array(new_val_inp_5)\n",
        "  new_train_label_5=np.array(new_train_label_5)\n",
        "  new_val_label_5=np.array(new_val_label_5)\n",
        "  new_train_mask_5=np.array(new_train_mask_5)\n",
        "  new_train_fnum_5=np.array(new_train_fnum_5)\n",
        "  new_val_fnum_5=np.array(new_val_fnum_5)\n",
        "  new_val_mask_5=np.array(new_val_mask_5)\n",
        "\n",
        "  print(new_val_fnum_5)\n",
        "  \n",
        "  log_dir='dbert_model'\n",
        "\n",
        "  model_save_path='./drive/MyDrive/Ensemble/bert-ensemble-512-model_5-'+str(f)+'-15labels.h5'\n",
        "\n",
        "  loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "  accuracy = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
        "  callbacks= [tf.keras.callbacks.ModelCheckpoint(filepath=model_save_path,monitor='val_accuracy',mode='max',save_best_only=True,save_weights_only=True),keras.callbacks.TensorBoard(log_dir=log_dir)]\n",
        "  model_5.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])\n",
        "  gpu_info = !nvidia-smi\n",
        "  gpu_info = '\\n'.join(gpu_info)\n",
        "  if gpu_info.find('failed') >= 0:\n",
        "    print('Not connected to a GPU')\n",
        "  else:\n",
        "    print(gpu_info)\n",
        "  \n",
        "  history=model_5.fit([new_train_inp_5,new_train_mask_5],new_train_label_5,batch_size=8,epochs=5,validation_data=([new_val_inp_5,new_val_mask_5],new_val_label_5),callbacks=callbacks)\n",
        "\n",
        "  pred_labels=[]\n",
        "\n",
        "  model_saved_5= create_model()\n",
        "  model_saved_5.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])\n",
        "  model_saved_5.load_weights('./drive/MyDrive/Ensemble/bert-ensemble-512-model_4-'+str(f)+'-15labels.h5')\n",
        "\n",
        "  for i in range(0,len(val_inp)):\n",
        "    pred=model_saved_5.predict([val_inp[i].reshape(1,512),val_mask[i].reshape(1,512)])\n",
        "    pred_label = pred.argmax(axis=1)\n",
        "    pred_labels.append(pred_label)\n",
        "  accuracy=accuracy_score(val_label, pred_labels)\n",
        "  print(\"Accuracy: \"+str(accuracy))\n",
        "  total_accuracy=total_accuracy+accuracy\n",
        "  \n",
        "  weighted_f1=f1_score(val_label,pred_labels, average='weighted')\n",
        "  print(\"Weighted F1: \"+ str(weighted_f1))\n",
        "  total_weighted_f1=total_weighted_f1+weighted_f1\n",
        "  micro_f1=f1_score(val_label,pred_labels, average='micro')\n",
        "  print(\"Micro F1: \"+ str(micro_f1))\n",
        "  total_micro_f1=total_micro_f1+micro_f1\n",
        "\n",
        "  weighted_precision=precision_score(val_label, pred_labels, average='weighted')\n",
        "  print(\"Weighted Precision: \" + str(weighted_precision))\n",
        "  total_weighted_precision=total_weighted_precision+weighted_precision\n",
        "  micro_precision=precision_score(val_label, pred_labels, average='micro')\n",
        "  print(\"Micro Precision: \" + str(micro_precision))\n",
        "  total_micro_precision=total_micro_precision+micro_precision\n",
        "\n",
        "  weighted_recall=recall_score(val_label, pred_labels, average='weighted')\n",
        "  print(\"Weighted Recall: \" + str(weighted_recall))\n",
        "  total_weighted_recall=total_weighted_recall+weighted_recall\n",
        "  micro_recall=recall_score(val_label, pred_labels, average='micro')\n",
        "  print(\"Micro Recall: \" + str(micro_recall))\n",
        "  total_micro_recall=total_micro_recall+micro_recall\n",
        "\n",
        "\n",
        "print(\"Average Accuracy: \"+str(total_accuracy/5))\n",
        "print(\"Average Weighted F1: \"+str(total_weighted_f1/5))\n",
        "print(\"Average Micro F1: \"+str(total_micro_f1/5))\n",
        "print(\"Average Weighted Precision: \"+str(total_weighted_precision/5))\n",
        "print(\"Average Micro Precision: \"+str(total_micro_precision/5))\n",
        "print(\"Average Weighted Recall: \"+str(total_weighted_recall/5))\n",
        "print(\"Average Micro Recall: \"+str(total_micro_recall/5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "aiQQ3PLLiLdn"
      },
      "outputs": [],
      "source": [
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "accuracy = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
        "\n",
        "def load_model(loss,accuracy,optimizer,id):\n",
        "  model= create_model()\n",
        "  model.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])\n",
        "  model.load_weights('./drive/MyDrive/Ensemble/bert-ensemble-512-model_'+str(id)+'-15labels.h5')\n",
        "  return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcfQEe8-lDZ7",
        "outputId": "f78ea049-0ac8-4bfb-f2be-7178427d6949"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6622   35 2227 6713 4989 8233 7444 5061 3942 8344  454 5464  317 2883\n",
            " 4515  796 3001 3410 6991 2875 1125 1657  354 3885 6647 4857 4504  214\n",
            " 3230 8340 2969 3847 4609 5092 6110 1346 5993 7358 2752 7651 7301 8188\n",
            " 7073 4782 6069 2346 6445 6483  542 1663 4952 5072 1967 4071 7590  860\n",
            " 6418 1997 5030 7297 2570  626 5463 6029 6302 3819 7256 2950 5667 1235\n",
            " 3993 1439 1323 4263 3835 2134 3054 2624 5801 4293 8129  320 5866 3247\n",
            " 5777 2373 5518   19 6059 6520 4837 3843 5695 1666  858 1418 5409 5274\n",
            " 6205 3115 3167 2356 8418 5997 6685  223 3444 3310 7721 5928 6139 2360\n",
            " 7197 4252 8343  909 4093 4354 4752 1421  649  829 1916 1003 3488 2496\n",
            " 5765 6612 4004 4052 1403 6697 5268 1010 6293  264 6380 4602 6057 1480\n",
            " 1851 4577 4236 7978 1350 5256 5083 6552 1058  361 7706 4186 2590 4452\n",
            " 4396 5386 2522  749  870 8351 3834  812 5108  463 2274 1743 6600 8264\n",
            " 8064 7121 8279 5556 1845 5119 2516 4673 2652 7947 8215 1882 3350 7617\n",
            " 7152 5199 3965 1374 4282 7672 1092 3342 8003 5025 5051  535 6024  933\n",
            " 3969 1927 3615 1221 8062 4920 1489 3296 7442 5459 2031 7351 1633  234\n",
            " 2313 8015 7416 7921 6574 6257 3535 8396  941 4520 3300 5640  240 7810\n",
            " 7425 1321 6580  928 1170 5404 1920 8273 5649 1919 6742 7005   94 7459\n",
            "    9 7861 7975 1018 5029 2322 2140 1415 7061 7767 4990 6261 8124 5927\n",
            " 6051 1739 3269 6916 5959 1503 4906 2846 6818 7052 7775 7582 8116 4240\n",
            " 4767 5248  430 1812 2175 6055 8331 3916  998 7271 1564 7326 1077 1559\n",
            "  292 3065 6764 6854 4915 5552 3583 1917  868 5902 2581 4229 2593 2144\n",
            " 7565 3712 7224 4502 1406 1651 6256 3827 6525 6318  791 2686 3616 5369\n",
            " 8235 1704 1011 3351 8089 8021  102  224 5671 8294 5874 1510 2459 7707\n",
            " 2985 6624 5146 1428 7381 3709 6824 6308  701 2281 6837 5436 1741 3520\n",
            " 5995 2649 4886  966 2863 5476 4058 1246 2406 1199 8387  513 5449 1028\n",
            " 2226 6166 4387  350 6123  678 5236 5041  734 3743 3439 3412 6085  587\n",
            " 2477  489 5760 3163 3376 4982  321 5115 4655 5575 3839 6743 7173  624\n",
            " 5724 7689 7787 1748  520  713 3214 8302 3150 5713 2247 8128 3044 4607\n",
            " 2089 6660 3505  983 4610 2866 4161 6905 7096  768 1062 4411 6087 2096\n",
            " 3536   21 4903 7546 6630 2928 4640 4510 7125 2816 1792 2063 1700 2305\n",
            " 7928 8095  737 5849 2311  922 3423 2359 6015 4489 4361 3250 5076 6081\n",
            " 6346 5014 5167 1023 8009 3335  268 2211 4777 5221 4323 6168 2580  759\n",
            "  711 5702 6129 7181 1860 5385 4701 7192 1347 1692 8278 5467 3378 6579\n",
            "   67 2693 6134 4841 4270 3073 8397 4611 1057 2943 6746 7696 3466 2573\n",
            " 1983 3950 4693 6125 4033  474 8370 6142 6131 3938  995 5548 2186 2112\n",
            " 1721 4153 5067 4015 5587 7092 3156   25 1749 5554 4878 1197 8248  458\n",
            " 8035 6836 1006 5310 2698 6960 2339 1650 6227 4104 5130 7089 6993 2400\n",
            " 1144 1289 4305 5585 2524 7933 2587 6284 7057 5324 3807 5790 7373 8347\n",
            " 2390 4815 5280 2841 7220 1384 6246 4205 4408 7693 8090 2671 5077 4171\n",
            " 5260  133 7451 6248 3957 6793 1719 7379 4743 4746 5435 6216 4858 4417\n",
            " 8355 4492 2648 5104 5278 6539  743 7308 5704  158 1185 7543 2168 5913\n",
            " 7111  864 3414 5788 7592   39 7157 5647 4555 1671 3956 1045 3617 4630\n",
            " 4384 3033 6596   96 4539 6749 7202 4710 2886 2413 4749 5122 8415 5052\n",
            " 1598 5929  252 5565  336 6008 4564 5580 7237 1833 2803 1466 1568 5958\n",
            " 7657 4327 5091 6918 6487 4082 2989 7448 4715   81 3382  467 2689 3784\n",
            " 4624 2317  764 4419 1117 2962 2571 3283 7954 7122 1430 6392  505 3174\n",
            " 1534  323 4164 3659 7526  498 5944 3510 4572 2307 2929 7480 4412  731\n",
            " 4563 7692 1869 2472 6966 4238 4702 6549 1594 2276 7269 2545 2216  605\n",
            " 6587 6315 5297 5282 4816 1607 3200 1619 5504 2536 6399 3459 3917 1212\n",
            " 6342 5754 5721 7385 7909 6126  555 7382 8382 4383 1777 5502 7130  485\n",
            " 5862 4936 7674 3575 5372 3893 7109 4605 5211 1895 5462 8065   50 4080\n",
            " 3334 8167 8378 5595 1873 8207 7076 4106 3719 4627 2090  395 2685  231\n",
            " 2405 5419 6973 3480 3246 5414 3593 5931 6252 8208 4825 6203 3019 1105\n",
            " 5822 6725   63 5854 6543 5787 4764 6701  334 6562 3822 1627 4861 3051\n",
            " 1315 4805 1149  246 1484 2629 8318 3862 1089 8169 2778 2036 8218 2756\n",
            " 1299 7549 2639  707 1284 1076 2465 3815  540 4636 3188  697   80 1455\n",
            "  381 7025 7127 3992   52 4026 2777 2038 4961  383 7531  744 1453 6640\n",
            " 1047 6684 1502 4297 7939  150 7384  446 7738 3287 6675  170 7752 5279\n",
            " 8196 3020 1203 4192 1552 2085 4253 2887 2556 5729 4127 1898 8362 3208\n",
            " 5973 7033 1431 5915 8312  881 5799 7105  511 3116 6404 2852 1561 4150\n",
            " 4835 1178]\n",
            "[6622   35 6713 4989 8233 7444 5061 3942 8344  454 5464  317 2883 4515\n",
            " 3001 3410 6991 2875 1125 3885 6647 4857 4504  214 3230 8340 2969 3847\n",
            " 4609 5092 1346 5993 7358 2752 7651 7301 8188 7073 4782 6069 2346 6445\n",
            " 6483  542 1663 4952 5072 1967 4071 7590  860 6418 5030 7297  626 5463\n",
            " 6029 6302 3819 7256 2950 5667 1235 3993 1439 1323 4263 3835 2134 2624\n",
            " 5801 4293 8129  320 5866 3247 5777   19 6059 6520 4837 3843 5695 1666\n",
            "  858 1418 5409 5274 6205 3167 2356 8418 5997 6685  223 3444 3310 7721\n",
            " 5928 2360 7197 4252 8343  909 4093 4354 4752 1421  649  829 1916 1003\n",
            " 3488 2496 5765 6612 4004 4052 1403 6697 5268 1010 6293 6380 4602 6057\n",
            " 1851 4577 4236 7978 1350 5083 6552 1058  361 7706 4186 2590 4452 4396\n",
            "  749  870 8351 3834  812 5108  463 1743 6600 8264 8064 7121 8279 5556\n",
            " 1845 5119 2516 4673 7947 8215 1882 3350 7617 7152 5199 4282 1092 3342\n",
            " 8003 5025 5051  535  933 3969 1927 3615 1221 8062 1489 3296 7442 5459\n",
            " 2031 7351  234 2313 8015 7416 7921 6574 6257 8396  941 4520 3300 5640\n",
            "  240 7810 7425 1321 6580  928 1170 5404 8273 6742 7005   94 7459    9\n",
            " 7861 7975 1018 5029 2322 2140 1415 7061 7767 4990 8124 5927 6051 3269\n",
            " 6916 5959 2846 6818 7052 7775 7582 8116 4240 4767 5248  430 1812 2175\n",
            " 6055 8331 3916  998 7271 1564 7326 1077 1559  292 3065 6764 6854 4915\n",
            " 5552 3583 1917  868 5902 4229 2144 7565 3712 7224 4502 1406 1651 6256\n",
            " 3827 6525 6318 2686 3616 5369 8235 1704 1011 3351 8089 8021  102  224\n",
            " 5671 8294 1510 7707 2985 6624 5146 1428 7381 3709 6824 6308  701 2281\n",
            " 6837 5436 1741 3520 5995 2649 4886  966 2863 5476 4058 1246 2406 1199\n",
            " 8387  513 5449 1028 2226 6166 4387  350 6123  678 5236 5041  734 3412\n",
            " 6085  587  489 5760 3163 3376 4982 5115 4655 5575 3839 6743 7173 5724\n",
            " 7689 7787  520  713 3214 8302 3150 5713 2247 8128 4607 6660 3505 4610\n",
            " 2866 4161 6905 7096  768 1062 4411 6087 2096   21 4903 7546 6630 2928\n",
            " 4640 7125 1792 2063 1700 2305 7928 8095  737 5849  922 3423 2359 6015\n",
            " 4489 4361 6081 6346 5014 5167 1023 8009 3335  268 4777 5221 4323 6168\n",
            "  759  711 5702 6129 7181 1860 7192 1347 1692 5467 3378 6579   67 2693\n",
            " 6134 4841 4270 3073 8397 4611 2943 6746 7696 3466 3950 4693 6125 8370\n",
            " 6142 6131  995 5548 2186 2112 1721 5067 4015 5587 7092 3156   25 1749\n",
            " 5554 4878 1197 8248  458 8035 6836 1006 5310 2698 6960 2339 1650 6227\n",
            " 4104 5130 7089 6993 2400 1289 4305 5585 2524 7933 6284 7057 5324 3807\n",
            " 5790 8347 2390 5280 2841 7220 1384 7693 8090 2671 4171 5260  133 7451\n",
            " 3957 6793 1719 7379 4743 4746 5435 6216 4858 4417 8355 4492 2648 5104\n",
            " 5278 6539  743 7308  158 7543 2168 5913 7111  864 3414 5788 7592   39\n",
            " 7157 5647 4555 3956 1045 3617 4630 4384 6596   96 4539 6749 7202 4710\n",
            " 2413 4749 5122 5052 1598 5929  252 5565  336 6008 4564 5580 7237 1833\n",
            " 1466 1568 5958 7657 4327 5091 6918 6487 4082 2989 7448 4715   81 3382\n",
            "  467 3784 4624  764 4419 1117 2962 7954 7122 1430 6392  505 3174 1534\n",
            "  323 4164 3659 7526  498 5944 3510 4572 2307 2929 7480 4412  731 4563\n",
            " 7692 1869 2472 6966 4238 4702 6549 1594 2276 7269 2545 2216  605 6587\n",
            " 6315 5297 5282 4816 3200 1619 5504 2536 6399 3459 3917 6342 5754 5721\n",
            " 7385 7909 6126 7382 8382 4383 5502 7130  485 5862 4936 7674 3575 5372\n",
            " 3893 7109 4605 5211 5462 8065   50 4080 3334 8167 8378 5595 1873 8207\n",
            " 7076 4106 3719 4627  395 2685  231 2405 5419 6973 3480 3246 5414 3593\n",
            " 5931 6252 8208 4825 6203 3019 1105 5822   63 5854 6543 5787 4764 6701\n",
            "  334 6562 3822 4861 3051 1315 4805  246 8318 1089 8169 8218 2756 7549\n",
            "  707 1284 1076 2465 3815  540 4636 3188  697   80 1455  381 7025 7127\n",
            " 3992 4026 2777 4961  383 7531  744 6640 1047 6684 4297 7939  150 7384\n",
            "  446 7738 3287 6675 7752 5279 8196 3020 1203 4192 1552 4253 2887 2556\n",
            " 5729 1898 3208 5973 7033 1431 5915 8312 5799 7105  511 6404 2852 1561\n",
            " 4150 4835 1178]\n",
            "[6622   35 6713 4989 8233 7444 5061 3942 8344  454 5464  317 4515 3001\n",
            " 6991 2875 1125 3885 6647 4857 4504  214 8340 2969 3847 4609 5092 1346\n",
            " 5993 7358 2752 7651 7301 8188 7073 4782 6069 2346 6445 6483 1663 4952\n",
            " 5072 1967 4071 7590  860 6418 5030 7297  626 5463 6029 6302 3819 7256\n",
            " 5667 1235 3993 1439 1323 4263 3835 2624 5801 4293 8129  320 5866 3247\n",
            " 5777   19 6059 4837 3843 1666  858 1418 5409 5274 6205 3167 2356 5997\n",
            " 6685  223 3444 3310 7721 5928 2360 7197 4252 8343  909 4354 4752 1421\n",
            "  649  829 1916 3488 2496 5765 6612 4004 4052 1403 6697 5268 1010 6293\n",
            " 6380 4602 6057 1851 4577 4236 7978 5083 6552 1058  361 7706 4186 4452\n",
            " 4396  749 8351 3834  812 5108  463 1743 6600 8264 8064 7121 8279 5556\n",
            " 5119 4673 7947 1882 3350 7617 7152 5199 4282 1092 8003 5025 5051  933\n",
            " 3969 1927 3615 1221 8062 1489 3296 7442 5459 2031 7351  234 2313 8015\n",
            " 7416 7921 6574 6257 8396  941 4520 3300 5640  240 7810 7425 1321 6580\n",
            "  928 1170 5404 8273 6742 7005   94 7459    9 7861 7975 1018 5029 2140\n",
            " 1415 7061 7767 4990 8124 5927 6051 3269 6916 5959 2846 6818 7052 7775\n",
            " 7582 8116 4240 4767 5248  430 1812 2175 6055 8331 3916  998 1564 7326\n",
            " 1559  292 3065 6764 6854 4915 5552 3583 1917 5902 4229 2144 7565 3712\n",
            " 7224 4502 1406 1651 6256 3827 6525 6318 2686 3616 5369 8235 1704 1011\n",
            " 3351 8089 8021  102  224 5671 8294 7707 2985 6624 5146 1428 7381 3709\n",
            " 6824 6308  701 2281 6837 5436 1741 3520 5995 2649 4886  966 5476 2406\n",
            " 1199 8387  513 5449 1028 2226 6166 4387  350 6123  678 5236 5041  734\n",
            " 3412 6085  587  489 5760 3376 4982 5115 4655 5575 3839 6743 7173 5724\n",
            " 7689 7787  520 8302 3150 5713 2247 8128 4607 6660 3505 4610 2866 4161\n",
            " 6905 7096  768 1062 4411 6087 2096 4903 7546 6630 2928 4640 7125 1792\n",
            " 2063 1700 2305 7928 8095  737 5849  922 2359 6015 4489 4361 6081 6346\n",
            " 5014 5167 1023 8009 3335  268 4777 5221 4323 6168  759  711 5702 6129\n",
            " 7181 1860 7192 1347 5467 3378 6579   67 2693 6134 4841 4270 4611 2943\n",
            " 6746 7696 3466 3950 4693 6125 8370 6142 6131  995 5548 2112 1721 5067\n",
            " 4015 7092 3156   25 1749 5554 4878 8248  458 6836 1006 5310 2698 6960\n",
            " 2339 1650 6227 4104 5130 7089 6993 2400 1289 4305 5585 7933 6284 7057\n",
            " 3807 5790 8347 2390 5280 2841 7220 1384 7693 8090 4171 5260  133 7451\n",
            " 3957 6793 1719 7379 4743 4746 5435 6216 4858 4417 8355 4492 2648 5104\n",
            " 5278 6539  743 7308  158 7543 2168 5913 7111 3414 5788 7592   39 7157\n",
            " 5647 4555 3956 1045 3617 4630 4384 6596   96 4539 6749 7202 2413 4749\n",
            " 5122 5052 1598 5929  252 5565  336 6008 4564 5580 7237 1466 1568 5958\n",
            " 7657 4327 5091 6918 6487 4082 2989 7448 4715 3382  467 3784 4624  764\n",
            " 4419 1117 7954 7122 6392  505 3174 1534  323 4164 3659 7526  498 5944\n",
            " 4572 2307 2929 7480 4412  731 4563 7692 1869 2472 6966 4238 6549 1594\n",
            " 2276 7269 2545 2216  605 6587 6315 5297 5282 4816 1619 5504 2536 6399\n",
            " 3917 6342 5754 5721 7385 7909 6126 7382 8382 4383 5502 7130  485 5862\n",
            " 4936 7674 3575 5372 3893 7109 4605 5211 5462 8065   50 4080 3334 8167\n",
            " 8378 5595 1873 8207 7076 4106 3719 4627  395  231 2405 5419 6973 3480\n",
            " 3246 5414 5931 8208 4825 6203 3019 5822   63 5854 6543 5787 4764 6701\n",
            " 6562 3822 4861 3051 1315 4805 8318 1089 8169 8218 7549  707 1284 1076\n",
            " 2465 3815 4636 3188  697   80 1455  381 7025 7127 3992 4026 2777 4961\n",
            " 7531  744 6640 1047 6684 4297 7939 7384  446 3287 6675 7752 5279 8196\n",
            " 3020 1203 4192 1552 4253 2887 2556 5729 1898 3208 5973 7033 1431 5915\n",
            " 8312 5799 7105  511 6404 2852 1561 4150 4835 1178]\n",
            "[6622   35 6713 4989 8233 7444 5061 3942 8344 5464  317 4515 3001 6991\n",
            " 2875 1125 3885 6647 4857 4504  214 8340 2969 3847 4609 1346 5993 7358\n",
            " 2752 7651 8188 7073 4782 6069 2346 6483 1663 4952 5072 1967 4071 7590\n",
            "  860 6418 5030 7297  626 5463 6029 6302 7256 5667 1235 3993 1323 4263\n",
            " 3835 2624 5801 4293 8129 5866 5777 6059 4837 3843 1418 5409 5274 6205\n",
            " 2356 5997 6685 3444 3310 7721 5928 2360 7197 4252 8343  909 4354 4752\n",
            "  649 1916 3488 2496 5765 6612 4004 4052 1403 6697 5268 6293 6380 4602\n",
            " 6057 1851 4577 4236 7978 5083 6552  361 7706 4186 4452 4396 8351 3834\n",
            "  812 5108  463 6600 8264 8064 7121 8279 5556 5119 7947 3350 7152 5199\n",
            " 4282 1092 5025 5051  933 1927 3615 1221 8062 1489 7442 5459 2031 7351\n",
            "  234 8015 7416 7921 6574 6257 8396  941 4520 3300 5640  240 7810 7425\n",
            " 1321 6580  928 1170 5404 8273 6742   94    9 7861 7975 1018 2140 1415\n",
            " 7061 7767 4990 8124 5927 6051 3269 6916 5959 2846 6818 7052 7775 7582\n",
            " 8116 4240 4767 5248 1812 2175 6055 8331  998 1564 7326 1559  292 3065\n",
            " 6764 6854 4915 5552 3583 5902 4229 2144 7565 3712 7224 4502 1406 1651\n",
            " 6256 3827 6525 6318 2686 3616 5369 8235 1704 1011 3351 8089 8021  102\n",
            "  224 5671 8294 7707 2985 6624 5146 7381 3709 6824 6308 2281 6837 5436\n",
            " 1741 3520 2649 4886  966 5476 1199 8387 5449 1028 2226 6166 4387  350\n",
            " 6123 5236 5041  734 3412 6085  587  489 5760 4982 5115 5575 3839 6743\n",
            " 7173 5724 7689 7787 8302 3150 2247 8128 6660 3505 4610 2866 4161 6905\n",
            " 7096  768 1062 4411 6087 2096 4903 7546 6630 2928 4640 7125 1792 2063\n",
            " 1700 7928 8095  737 5849  922 2359 6015 4489 4361 6081 6346 5014 5167\n",
            " 1023 8009 3335  268 4777 5221 4323 6168  711 5702 6129 7181 1860 7192\n",
            " 1347 5467 6579   67 2693 6134 4841 4270 4611 2943 6746 7696 3466 3950\n",
            " 6125 8370 6142 6131  995 5548 1721 5067 4015 7092 3156   25 1749 5554\n",
            " 4878 8248 6836 5310 2698 6960 2339 1650 6227 4104 5130 7089 6993 2400\n",
            " 1289 4305 5585 7933 6284 7057 3807 5790 8347 5280 2841 7220 1384 7693\n",
            " 8090 4171 5260  133 7451 3957 1719 7379 4743 4746 5435 6216 4858 4417\n",
            " 8355 4492 2648 5104 5278  743 7308  158 7543 2168 5913 7111 3414 7592\n",
            "   39 7157 5647 4555 3956 3617 4630 4384 6596   96 4539 6749 7202 2413\n",
            " 4749 5122 5052 5929  252 5565  336 6008 4564 5580 7237 1466 1568 5958\n",
            " 7657 4327 5091 6918 6487 4082 2989 7448 4715 3382  467 3784 4624 4419\n",
            " 1117 7954 7122 6392 3174 1534 4164 3659 7526  498 5944 4572 2307 2929\n",
            " 7480 4412  731 4563 7692 1869 2472 6966 4238 6549 1594 2276 7269 2545\n",
            " 2216  605 6587 6315 5297 5282 1619 5504 6399 3917 6342 5754 5721 7385\n",
            " 7909 6126 7382 8382 4383 5502 7130  485 4936 7674 5372 3893 7109 4605\n",
            " 5211 5462 8065   50 4080 3334 8167 8378 5595 1873 8207 7076 4106 3719\n",
            " 4627  395  231 2405 6973 3480 3246 5414 5931 8208 4825 6203 3019 5822\n",
            "   63 5854 6543 5787 4764 6701 6562 3822 4861 3051 1315 8318 1089 8169\n",
            " 8218 7549  707 1284 1076 2465 3815 4636   80 7025 7127 4026 2777 4961\n",
            " 7531  744 6640 6684 4297 7939 7384  446 3287 6675 7752 5279 8196 3020\n",
            " 1203 4192 4253 2887 2556 5729 1898 3208 5973 7033 1431 5915 8312 5799\n",
            " 7105  511 6404 2852 1561 4150 1178]\n",
            "[6622   35 6713 4989 8233 7444 5061 8344 5464  317 4515 3001 6991 2875\n",
            " 3885 6647 4857 4504  214 8340 2969 3847 4609 1346 5993 7358 2752 7651\n",
            " 8188 7073 6069 2346 6483 4952 5072 1967 4071 7590 5030 7297  626 5463\n",
            " 6029 6302 7256 3993 4263 3835 5801 4293 8129 5866 5777 4837 3843 1418\n",
            " 5274 6205 2356 5997 6685 3310 7721 5928 2360 7197 4252 8343  909 4354\n",
            " 4752  649 3488 2496 5765 6612 4004 4052 1403 6697 5268 6293 6380 4602\n",
            " 6057 4577 4236 7978 5083 6552  361 7706 4186 8351  812 5108 6600 8264\n",
            " 8064 7121 8279 5556 5119 7947 3350 7152 5199 4282 5025 5051 1927 3615\n",
            " 1221 8062 1489 2031 7351  234 8015 7416 7921 6574 6257 8396  941 4520\n",
            " 3300 5640  240 7810 7425 1321 6580 5404 8273 6742   94    9 7861 7975\n",
            " 1018 2140 1415 7061 7767 4990 8124 5927 6051 3269 6916 2846 6818 7052\n",
            " 7775 7582 8116 4767 5248 1812 6055 8331  998 1564 7326  292 3065 6764\n",
            " 6854 4915 3583 5902 4229 2144 7565 3712 7224 4502 1406 1651 3827 6525\n",
            " 6318 2686 5369 8235 1704 1011 3351 8021  102  224 8294 7707 2985 5146\n",
            " 3709 6824 2281 6837 5436 2649 4886  966 5476 1199 8387 5449 1028 2226\n",
            " 6166 4387  350 6123 5236 5041  734 3412 6085  587  489 4982 5115 5575\n",
            " 3839 6743 7173 5724 7689 7787 8302 3150 2247 8128 6660 3505 4610 2866\n",
            " 4161 6905 7096  768 1062 4411 6087 2096 4903 7546 6630 2928 4640 7125\n",
            " 1792 2063 1700 7928 8095  737 5849  922 2359 6015 4489 4361 6081 6346\n",
            " 5014 5167 1023 8009  268 4777 5221 4323 6168 5702 6129 7181 1860 7192\n",
            " 5467 6579   67 2693 6134 4270 4611 6746 7696 3466 3950 6125 8370 6142\n",
            " 6131  995 5548 1721 5067 4015 7092 3156   25 1749 4878 8248 6836 5310\n",
            " 6960 2339 1650 6227 4104 5130 7089 6993 1289 4305 5585 7933 6284 7057\n",
            " 3807 5790 8347 5280 2841 7693 8090 4171 5260  133 7451 3957 1719 4743\n",
            " 4746 5435 6216 4858 4417 8355 4492 2648 5104 5278 7308  158 7543 2168\n",
            " 5913 7111 3414 7592   39 7157 5647 4555 3956 3617 4630 4384 6596 4539\n",
            " 6749 7202 2413 4749 5122 5929  252 5565  336 6008 4564 5580 7237 5958\n",
            " 7657 4327 5091 6918 6487 2989 7448 4715 3382  467 3784 4419 1117 7954\n",
            " 7122 3174 1534 4164 7526  498 5944 4572 2307 2929 7480 4412 4563 7692\n",
            " 1869 2472 6966 6549 1594 2276 7269 2545 2216  605 6587 6315 5297 5282\n",
            " 1619 5504 6399 3917 6342 5721 7909 6126 8382 4383 5502 7130  485 4936\n",
            " 7674 5372 7109 4605 5211 5462 8065   50 4080 3334 8167 8378 5595 1873\n",
            " 8207 7076 4106 3719 4627  231 2405 6973 3480 3246 5931 8208 4825 6203\n",
            " 3019 5822   63 5854 6543 5787 4764 6701 6562 3822 4861 1315 8318 8169\n",
            " 8218 7549  707 1076 2465 4636   80 7025 7127 4026 4961 7531  744 6684\n",
            " 4297 7939 7384  446 6675 7752 5279 8196 3020 4192 4253 2887 2556 5729\n",
            " 1898 3208 5973 7033 1431 5915 8312 5799 7105  511 6404 2852 1561]\n",
            "[6622   35 6713 4989 8233 7444 5061 8344 5464  317 4515 3001 6991 2875\n",
            " 3885 6647 4857 4504  214 8340 2969 3847 4609 7358 2752 7651 8188 7073\n",
            " 6483 4952 5072 1967 4071 7590 5030 7297  626 6029 6302 7256 3993 4263\n",
            " 3835 4293 8129 4837 3843 1418 5274 6205 5997 6685 3310 7721 2360 7197\n",
            " 4252 8343  909 4354  649 3488 2496 5765 4004 4052 1403 6697 6293 6380\n",
            " 4602 6057 4236 7978 5083 6552  361 7706 4186 8351  812 6600 8264 8064\n",
            " 7121 8279 5556 5119 7947 3350 7152 4282 5025 5051 3615 8062 1489 7351\n",
            "  234 7416 7921 6574 6257 8396 4520 3300 5640  240 7810 1321 6580 5404\n",
            " 8273 6742 7861 7975 1018 2140 7061 7767 4990 8124 5927 6051 6916 2846\n",
            " 6818 7052 7775 7582 8116 4767 5248 6055 8331 1564 7326 3065 6764 6854\n",
            " 4915 3583 5902 4229 7565 3712 7224 4502 3827 6525 6318 2686 5369 8235\n",
            " 3351 8021  102  224 8294 7707 2985 5146 3709 6824 2281 6837 5436 2649\n",
            " 4886  966 5476 8387 6166 4387  350 6123 5236 5041  734 3412 6085  489\n",
            " 4982 5115 6743 7173 5724 7689 7787 8302 3150 2247 8128 6660 4610 2866\n",
            " 6905 7096  768 4411 6087 2096 4903 7546 6630 2928 4640 7125 1792 1700\n",
            " 7928 8095  737 5849 2359 6015 4361 6081 5014 5167 1023 8009  268 4777\n",
            " 5702 6129 7181 1860 7192 5467   67 6134 4270 4611 6746 7696 3466 3950\n",
            " 6125 8370 6142 6131 5548 1721 5067 4015 7092   25 1749 4878 8248 6836\n",
            " 5310 6960 2339 1650 6227 4104 5130 7089 6993 1289 4305 5585 7933 6284\n",
            " 7057 3807 8347 5280 2841 7693 8090 4171 5260  133 7451 3957 1719 5435\n",
            " 6216 4858 4417 8355 4492 2648 5278 7308  158 7543 2168 5913 7111 3414\n",
            " 7592   39 7157 5647 4555 3956 3617 4630 4384 6596 4539 6749 7202 2413\n",
            " 4749 5122 5929  252 5565  336 6008 4564 5580 7237 5958 6918 6487 2989\n",
            " 7448 3382  467 3784 4419 1117 7954 7122 3174 1534 4164 7526  498 5944\n",
            " 4572 2307 2929 7480 4412 4563 7692 1869 2472 6966 6549 1594 2276 7269\n",
            " 2545 2216 6587 6315 5297 5282 6399 3917 6342 5721 7909 8382 4383 7130\n",
            "  485 4936 7674 5372 7109 4605 5211 5462 8065   50 4080 3334 8167 8378\n",
            " 5595 1873 8207 7076 3719 4627  231 2405 6973 3246 5931 8208 4825 6203\n",
            " 3019 5822   63 5854 5787 4764 6701 3822 8318 8169 8218 7549  707 1076\n",
            " 2465 4636 7025 7127 4026 4961 7531  744 6684 4297 7939 7384  446 6675\n",
            " 7752 5279 8196 3020 4192 4253 2887 1898 3208 5973 7033 1431 5915 8312\n",
            " 5799 7105  511 6404 1561]\n",
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_5 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_6 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_5[0][0]',                \n",
            "                                thPoolingAndCrossAt               'input_6[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_2 (Sl  (None, 768)         0           ['tf_bert_model[2][0]']          \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_2[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_39 (Dropout)           (None, 512)          0           ['dense_4[0][0]']                \n",
            "                                                                                                  \n",
            " dense_5 (Dense)                (None, 15)           7695        ['dropout_39[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_7 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_8 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_7[0][0]',                \n",
            "                                thPoolingAndCrossAt               'input_8[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_3 (Sl  (None, 768)         0           ['tf_bert_model[3][0]']          \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_6 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_3[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_40 (Dropout)           (None, 512)          0           ['dense_6[0][0]']                \n",
            "                                                                                                  \n",
            " dense_7 (Dense)                (None, 15)           7695        ['dropout_40[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_9 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_10 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_9[0][0]',                \n",
            "                                thPoolingAndCrossAt               'input_10[0][0]']               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_4 (Sl  (None, 768)         0           ['tf_bert_model[4][0]']          \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_8 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_4[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_41 (Dropout)           (None, 512)          0           ['dense_8[0][0]']                \n",
            "                                                                                                  \n",
            " dense_9 (Dense)                (None, 15)           7695        ['dropout_41[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model: \"model_5\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_11 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_12 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_11[0][0]',               \n",
            "                                thPoolingAndCrossAt               'input_12[0][0]']               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_5 (Sl  (None, 768)         0           ['tf_bert_model[5][0]']          \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_10 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_5[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_42 (Dropout)           (None, 512)          0           ['dense_10[0][0]']               \n",
            "                                                                                                  \n",
            " dense_11 (Dense)               (None, 15)           7695        ['dropout_42[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model: \"model_6\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_13 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_14 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_13[0][0]',               \n",
            "                                thPoolingAndCrossAt               'input_14[0][0]']               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_6 (Sl  (None, 768)         0           ['tf_bert_model[6][0]']          \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_12 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_6[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_43 (Dropout)           (None, 512)          0           ['dense_12[0][0]']               \n",
            "                                                                                                  \n",
            " dense_13 (Dense)               (None, 15)           7695        ['dropout_43[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model: \"model_7\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_15 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_16 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_15[0][0]',               \n",
            "                                thPoolingAndCrossAt               'input_16[0][0]']               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_7 (Sl  (None, 768)         0           ['tf_bert_model[7][0]']          \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_14 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_7[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_44 (Dropout)           (None, 512)          0           ['dense_14[0][0]']               \n",
            "                                                                                                  \n",
            " dense_15 (Dense)               (None, 15)           7695        ['dropout_44[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "[1, 9, 3, 1, 2, 8, 3, 6, 2, 1, 8, 4, 1, 4, 10, 7, 2, 9, 1, 2, 8, 9, 9, 4, 2, 10, 2, 8, 1, 8, 8, 1, 8, 1, 3, 1, 9, 2, 2, 3, 9, 1, 10, 1, 2, 8, 1, 3, 4, 8, 3, 12, 1, 7, 2, 8, 9, 1, 9, 9, 3, 3, 6, 2, 1, 1, 10, 1, 10, 1, 3, 8, 7, 1, 2, 2, 9, 10, 1, 3, 1, 9, 10, 1, 7, 9, 1, 12, 8, 1, 2, 8, 1, 2, 1, 1, 2, 3, 8, 2, 7, 3, 11, 2, 1, 9, 7, 4, 8, 9, 9, 2, 1, 1, 10, 9, 3, 3, 9, 1, 8, 8, 7, 1, 2, 9, 3, 2, 8, 2, 9, 1, 8, 1, 9, 8, 1, 1, 1, 2, 1, 1, 10, 9, 1, 9, 8, 3, 8, 9, 2, 1, 3, 8, 4, 8, 9, 10, 8, 10, 4, 8, 4, 4, 12, 1, 8, 9, 4, 7, 3, 9, 10, 2, 8, 9, 3, 9, 12, 7, 3, 9, 9, 3, 9, 8, 9, 9, 12, 1, 1, 10, 10, 1, 2, 9, 11, 12, 1, 9, 3, 9, 1, 3, 10, 8, 8, 9, 2, 8, 8, 2, 2, 9, 1, 1, 2, 8, 2, 2, 3, 9, 8, 2, 1, 7, 3, 2, 4, 2, 2, 1, 9, 1, 9, 2, 9, 1, 8, 3, 1, 8, 8, 12, 1, 1, 7, 8, 1, 2, 9, 2, 12, 1, 9, 3, 1, 4, 9, 2, 8, 3, 1, 2, 8, 8, 2, 10, 8, 8, 2, 2, 8, 2, 3, 1, 8, 2, 2, 3, 7, 12, 2, 12, 1, 1, 9, 12, 1, 3, 2, 7, 3, 1, 8, 2, 1, 5, 1, 3, 2, 1, 2, 10, 8, 8, 2, 2, 8, 12, 10, 1, 10, 5, 1, 1, 1, 8, 9, 3, 9, 9, 2, 1, 1, 7, 1, 10, 9, 1, 9, 5, 1, 1, 1, 3, 1, 8, 2, 8, 1, 8, 7, 3, 1, 9, 8, 1, 2, 8, 1, 3, 3, 8, 1, 2, 1, 1, 9, 8, 1, 12, 3, 1, 1, 8, 4, 3, 3, 2, 9, 1, 2, 2, 2, 3, 6, 2, 1, 1, 10, 12, 8, 4, 1, 9, 10, 9, 12, 1, 9, 8, 8, 7, 9, 8, 8, 8, 8, 1, 8, 2, 1, 3, 6, 2, 3, 2, 2, 2, 8, 3, 1, 9, 1, 9, 1, 7, 9, 1, 1, 2, 10, 12, 9, 1, 2, 8, 10, 1, 2, 3, 3, 3, 8, 10, 10, 1, 12, 3, 1, 2, 4, 8, 9, 1, 2, 9, 2, 8, 3, 8, 8, 0, 8, 1, 8, 1, 1, 6, 8, 8, 7, 1, 10, 1, 2, 9, 3, 6, 12, 1, 3, 4, 2, 3, 1, 8, 8, 1, 9, 9, 9, 9, 1, 9, 9, 8, 8, 9, 1, 3, 1, 9, 2, 1, 3, 1, 2, 9, 10, 8, 1, 1, 1, 9, 4, 1, 2, 9, 3, 6, 12, 10, 1, 1, 2, 8, 2, 3, 2, 5, 3, 1, 3, 9, 1, 1, 8, 1, 1, 8, 8, 2, 8, 8, 1, 8, 1, 8, 10, 6, 1, 9, 1, 1, 8, 9, 3, 10, 4, 8, 2, 1, 8, 9, 8, 2, 5, 4, 7, 2, 8, 9, 8, 12, 9, 10, 9, 1, 2, 1, 3, 8, 1, 10, 1, 1, 8, 9, 9, 8, 1, 1, 2, 2, 5, 8, 12, 2, 4, 7, 9, 8, 1, 10, 9, 9, 1, 8, 11, 1, 1, 1, 3, 1, 1, 1, 8, 2, 3, 9, 10, 10, 1, 8, 10, 9, 9, 2, 8, 9, 8, 3, 2, 2, 8, 8, 2, 1, 1, 9, 3, 10, 8, 3, 8, 2, 1, 1, 3, 1, 8, 2, 9, 1, 1, 9, 1, 9, 3, 3, 8, 9, 2, 3, 10, 2, 8, 3, 12, 1, 1, 12, 12, 1, 7, 8, 11, 1, 3, 9, 3, 1, 8, 7, 4, 8, 2, 8, 2, 8, 1, 2, 8, 8, 6, 10, 2, 9, 1, 8, 9, 9, 5, 8, 1, 1, 2, 8, 7, 9, 1, 2, 1, 8, 3, 1, 1, 2, 8, 4, 3, 8, 1, 1, 1, 9, 8, 2, 8, 8, 9, 8, 1, 7, 1, 2, 1, 8, 9, 10, 8, 10, 9, 9, 1, 9, 2, 1, 1, 9, 1, 2, 8, 10, 10, 8, 8, 2, 9, 1, 8, 2, 9, 7, 8, 1, 8, 1, 1, 3, 1, 1, 4, 1, 8, 9, 9, 1, 2, 2, 3, 2, 1, 3, 3, 1, 4, 7, 2, 8, 8, 1, 12, 8, 2, 1, 7, 1, 8, 2, 2, 12, 3, 3, 8, 9, 1, 8, 8, 1, 1, 2, 10, 1, 9, 1, 10, 9, 9, 9, 1, 1, 3, 2, 9, 10, 2, 9, 1, 1, 9, 8, 8, 1, 10, 9, 8, 1, 2, 12, 8, 9, 8, 9, 7, 12, 12, 8, 2, 1]\n",
            "[1, 9, -1, 1, 2, 8, 3, 6, 2, 1, 8, 4, 8, 4, 8, -1, 2, 4, 1, 2, 8, -1, -1, 4, 2, 8, 2, 8, 1, 8, 8, 1, 8, 1, -1, 1, 1, 2, 2, 3, 2, 1, 10, 1, 2, 8, 1, 3, 4, 8, 3, 12, 1, 7, 2, 9, 9, -1, 9, 4, -1, 2, 6, 2, 1, 1, 2, 1, 8, 1, 3, 8, 7, 1, 2, 2, -1, 10, 1, 3, 1, 11, 10, 1, 7, -1, -1, 12, 8, 1, 2, 8, 1, 2, 1, 1, 2, 10, 8, -1, 7, 3, 11, 2, 1, 9, 10, 4, 8, 9, -1, 4, 1, 1, 10, 9, 3, 3, 9, 1, 8, 8, 7, 1, 2, 9, 3, 2, 8, 2, 9, 1, 9, 3, 9, -1, 1, 1, 1, -1, 1, 1, 9, 9, 1, -1, 8, 3, 8, 9, 2, 1, 3, 7, 4, -1, -1, 8, 8, 2, 4, 8, 4, 4, -1, 1, 8, 9, 4, 7, 2, 5, 10, 10, 8, 9, -1, 9, 12, 7, 3, 1, 9, 3, -1, -1, 9, -1, 12, 1, 1, 8, 10, 1, -1, 9, 10, 12, 1, 9, 3, -1, 1, 3, 10, 2, 8, 9, -1, 8, 8, 2, 2, 9, 1, 1, -1, 10, 2, 2, 8, 9, 10, 2, 1, 7, 1, 2, 4, 2, -1, 1, -1, -1, 9, 10, 4, 1, 7, 3, 1, 8, 8, 12, 1, 1, 7, 8, 1, -1, 2, 2, 12, -1, 9, 3, 1, -1, -1, 2, 8, 5, 1, 3, 9, 2, 2, 8, 8, 8, 2, 2, 8, 2, 7, 1, 8, 2, 2, 3, 7, 12, 2, 12, 1, 1, 9, 12, 1, 3, -1, 7, -1, 1, 8, 2, 1, 5, 1, 3, 9, 1, 2, 5, -1, 8, 2, 3, 8, 12, 7, 1, 10, 5, 1, 1, 1, 8, -1, 3, -1, 9, 10, 4, 1, 7, 1, 8, 2, 1, 7, 5, 1, 1, 1, 3, 1, 8, 2, 8, 1, 8, 7, 5, 1, 8, 8, 1, 2, 8, 1, 3, 8, 8, 1, 2, 1, 1, 1, -1, -1, 12, 3, 1, -1, 8, 4, 3, 3, 2, -1, 1, 2, 2, 2, 3, 4, -1, 1, 1, 10, -1, 8, 8, 1, 9, 8, 1, 12, 1, -1, 8, -1, 7, 3, -1, 8, 8, 8, 1, 8, 2, 1, 3, 6, 2, -1, 3, 2, 2, 10, 3, 1, -1, 1, -1, 1, 7, 5, 4, 1, 2, 8, 12, -1, 1, 2, 8, 10, 1, 2, -1, -1, 1, 8, 9, 10, 1, 12, 3, 1, -1, 4, 7, 9, 1, -1, 9, 2, 8, 3, 8, 8, -1, -1, 1, 8, 1, -1, 6, 8, 8, 7, 1, 4, 1, 2, 2, 5, 6, -1, 1, 3, 4, 4, -1, -1, 2, 8, 1, -1, -1, 9, 9, 1, -1, 8, 4, 8, 9, 3, -1, 1, 5, 2, 1, 3, 1, 2, 9, 10, 8, 1, 4, 1, 1, 4, 1, 1, 9, 10, 3, 12, 8, 1, 1, 2, 8, -1, 3, 2, 5, 1, 1, -1, 9, 1, 1, 8, 1, -1, 9, 8, -1, 8, 8, 1, 8, -1, -1, -1, 6, 1, 10, -1, 1, 8, 9, 3, -1, 8, 8, 3, 1, 8, 9, 8, 9, 5, 4, 7, 2, 8, 9, 8, 12, 8, 10, -1, 1, -1, 1, 3, 8, 1, 8, 1, 1, 8, 9, 2, 8, 1, -1, 2, 2, 1, 8, 12, -1, 4, 7, 9, 8, 1, 10, -1, 9, 1, 8, -1, 1, 3, 1, 10, 1, 8, 1, 8, 2, 3, 8, -1, 10, 1, 8, 10, 9, 9, 2, 8, 9, 3, 3, 4, 2, 8, 4, -1, 1, 1, -1, 3, 8, 8, 3, -1, -1, 1, 1, 8, 1, 8, 2, 9, 1, 1, 4, 1, 8, 3, 3, 8, 9, 2, 3, 10, 2, 8, 3, 12, 1, 1, 12, 12, 1, 7, 8, 11, 1, 3, 4, 3, 1, 8, 2, 4, -1, 2, 8, 4, 8, 4, 2, 8, -1, 6, 10, 2, 9, 1, 7, -1, 9, 5, 8, -1, 1, 2, 8, 7, 9, 1, 2, 1, 8, 8, 1, 1, -1, 8, 4, 3, 8, 1, 1, 1, 3, 8, 2, 8, 8, 8, 8, -1, 7, 1, 2, 1, 8, 9, 10, 8, 9, 4, 10, 1, 1, 2, 1, 1, 8, 1, -1, 8, 10, 10, 8, 8, 2, 9, 1, 8, -1, 9, 7, 8, 1, -1, 1, -1, -1, 1, -1, 4, 1, -1, -1, 10, 1, -1, 2, -1, 1, 1, 3, 3, 1, 8, 7, 2, 8, 8, 1, 4, 8, 2, 1, -1, 1, 8, -1, 2, 12, 1, 3, -1, 9, 1, 8, -1, 1, 1, 2, 10, 3, 9, 1, 10, -1, 10, 2, 1, 1, 2, 2, 9, -1, 2, 5, 1, 1, -1, 4, -1, 1, 10, 9, 8, 1, 2, -1, 2, 4, 8, -1, 7, 12, 12, 8, 2, 1]\n",
            "[1, 9, -1, 1, 2, 8, 3, 6, 2, 1, 8, 4, 1, -1, 1, -1, 2, -1, 1, 2, 8, -1, -1, 4, 9, 10, 2, 8, -1, 8, 8, 1, 8, 1, -1, 1, 9, 8, 2, 3, 2, 1, 10, 1, 2, 8, 1, 3, -1, 8, 3, 12, 3, 7, 2, 9, 9, -1, 9, 9, -1, 3, 6, 2, 1, 1, 9, -1, 8, 1, 3, 8, 7, 1, 2, -1, -1, 10, 1, 3, 1, 9, 10, 1, 7, -1, -1, 12, 8, -1, 2, 8, -1, 2, 1, 1, 2, 10, 8, -1, 7, 3, -1, 4, 1, 9, 7, 4, 8, 9, -1, 9, 1, 1, 10, 9, -1, 3, 9, 1, 8, 8, 7, -1, 2, 9, 3, 2, 8, 2, 9, 1, 8, 3, 9, -1, 1, 1, 1, -1, 1, 1, 10, 9, -1, -1, 8, 3, 8, 9, 1, 1, -1, 7, 2, -1, -1, 8, -1, 1, 9, 8, 4, 4, -1, 1, 8, 9, 9, 7, 3, 5, -1, 2, -1, 9, -1, 9, -1, 7, 3, 1, 9, 3, -1, -1, 9, -1, 12, -1, 1, 8, 10, -1, -1, 9, 10, 12, 1, 9, 3, -1, 1, 2, 8, 8, 8, 9, -1, 8, 8, 2, 2, 9, 1, 1, -1, 8, 2, 2, 8, 9, 4, 2, 1, 7, 1, 2, 1, 2, -1, 1, -1, -1, 9, 2, 9, 1, 7, 3, 1, 8, 8, -1, 1, 1, 7, 8, 1, -1, 2, 2, 12, -1, 9, 2, 1, -1, -1, 2, 8, 1, 1, 2, 9, 8, 2, 10, 8, 8, 2, 2, 8, 2, 7, -1, 8, 2, -1, 3, 7, 1, 2, 12, 1, 1, 9, 12, -1, 3, -1, 7, -1, 1, 9, 2, 1, 5, 1, 3, 9, 1, 2, 1, -1, 8, 2, 3, 8, 12, 7, 1, 8, 5, 1, 1, 1, 12, -1, -1, -1, 9, 11, 4, 1, 9, 1, 9, 9, 1, 7, 5, 1, 1, 1, 3, 1, 8, 2, 8, -1, 8, -1, -1, 1, 8, 8, 1, 2, 8, 1, 3, 8, 8, 1, 2, 1, 1, 9, -1, -1, 12, 3, 1, -1, 8, 1, -1, 1, 2, -1, 1, 2, 2, 2, 3, 9, -1, 9, 1, 10, -1, 8, -1, -1, 9, 10, 9, 12, 1, -1, 8, -1, 7, 3, -1, 8, 8, 8, 1, 8, 2, 1, 3, 6, 2, -1, -1, 2, 2, 8, 3, 1, -1, 1, -1, 1, 7, 4, 1, 1, 1, 10, 12, -1, 1, -1, 8, 8, 1, 2, -1, -1, 1, 8, 9, 10, 1, 12, 3, 1, -1, 8, 7, 9, 1, -1, 8, 1, 7, 8, 4, 8, -1, -1, 1, 8, -1, -1, 2, 8, 8, 7, 1, 4, 1, 2, -1, -1, 6, -1, 1, 3, 3, 1, -1, -1, 8, 8, 1, -1, -1, 9, 9, 1, -1, 8, 9, -1, 9, 1, -1, 1, 5, -1, 1, 1, 1, 2, 9, 7, -1, 1, 9, -1, 1, 4, 1, 1, 9, 3, 3, 12, 10, 1, 1, 2, 8, -1, 3, 2, 5, -1, 1, -1, 9, 1, -1, 7, 1, -1, 9, 8, -1, 8, 8, 1, 8, -1, -1, -1, 6, 1, -1, -1, 1, 8, 9, 10, -1, 8, 8, 2, 1, 8, 9, 8, 9, 5, 4, 7, 2, 9, 9, 8, 12, 8, 7, -1, 1, -1, 1, 3, 8, 1, -1, 1, 1, 8, 9, 9, 8, 1, -1, 2, 2, 1, 11, 12, -1, 4, 7, 9, 1, 1, -1, -1, 9, 1, 8, -1, 1, 1, 1, 8, 1, 1, 1, 8, 2, 3, -1, -1, 7, 1, 8, 10, 9, 9, 2, 8, 9, 12, 3, 4, -1, 8, 11, -1, 9, 1, -1, 3, 10, 8, -1, -1, -1, 1, 1, -1, 1, 8, 2, 9, 2, 1, 9, 1, 9, 3, -1, 8, 9, 2, 3, 7, 1, 8, 3, 12, 1, 1, 12, -1, 1, 7, 8, 11, 1, 3, 4, 3, 1, 8, 2, 1, -1, -1, 8, 4, 8, 9, -1, 8, -1, 6, 8, 2, 9, 1, 7, -1, 9, 5, 8, -1, 1, 2, 8, 7, 9, 1, 2, 1, 8, 8, 1, 1, -1, 10, 9, 3, 8, 1, 1, 1, 9, 8, 2, 8, 8, 9, 8, -1, 7, -1, 2, 1, 8, 9, 10, 8, 9, -1, 9, -1, 1, 2, 1, 1, -1, 1, -1, 8, 10, 10, 8, 8, 2, -1, 1, 8, -1, 9, 7, 8, 1, -1, -1, -1, -1, 1, -1, 8, 1, -1, -1, 10, -1, -1, 2, -1, 1, 1, 1, 3, 1, -1, 7, 9, 8, 8, 1, 12, 9, 2, 1, -1, 1, 8, -1, 2, -1, 1, 3, -1, 9, 1, 8, -1, 1, 1, -1, 10, 3, -1, 1, 10, -1, 10, 9, 1, 1, 2, 2, 9, -1, 2, 1, 1, 1, -1, 9, -1, 1, 10, 9, 8, 1, 9, -1, 9, 9, 8, -1, 7, 12, 12, 8, 2, 1]\n",
            "[1, 8, -1, 1, 2, 8, 3, 6, 2, 1, -1, 4, 1, -1, 8, -1, 2, -1, 1, 2, 8, -1, -1, 4, 2, 8, 2, 8, -1, 8, 12, 1, 8, -1, -1, 1, 9, 2, 2, 3, -1, 1, 8, 2, 2, 8, -1, 3, -1, 8, 3, 12, 1, 7, 1, 8, 9, -1, 9, 9, -1, 3, 6, 2, 1, -1, 2, -1, 10, 1, 3, -1, 7, 1, 2, -1, -1, 10, 1, 3, 1, -1, 10, -1, 7, -1, -1, -1, 8, -1, 2, 8, -1, -1, -1, 1, 2, 10, 8, -1, -1, 3, -1, 2, 1, -1, 7, 2, 8, 5, -1, 2, 1, 1, 10, 9, -1, 3, 9, -1, 8, -1, 7, -1, 2, 1, 3, 2, 8, 2, 9, 1, 8, -1, 9, -1, 1, 1, 1, -1, 1, 1, 8, 9, -1, -1, 8, 3, -1, 9, 1, 1, -1, 7, 4, -1, -1, -1, -1, 2, 4, 8, 4, 4, -1, -1, 8, 9, 4, 7, 3, 5, -1, 2, -1, -1, -1, 9, -1, -1, 3, -1, 9, 8, -1, -1, 8, -1, 12, -1, -1, 8, 10, -1, -1, 9, -1, 12, 1, 9, 3, -1, 1, -1, 8, 8, 8, 9, -1, 8, -1, 2, 2, 9, 1, 1, -1, 8, 2, 2, 8, 9, 4, 2, 1, 7, 1, 2, 4, 2, -1, 1, -1, -1, 9, -1, 9, -1, 7, 3, 1, 8, -1, -1, 1, 1, 7, 8, 1, -1, 2, 2, 3, -1, 9, 2, 1, -1, -1, 2, 8, 5, 1, 2, 9, 2, 2, 8, -1, 8, 2, 2, 8, -1, 7, -1, 9, 2, -1, 3, 7, 12, 2, 12, 1, 1, 9, -1, -1, 3, -1, 7, -1, 1, 8, 2, 1, 5, 1, 3, 2, 1, 2, 1, -1, 8, 2, 3, 8, 12, 7, 1, 8, 5, 1, 1, 1, 8, -1, -1, -1, 9, 2, 2, 1, -1, 1, 9, 2, 1, -1, 5, 1, 1, 1, 3, -1, 8, 2, 8, -1, 8, -1, -1, -1, 8, 9, -1, 2, 8, 1, 3, 3, 8, 1, -1, 1, 1, 9, -1, -1, 12, 3, 4, -1, 8, 4, -1, -1, 2, -1, 1, -1, 2, 2, 3, 4, -1, 1, 1, 10, -1, -1, -1, -1, 9, 8, -1, 12, 1, -1, -1, -1, 7, 3, -1, 8, 8, 8, 1, 12, 2, 1, 3, 2, 2, -1, -1, 2, 2, 8, 3, 1, -1, 1, -1, 1, 7, 5, -1, 1, 1, 8, 12, -1, 1, -1, 8, 8, 1, 2, -1, -1, 1, 8, 9, 10, 1, 12, 3, 1, -1, 8, 7, 9, 1, -1, -1, 2, 8, 8, 8, 8, -1, -1, 1, 8, -1, -1, 2, -1, 8, 7, 1, 4, 1, 2, -1, -1, 6, -1, 1, 3, 1, 2, -1, -1, 8, -1, 1, -1, -1, 9, 8, 1, -1, 9, 4, -1, -1, 1, -1, 1, 5, -1, 1, 3, 1, 2, 2, 7, -1, 2, -1, -1, 1, -1, 1, 1, 9, 10, 3, 12, 8, 1, 1, 2, 8, -1, 3, 2, 5, -1, 1, -1, 7, 1, -1, 7, 1, -1, 9, -1, -1, 8, 8, 1, 8, -1, -1, -1, 6, 1, -1, -1, 1, 8, 9, 3, -1, 4, -1, 2, 1, 8, 9, 8, 2, 5, 4, 7, 2, 9, 9, 8, -1, 8, 10, -1, 1, -1, 2, 3, 8, 1, -1, 1, -1, 8, 9, 9, 8, 1, -1, 2, -1, 1, 8, 12, -1, 2, 7, 3, 10, 1, -1, -1, 9, 1, 8, -1, 1, -1, 1, 3, 1, 1, 1, 8, 2, 3, -1, -1, 10, 1, 8, 10, 9, 9, 2, 8, 9, 3, 3, 2, -1, 8, 4, -1, 1, 1, -1, -1, 8, 8, -1, -1, -1, 1, 1, -1, 1, -1, 2, 8, -1, 1, 8, 1, 8, 3, -1, 8, 9, 2, 3, 10, 2, 8, 3, 12, 1, 10, 12, -1, 1, 7, 8, 11, 1, 3, 2, 2, 1, 8, 7, -1, -1, -1, 8, 2, -1, 1, -1, 8, -1, 6, 8, 2, 9, 1, 7, -1, 9, 5, 8, -1, 1, 2, 8, -1, 3, 1, -1, 1, 8, 9, 1, 1, -1, 8, 4, 3, 8, 1, 1, 1, 9, 8, 2, 8, 8, 9, 8, -1, 7, -1, 2, 1, -1, 9, 10, 8, 9, -1, 10, -1, 9, 2, 1, 1, -1, 1, -1, 8, 10, 10, 8, 8, 2, -1, 1, 8, -1, 9, 7, 8, -1, -1, -1, -1, -1, 1, -1, 10, 1, -1, -1, 10, -1, -1, 2, -1, 2, 1, 3, 3, 1, -1, 7, -1, -1, 8, -1, -1, 8, 2, -1, -1, 1, 9, -1, 2, -1, 1, 3, -1, 9, -1, 8, -1, 1, 1, -1, 10, 1, -1, 1, 10, -1, 10, 9, 1, 1, 2, 2, -1, -1, 2, 1, 1, 1, -1, 4, -1, 1, 10, 9, 7, 1, 9, -1, 9, 9, 8, -1, 7, 12, 12, 8, -1, 1]\n",
            "[1, 9, -1, 1, 2, 9, 3, 6, -1, 1, -1, 4, 1, -1, 1, -1, 2, -1, 1, 2, -1, -1, -1, 4, 2, 10, 2, 8, -1, 9, 8, 1, 8, -1, -1, 1, 1, 2, 2, 3, -1, 1, 9, -1, 6, 8, -1, 3, -1, -1, 3, 12, 3, 7, 2, -1, -1, -1, 9, 9, -1, 3, 6, 2, 1, -1, 2, -1, -1, -1, 3, -1, -1, 1, 2, -1, -1, -1, 1, 3, 1, -1, 10, -1, 7, -1, -1, -1, -1, -1, 2, 8, -1, -1, -1, 1, -1, 3, 8, -1, -1, 3, -1, 2, 1, -1, -1, 4, 8, 9, -1, 2, 1, 1, 7, 9, -1, 3, 9, -1, 8, -1, -1, -1, 2, 1, 3, 2, 8, 2, 9, 1, 8, -1, 9, -1, 1, 1, 1, -1, -1, 1, 9, 9, -1, -1, 8, 3, -1, 9, 2, 1, -1, -1, -1, -1, -1, -1, -1, 2, -1, 8, 4, -1, -1, -1, 4, 9, 4, 7, 3, 5, -1, 2, -1, -1, -1, 9, -1, -1, 3, -1, 1, 3, -1, -1, 9, -1, -1, -1, -1, 8, 9, -1, -1, -1, -1, 12, 1, 9, 3, -1, 1, -1, -1, -1, 9, 9, -1, 8, -1, 2, 2, 9, 1, 1, -1, 8, 2, 2, 8, 9, 4, 2, 1, 7, 3, -1, -1, 2, -1, 1, -1, -1, 9, -1, 9, -1, 7, 3, 1, 9, -1, -1, 1, 1, 7, 8, 1, -1, 9, 2, 3, -1, 9, 3, -1, -1, -1, 2, 8, 6, 1, 3, 3, -1, 4, 10, -1, 8, -1, 2, 8, -1, 7, -1, 8, 3, -1, -1, 7, 1, 2, 8, 1, -1, 9, -1, -1, 3, -1, 7, -1, 1, 9, 2, 1, 5, 1, 3, -1, 1, 2, 1, -1, 8, -1, 3, 8, 12, 3, 1, -1, 5, 1, 1, -1, 8, -1, -1, -1, 9, 8, -1, 1, -1, -1, 9, 2, -1, -1, 5, 1, 1, -1, -1, -1, 8, 2, 8, -1, 7, -1, -1, -1, 9, 9, -1, 2, 8, 1, 3, 3, 9, 1, -1, 1, 1, 9, -1, -1, 12, 3, 1, -1, 9, -1, -1, -1, 2, -1, 1, -1, 2, 2, 3, 4, -1, 1, 1, 10, -1, -1, -1, -1, 9, 10, -1, 12, 1, -1, -1, -1, 7, 3, -1, 8, 8, 8, 1, 9, 2, 1, 3, 6, 2, -1, -1, 2, 1, 9, 2, 1, -1, 1, -1, 1, 7, 5, -1, 1, 1, 10, 12, -1, 3, -1, 8, 10, 1, 2, -1, -1, 1, 8, 9, 8, 1, 12, -1, 1, -1, 4, 7, 9, 1, -1, -1, -1, 9, 3, 8, 8, -1, -1, 1, -1, -1, -1, 6, -1, 6, 7, 1, 4, -1, 2, -1, -1, 6, -1, -1, 3, 3, 4, -1, -1, 8, -1, 1, -1, -1, 9, 9, 1, -1, 9, 4, -1, -1, 1, -1, 4, 5, -1, 1, 3, 1, 2, -1, 10, -1, 1, -1, -1, 1, -1, 1, -1, 9, 3, 3, 12, 10, 1, 1, 3, -1, -1, 3, 2, 5, -1, 1, -1, 9, 1, -1, 10, 1, -1, 9, -1, -1, 8, 8, -1, -1, -1, -1, -1, 6, 1, -1, -1, 1, 8, 9, 9, -1, 8, -1, 3, -1, 8, 9, 8, 9, 5, 4, 7, 2, 9, 9, 8, -1, -1, 9, -1, 1, -1, 1, 3, 8, 1, -1, 1, -1, 8, 9, 9, 2, 1, -1, 2, -1, 1, 8, 12, -1, 4, -1, 9, 1, 1, -1, -1, 9, 1, 8, -1, -1, -1, 1, 3, 1, 1, 1, 1, 2, 3, -1, -1, -1, -1, 8, 10, 9, 9, 2, 8, -1, 3, 3, 4, -1, 8, 8, -1, 1, -1, -1, -1, 10, 8, -1, -1, -1, 1, 1, -1, -1, -1, 2, 9, -1, 1, -1, 1, 9, 3, -1, 8, 9, 2, 3, 10, -1, 9, 3, 12, 1, 1, -1, -1, 1, 7, 9, 11, 1, 3, 4, 3, 1, 8, 2, -1, -1, -1, 8, 4, -1, 1, -1, 8, -1, 6, -1, 2, -1, 1, 7, -1, -1, 5, 8, -1, 1, 2, 8, -1, 3, 1, -1, 1, -1, 3, 1, 1, -1, 8, 4, 3, 9, 1, 1, 1, 9, 8, 2, 8, 8, 9, 8, -1, -1, -1, 2, 1, -1, 9, 3, 8, -1, -1, 9, -1, 1, 2, 1, 1, -1, 1, -1, 8, 10, 10, 8, 9, 2, -1, 1, 8, -1, 9, -1, 8, -1, -1, -1, -1, -1, 1, -1, -1, 1, -1, -1, 9, -1, -1, 2, -1, 1, -1, 3, 2, -1, -1, 7, -1, -1, 8, -1, -1, 9, 2, -1, -1, 1, -1, -1, 2, -1, 3, 3, -1, -1, -1, 8, -1, 1, 1, -1, 10, 1, -1, -1, 10, -1, 9, 9, 1, 1, -1, 2, -1, -1, 4, 1, 1, 1, -1, 8, -1, 1, 3, 9, 7, 1, 9, -1, 9, 9, 8, -1, 7, 12, 12, -1, -1, -1]\n",
            "[1, 9, -1, 1, 2, 8, 3, 6, -1, 1, -1, 4, 1, -1, 10, -1, 2, -1, 1, 2, -1, -1, -1, 4, 2, 8, 2, 8, -1, 8, 8, 2, 8, -1, -1, -1, -1, 2, 2, 3, -1, 1, 1, -1, -1, -1, -1, 3, -1, -1, 3, 12, 3, 7, 2, -1, -1, -1, 9, 9, -1, 2, -1, 2, 1, -1, 9, -1, -1, -1, 3, -1, -1, 1, 2, -1, -1, -1, -1, 3, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 2, 8, -1, -1, -1, 1, -1, 10, 8, -1, -1, -1, -1, 2, 1, -1, -1, 4, 8, -1, -1, 2, 1, 1, 10, 9, -1, 3, -1, -1, 8, -1, -1, -1, 2, 1, 3, -1, 8, 2, 9, 1, -1, -1, 9, -1, 1, 1, 1, -1, -1, -1, 9, 9, -1, -1, 8, 3, -1, 9, 2, 1, -1, -1, -1, -1, -1, -1, -1, 2, -1, 8, -1, -1, -1, -1, 8, 9, 4, 7, 3, 5, -1, 2, -1, -1, -1, 9, -1, -1, 2, -1, 9, -1, -1, -1, 8, -1, -1, -1, -1, 8, 10, -1, -1, -1, -1, -1, 1, -1, 3, -1, 1, -1, -1, -1, -1, 9, -1, 8, -1, -1, 2, 9, 1, 8, -1, 8, -1, 2, 8, 9, 9, 2, -1, 7, 1, -1, -1, 2, -1, 1, -1, -1, 9, -1, -1, -1, -1, 3, 1, 8, -1, -1, 1, -1, 7, 8, 1, -1, 2, 2, 3, -1, -1, 3, -1, -1, -1, 2, 8, 1, 1, 3, 9, -1, 2, 8, -1, -1, -1, 2, 8, -1, -1, -1, 8, 2, -1, -1, -1, 12, 2, 8, 1, -1, 9, -1, -1, 3, -1, 7, -1, -1, 8, 2, 1, 5, -1, -1, -1, 1, 2, 1, -1, 8, -1, 1, 8, -1, -1, 1, -1, 5, 1, 1, -1, 8, -1, -1, -1, 9, 2, -1, 1, -1, -1, 8, 2, -1, -1, 5, 1, 1, -1, -1, -1, 8, 2, 8, -1, 8, -1, -1, -1, -1, 6, -1, -1, -1, -1, 3, 3, 8, 1, -1, 1, 1, 9, -1, -1, 12, 3, -1, -1, 8, -1, -1, -1, 2, -1, 1, -1, -1, -1, 3, 4, -1, 1, 1, 10, -1, -1, -1, -1, 9, 8, -1, 12, 1, -1, -1, -1, 7, -1, -1, 8, 8, -1, 1, 8, 2, -1, 3, 6, 2, -1, -1, 2, 2, 8, 2, 1, -1, 1, -1, 1, -1, 5, -1, 1, 1, 8, 12, -1, -1, -1, 8, 8, -1, 2, -1, -1, 1, -1, 9, 8, 1, 12, -1, 1, -1, 4, -1, -1, -1, -1, -1, -1, 8, 8, 8, 8, -1, -1, 1, -1, -1, -1, 2, -1, -1, 7, -1, 8, -1, 2, -1, -1, 6, -1, -1, 3, 3, 2, -1, -1, 2, -1, 1, -1, -1, 9, 9, 1, -1, -1, 2, -1, -1, 1, -1, 4, 5, -1, 1, -1, 1, 2, -1, 10, -1, 1, -1, -1, 1, -1, 1, -1, 9, 10, 3, 12, 10, 1, 1, 2, -1, -1, 3, 2, 5, -1, 1, -1, 9, 1, -1, 7, -1, -1, 8, -1, -1, 8, 8, -1, -1, -1, -1, -1, 6, 1, -1, -1, 1, 8, 9, 2, -1, 8, -1, 2, -1, -1, -1, 8, 2, 5, 8, 7, 2, 8, -1, 8, -1, -1, 7, -1, 1, -1, 1, 3, 8, 1, -1, 1, -1, 8, 9, 2, 8, 1, -1, 2, -1, 1, 8, 12, -1, 2, -1, 2, 8, 1, -1, -1, 9, 1, 8, -1, -1, -1, 1, 3, 1, 8, 1, 8, 2, 3, -1, -1, -1, -1, 8, -1, -1, -1, 2, 8, -1, 12, 3, -1, -1, 8, 8, -1, 1, -1, -1, -1, 12, 8, -1, -1, -1, 1, 1, -1, -1, -1, 1, 8, -1, 1, -1, 1, 8, 3, -1, 12, 9, 2, 3, 7, -1, 8, 3, 12, 1, 1, -1, -1, 1, 7, 8, 11, 1, 3, -1, 3, 1, 8, 2, -1, -1, -1, -1, -1, -1, 1, -1, 8, -1, 6, -1, 2, -1, 1, -1, -1, -1, 5, 8, -1, -1, 2, 8, -1, 9, 1, -1, 1, -1, 8, 1, 1, -1, 8, 4, 3, 8, 1, 1, 1, 3, 8, 2, 8, -1, 8, 8, -1, -1, -1, 2, 1, -1, 9, -1, 8, -1, -1, 9, -1, 1, 2, 1, 1, -1, 1, -1, 12, 8, -1, 8, 8, 2, -1, -1, 8, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, 1, -1, -1, 10, -1, -1, 2, -1, 1, -1, 9, 2, -1, -1, 7, -1, -1, -1, -1, -1, 8, 2, -1, -1, 1, -1, -1, 2, -1, 1, 3, -1, -1, -1, 8, -1, 1, 1, -1, 10, 1, -1, -1, 9, -1, 9, 2, 1, 1, -1, 2, -1, -1, 2, 9, -1, -1, -1, 8, -1, 1, 10, 9, 8, 1, 9, -1, 2, 9, 8, -1, 7, -1, 12, -1, -1, -1]\n",
            "0.7529691211401425\n",
            "[1, 9, 3, 1, 2, 8, 3, 6, 2, 1, 8, 4, 1, 4, 10, 7, 2, 9, 1, 2, 8, 9, 9, 4, 2, 10, 2, 8, 1, 8, 8, 1, 8, 1, 3, 1, 9, 2, 2, 3, 9, 1, 10, 1, 2, 8, 1, 3, 4, 8, 3, 12, 1, 7, 2, 8, 9, 1, 9, 9, 3, 3, 6, 2, 1, 1, 10, 1, 10, 1, 3, 8, 7, 1, 2, 2, 9, 10, 1, 3, 1, 9, 10, 1, 7, 9, 1, 12, 8, 1, 2, 8, 1, 2, 1, 1, 2, 3, 8, 2, 7, 3, 11, 2, 1, 9, 7, 4, 8, 9, 9, 2, 1, 1, 10, 9, 3, 3, 9, 1, 8, 8, 7, 1, 2, 9, 3, 2, 8, 2, 9, 1, 8, 1, 9, 8, 1, 1, 1, 2, 1, 1, 10, 9, 1, 9, 8, 3, 8, 9, 2, 1, 3, 8, 4, 8, 9, 10, 8, 10, 4, 8, 4, 4, 12, 1, 8, 9, 4, 7, 3, 9, 10, 2, 8, 9, 3, 9, 12, 7, 3, 9, 9, 3, 9, 8, 9, 9, 12, 1, 1, 10, 10, 1, 2, 9, 11, 12, 1, 9, 3, 9, 1, 3, 10, 8, 8, 9, 2, 8, 8, 2, 2, 9, 1, 1, 2, 8, 2, 2, 3, 9, 8, 2, 1, 7, 3, 2, 4, 2, 2, 1, 9, 1, 9, 2, 9, 1, 8, 3, 1, 8, 8, 12, 1, 1, 7, 8, 1, 2, 9, 2, 12, 1, 9, 3, 1, 4, 9, 2, 8, 3, 1, 2, 8, 8, 2, 10, 8, 8, 2, 2, 8, 2, 3, 1, 8, 2, 2, 3, 7, 12, 2, 12, 1, 1, 9, 12, 1, 3, 2, 7, 3, 1, 8, 2, 1, 5, 1, 3, 2, 1, 2, 10, 8, 8, 2, 2, 8, 12, 10, 1, 10, 5, 1, 1, 1, 8, 9, 3, 9, 9, 2, 1, 1, 7, 1, 10, 9, 1, 9, 5, 1, 1, 1, 3, 1, 8, 2, 8, 1, 8, 7, 3, 1, 9, 8, 1, 2, 8, 1, 3, 3, 8, 1, 2, 1, 1, 9, 8, 1, 12, 3, 1, 1, 8, 4, 3, 3, 2, 9, 1, 2, 2, 2, 3, 6, 2, 1, 1, 10, 12, 8, 4, 1, 9, 10, 9, 12, 1, 9, 8, 8, 7, 9, 8, 8, 8, 8, 1, 8, 2, 1, 3, 6, 2, 3, 2, 2, 2, 8, 3, 1, 9, 1, 9, 1, 7, 9, 1, 1, 2, 10, 12, 9, 1, 2, 8, 10, 1, 2, 3, 3, 3, 8, 10, 10, 1, 12, 3, 1, 2, 4, 8, 9, 1, 2, 9, 2, 8, 3, 8, 8, 0, 8, 1, 8, 1, 1, 6, 8, 8, 7, 1, 10, 1, 2, 9, 3, 6, 12, 1, 3, 4, 2, 3, 1, 8, 8, 1, 9, 9, 9, 9, 1, 9, 9, 8, 8, 9, 1, 3, 1, 9, 2, 1, 3, 1, 2, 9, 10, 8, 1, 1, 1, 9, 4, 1, 2, 9, 3, 6, 12, 10, 1, 1, 2, 8, 2, 3, 2, 5, 3, 1, 3, 9, 1, 1, 8, 1, 1, 8, 8, 2, 8, 8, 1, 8, 1, 8, 10, 6, 1, 9, 1, 1, 8, 9, 3, 10, 4, 8, 2, 1, 8, 9, 8, 2, 5, 4, 7, 2, 8, 9, 8, 12, 9, 10, 9, 1, 2, 1, 3, 8, 1, 10, 1, 1, 8, 9, 9, 8, 1, 1, 2, 2, 5, 8, 12, 2, 4, 7, 9, 8, 1, 10, 9, 9, 1, 8, 11, 1, 1, 1, 3, 1, 1, 1, 8, 2, 3, 9, 10, 10, 1, 8, 10, 9, 9, 2, 8, 9, 8, 3, 2, 2, 8, 8, 2, 1, 1, 9, 3, 10, 8, 3, 8, 2, 1, 1, 3, 1, 8, 2, 9, 1, 1, 9, 1, 9, 3, 3, 8, 9, 2, 3, 10, 2, 8, 3, 12, 1, 1, 12, 12, 1, 7, 8, 11, 1, 3, 9, 3, 1, 8, 7, 4, 8, 2, 8, 2, 8, 1, 2, 8, 8, 6, 10, 2, 9, 1, 8, 9, 9, 5, 8, 1, 1, 2, 8, 7, 9, 1, 2, 1, 8, 3, 1, 1, 2, 8, 4, 3, 8, 1, 1, 1, 9, 8, 2, 8, 8, 9, 8, 1, 7, 1, 2, 1, 8, 9, 10, 8, 10, 9, 9, 1, 9, 2, 1, 1, 9, 1, 2, 8, 10, 10, 8, 8, 2, 9, 1, 8, 2, 9, 7, 8, 1, 8, 1, 1, 3, 1, 1, 4, 1, 8, 9, 9, 1, 2, 2, 3, 2, 1, 3, 3, 1, 4, 7, 2, 8, 8, 1, 12, 8, 2, 1, 7, 1, 8, 2, 2, 12, 3, 3, 8, 9, 1, 8, 8, 1, 1, 2, 10, 1, 9, 1, 10, 9, 9, 9, 1, 1, 3, 2, 9, 10, 2, 9, 1, 1, 9, 8, 8, 1, 10, 9, 8, 1, 2, 12, 8, 9, 8, 9, 7, 12, 12, 8, 2, 1]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "final_pred_0=[]\n",
        "\n",
        "print(new_val_fnum_0)\n",
        "print(new_val_fnum_1)\n",
        "print(new_val_fnum_2)\n",
        "print(new_val_fnum_3)\n",
        "print(new_val_fnum_4)\n",
        "print(new_val_fnum_5)\n",
        "\n",
        "num_correct=0\n",
        "model_0_0=load_model(loss,accuracy,optimizer,'0-0')\n",
        "mod_labels_0=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_0:\n",
        "    pred_test_0_0=model_0_0.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_0_0 = pred_test_0_0.argmax(axis=1)\n",
        "    mod_labels_0.append(pred_labels_0_0[0])\n",
        "  else:\n",
        "    mod_labels_0.append(-1)\n",
        "\n",
        "model_1_0=load_model(loss,accuracy,optimizer,'1-0')\n",
        "mod_labels_1=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        " \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_1:\n",
        "    pred_test_1_0=model_1_0.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_1_0 = pred_test_1_0.argmax(axis=1)\n",
        "    mod_labels_1.append(pred_labels_1_0[0])\n",
        "  else:\n",
        "    mod_labels_1.append(-1)\n",
        "\n",
        "model_2_0=load_model(loss,accuracy,optimizer,'2-0')\n",
        "mod_labels_2=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_2:\n",
        "    pred_test_2_0=model_2_0.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_2_0 = pred_test_2_0.argmax(axis=1)\n",
        "    mod_labels_2.append(pred_labels_2_0[0])\n",
        "  else:\n",
        "    mod_labels_2.append(-1)\n",
        "\n",
        "model_3_0=load_model(loss,accuracy,optimizer,'3-0')\n",
        "mod_labels_3=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_3:\n",
        "    pred_test_3_0=model_3_0.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_3_0 = pred_test_3_0.argmax(axis=1)\n",
        "    mod_labels_3.append(pred_labels_3_0[0])\n",
        "  else:\n",
        "    mod_labels_3.append(-1)\n",
        "\n",
        "model_4_0=load_model(loss,accuracy,optimizer,'4-0')\n",
        "mod_labels_4=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_4:\n",
        "    pred_test_4_0=model_4_0.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_4_0 = pred_test_4_0.argmax(axis=1)\n",
        "    mod_labels_4.append(pred_labels_4_0[0])\n",
        "  else:\n",
        "    mod_labels_4.append(-1)\n",
        "\n",
        "model_5_0=load_model(loss,accuracy,optimizer,'5-0')\n",
        "mod_labels_5=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_5:\n",
        "    pred_test_5_0=model_5_0.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_5_0 = pred_test_5_0.argmax(axis=1)\n",
        "    mod_labels_5.append(pred_labels_5_0[0])\n",
        "  else:\n",
        "    mod_labels_5.append(-1)\n",
        "\n",
        "print(mod_labels_0)\n",
        "print(mod_labels_1)\n",
        "print(mod_labels_2)\n",
        "print(mod_labels_3)\n",
        "print(mod_labels_4)\n",
        "print(mod_labels_5)\n",
        "\n",
        "\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  fin_labels=[]\n",
        "  if mod_labels_0[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  if mod_labels_1[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  if mod_labels_2[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  if mod_labels_3[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  if mod_labels_4[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  if mod_labels_5[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  res=max(set(fin_labels), key = fin_labels.count)\n",
        "  final_pred_0.append(res)\n",
        "\n",
        "  if res==new_val_label_0[i]:\n",
        "    num_correct=num_correct+1\n",
        "\n",
        "accuracy=num_correct/len(new_val_label_0)\n",
        "print(accuracy)\n",
        "  \n",
        "print(final_pred_0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSrkJ8k6oBkG",
        "outputId": "225031d2-0c8b-4f47-ecb0-79fb0cc23f29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7529691211401425\n",
            "Weighted F1: 0.7520917735523328\n",
            "Micro F1: 0.7529691211401426\n",
            "Weighted Precision: 0.7551276057124311\n",
            "Micro Precision: 0.7529691211401425\n",
            "Weighted Recall: 0.7529691211401425\n",
            "Micro Recall: 0.7529691211401425\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "total_accuracy=0\n",
        "total_weighted_f1=0\n",
        "total_micro_f1=0\n",
        "total_weighted_precision=0\n",
        "total_micro_precision=0\n",
        "total_weighted_recall=0\n",
        "total_micro_recall=0\n",
        "\n",
        "accuracy=accuracy_score(new_val_label_0, final_pred_0)\n",
        "print(\"Accuracy: \"+str(accuracy))\n",
        "total_accuracy=total_accuracy+accuracy\n",
        "\n",
        "weighted_f1=f1_score(new_val_label_0,final_pred_0, average='weighted')\n",
        "print(\"Weighted F1: \"+ str(weighted_f1))\n",
        "total_weighted_f1=total_weighted_f1+weighted_f1\n",
        "micro_f1=f1_score(new_val_label_0,final_pred_0, average='micro')\n",
        "print(\"Micro F1: \"+ str(micro_f1))\n",
        "total_micro_f1=total_micro_f1+micro_f1\n",
        "\n",
        "weighted_precision=precision_score(new_val_label_0, final_pred_0, average='weighted')\n",
        "print(\"Weighted Precision: \" + str(weighted_precision))\n",
        "total_weighted_precision=total_weighted_precision+weighted_precision\n",
        "micro_precision=precision_score(new_val_label_0, final_pred_0, average='micro')\n",
        "print(\"Micro Precision: \" + str(micro_precision))\n",
        "total_micro_precision=total_micro_precision+micro_precision\n",
        "\n",
        "weighted_recall=recall_score(new_val_label_0, final_pred_0, average='weighted')\n",
        "print(\"Weighted Recall: \" + str(weighted_recall))\n",
        "total_weighted_recall=total_weighted_recall+weighted_recall\n",
        "micro_recall=recall_score(new_val_label_0, final_pred_0, average='micro')\n",
        "print(\"Micro Recall: \" + str(micro_recall))\n",
        "total_micro_recall=total_micro_recall+micro_recall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9SEXW7xUkmvH",
        "outputId": "b1efab5a-471a-4b28-f7e0-cf6a4841ef75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_8\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_17 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_18 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_17[0][0]',               \n",
            "                                thPoolingAndCrossAt               'input_18[0][0]']               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_8 (Sl  (None, 768)         0           ['tf_bert_model[8][0]']          \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_16 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_8[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_45 (Dropout)           (None, 512)          0           ['dense_16[0][0]']               \n",
            "                                                                                                  \n",
            " dense_17 (Dense)               (None, 15)           7695        ['dropout_45[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model: \"model_9\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_19 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_20 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_19[0][0]',               \n",
            "                                thPoolingAndCrossAt               'input_20[0][0]']               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_9 (Sl  (None, 768)         0           ['tf_bert_model[9][0]']          \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_18 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_9[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_46 (Dropout)           (None, 512)          0           ['dense_18[0][0]']               \n",
            "                                                                                                  \n",
            " dense_19 (Dense)               (None, 15)           7695        ['dropout_46[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model: \"model_10\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_21 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_22 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_21[0][0]',               \n",
            "                                thPoolingAndCrossAt               'input_22[0][0]']               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_10 (S  (None, 768)         0           ['tf_bert_model[10][0]']         \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " dense_20 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_10[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dropout_47 (Dropout)           (None, 512)          0           ['dense_20[0][0]']               \n",
            "                                                                                                  \n",
            " dense_21 (Dense)               (None, 15)           7695        ['dropout_47[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model: \"model_11\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_23 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_24 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_23[0][0]',               \n",
            "                                thPoolingAndCrossAt               'input_24[0][0]']               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_11 (S  (None, 768)         0           ['tf_bert_model[11][0]']         \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " dense_22 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_11[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dropout_48 (Dropout)           (None, 512)          0           ['dense_22[0][0]']               \n",
            "                                                                                                  \n",
            " dense_23 (Dense)               (None, 15)           7695        ['dropout_48[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model: \"model_12\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_25 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_26 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_25[0][0]',               \n",
            "                                thPoolingAndCrossAt               'input_26[0][0]']               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_12 (S  (None, 768)         0           ['tf_bert_model[12][0]']         \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " dense_24 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_12[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dropout_49 (Dropout)           (None, 512)          0           ['dense_24[0][0]']               \n",
            "                                                                                                  \n",
            " dense_25 (Dense)               (None, 15)           7695        ['dropout_49[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model: \"model_13\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_27 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_28 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_27[0][0]',               \n",
            "                                thPoolingAndCrossAt               'input_28[0][0]']               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_13 (S  (None, 768)         0           ['tf_bert_model[13][0]']         \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " dense_26 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_13[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dropout_50 (Dropout)           (None, 512)          0           ['dense_26[0][0]']               \n",
            "                                                                                                  \n",
            " dense_27 (Dense)               (None, 15)           7695        ['dropout_50[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "[1, 9, 3, 1, 2, 8, 3, 6, 2, 1, 8, 4, 1, 4, 1, 7, 2, 4, 1, 2, 8, 9, 9, 4, 2, 8, 2, 8, 1, 8, 8, 1, 8, 1, 3, 1, 1, 2, 2, 3, 2, 1, 8, 2, 2, 8, 9, 3, 4, 8, 3, 12, 3, 7, 2, 9, 9, 2, 9, 9, 3, 2, 6, 2, 1, 1, 2, 1, 10, 1, 3, 8, 7, 1, 2, 2, 9, 4, 1, 3, 1, 9, 10, 1, 7, 9, 9, 12, 8, 1, 2, 8, 1, 2, 1, 1, 2, 10, 8, 2, 7, 3, 11, 9, 1, 9, 10, 4, 8, 9, 6, 2, 1, 1, 10, 9, 3, 3, 9, 1, 8, 8, 7, 1, 2, 9, 3, 2, 8, 2, 9, 1, 8, 1, 9, 8, 1, 1, 1, 2, 1, 1, 8, 9, 1, 9, 8, 3, 8, 9, 2, 1, 3, 7, 2, 10, 3, 8, 8, 2, 9, 8, 8, 4, 12, 1, 8, 9, 8, 7, 3, 5, 10, 2, 8, 9, 3, 9, 12, 7, 2, 1, 9, 9, 9, 8, 9, 9, 12, 1, 1, 10, 8, 1, 2, 9, 10, 12, 1, 9, 3, 2, 1, 3, 8, 2, 9, 9, 2, 8, 8, 2, 2, 9, 1, 1, 2, 8, 2, 2, 8, 9, 4, 2, 1, 7, 1, 2, 1, 2, 2, 1, 9, 1, 9, 2, 9, 1, 7, 3, 1, 9, 8, 12, 1, 1, 7, 8, 1, 2, 2, 2, 12, 1, 9, 3, 1, 1, 9, 2, 8, 5, 1, 3, 2, 8, 2, 10, 8, 8, 2, 2, 8, 2, 7, 1, 8, 2, 2, 3, 9, 12, 2, 12, 1, 1, 9, 12, 1, 3, 2, 7, 3, 1, 9, 2, 1, 5, 1, 3, 2, 1, 2, 1, 8, 8, 2, 1, 8, 12, 10, 1, 10, 5, 1, 1, 1, 8, 9, 3, 9, 9, 10, 1, 1, 9, 1, 9, 2, 1, 9, 5, 1, 1, 1, 3, 9, 8, 2, 8, 1, 8, 7, 1, 1, 9, 8, 1, 2, 8, 1, 3, 3, 8, 1, 2, 1, 1, 9, 9, 1, 12, 3, 1, 2, 8, 4, 3, 1, 2, 9, 1, 2, 2, 2, 3, 4, 2, 1, 1, 10, 8, 8, 8, 1, 9, 8, 9, 12, 1, 9, 8, 9, 7, 9, 8, 8, 8, 8, 1, 8, 2, 1, 3, 6, 2, 3, 2, 2, 8, 10, 2, 1, 9, 1, 9, 1, 7, 9, 1, 8, 1, 10, 12, 9, 3, 2, 8, 10, 1, 2, 3, 9, 1, 8, 9, 10, 1, 12, 3, 1, 2, 8, 7, 9, 1, 2, 9, 2, 8, 8, 8, 8, 2, 8, 1, 8, 1, 1, 2, 8, 8, 7, 1, 4, 1, 2, 2, 5, 6, 12, 1, 3, 9, 2, 3, 1, 8, 8, 1, 9, 9, 9, 2, 1, 9, 9, 8, 8, 9, 1, 3, 1, 5, 2, 1, 3, 1, 2, 9, 10, 8, 2, 4, 1, 1, 4, 1, 1, 9, 10, 3, 12, 10, 1, 1, 2, 8, 1, 3, 2, 5, 9, 1, 3, 9, 1, 1, 10, 1, 1, 9, 8, 2, 8, 8, 1, 8, 1, 4, 8, 6, 1, 9, 1, 1, 8, 9, 3, 10, 10, 8, 2, 1, 8, 9, 8, 10, 5, 4, 7, 2, 8, 9, 8, 12, 9, 7, 9, 1, 2, 1, 3, 8, 1, 10, 1, 1, 8, 9, 2, 8, 1, 1, 2, 2, 1, 8, 12, 2, 2, 7, 2, 8, 1, 10, 2, 9, 1, 8, 11, 1, 1, 1, 3, 1, 1, 1, 8, 2, 3, 9, 2, 10, 1, 8, 8, 9, 2, 2, 8, 9, 3, 3, 2, 2, 8, 8, 2, 9, 1, 9, 1, 10, 8, 3, 3, 2, 1, 1, 3, 1, 8, 2, 9, 2, 1, 9, 1, 9, 3, 3, 8, 9, 2, 3, 10, 2, 8, 3, 12, 1, 1, 12, 12, 1, 7, 8, 11, 1, 3, 2, 3, 1, 8, 2, 4, 8, 2, 8, 2, 8, 1, 2, 8, 8, 6, 10, 2, 9, 1, 8, 9, 9, 5, 8, 1, 1, 2, 8, 7, 9, 1, 2, 1, 8, 8, 1, 1, 2, 8, 4, 3, 8, 1, 1, 1, 9, 8, 2, 8, 8, 9, 8, 1, 7, 1, 2, 1, 8, 9, 10, 8, 2, 9, 10, 8, 2, 2, 1, 1, 9, 1, 2, 8, 10, 10, 8, 8, 2, 9, 1, 8, 9, 9, 7, 8, 1, 8, 1, 1, 3, 1, 1, 8, 1, 8, 2, 10, 1, 1, 2, 3, 1, 1, 3, 3, 1, 8, 7, 2, 8, 8, 1, 4, 9, 2, 1, 7, 1, 8, 2, 2, 12, 3, 3, 8, 9, 1, 8, 8, 1, 1, 2, 10, 1, 9, 3, 10, 8, 10, 9, 1, 1, 2, 2, 9, 10, 2, 9, 1, 1, 9, 9, 2, 1, 10, 9, 8, 1, 2, 12, 2, 9, 8, 1, 7, 12, 12, 2, 2, 1]\n",
            "[1, 9, -1, 1, 2, 8, 3, 6, 8, 1, 8, 4, 1, 2, 1, -1, 2, 4, 1, 2, 8, -1, -1, 4, 2, 8, 2, 8, 1, 8, 12, 1, 8, 1, -1, 1, 1, 2, 2, 3, 2, 1, 1, 2, 2, 8, 1, 3, 4, 8, 3, 12, 1, 7, 2, 8, 9, -1, 9, 9, -1, 2, 2, 2, 1, 1, 8, 1, 8, 1, 3, 8, 7, 1, 2, 2, -1, 2, 1, 3, 1, 11, 10, 1, 7, -1, -1, 12, 10, 1, 2, 8, 1, 2, 1, 1, 2, 10, 8, -1, 7, 3, 11, 2, 1, 9, 9, 1, 8, 5, -1, 2, 1, 1, 10, 9, 3, 3, 9, 1, 8, 8, 7, 1, 2, 9, 3, 2, 9, 2, 9, 1, 12, 3, 9, -1, 1, 1, 1, -1, 1, 1, 10, 9, 1, -1, 8, 3, 8, 9, 2, 1, 3, 7, 2, -1, -1, 8, 8, 2, 9, 8, 12, 8, -1, 1, 8, 9, 4, 7, 3, 5, 7, 2, 8, 9, -1, 9, 12, 7, 2, 1, 9, 8, -1, -1, 8, -1, 12, 1, 1, 8, 10, 1, -1, 9, 10, 12, 1, 9, 8, -1, 1, 3, 10, 8, 8, 9, -1, 8, 8, 2, 2, 9, 1, 1, -1, 10, 2, 2, 8, 9, 8, 2, 1, 7, 1, 2, 4, 2, -1, 1, -1, -1, 9, 10, 9, 1, 12, 7, 1, 8, 8, 12, 1, 1, 7, 8, 1, -1, 2, 2, 12, -1, 9, 2, 1, -1, -1, 2, 8, 1, 1, 2, 6, 2, 2, 8, 8, 8, 2, 2, 8, 2, 7, 1, 8, 2, 2, 3, 7, 12, 2, 12, 1, 1, 9, 12, 1, 3, -1, 7, -1, 1, 8, 2, 1, 5, 1, 3, 9, 1, 2, 9, -1, 8, 2, 1, 8, 12, 7, 1, 8, 5, 1, 1, 1, 8, -1, 3, -1, 9, 10, 1, 1, 9, 1, 8, 2, 1, 7, 5, 1, 1, 1, 3, 1, 8, 2, 8, 1, 8, 7, 3, 1, 9, 8, 1, 2, 8, 1, 3, 8, 8, 1, 2, 1, 1, 9, -1, -1, 12, 3, 1, -1, 8, 4, 3, 1, 2, -1, 1, 2, 2, 2, 3, 6, -1, 1, 1, 10, -1, 8, 4, 1, 9, 8, 1, 12, 1, -1, 8, -1, 7, 9, -1, 8, 8, 8, 1, 12, 2, 1, 9, 6, 2, -1, 2, 2, 2, 9, 3, 1, -1, 1, -1, 1, 7, 9, 1, 1, 2, 10, 12, -1, 1, 2, 8, 10, 1, 2, -1, -1, 1, 8, 9, 8, 1, 12, 3, 1, -1, 8, 7, 9, 1, -1, 9, 2, 8, 8, 8, 8, -1, -1, 1, 8, 1, -1, 2, 8, 8, 7, 1, 8, 1, 2, 9, 3, 6, -1, 1, 3, 2, 2, -1, -1, 8, 8, 1, -1, -1, 9, 9, 1, -1, 8, 8, 8, 9, 1, -1, 1, 5, 2, 1, 1, 1, 2, 9, 10, 8, 1, 1, 1, 1, 4, 1, 2, 9, 10, 3, 12, 8, 1, 1, 2, 8, -1, 3, 2, 5, 1, 1, -1, 9, 1, 1, 7, 1, -1, 9, 8, -1, 8, 8, 1, 8, -1, -1, -1, 2, 1, 9, -1, 1, 8, 9, 9, -1, 8, 8, 2, 1, 8, 9, 8, 9, 5, 4, 7, 2, 9, 9, 8, 12, 9, 9, -1, 1, -1, 1, 3, 8, 1, 8, 1, 1, 8, 9, 9, 8, 1, -1, 2, 2, 12, 8, 12, -1, 4, 7, 9, 8, 1, 10, -1, 8, 1, 12, -1, 1, 9, 1, 1, 1, 1, 1, 8, 2, 3, 9, -1, 10, 1, 8, 8, 9, 2, 2, 8, 9, 3, 3, 2, 2, 8, 8, -1, 9, 1, -1, 3, 12, 8, 9, -1, -1, 1, 1, 9, 1, 8, 2, 8, 1, 1, 9, 1, 8, 3, 3, 8, 10, 2, 3, 10, 2, 8, 3, 12, 1, 1, 12, 12, 1, 7, 8, 10, 1, 3, 2, 3, 1, 8, 7, 4, -1, 2, 8, 2, 8, 4, 2, 8, -1, 6, 10, 2, 1, 1, 8, -1, 9, 5, 8, -1, 1, 2, 8, 7, 9, 1, 2, 1, 8, 8, 1, 1, -1, 8, 8, 3, 8, 1, 1, 1, 9, 8, 2, 8, 8, 8, 8, -1, 7, 1, 2, 1, 8, 9, 10, 8, 9, 9, 10, 1, 1, 2, 1, 1, 8, 1, -1, 8, 10, 10, 8, 9, 2, 9, 1, 8, -1, 8, 7, 8, 1, -1, 1, -1, -1, 1, -1, 8, 1, -1, -1, 9, 1, -1, 2, -1, 1, 1, 8, 3, 1, 8, 7, 2, 8, 12, 1, 12, 8, 2, 1, -1, 1, 8, -1, 2, 12, 1, 3, -1, 9, 1, 8, -1, 1, 1, 2, 10, 1, 9, 1, 8, -1, 10, 2, 1, 1, 2, 2, 9, -1, 2, 1, 1, 1, -1, 8, -1, 1, 7, 9, 8, 1, 2, -1, 8, 8, 8, -1, 7, 12, 12, 8, 2, 1]\n",
            "[1, 9, -1, 1, 2, 10, 3, 6, 2, 1, 8, 4, 1, -1, 10, -1, 2, -1, 1, 2, 8, -1, -1, 4, 9, 10, 2, 8, -1, 8, 8, 1, 8, 1, -1, 1, 1, 9, 2, 3, 2, 1, 10, 2, 2, 8, 1, 3, -1, 8, 3, 12, 3, 7, 2, 8, 9, -1, 9, 9, -1, 3, 6, 2, 1, 1, 9, -1, 10, 1, 3, 8, 7, 1, 2, -1, -1, 4, 1, 3, 1, 9, 10, 1, 7, -1, -1, 12, 8, -1, 2, 8, -1, 2, 1, 1, 2, 10, 8, -1, 7, 3, -1, 4, 1, 9, 10, 4, 8, 9, -1, 2, 1, 1, 10, 9, -1, 3, 9, 1, 8, 8, 7, -1, 2, 9, 3, 2, 8, 2, 9, 1, 12, 3, 9, -1, 1, 1, 1, -1, 1, 1, 10, 9, -1, -1, 8, 3, 8, 9, 1, 1, -1, 7, 4, -1, -1, 10, -1, 1, 4, 8, 4, 4, -1, 1, 8, 9, 8, 7, 3, 5, -1, 10, -1, 10, -1, 9, -1, 7, 2, 9, 9, 3, -1, -1, 8, -1, 12, -1, 1, 10, 10, -1, -1, 7, 10, 12, 1, 9, 3, -1, 1, 3, 8, 8, 8, 9, -1, 8, 8, 2, 2, 9, 1, 1, -1, 10, 2, 2, 8, 9, 4, 2, 1, 7, 3, 2, 1, 2, -1, 1, -1, -1, 9, 2, 9, 1, 7, 3, 1, 8, 8, -1, 1, 1, 7, 10, 1, -1, 2, 2, 12, -1, 9, 3, 1, -1, -1, 2, 8, 5, 1, 2, 2, 10, 2, 10, 8, 8, 2, 2, 8, 2, 7, -1, 8, 2, -1, 3, 7, 1, 2, 12, 1, 1, 9, 12, -1, 3, -1, 7, -1, 1, 9, 2, 1, 5, 1, 3, 9, 1, 2, 1, -1, 8, 2, 3, 8, 12, 7, 1, 10, 5, 1, 1, 1, 8, -1, -1, -1, 9, 10, 4, 1, 7, 1, 9, 2, 1, 7, 5, 1, 1, 1, 3, 1, 8, 2, 8, -1, 8, -1, -1, 1, 10, 10, 1, 2, 8, 1, 3, 3, 8, 1, 2, 1, 1, 9, -1, -1, 12, 3, 1, -1, 8, 4, -1, 10, 2, -1, 1, 2, 2, 2, 3, 4, -1, 9, 1, 10, -1, 8, -1, -1, 9, 10, 9, 12, 1, -1, 8, -1, 7, 3, -1, 8, 8, 8, 1, 8, 2, 1, 3, 6, 2, -1, -1, 2, 2, 10, 3, 1, -1, 1, -1, 1, 7, 4, 1, 1, 1, 10, 12, -1, 3, -1, 8, 10, 1, 2, -1, -1, 1, 8, 9, 10, 1, 12, 3, 1, -1, 8, 7, 9, 1, -1, 8, 2, 9, 8, 8, 8, -1, -1, 1, 8, -1, -1, 6, 8, 8, 7, 1, 4, 1, 2, -1, -1, 6, -1, 1, 3, 1, 2, -1, -1, 10, 8, 1, -1, -1, 3, 9, 1, -1, 9, 9, -1, 9, 3, -1, 1, 9, -1, 1, 3, 1, 2, 9, 10, -1, 1, 8, -1, 1, 4, 1, 1, 9, 3, 3, 12, 10, 1, 1, 2, 8, -1, 3, 2, 5, -1, 1, -1, 9, 1, -1, 7, 1, -1, 9, 8, -1, 10, 8, 1, 8, -1, -1, -1, 6, 1, -1, -1, 1, 8, 9, 3, -1, 4, 8, 2, 1, 8, 9, 8, 9, 5, 4, 7, 2, 8, 9, 8, 12, 10, 7, -1, 1, -1, 1, 3, 8, 1, -1, 1, 1, 8, 9, 9, 10, 1, -1, 2, 2, 1, 10, 12, -1, 2, 7, 9, 8, 1, -1, -1, 8, 1, 8, -1, 1, 3, 1, 3, 1, 9, 1, 8, 2, 3, -1, -1, 7, 1, 8, 10, 9, 9, 2, 8, 9, 3, 3, 2, -1, 8, 10, -1, 1, 1, -1, 3, 10, 8, -1, -1, -1, 1, 1, -1, 1, 8, 2, 8, 1, 1, 9, 1, 9, 3, -1, 8, 10, 2, 3, 7, 2, 8, 3, 12, 4, 1, 12, -1, 1, 7, 8, 11, 1, 3, 4, 3, 1, 8, 2, 4, -1, -1, 8, 4, 8, 1, -1, 8, -1, 6, 10, 2, 9, 1, 7, -1, 9, 5, 8, -1, 1, 2, 8, 7, 10, 1, 2, 1, 8, 8, 1, 1, -1, 8, 9, 3, 8, 1, 1, 1, 3, 8, 2, 8, 8, 8, 8, -1, 7, -1, 2, 1, 8, 9, 10, 8, 2, -1, 9, -1, 1, 1, 1, 1, -1, 1, -1, 8, 10, 10, 8, 8, 2, -1, 1, 8, -1, 9, 7, 8, 1, -1, -1, -1, -1, 1, -1, 8, 1, -1, -1, 10, -1, -1, 2, -1, 1, 1, 3, 3, 1, -1, 7, 2, 8, 12, 1, 12, 9, 2, 1, -1, 1, 8, -1, 2, -1, 3, 3, -1, 9, 1, 8, -1, 1, 1, -1, 10, 3, -1, 3, 10, -1, 10, 9, 1, 1, 2, 2, 9, -1, 2, 1, 1, 1, -1, 9, -1, 1, 10, 9, 7, 1, 9, -1, 8, 9, 8, -1, 7, 12, 12, 2, 2, 1]\n",
            "[1, 9, -1, 1, 2, 8, 3, 6, 2, 1, -1, 4, 1, -1, 8, -1, 2, -1, 1, 2, 8, -1, -1, 4, 9, 10, 2, 8, -1, 8, 8, 1, 8, -1, -1, 4, 9, 2, 2, 2, -1, 1, 10, 2, 10, 8, -1, 3, -1, 8, 3, 12, 1, 7, 2, 9, 9, -1, 9, 9, -1, 3, 6, 2, 1, -1, 7, -1, 10, 1, 3, -1, 7, 1, 2, -1, -1, 8, 1, 3, 1, -1, 10, -1, 2, -1, -1, -1, 8, -1, 2, 8, -1, -1, -1, 1, 2, 10, 8, -1, -1, 3, -1, 2, 1, -1, 10, 4, 8, 9, -1, 2, 1, 1, 10, 9, -1, 3, 9, -1, 8, -1, 7, -1, 2, 1, 3, 2, 8, 2, 9, 1, 8, -1, 9, -1, 4, 1, 1, -1, 1, 1, 10, 9, -1, -1, 8, 3, -1, 9, 1, 1, -1, 2, 4, -1, -1, -1, -1, 2, 4, 9, 4, 4, -1, -1, 8, 9, 4, 7, 3, 5, -1, 10, -1, -1, -1, 9, -1, -1, 2, -1, 9, 3, -1, -1, 8, -1, 12, -1, -1, 10, 10, -1, -1, 9, -1, 12, 1, 9, 8, -1, 1, -1, 8, 8, 8, 9, -1, 8, -1, 2, 2, 9, 1, 1, -1, 10, 2, 2, 8, 9, 8, 2, 1, 7, 3, 2, 4, 2, -1, 1, -1, -1, 9, -1, 9, -1, 7, 3, 1, 8, -1, -1, 1, 1, 7, 8, 1, -1, 9, 2, 12, -1, 9, 2, 1, -1, -1, 2, 8, 3, 1, 2, 2, 2, 2, 10, -1, 8, 2, 9, 8, -1, 7, -1, 8, 2, -1, 3, 2, 1, 2, 12, 1, 1, 2, -1, -1, 3, -1, 7, -1, 1, 8, 2, 1, 5, 1, 3, 2, 1, 2, 10, -1, 8, 2, 1, 8, 12, 7, 1, 8, 5, 1, 1, 1, 8, -1, -1, -1, 2, 2, 4, 9, -1, 1, 10, 2, 1, -1, 5, 1, 1, 1, 3, -1, 8, 2, 8, -1, 8, -1, -1, -1, 8, 2, -1, 2, 8, 1, 3, 3, 9, 1, -1, 1, 1, 9, -1, -1, 12, 3, 4, -1, 8, 4, -1, -1, 2, -1, 1, -1, 2, 2, 3, 4, -1, 1, 1, 10, -1, -1, -1, -1, 9, 8, -1, 12, 1, -1, -1, -1, 7, 3, -1, 8, 8, 8, 9, 9, 2, 1, 3, 2, 2, -1, -1, 2, 2, 10, 2, 1, -1, 1, -1, 1, 7, 5, -1, 1, 2, 10, 12, -1, 3, -1, 8, 10, 1, 2, -1, -1, 3, 8, 9, 10, 4, 12, 3, 1, -1, 4, 7, 9, 1, -1, -1, 2, 9, 8, 8, 8, -1, -1, 1, 8, -1, -1, 2, -1, 8, 7, 1, 4, 1, 2, -1, -1, 6, -1, 1, 3, 3, 4, -1, -1, 8, -1, 1, -1, -1, 10, 2, 1, -1, 9, 4, -1, -1, 1, -1, 1, 5, -1, 1, 3, 1, 2, 2, 10, -1, 8, -1, -1, 1, -1, 1, 2, 9, 10, 3, 12, 8, 1, 1, 2, 8, -1, 3, 2, 5, -1, 1, -1, 7, 1, -1, 10, 1, -1, 9, -1, -1, 8, 8, 1, 8, -1, -1, -1, 10, 1, -1, -1, 1, 8, 9, 4, -1, 8, -1, 2, 1, 8, 9, 8, 9, 5, 8, 2, 2, 8, 9, 8, -1, 9, 9, -1, 1, -1, 1, 3, 8, 1, -1, 1, -1, 8, 9, 9, 8, 1, -1, 2, -1, 1, 8, 12, -1, 2, 2, 9, 10, 1, -1, -1, 9, 1, 8, -1, 1, -1, 1, 3, 1, 1, 1, 8, 2, 3, -1, -1, 10, 1, 8, 10, 9, 9, 2, 8, 9, 3, 3, 4, -1, 8, 4, -1, 9, 1, -1, -1, 8, 8, -1, -1, -1, 1, 1, -1, 1, -1, 2, 9, -1, 1, 9, 1, 8, 3, -1, 8, 9, 2, 3, 10, 2, 8, 3, 12, 4, 10, 12, -1, 1, 7, 9, 11, 1, 3, 8, 2, 1, 8, 2, -1, -1, -1, 8, 2, -1, 1, -1, 8, -1, 6, 10, 2, 9, 1, 7, -1, 9, 5, 8, -1, 1, 2, 8, -1, 9, 1, -1, 1, 8, 9, 1, 1, -1, 8, 4, 3, 8, 1, 1, 1, 3, 8, 2, 8, 8, 9, 8, -1, 7, -1, 2, 1, -1, 9, 10, 10, 9, -1, 9, -1, 1, 2, 1, 1, -1, 1, -1, 8, 10, 10, 8, 8, 2, -1, 1, 8, -1, 9, 7, 8, -1, -1, -1, -1, -1, 1, -1, 8, 1, -1, -1, 10, -1, -1, 2, -1, 1, 1, 3, 3, 4, -1, 7, -1, -1, 8, -1, -1, 9, 2, -1, -1, 1, 8, -1, 2, -1, 1, 3, -1, 9, -1, 8, -1, 1, 1, -1, 10, 1, -1, 1, 8, -1, 10, 9, 1, 1, 2, 2, -1, -1, 2, 9, 1, 1, -1, 4, -1, 1, 9, 9, 9, 1, 2, -1, 2, 9, 8, -1, 7, 12, 12, 2, -1, 1]\n",
            "[1, 8, -1, 1, 2, 8, 3, 6, -1, 1, -1, 4, 1, -1, 10, -1, 2, -1, 1, 2, -1, -1, -1, 4, 2, 10, 2, 8, -1, 8, 8, 1, 8, -1, -1, 1, 1, 2, 2, 3, -1, 1, 10, -1, 2, 8, -1, 3, -1, -1, 3, 12, 1, 7, 2, -1, -1, -1, 9, 9, -1, 1, 6, 2, 1, -1, 2, -1, -1, -1, 3, -1, -1, 1, 2, -1, -1, -1, 1, 3, 1, -1, 10, -1, 7, -1, -1, -1, -1, -1, 2, 8, -1, -1, -1, 1, -1, 10, 8, -1, -1, 3, -1, 4, 1, -1, -1, 4, 8, 5, -1, 2, 1, 1, 10, 8, -1, 3, 2, -1, 8, -1, -1, -1, 2, 1, 3, 2, 8, 2, 9, 1, 8, -1, 9, -1, 1, 1, 1, -1, -1, 1, 10, 9, -1, -1, 9, 3, -1, 9, 2, 1, -1, -1, -1, -1, -1, -1, -1, 2, -1, 8, 4, -1, -1, -1, 8, 9, 4, 7, 3, 5, -1, 2, -1, -1, -1, 9, -1, -1, 3, -1, 9, 4, -1, -1, 8, -1, -1, -1, -1, 10, 9, -1, -1, -1, -1, 12, 1, 9, 3, -1, 1, -1, -1, -1, 8, 9, -1, 8, -1, 2, 2, 9, 1, 1, -1, 8, 2, 2, 8, 9, 8, 2, 1, 7, 1, -1, -1, 2, -1, 1, -1, -1, 9, -1, 9, -1, 7, 3, 1, 8, -1, -1, 1, 1, 7, 8, 1, -1, 9, 2, 3, -1, 9, 3, -1, -1, -1, 2, 8, 1, 1, 3, 2, -1, 2, 10, -1, 8, -1, 2, 8, -1, 7, -1, 8, 2, -1, -1, 7, 1, 2, 8, 1, -1, 9, -1, -1, 3, -1, 7, -1, 1, 9, 2, 1, 5, 1, 3, -1, 1, 2, 1, -1, 8, -1, 1, 8, 12, 7, 1, -1, 5, 1, 1, -1, 8, -1, -1, -1, 9, 2, -1, 1, -1, -1, 8, 2, -1, -1, 5, 1, 1, -1, -1, -1, 8, 2, 8, -1, 7, -1, -1, -1, 10, 6, -1, 2, 8, 1, 3, 3, 8, 1, -1, 1, 1, 9, -1, -1, 12, 3, 1, -1, 8, -1, -1, -1, 2, -1, 1, -1, 2, 10, 3, 4, -1, 1, 1, 10, -1, -1, -1, -1, 9, 8, -1, 12, 1, -1, -1, -1, 7, 3, -1, 8, 8, 8, 1, 9, 2, 1, 3, 6, 2, -1, -1, 2, 1, 8, 3, 1, -1, 1, -1, 1, 3, 5, -1, 8, 1, 10, 12, -1, 1, -1, 8, 10, 1, 2, -1, -1, 1, 8, 9, 10, 1, 12, -1, 1, -1, 4, 7, 9, 1, -1, -1, -1, 9, 3, 4, 8, -1, -1, 1, -1, -1, -1, 6, -1, 6, 7, 1, 4, -1, 2, -1, -1, 6, -1, -1, 3, 4, 1, -1, -1, 8, -1, 1, -1, -1, 9, 9, 1, -1, 8, 4, -1, -1, 1, -1, 4, 5, -1, 1, 1, 1, 2, -1, 10, -1, 1, -1, -1, 1, -1, 1, -1, 9, 3, 3, 12, 10, 1, 1, 3, -1, -1, 3, 2, 5, -1, 1, -1, 9, 1, -1, 7, 1, -1, 9, -1, -1, 8, 8, -1, -1, -1, -1, -1, 6, 1, -1, -1, 1, 8, 9, 3, -1, 4, -1, 2, -1, 8, 9, 8, 10, 5, 4, 7, 2, 8, 9, 8, -1, -1, 9, -1, 1, -1, 1, 3, 8, 1, -1, 1, -1, 8, 9, 9, 9, 1, -1, 2, -1, 1, 10, 12, -1, 4, -1, 3, 8, 1, -1, -1, 9, 1, 8, -1, -1, -1, 1, 3, 1, 1, 1, 8, 2, 3, -1, -1, -1, -1, 8, 10, 9, 9, 2, 8, -1, 3, 3, 4, -1, 8, 10, -1, 1, -1, -1, -1, 10, 8, -1, -1, -1, 1, 1, -1, -1, -1, 1, 9, -1, 1, -1, 1, 8, 3, -1, 8, 9, 2, 3, 10, -1, 9, 3, 12, 1, 1, -1, -1, 1, 7, 8, 10, 1, 3, 2, 3, 1, 8, 2, -1, -1, -1, 8, 4, -1, 1, -1, 8, -1, 6, -1, 2, -1, 1, 7, -1, -1, 5, 8, -1, 1, 2, 8, -1, 3, 1, -1, 1, -1, 3, 1, 1, -1, 8, 4, 3, 8, 1, 1, 1, 9, 8, 2, 8, 8, 9, 8, -1, -1, -1, 2, 1, -1, 9, 10, 8, -1, -1, 9, -1, 1, 2, 1, 1, -1, 1, -1, 8, 10, 10, 8, 10, 2, -1, 1, 8, -1, 9, -1, 8, -1, -1, -1, -1, -1, 1, -1, -1, 1, -1, -1, 10, -1, -1, 2, -1, 1, -1, 3, 3, -1, -1, 7, -1, -1, 8, -1, -1, 9, 2, -1, -1, 1, -1, -1, 2, -1, 1, 3, -1, -1, -1, 8, -1, 1, 1, -1, 10, 1, -1, -1, 9, -1, 9, 9, 1, 1, -1, 2, -1, -1, 8, 1, 1, 1, -1, 8, -1, 1, 9, 9, 8, 1, 9, -1, 8, 9, 8, -1, 7, 12, 12, -1, -1, -1]\n",
            "[1, 8, -1, 1, 2, 8, 3, 6, -1, 1, -1, 4, 1, -1, 10, -1, 2, -1, 1, 2, -1, -1, -1, 4, 2, 8, 2, 8, -1, 8, 8, 1, 8, -1, -1, -1, -1, 2, 2, 3, -1, 1, 9, -1, -1, -1, -1, 3, -1, -1, 3, 12, 1, 7, 2, -1, -1, -1, 9, 9, -1, 3, -1, 2, 1, -1, 9, -1, -1, -1, 3, -1, -1, 1, 2, -1, -1, -1, -1, 6, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 2, 8, -1, -1, -1, 1, -1, 3, 8, -1, -1, -1, -1, 4, 1, -1, -1, 4, 8, -1, -1, 2, 1, 1, 10, 9, -1, 3, -1, -1, 8, -1, -1, -1, 2, 9, 3, -1, 8, 2, 9, 1, -1, -1, 9, -1, 4, 1, 1, -1, -1, -1, 9, 9, -1, -1, 8, 3, -1, 9, 1, 1, -1, -1, -1, -1, -1, -1, -1, 2, -1, 8, -1, -1, -1, -1, 8, 9, 4, 7, 3, 5, -1, 2, -1, -1, -1, 9, -1, -1, 2, -1, 9, -1, -1, -1, 8, -1, -1, -1, -1, 8, 9, -1, -1, -1, -1, -1, 1, -1, 3, -1, 1, -1, -1, -1, -1, 9, -1, 8, -1, -1, 2, 9, 1, 8, -1, 8, -1, 2, 8, 9, 8, 2, -1, 7, 1, -1, -1, 2, -1, 1, -1, -1, 9, -1, -1, -1, -1, 3, 1, 9, -1, -1, 1, -1, 7, 8, 1, -1, 2, 2, 3, -1, -1, 3, -1, -1, -1, 2, 8, 1, 1, 2, 2, -1, 2, 8, -1, -1, -1, 9, 8, -1, -1, -1, 8, 2, -1, -1, -1, 12, 2, 8, 1, -1, 4, -1, -1, 3, -1, 7, -1, -1, 8, 2, 4, 5, -1, -1, -1, 1, 2, 9, -1, 8, -1, 3, 8, -1, -1, 1, -1, 5, 1, 1, -1, 8, -1, -1, -1, 9, 2, -1, 1, -1, -1, 8, 9, -1, -1, 5, 1, 1, -1, -1, -1, 8, 2, 8, -1, 8, -1, -1, -1, -1, 8, -1, -1, -1, -1, 3, 3, 9, 1, -1, 1, 1, 9, -1, -1, 12, 3, -1, -1, 8, -1, -1, -1, 2, -1, 1, -1, -1, -1, 3, 4, -1, 1, 1, 10, -1, -1, -1, -1, 9, 8, -1, 12, 1, -1, -1, -1, 7, -1, -1, 8, 8, -1, 1, 9, 2, -1, 3, 6, 2, -1, -1, 2, 8, 8, 2, 1, -1, 1, -1, 1, -1, 9, -1, 1, 1, 8, 12, -1, -1, -1, 8, 10, -1, 2, -1, -1, 3, -1, 9, 10, 1, 8, -1, 1, -1, 8, -1, -1, -1, -1, -1, -1, 9, 8, 8, 8, -1, -1, 1, -1, -1, -1, 6, -1, -1, 7, -1, 10, -1, 2, -1, -1, 6, -1, -1, 3, 9, 2, -1, -1, 8, -1, 1, -1, -1, 9, 2, 1, -1, -1, 9, -1, -1, 1, -1, 4, 5, -1, 1, -1, 1, 2, -1, 7, -1, 1, -1, -1, 1, -1, 1, -1, 9, 3, 1, 12, 10, 1, 1, 8, -1, -1, 3, 2, 5, -1, 1, -1, 9, 1, -1, 8, -1, -1, 9, -1, -1, 8, 8, -1, -1, -1, -1, -1, 6, 1, -1, -1, 1, 8, 9, 9, -1, 8, -1, 2, -1, -1, -1, 8, 9, 5, 4, 7, 2, 8, -1, 8, -1, -1, 9, -1, 1, -1, 1, 3, 8, 1, -1, 1, -1, 8, 9, 9, 8, 1, -1, 2, -1, 5, 8, 12, -1, 2, -1, 9, 8, 1, -1, -1, 9, 1, 8, -1, -1, -1, 1, 3, 4, 1, 1, 8, 2, 3, -1, -1, -1, -1, 8, -1, -1, -1, 2, 8, -1, 8, 3, -1, -1, 8, 8, -1, 4, -1, -1, -1, 8, 8, -1, -1, -1, 1, 1, -1, -1, -1, 2, 8, -1, 1, -1, 1, 9, 3, -1, 8, 9, 2, 3, 10, -1, 8, 3, 12, 1, 10, -1, -1, 1, 7, 8, 11, 1, 3, -1, 3, 1, 8, 7, -1, -1, -1, -1, -1, -1, 4, -1, 8, -1, 6, -1, 2, -1, 1, -1, -1, -1, 5, 8, -1, -1, 2, 8, -1, 9, 1, -1, 1, -1, 3, 1, 1, -1, 8, 4, 3, 8, 1, 1, 1, 3, 8, 2, 8, -1, 9, 8, -1, -1, -1, 2, 1, -1, 9, -1, 8, -1, -1, 9, -1, 2, 2, 1, 1, -1, 1, -1, 8, 8, -1, 8, 8, 2, -1, -1, 8, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, 1, -1, -1, 9, -1, -1, 2, -1, 1, -1, 7, 3, -1, -1, 7, -1, -1, -1, -1, -1, 8, 2, -1, -1, 1, -1, -1, 2, -1, 1, 3, -1, -1, -1, 8, -1, 1, 1, -1, 10, 1, -1, -1, 9, -1, 9, 9, 1, 1, -1, 2, -1, -1, 2, 4, -1, -1, -1, 8, -1, 1, 10, 9, 8, 1, 9, -1, 8, 9, 8, -1, 7, -1, 12, -1, -1, -1]\n",
            "0.7589073634204275\n",
            "[1, 9, 3, 1, 2, 8, 3, 6, 2, 1, 8, 4, 1, 4, 1, 7, 2, 4, 1, 2, 8, 9, 9, 4, 2, 8, 2, 8, 1, 8, 8, 1, 8, 1, 3, 1, 1, 2, 2, 3, 2, 1, 8, 2, 2, 8, 9, 3, 4, 8, 3, 12, 3, 7, 2, 9, 9, 2, 9, 9, 3, 2, 6, 2, 1, 1, 2, 1, 10, 1, 3, 8, 7, 1, 2, 2, 9, 4, 1, 3, 1, 9, 10, 1, 7, 9, 9, 12, 8, 1, 2, 8, 1, 2, 1, 1, 2, 10, 8, 2, 7, 3, 11, 9, 1, 9, 10, 4, 8, 9, 6, 2, 1, 1, 10, 9, 3, 3, 9, 1, 8, 8, 7, 1, 2, 9, 3, 2, 8, 2, 9, 1, 8, 1, 9, 8, 1, 1, 1, 2, 1, 1, 8, 9, 1, 9, 8, 3, 8, 9, 2, 1, 3, 7, 2, 10, 3, 8, 8, 2, 9, 8, 8, 4, 12, 1, 8, 9, 8, 7, 3, 5, 10, 2, 8, 9, 3, 9, 12, 7, 2, 1, 9, 9, 9, 8, 9, 9, 12, 1, 1, 10, 8, 1, 2, 9, 10, 12, 1, 9, 3, 2, 1, 3, 8, 2, 9, 9, 2, 8, 8, 2, 2, 9, 1, 1, 2, 8, 2, 2, 8, 9, 4, 2, 1, 7, 1, 2, 1, 2, 2, 1, 9, 1, 9, 2, 9, 1, 7, 3, 1, 9, 8, 12, 1, 1, 7, 8, 1, 2, 2, 2, 12, 1, 9, 3, 1, 1, 9, 2, 8, 5, 1, 3, 2, 8, 2, 10, 8, 8, 2, 2, 8, 2, 7, 1, 8, 2, 2, 3, 9, 12, 2, 12, 1, 1, 9, 12, 1, 3, 2, 7, 3, 1, 9, 2, 1, 5, 1, 3, 2, 1, 2, 1, 8, 8, 2, 1, 8, 12, 10, 1, 10, 5, 1, 1, 1, 8, 9, 3, 9, 9, 10, 1, 1, 9, 1, 9, 2, 1, 9, 5, 1, 1, 1, 3, 9, 8, 2, 8, 1, 8, 7, 1, 1, 9, 8, 1, 2, 8, 1, 3, 3, 8, 1, 2, 1, 1, 9, 9, 1, 12, 3, 1, 2, 8, 4, 3, 1, 2, 9, 1, 2, 2, 2, 3, 4, 2, 1, 1, 10, 8, 8, 8, 1, 9, 8, 9, 12, 1, 9, 8, 9, 7, 9, 8, 8, 8, 8, 1, 8, 2, 1, 3, 6, 2, 3, 2, 2, 8, 10, 2, 1, 9, 1, 9, 1, 7, 9, 1, 8, 1, 10, 12, 9, 3, 2, 8, 10, 1, 2, 3, 9, 1, 8, 9, 10, 1, 12, 3, 1, 2, 8, 7, 9, 1, 2, 9, 2, 8, 8, 8, 8, 2, 8, 1, 8, 1, 1, 2, 8, 8, 7, 1, 4, 1, 2, 2, 5, 6, 12, 1, 3, 9, 2, 3, 1, 8, 8, 1, 9, 9, 9, 2, 1, 9, 9, 8, 8, 9, 1, 3, 1, 5, 2, 1, 3, 1, 2, 9, 10, 8, 2, 4, 1, 1, 4, 1, 1, 9, 10, 3, 12, 10, 1, 1, 2, 8, 1, 3, 2, 5, 9, 1, 3, 9, 1, 1, 10, 1, 1, 9, 8, 2, 8, 8, 1, 8, 1, 4, 8, 6, 1, 9, 1, 1, 8, 9, 3, 10, 10, 8, 2, 1, 8, 9, 8, 10, 5, 4, 7, 2, 8, 9, 8, 12, 9, 7, 9, 1, 2, 1, 3, 8, 1, 10, 1, 1, 8, 9, 2, 8, 1, 1, 2, 2, 1, 8, 12, 2, 2, 7, 2, 8, 1, 10, 2, 9, 1, 8, 11, 1, 1, 1, 3, 1, 1, 1, 8, 2, 3, 9, 2, 10, 1, 8, 8, 9, 2, 2, 8, 9, 3, 3, 2, 2, 8, 8, 2, 9, 1, 9, 1, 10, 8, 3, 3, 2, 1, 1, 3, 1, 8, 2, 9, 2, 1, 9, 1, 9, 3, 3, 8, 9, 2, 3, 10, 2, 8, 3, 12, 1, 1, 12, 12, 1, 7, 8, 11, 1, 3, 2, 3, 1, 8, 2, 4, 8, 2, 8, 2, 8, 1, 2, 8, 8, 6, 10, 2, 9, 1, 8, 9, 9, 5, 8, 1, 1, 2, 8, 7, 9, 1, 2, 1, 8, 8, 1, 1, 2, 8, 4, 3, 8, 1, 1, 1, 9, 8, 2, 8, 8, 9, 8, 1, 7, 1, 2, 1, 8, 9, 10, 8, 2, 9, 10, 8, 2, 2, 1, 1, 9, 1, 2, 8, 10, 10, 8, 8, 2, 9, 1, 8, 9, 9, 7, 8, 1, 8, 1, 1, 3, 1, 1, 8, 1, 8, 2, 10, 1, 1, 2, 3, 1, 1, 3, 3, 1, 8, 7, 2, 8, 8, 1, 4, 9, 2, 1, 7, 1, 8, 2, 2, 12, 3, 3, 8, 9, 1, 8, 8, 1, 1, 2, 10, 1, 9, 3, 10, 8, 10, 9, 1, 1, 2, 2, 9, 10, 2, 9, 1, 1, 9, 9, 2, 1, 10, 9, 8, 1, 2, 12, 2, 9, 8, 1, 7, 12, 12, 2, 2, 1]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "final_pred_1=[]\n",
        "\n",
        "num_correct=0\n",
        "model_0_1=load_model(loss,accuracy,optimizer,'0-1')\n",
        "mod_labels_0=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_0:\n",
        "    pred_test_0_1=model_0_1.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_0_1 = pred_test_0_1.argmax(axis=1)\n",
        "    mod_labels_0.append(pred_labels_0_1[0])\n",
        "  else:\n",
        "    mod_labels_0.append(-1)\n",
        "\n",
        "model_1_1=load_model(loss,accuracy,optimizer,'1-1')\n",
        "mod_labels_1=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        " \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_1:\n",
        "    pred_test_1_1=model_1_1.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_1_1 = pred_test_1_1.argmax(axis=1)\n",
        "    mod_labels_1.append(pred_labels_1_1[0])\n",
        "  else:\n",
        "    mod_labels_1.append(-1)\n",
        "\n",
        "model_2_1=load_model(loss,accuracy,optimizer,'2-1')\n",
        "mod_labels_2=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_2:\n",
        "    pred_test_2_1=model_2_1.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_2_1 = pred_test_2_1.argmax(axis=1)\n",
        "    mod_labels_2.append(pred_labels_2_1[0])\n",
        "  else:\n",
        "    mod_labels_2.append(-1)\n",
        "\n",
        "model_3_1=load_model(loss,accuracy,optimizer,'3-1')\n",
        "mod_labels_3=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_3:\n",
        "    pred_test_3_1=model_3_1.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_3_1 = pred_test_3_1.argmax(axis=1)\n",
        "    mod_labels_3.append(pred_labels_3_1[0])\n",
        "  else:\n",
        "    mod_labels_3.append(-1)\n",
        "\n",
        "model_4_1=load_model(loss,accuracy,optimizer,'4-1')\n",
        "mod_labels_4=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_4:\n",
        "    pred_test_4_1=model_4_1.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_4_1 = pred_test_4_1.argmax(axis=1)\n",
        "    mod_labels_4.append(pred_labels_4_1[0])\n",
        "  else:\n",
        "    mod_labels_4.append(-1)\n",
        "\n",
        "model_5_1=load_model(loss,accuracy,optimizer,'5-1')\n",
        "mod_labels_5=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_5:\n",
        "    pred_test_5_1=model_5_1.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_5_1 = pred_test_5_1.argmax(axis=1)\n",
        "    mod_labels_5.append(pred_labels_5_1[0])\n",
        "  else:\n",
        "    mod_labels_5.append(-1)\n",
        "\n",
        "print(mod_labels_0)\n",
        "print(mod_labels_1)\n",
        "print(mod_labels_2)\n",
        "print(mod_labels_3)\n",
        "print(mod_labels_4)\n",
        "print(mod_labels_5)\n",
        "\n",
        "\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  fin_labels=[]\n",
        "  if mod_labels_0[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  if mod_labels_1[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  if mod_labels_2[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  if mod_labels_3[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  if mod_labels_4[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  if mod_labels_5[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  res=max(set(fin_labels), key = fin_labels.count)\n",
        "  final_pred_1.append(res)\n",
        "\n",
        "  if res==new_val_label_0[i]:\n",
        "    num_correct=num_correct+1\n",
        "\n",
        "accuracy=num_correct/len(new_val_label_0)\n",
        "print(accuracy)\n",
        "  \n",
        "print(final_pred_1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhhwGEPul5A4",
        "outputId": "cae283d9-a2ce-48fb-ac07-02d6cff9e50b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7589073634204275\n",
            "Weighted F1: 0.7562997066198456\n",
            "Micro F1: 0.7589073634204275\n",
            "Weighted Precision: 0.7622027892195677\n",
            "Micro Precision: 0.7589073634204275\n",
            "Weighted Recall: 0.7589073634204275\n",
            "Micro Recall: 0.7589073634204275\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "\n",
        "accuracy=accuracy_score(new_val_label_0, final_pred_1)\n",
        "print(\"Accuracy: \"+str(accuracy))\n",
        "total_accuracy=total_accuracy+accuracy\n",
        "\n",
        "weighted_f1=f1_score(new_val_label_0,final_pred_1, average='weighted')\n",
        "print(\"Weighted F1: \"+ str(weighted_f1))\n",
        "total_weighted_f1=total_weighted_f1+weighted_f1\n",
        "micro_f1=f1_score(new_val_label_0,final_pred_1, average='micro')\n",
        "print(\"Micro F1: \"+ str(micro_f1))\n",
        "total_micro_f1=total_micro_f1+micro_f1\n",
        "\n",
        "weighted_precision=precision_score(new_val_label_0, final_pred_1, average='weighted')\n",
        "print(\"Weighted Precision: \" + str(weighted_precision))\n",
        "total_weighted_precision=total_weighted_precision+weighted_precision\n",
        "micro_precision=precision_score(new_val_label_0, final_pred_1, average='micro')\n",
        "print(\"Micro Precision: \" + str(micro_precision))\n",
        "total_micro_precision=total_micro_precision+micro_precision\n",
        "\n",
        "weighted_recall=recall_score(new_val_label_0, final_pred_1, average='weighted')\n",
        "print(\"Weighted Recall: \" + str(weighted_recall))\n",
        "total_weighted_recall=total_weighted_recall+weighted_recall\n",
        "micro_recall=recall_score(new_val_label_0, final_pred_1, average='micro')\n",
        "print(\"Micro Recall: \" + str(micro_recall))\n",
        "total_micro_recall=total_micro_recall+micro_recall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Pe7iq0ql_Vy",
        "outputId": "48ca14e8-1189-44ed-9e2a-2db25db821bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_14\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_29 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_30 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_29[0][0]',               \n",
            "                                thPoolingAndCrossAt               'input_30[0][0]']               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_14 (S  (None, 768)         0           ['tf_bert_model[14][0]']         \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " dense_28 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_14[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dropout_51 (Dropout)           (None, 512)          0           ['dense_28[0][0]']               \n",
            "                                                                                                  \n",
            " dense_29 (Dense)               (None, 15)           7695        ['dropout_51[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model: \"model_15\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_31 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_32 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_31[0][0]',               \n",
            "                                thPoolingAndCrossAt               'input_32[0][0]']               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_15 (S  (None, 768)         0           ['tf_bert_model[15][0]']         \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " dense_30 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_15[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dropout_52 (Dropout)           (None, 512)          0           ['dense_30[0][0]']               \n",
            "                                                                                                  \n",
            " dense_31 (Dense)               (None, 15)           7695        ['dropout_52[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model: \"model_16\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_33 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_34 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_33[0][0]',               \n",
            "                                thPoolingAndCrossAt               'input_34[0][0]']               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_16 (S  (None, 768)         0           ['tf_bert_model[16][0]']         \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " dense_32 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_16[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dropout_53 (Dropout)           (None, 512)          0           ['dense_32[0][0]']               \n",
            "                                                                                                  \n",
            " dense_33 (Dense)               (None, 15)           7695        ['dropout_53[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model: \"model_17\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_35 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_36 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_35[0][0]',               \n",
            "                                thPoolingAndCrossAt               'input_36[0][0]']               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_17 (S  (None, 768)         0           ['tf_bert_model[17][0]']         \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " dense_34 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_17[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dropout_54 (Dropout)           (None, 512)          0           ['dense_34[0][0]']               \n",
            "                                                                                                  \n",
            " dense_35 (Dense)               (None, 15)           7695        ['dropout_54[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model: \"model_18\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_37 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_38 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_37[0][0]',               \n",
            "                                thPoolingAndCrossAt               'input_38[0][0]']               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_18 (S  (None, 768)         0           ['tf_bert_model[18][0]']         \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " dense_36 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_18[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dropout_55 (Dropout)           (None, 512)          0           ['dense_36[0][0]']               \n",
            "                                                                                                  \n",
            " dense_37 (Dense)               (None, 15)           7695        ['dropout_55[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model: \"model_19\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_39 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_40 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_39[0][0]',               \n",
            "                                thPoolingAndCrossAt               'input_40[0][0]']               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_19 (S  (None, 768)         0           ['tf_bert_model[19][0]']         \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " dense_38 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_19[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dropout_56 (Dropout)           (None, 512)          0           ['dense_38[0][0]']               \n",
            "                                                                                                  \n",
            " dense_39 (Dense)               (None, 15)           7695        ['dropout_56[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "[1, 8, 3, 1, 2, 8, 3, 6, 4, 1, 8, 4, 1, 4, 1, 7, 2, 4, 9, 2, 8, 9, 9, 4, 2, 10, 2, 8, 1, 8, 8, 1, 8, 1, 7, 1, 9, 2, 2, 3, 9, 1, 9, 2, 10, 8, 1, 3, 4, 8, 3, 12, 3, 7, 2, 8, 9, 1, 9, 9, 3, 2, 6, 2, 1, 1, 8, 1, 10, 1, 3, 8, 7, 1, 2, 2, 9, 4, 1, 3, 1, 11, 10, 1, 7, 9, 9, 12, 8, 1, 2, 8, 1, 2, 1, 1, 2, 10, 8, 1, 7, 3, 11, 2, 1, 9, 9, 4, 8, 9, 9, 2, 1, 1, 10, 9, 3, 4, 9, 1, 8, 8, 7, 1, 2, 9, 3, 2, 8, 9, 9, 1, 9, 3, 9, 8, 1, 1, 1, 9, 1, 1, 9, 9, 1, 9, 8, 3, 8, 9, 2, 1, 3, 7, 4, 0, 9, 8, 8, 2, 9, 8, 4, 4, 12, 1, 8, 9, 4, 7, 3, 5, 10, 2, 8, 9, 3, 9, 12, 7, 3, 9, 1, 8, 10, 8, 8, 9, 12, 1, 1, 10, 10, 1, 2, 9, 11, 12, 1, 9, 3, 9, 1, 9, 8, 8, 8, 9, 9, 8, 8, 9, 2, 9, 1, 1, 2, 10, 2, 2, 8, 9, 8, 2, 1, 7, 1, 9, 4, 2, 2, 1, 9, 1, 9, 2, 9, 1, 7, 3, 1, 8, 8, 12, 1, 1, 7, 8, 1, 2, 2, 2, 12, 1, 9, 3, 1, 9, 9, 2, 8, 5, 1, 2, 9, 1, 2, 8, 8, 8, 2, 9, 8, 2, 7, 1, 8, 9, 2, 3, 1, 12, 2, 8, 1, 1, 9, 12, 4, 3, 2, 7, 3, 1, 9, 2, 1, 5, 1, 3, 9, 1, 2, 1, 8, 8, 2, 1, 8, 12, 7, 1, 8, 5, 1, 1, 1, 8, 9, 3, 9, 9, 10, 4, 1, 9, 2, 10, 2, 1, 9, 5, 1, 1, 1, 3, 9, 8, 2, 8, 1, 8, 7, 3, 1, 10, 10, 1, 2, 8, 1, 3, 3, 8, 1, 2, 1, 1, 9, 8, 1, 12, 3, 1, 1, 8, 4, 3, 1, 2, 9, 1, 2, 2, 2, 3, 8, 9, 1, 1, 10, 8, 8, 10, 1, 9, 10, 9, 12, 1, 9, 8, 8, 7, 3, 8, 8, 8, 8, 1, 8, 2, 1, 3, 6, 2, 3, 3, 2, 2, 10, 3, 1, 3, 1, 9, 1, 7, 9, 1, 1, 2, 8, 12, 1, 1, 2, 8, 8, 1, 2, 3, 9, 1, 8, 9, 10, 1, 8, 3, 1, 2, 4, 7, 9, 1, 2, 9, 3, 7, 8, 8, 8, 0, 8, 1, 8, 1, 1, 6, 8, 8, 7, 1, 4, 1, 2, 1, 2, 6, 12, 1, 3, 1, 4, 3, 1, 8, 8, 1, 9, 9, 9, 9, 1, 9, 8, 4, 8, 9, 1, 3, 1, 2, 2, 1, 3, 1, 2, 9, 7, 8, 1, 4, 1, 1, 4, 1, 1, 9, 3, 3, 12, 10, 1, 1, 2, 8, 2, 3, 2, 5, 8, 1, 3, 7, 1, 1, 7, 1, 1, 9, 8, 2, 8, 8, 1, 8, 1, 8, 10, 6, 1, 9, 1, 1, 8, 8, 2, 10, 4, 8, 2, 1, 8, 9, 8, 2, 5, 4, 7, 2, 7, 9, 8, 12, 9, 8, 9, 1, 2, 1, 3, 8, 1, 10, 1, 1, 8, 9, 9, 8, 1, 1, 2, 2, 1, 8, 12, 9, 2, 7, 9, 1, 1, 9, 2, 8, 1, 8, 11, 1, 9, 1, 3, 1, 1, 1, 8, 2, 3, 8, 10, 7, 1, 10, 8, 9, 9, 2, 8, 9, 4, 3, 2, 2, 8, 4, 2, 1, 1, 9, 3, 8, 8, 3, 3, 9, 1, 1, 8, 1, 8, 2, 8, 1, 1, 9, 1, 8, 3, 3, 8, 9, 2, 3, 10, 3, 8, 3, 12, 1, 1, 12, 12, 1, 7, 8, 11, 1, 3, 2, 3, 1, 8, 7, 4, 8, 1, 8, 4, 8, 1, 2, 8, 8, 6, 10, 2, 9, 1, 7, 9, 9, 5, 8, 1, 1, 2, 8, 9, 3, 1, 2, 1, 8, 8, 1, 1, 1, 8, 4, 3, 8, 1, 1, 1, 8, 8, 2, 8, 8, 9, 8, 1, 7, 1, 2, 1, 8, 9, 10, 8, 4, 9, 9, 1, 1, 2, 1, 1, 8, 1, 2, 8, 10, 10, 8, 8, 2, 9, 1, 8, 2, 9, 7, 8, 1, 8, 1, 1, 3, 1, 1, 10, 1, 8, 1, 9, 1, 1, 2, 3, 1, 1, 9, 3, 1, 8, 7, 2, 8, 8, 1, 4, 9, 2, 1, 7, 1, 9, 9, 2, 12, 1, 3, 8, 9, 1, 8, 8, 1, 1, 2, 10, 1, 9, 1, 8, 2, 9, 2, 1, 1, 2, 2, 9, 10, 2, 9, 1, 1, 9, 9, 2, 1, 7, 9, 9, 1, 2, 12, 8, 9, 8, 1, 7, 12, 12, 8, 2, 1]\n",
            "[1, 8, -1, 1, 2, 8, 3, 6, 2, 1, 8, 4, 1, 4, 8, -1, 2, 4, 1, 2, 8, -1, -1, 4, 2, 8, 2, 8, 1, 8, 8, 1, 8, 1, -1, 1, 1, 2, 2, 3, 2, 1, 10, 2, 2, 8, 1, 3, 4, 8, 3, 12, 1, 7, 2, 8, 9, -1, 9, 9, -1, 2, 6, 2, 1, 1, 8, 1, 10, 1, 3, 8, 7, 2, 2, 2, -1, 2, 1, 3, 1, 11, 10, 1, 7, -1, -1, 12, 10, 1, 2, 8, 1, 2, 1, 1, 2, 10, 8, -1, 7, 3, 11, 2, 1, 9, 10, 4, 8, 5, -1, 2, 1, 1, 10, 8, 3, 3, 9, 1, 8, 8, 7, 1, 2, 9, 3, 2, 8, 2, 4, 1, 8, 9, 9, -1, 1, 1, 1, -1, 1, 1, 10, 9, 1, -1, 8, 3, 8, 9, 2, 1, 3, 7, 2, -1, -1, 8, 8, 2, 4, 8, 8, 4, -1, 1, 8, 9, 8, 7, 3, 5, 10, 2, 8, 9, -1, 9, 12, 7, 2, 1, 9, 3, -1, -1, 9, -1, 12, 1, 1, 8, 10, 1, -1, 9, 10, 12, 1, 9, 8, -1, 1, 3, 8, 8, 8, 9, -1, 8, 8, 2, 2, 9, 1, 1, -1, 10, 2, 2, 8, 9, 8, 2, 1, 7, 1, 2, 1, 2, -1, 1, -1, -1, 9, 2, 8, 1, 8, 3, 1, 8, 8, 12, 1, 1, 7, 8, 1, -1, 2, 2, 12, -1, 9, 3, 1, -1, -1, 2, 8, 1, 1, 3, 2, 8, 2, 8, 8, 8, 2, 2, 8, 2, 7, 1, 8, 2, 2, 3, 7, 12, 2, 8, 1, 1, 9, 12, 1, 3, -1, 7, -1, 1, 8, 2, 1, 5, 1, 3, 2, 1, 2, 1, -1, 8, 2, 3, 8, 12, 7, 1, 8, 5, 1, 1, 1, 8, -1, 3, -1, 9, 10, 1, 1, 8, 1, 8, 2, 1, 7, 5, 1, 1, 1, 3, 1, 8, 2, 8, 1, 8, 7, 1, 1, 8, 8, 1, 2, 8, 1, 3, 3, 8, 1, 2, 1, 1, 9, -1, -1, 12, 3, 1, -1, 8, 4, 3, 1, 2, -1, 1, 2, 2, 2, 3, 4, -1, 8, 1, 10, -1, 8, 8, 1, 9, 10, 1, 12, 1, -1, 8, -1, 7, 9, -1, 8, 8, 8, 1, 8, 2, 1, 3, 6, 2, -1, 2, 2, 2, 10, 3, 1, -1, 1, -1, 1, 7, 9, 1, 8, 2, 8, 12, -1, 1, 2, 8, 10, 1, 2, -1, -1, 1, 8, 9, 10, 1, 8, 3, 1, -1, 4, 7, 8, 1, -1, 8, 2, 8, 8, 8, 8, -1, -1, 1, 8, 1, -1, 2, 8, 8, 7, 1, 4, 1, 2, 2, 3, 6, -1, 1, 3, 4, 2, -1, -1, 8, 8, 1, -1, -1, 8, 9, 1, -1, 8, 9, 8, 9, 1, -1, 1, 5, 2, 1, 3, 1, 2, 9, 10, 8, 1, 4, 1, 1, 4, 1, 2, 9, 3, 3, 12, 8, 1, 1, 2, 8, -1, 3, 2, 5, 1, 1, -1, 9, 1, 1, 8, 1, -1, 9, 8, -1, 8, 8, 1, 8, -1, -1, -1, 6, 1, 9, -1, 1, 8, 9, 8, -1, 8, 8, 2, 1, 8, 9, 8, 9, 5, 8, 7, 2, 8, 9, 8, 12, 8, 8, -1, 1, -1, 1, 3, 8, 1, 8, 1, 1, 8, 9, 2, 8, 1, -1, 2, 2, 1, 8, 12, -1, 2, 7, 9, 8, 1, 10, -1, 8, 1, 8, -1, 1, 3, 1, 8, 1, 8, 1, 8, 2, 3, 8, -1, 7, 1, 8, 10, 9, 9, 2, 8, 9, 3, 3, 2, 2, 8, 8, -1, 1, 1, -1, 3, 10, 8, 9, -1, -1, 1, 1, 8, 1, 8, 1, 8, 2, 1, 8, 1, 8, 3, 3, 8, 10, 2, 3, 10, 2, 8, 3, 12, 1, 1, 12, 12, 1, 7, 8, 11, 1, 3, 2, 3, 1, 8, 2, 4, -1, 2, 8, 2, 8, 2, 2, 8, -1, 2, 10, 2, 2, 1, 8, -1, 9, 5, 8, -1, 1, 2, 8, 7, 9, 1, 2, 1, 8, 3, 1, 1, -1, 8, 8, 3, 8, 1, 1, 1, 9, 8, 2, 8, 8, 8, 8, -1, 7, 1, 2, 1, 8, 9, 10, 8, 10, 9, 10, 1, 2, 2, 1, 1, 8, 1, -1, 8, 10, 10, 8, 8, 2, 9, 1, 8, -1, 9, 7, 8, 1, -1, 1, -1, -1, 1, -1, 8, 1, -1, -1, 10, 1, -1, 2, -1, 1, 1, 9, 3, 1, 8, 7, 2, 8, 8, 8, 8, 8, 2, 1, -1, 1, 8, -1, 2, 12, 1, 3, -1, 9, 1, 8, -1, 1, 1, 2, 10, 1, 9, 1, 10, -1, 9, 2, 1, 1, 2, 2, 8, -1, 2, 4, 1, 1, -1, 8, -1, 1, 9, 9, 9, 1, 2, -1, 2, 8, 8, -1, 7, 12, 12, 8, 2, 1]\n",
            "[1, 8, -1, 1, 2, 8, 3, 6, 2, 1, 8, 4, 1, -1, 10, -1, 2, -1, 9, 2, 8, -1, -1, 4, 2, 10, 2, 8, -1, 8, 8, 1, 8, 1, -1, 1, 1, 2, 2, 3, 2, 1, 10, 2, 2, 8, 1, 3, -1, 8, 3, 12, 1, 7, 1, 8, 9, -1, 9, 9, -1, 3, 6, 2, 1, 1, 2, -1, 8, 1, 3, 8, 7, 1, 2, -1, -1, 4, 1, 6, 1, 11, 10, 1, 7, -1, -1, 12, 8, -1, 2, 8, -1, 2, 1, 1, 2, 10, 8, -1, 7, 3, -1, 2, 1, 9, 7, 4, 8, 9, -1, 2, 1, 1, 8, 8, -1, 3, 9, 1, 8, 8, 7, -1, 2, 9, 3, 2, 8, 2, 9, 1, 8, 3, 9, -1, 1, 1, 1, -1, 1, 1, 9, 9, -1, -1, 8, 3, 8, 9, 1, 1, -1, 8, 2, -1, -1, 8, -1, 2, 4, 8, 4, 4, -1, 1, 8, 9, 4, 7, 3, 5, -1, 2, -1, 9, -1, 9, -1, 7, 3, 1, 1, 8, -1, -1, 8, -1, 12, -1, 1, 8, 10, -1, -1, 9, 11, 12, 1, 1, 3, -1, 1, 3, 8, 8, 8, 9, -1, 8, 8, 2, 2, 9, 1, 2, -1, 8, 2, 2, 8, 9, 8, 2, 1, 7, 1, 2, 1, 2, -1, 1, -1, -1, 9, 2, 9, 1, 8, 3, 1, 8, 8, -1, 1, 1, 7, 8, 1, -1, 3, 2, 12, -1, 9, 3, 1, -1, -1, 2, 8, 3, 1, 2, 2, 10, 2, 8, 8, 8, 2, 2, 8, 2, 7, -1, 8, 2, -1, 3, 7, 12, 2, 8, 1, 1, 9, 8, -1, 3, -1, 7, -1, 1, 8, 2, 1, 5, 1, 3, 2, 1, 2, 1, -1, 8, 2, 3, 8, 12, 7, 1, 10, 5, 1, 1, 1, 8, -1, -1, -1, 2, 2, 1, 9, 9, 1, 8, 2, 1, 3, 5, 1, 1, 1, 3, 1, 8, 2, 8, -1, 8, -1, -1, 1, 9, 8, 1, 2, 8, 1, 3, 9, 9, 1, 2, 1, 1, 9, -1, -1, 12, 3, 1, -1, 8, 4, -1, 3, 2, -1, 1, 2, 2, 2, 3, 6, -1, 1, 1, 10, -1, 8, -1, -1, 9, 10, 9, 12, 1, -1, 8, -1, 7, 3, -1, 8, 8, 8, 1, 9, 2, 1, 3, 6, 2, -1, -1, 2, 2, 8, 3, 1, -1, 1, -1, 1, 7, 9, 1, 8, 2, 10, 12, -1, 1, -1, 8, 10, 1, 2, -1, -1, 5, 8, 9, 10, 1, 12, 3, 1, -1, 4, 8, 9, 1, -1, 9, 3, 8, 8, 8, 8, -1, -1, 1, 8, -1, -1, 2, 8, 8, 7, 1, 4, 1, 2, -1, -1, 6, -1, 1, 8, 3, 2, -1, -1, 8, 8, 1, -1, -1, 9, 2, 1, -1, 9, 4, -1, 9, 1, -1, 1, 9, -1, 1, 3, 1, 2, 2, 7, -1, 1, 8, -1, 1, 4, 1, 9, 9, 3, 3, 12, 8, 1, 1, 2, 8, -1, 3, 2, 5, -1, 1, -1, 9, 1, -1, 7, 1, -1, 9, 8, -1, 8, 8, 1, 8, -1, -1, -1, 6, 1, -1, -1, 1, 8, 9, 3, -1, 4, 8, 2, 1, 8, 9, 8, 2, 5, 4, 7, 2, 8, 9, 8, 8, 8, 7, -1, 1, -1, 1, 3, 8, 1, -1, 1, 1, 8, 9, 9, 8, 1, -1, 2, 2, 5, 8, 12, -1, 2, 6, 9, 10, 1, -1, -1, 8, 1, 8, -1, 1, 3, 1, 3, 1, 8, 1, 8, 2, 3, -1, -1, 10, 1, 8, 10, 9, 9, 2, 8, 9, 3, 3, 2, -1, 8, 4, -1, 9, 1, -1, 3, 8, 8, -1, -1, -1, 1, 1, -1, 1, 8, 2, 8, 1, 1, 8, 1, 8, 3, -1, 8, 9, 2, 3, 10, 2, 8, 3, 12, 1, 1, 12, -1, 1, 7, 8, 11, 1, 3, 8, 3, 1, 8, 2, 4, -1, -1, 8, 2, 8, 4, -1, 8, -1, 6, 8, 2, 9, 1, 8, -1, 9, 5, 8, -1, 1, 2, 8, 7, 3, 1, 2, 1, 8, 8, 1, 1, -1, 8, 4, 3, 8, 1, 1, 1, 9, 8, 2, 8, 8, 8, 8, -1, 7, -1, 2, 1, 8, 9, 8, 8, 2, -1, 9, -1, 1, 2, 1, 1, -1, 1, -1, 8, 10, 10, 8, 10, 2, -1, 1, 8, -1, 9, 7, 8, 1, -1, -1, -1, -1, 1, -1, 10, 1, -1, -1, 10, -1, -1, 2, -1, 1, 1, 9, 3, 1, -1, 7, 2, 8, 8, 1, 8, 8, 2, 1, -1, 1, 8, -1, 2, -1, 1, 3, -1, 9, 1, 8, -1, 1, 1, -1, 10, 1, -1, 1, 10, -1, 9, 9, 1, 1, 2, 2, 9, -1, 2, 1, 1, 1, -1, 9, -1, 1, 10, 9, 8, 1, 2, -1, 8, 9, 8, -1, 7, 12, 12, 8, 2, 1]\n",
            "[1, 8, -1, 1, 2, 8, 3, 6, 2, 1, -1, 4, 1, -1, 3, -1, 2, -1, 1, 2, 8, -1, -1, 4, 2, 10, 2, 8, -1, 8, 8, 1, 8, -1, -1, 4, 9, 2, 2, 3, -1, 1, 10, 2, 2, 8, -1, 3, -1, 8, 3, 12, 3, 7, 2, 9, 9, -1, 9, 9, -1, 3, 6, 2, 1, -1, 2, -1, 8, 1, 3, -1, 7, 1, 2, -1, -1, 10, 1, 3, 1, -1, 10, -1, 7, -1, -1, -1, 8, -1, 2, 8, -1, -1, -1, 1, 2, 10, 8, -1, -1, 3, -1, 4, 1, -1, 10, 4, 8, 9, -1, 2, 1, 1, 10, 8, -1, 3, 9, -1, 8, -1, 7, -1, 2, 1, 3, 2, 8, 2, 9, 1, 8, -1, 9, -1, 4, 1, 1, -1, 1, 1, 10, 9, -1, -1, 8, 3, -1, 9, 1, 1, -1, 7, 4, -1, -1, -1, -1, 2, 4, 4, 4, 4, -1, -1, 4, 9, 4, 7, 3, 5, -1, 2, -1, -1, -1, 9, -1, -1, 3, -1, 9, 4, -1, -1, 8, -1, 12, -1, -1, 10, 10, -1, -1, 9, -1, 12, 1, 9, 3, -1, 1, -1, 9, 8, 8, 9, -1, 8, -1, 2, 2, 9, 1, 1, -1, 8, 2, 2, 8, 9, 4, 2, 1, 7, 3, 2, 4, 2, -1, 1, -1, -1, 9, -1, 9, -1, 2, 3, 1, 9, -1, -1, 1, 1, 7, 8, 1, -1, 9, 2, 3, -1, 9, 3, 1, -1, -1, 2, 8, 2, 1, 3, 2, 2, 2, 8, -1, 8, 2, 2, 8, -1, 7, -1, 8, 2, -1, 3, 7, 12, 2, 8, 1, 1, 9, -1, -1, 3, -1, 7, -1, 1, 9, 2, 4, 5, 1, 3, 2, 1, 2, 1, -1, 8, 2, 3, 8, 12, 7, 4, 8, 5, 1, 1, 1, 8, -1, -1, -1, 1, 11, 4, 9, -1, 1, 8, 9, 1, -1, 5, 1, 1, 4, 3, -1, 4, 2, 8, -1, 8, -1, -1, -1, 9, 2, -1, 2, 8, 1, 3, 8, 8, 1, -1, 1, 1, 9, -1, -1, 12, 3, 4, -1, 8, 4, -1, -1, 2, -1, 1, -1, 2, 2, 3, 6, -1, 1, 1, 10, -1, -1, -1, -1, 9, 8, -1, 12, 1, -1, -1, -1, 7, 3, -1, 8, 8, 8, 1, 9, 2, 1, 3, 2, 2, -1, -1, 2, 2, 8, 3, 1, -1, 1, -1, 1, 7, 4, -1, 1, 2, 10, 12, -1, 3, -1, 8, 10, 1, 2, -1, -1, 1, 8, 9, 10, 4, 12, 3, 1, -1, 4, 7, 9, 1, -1, -1, 2, 8, 9, 8, 8, -1, -1, 1, 8, -1, -1, 6, -1, 8, 7, 1, 4, 1, 2, -1, -1, 6, -1, 4, 3, 3, 4, -1, -1, 2, -1, 1, -1, -1, 9, 9, 1, -1, 9, 4, -1, -1, 3, -1, 4, 5, -1, 1, 3, 1, 2, 2, 10, -1, 1, -1, -1, 1, -1, 1, 2, 9, 10, 3, 12, 10, 1, 1, 2, 8, -1, 3, 2, 5, -1, 1, -1, 9, 1, -1, 7, 1, -1, 9, -1, -1, 8, 8, 1, 8, -1, -1, -1, 6, 1, -1, -1, 1, 8, 9, 4, -1, 4, -1, 2, 1, 8, 9, 8, 9, 5, 4, 7, 2, 8, 9, 8, -1, 8, 9, -1, 1, -1, 1, 3, 8, 1, -1, 1, -1, 8, 9, 9, 8, 1, -1, 2, -1, 1, 8, 12, -1, 4, 7, 2, 8, 1, -1, -1, 9, 1, 8, -1, 1, -1, 1, 3, 1, 1, 1, 8, 2, 3, -1, -1, 10, 1, 4, 10, 9, 2, 2, 8, 9, 12, 3, 4, -1, 8, 4, -1, 4, 1, -1, -1, 8, 8, -1, -1, -1, 1, 1, -1, 1, -1, 2, 9, -1, 1, 8, 1, 9, 3, -1, 8, 9, 2, 3, 10, 2, 9, 3, 12, 4, 1, 12, -1, 1, 7, 9, 11, 1, 3, 4, 3, 1, 8, 2, -1, -1, -1, 8, 2, -1, 1, -1, 8, -1, 6, 10, 2, 9, 1, 7, -1, 9, 5, 8, -1, 4, 2, 8, -1, 9, 1, -1, 1, 8, 9, 1, 1, -1, 8, 4, 3, 8, 1, 1, 1, 3, 8, 2, 8, 8, 9, 8, -1, 7, -1, 2, 1, -1, 9, 10, 8, 4, -1, 9, -1, 2, 2, 1, 1, -1, 1, -1, 8, 10, 10, 8, 10, 2, -1, 1, 8, -1, 9, 7, 8, -1, -1, -1, -1, -1, 1, -1, 10, 1, -1, -1, 9, -1, -1, 2, -1, 1, 1, 3, 3, 1, -1, 7, -1, -1, 8, -1, -1, 8, 2, -1, -1, 1, 8, -1, 2, -1, 1, 3, -1, 9, -1, 8, -1, 1, 1, -1, 10, 3, -1, 1, 10, -1, 10, 9, 1, 1, 2, 2, -1, -1, 2, 4, 4, 1, -1, 9, -1, 4, 7, 9, 8, 1, 2, -1, 2, 9, 8, -1, 7, 12, 12, 8, -1, 1]\n",
            "[1, 9, -1, 1, 2, 9, 3, 6, -1, 1, -1, 4, 1, -1, 1, -1, 2, -1, 1, 2, -1, -1, -1, 4, 2, 8, 2, 8, -1, 9, 8, 1, 8, -1, -1, 1, 1, 2, 2, 3, -1, 1, 9, -1, 9, 8, -1, 3, -1, -1, 3, 4, 3, 7, 2, -1, -1, -1, 9, 9, -1, 2, 6, 2, 1, -1, 2, -1, -1, -1, 3, -1, -1, 1, 2, -1, -1, -1, 1, 3, 1, -1, 10, -1, 7, -1, -1, -1, -1, -1, 2, 8, -1, -1, -1, 1, -1, 3, 8, -1, -1, 3, -1, 4, 1, -1, -1, 4, 8, 9, -1, 2, 1, 1, 8, 9, -1, 3, 2, -1, 8, -1, -1, -1, 2, 1, 3, 2, 8, 2, 9, 1, 9, -1, 9, -1, 1, 1, 3, -1, -1, 1, 9, 9, -1, -1, 8, 3, -1, 9, 2, 1, -1, -1, -1, -1, -1, -1, -1, 2, -1, 8, 4, -1, -1, -1, 8, 9, 9, 7, 3, 5, -1, 2, -1, -1, -1, 9, -1, -1, 3, -1, 1, 3, -1, -1, 9, -1, -1, -1, -1, 8, 9, -1, -1, -1, -1, 12, 1, 9, 3, -1, 1, -1, -1, -1, 8, 9, -1, 8, -1, 2, 2, 9, 1, 1, -1, 8, 2, 2, 8, 9, 8, 2, 1, 7, 3, -1, -1, 2, -1, 1, -1, -1, 9, -1, 9, -1, 7, 3, 1, 8, -1, -1, 1, 1, 7, 8, 1, -1, 9, 2, 3, -1, 9, 3, -1, -1, -1, 2, 8, 1, 1, 3, 3, -1, 2, 10, -1, 8, -1, 2, 8, -1, 7, -1, 9, 2, -1, -1, 7, 1, 2, 8, 1, -1, 9, -1, -1, 3, -1, 7, -1, 1, 9, 2, 1, 5, 1, 3, -1, 1, 2, 1, -1, 8, -1, 3, 8, 12, 10, 1, -1, 5, 1, 1, -1, 8, -1, -1, -1, 9, 11, -1, 1, -1, -1, 9, 2, -1, -1, 5, 1, 1, -1, -1, -1, 8, 2, 8, -1, 7, -1, -1, -1, 9, 6, -1, 2, 8, 1, 3, 3, 9, 1, -1, 1, 1, 9, -1, -1, 12, 3, 1, -1, 8, -1, -1, -1, 2, -1, 1, -1, 2, 4, 3, 6, -1, 1, 1, 10, -1, -1, -1, -1, 9, 8, -1, 12, 1, -1, -1, -1, 7, 3, -1, 8, 8, 8, 1, 9, 2, 1, 3, 2, 2, -1, -1, 2, 2, 8, 3, 1, -1, 1, -1, 1, 7, 5, -1, 9, 1, 10, 12, -1, 1, -1, 8, 8, 1, 2, -1, -1, 1, 8, 9, 10, 1, 12, -1, 1, -1, 4, 7, 9, 1, -1, -1, -1, 9, 3, 8, 8, -1, -1, 1, -1, -1, -1, 6, -1, 8, 7, 1, 4, -1, 2, -1, -1, 6, -1, -1, 3, 3, 1, -1, -1, 8, -1, 1, -1, -1, 9, 9, 1, -1, 9, 4, -1, -1, 1, -1, 4, 5, -1, 1, 3, 1, 2, -1, 10, -1, 1, -1, -1, 1, -1, 1, -1, 9, 10, 3, 3, 8, 1, 1, 3, -1, -1, 3, 2, 5, -1, 1, -1, 9, 1, -1, 7, 1, -1, 9, -1, -1, 8, 8, -1, -1, -1, -1, -1, 6, 1, -1, -1, 1, 8, 9, 3, -1, 8, -1, 3, -1, 9, 9, 8, 9, 5, 4, 7, 2, 8, 9, 8, -1, -1, 9, -1, 1, -1, 1, 3, 8, 1, -1, 1, -1, 8, 9, 9, 8, 1, -1, 2, -1, 1, 10, 12, -1, 4, -1, 2, 8, 1, -1, -1, 9, 1, 8, -1, -1, -1, 1, 3, 1, 1, 1, 8, 2, 3, -1, -1, -1, -1, 8, 10, 9, 2, 2, 8, -1, 3, 3, 4, -1, 8, 10, -1, 1, -1, -1, -1, 8, 8, -1, -1, -1, 1, 1, -1, -1, -1, 2, 9, -1, 1, -1, 1, 8, 3, -1, 8, 9, 2, 3, 10, -1, 9, 3, 12, 1, 1, -1, -1, 1, 7, 8, 11, 1, 3, 9, 3, 1, 8, 2, -1, -1, -1, 8, 4, -1, 1, -1, 9, -1, 6, -1, 2, -1, 1, 7, -1, -1, 5, 8, -1, 1, 2, 8, -1, 3, 1, -1, 1, -1, 3, 1, 1, -1, 8, 4, 3, 8, 1, 1, 1, 9, 8, 2, 8, 8, 8, 8, -1, -1, -1, 2, 1, -1, 9, 10, 8, -1, -1, 9, -1, 1, 2, 1, 1, -1, 1, -1, 8, 10, 10, 8, 9, 2, -1, 1, 8, -1, 9, -1, 8, -1, -1, -1, -1, -1, 1, -1, -1, 1, -1, -1, 9, -1, -1, 2, -1, 1, -1, 3, 3, -1, -1, 7, -1, -1, 8, -1, -1, 9, 2, -1, -1, 1, -1, -1, 2, -1, 1, 3, -1, -1, -1, 8, -1, 1, 1, -1, 10, 1, -1, -1, 9, -1, 9, 9, 1, 1, -1, 2, -1, -1, 2, 9, 1, 1, -1, 9, -1, 1, 9, 9, 7, 1, 9, -1, 8, 9, 8, -1, 7, 12, 12, -1, -1, -1]\n",
            "[1, 8, -1, 1, 2, 9, 3, 6, -1, 1, -1, 4, 1, -1, 8, -1, 2, -1, 1, 2, -1, -1, -1, 4, 2, 8, 2, 8, -1, 8, 8, 1, 8, -1, -1, -1, -1, 2, 2, 3, -1, 1, 10, -1, -1, -1, -1, 3, -1, -1, 3, 12, 1, 7, 2, -1, -1, -1, 9, 4, -1, 3, -1, 2, 1, -1, 10, -1, -1, -1, 3, -1, -1, 1, 2, -1, -1, -1, -1, 6, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 2, 8, -1, -1, -1, 1, -1, 10, 8, -1, -1, -1, -1, 1, 1, -1, -1, 4, 8, -1, -1, 2, 1, 1, 10, 8, -1, 3, -1, -1, 8, -1, -1, -1, 2, 9, 3, -1, 8, 2, 9, 1, -1, -1, 9, -1, 1, 1, 1, -1, -1, -1, 9, 9, -1, -1, 8, 3, -1, 9, 2, 1, -1, -1, -1, -1, -1, -1, -1, 2, -1, 8, -1, -1, -1, -1, 8, 9, 4, 7, 3, 5, -1, 2, -1, -1, -1, 9, -1, -1, 2, -1, 9, -1, -1, -1, 9, -1, -1, -1, -1, 8, 9, -1, -1, -1, -1, -1, 1, -1, 7, -1, 1, -1, -1, -1, -1, 9, -1, 8, -1, -1, 2, 9, 1, 8, -1, 8, -1, 2, 8, 9, 4, 2, -1, 7, 1, -1, -1, 2, -1, 1, -1, -1, 8, -1, -1, -1, -1, 3, 1, 8, -1, -1, 1, -1, 7, 8, 1, -1, 2, 2, 3, -1, -1, 3, -1, -1, -1, 2, 10, 1, 1, 2, 9, -1, 2, 10, -1, -1, -1, 2, 8, -1, -1, -1, 8, 2, -1, -1, -1, 12, 2, 8, 1, -1, 9, -1, -1, 3, -1, 7, -1, -1, 8, 2, 1, 5, -1, -1, -1, 1, 2, 1, -1, 8, -1, 3, 8, -1, -1, 1, -1, 5, 1, 1, -1, 8, -1, -1, -1, 9, 2, -1, 1, -1, -1, 8, 9, -1, -1, 5, 1, 1, -1, -1, -1, 8, 2, 8, -1, 8, -1, -1, -1, -1, 6, -1, -1, -1, -1, 3, 10, 8, 1, -1, 1, 1, 9, -1, -1, 12, 3, -1, -1, 2, -1, -1, -1, 2, -1, 1, -1, -1, -1, 3, 8, -1, 1, 1, 10, -1, -1, -1, -1, 9, 10, -1, 12, 1, -1, -1, -1, 7, -1, -1, 8, 8, -1, 1, 8, 2, -1, 7, 2, 2, -1, -1, 2, 2, 8, 2, 1, -1, 1, -1, 1, -1, 4, -1, 1, 1, 10, 12, -1, -1, -1, 8, 10, -1, 2, -1, -1, 1, -1, 9, 10, 1, 12, -1, 1, -1, 8, -1, -1, -1, -1, -1, -1, 8, 8, 8, 8, -1, -1, 1, -1, -1, -1, 6, -1, -1, 7, -1, 8, -1, 2, -1, -1, 6, -1, -1, 3, 4, 4, -1, -1, 2, -1, 1, -1, -1, 9, 9, 1, -1, -1, 8, -1, -1, 1, -1, 1, 5, -1, 1, -1, 1, 2, -1, 10, -1, 1, -1, -1, 1, -1, 1, -1, 9, 3, 3, 12, 10, 1, 1, 2, -1, -1, 3, 2, 5, -1, 1, -1, 9, 1, -1, 10, -1, -1, 8, -1, -1, 8, 8, -1, -1, -1, -1, -1, 6, 1, -1, -1, 1, 9, 9, 3, -1, 4, -1, 2, -1, -1, -1, 8, 9, 5, 8, 7, 2, 8, -1, 8, -1, -1, 10, -1, 1, -1, 1, 3, 8, 1, -1, 1, -1, 8, 9, 9, 8, 1, -1, 2, -1, 1, 8, 12, -1, 2, -1, 2, 8, 1, -1, -1, 8, 1, 8, -1, -1, -1, 1, 8, 1, 8, 1, 8, 2, 3, -1, -1, -1, -1, 8, -1, -1, -1, 2, 8, -1, 3, 3, -1, -1, 8, 8, -1, 1, -1, -1, -1, 8, 8, -1, -1, -1, 1, 1, -1, -1, -1, 1, 8, -1, 1, -1, 1, 8, 3, -1, 8, 9, 2, 3, 10, -1, 8, 3, 12, 1, 1, -1, -1, 1, 7, 8, 11, 1, 3, -1, 3, 1, 8, 7, -1, -1, -1, -1, -1, -1, 4, -1, 9, -1, 6, -1, 2, -1, 1, -1, -1, -1, 5, 8, -1, -1, 2, 8, -1, 9, 1, -1, 1, -1, 9, 1, 1, -1, 8, 4, 3, 8, 1, 1, 1, 3, 8, 2, 8, -1, 9, 8, -1, -1, -1, 2, 1, -1, 9, -1, 8, -1, -1, 9, -1, 1, 2, 1, 1, -1, 1, -1, 8, 10, -1, 8, 9, 2, -1, -1, 8, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, 1, -1, -1, 9, -1, -1, 2, -1, 1, -1, 7, 3, -1, -1, 7, -1, -1, -1, -1, -1, 8, 2, -1, -1, 1, -1, -1, 2, -1, 1, 3, -1, -1, -1, 8, -1, 1, 1, -1, 10, 1, -1, -1, 8, -1, 9, 9, 1, 1, -1, 2, -1, -1, 2, 9, -1, -1, -1, 8, -1, 1, 10, 9, 7, 1, 9, -1, 8, 9, 8, -1, 7, -1, 12, -1, -1, -1]\n",
            "0.7600950118764845\n",
            "[1, 8, 3, 1, 2, 8, 3, 6, 4, 1, 8, 4, 1, 4, 1, 7, 2, 4, 9, 2, 8, 9, 9, 4, 2, 10, 2, 8, 1, 8, 8, 1, 8, 1, 7, 1, 9, 2, 2, 3, 9, 1, 9, 2, 10, 8, 1, 3, 4, 8, 3, 12, 3, 7, 2, 8, 9, 1, 9, 9, 3, 2, 6, 2, 1, 1, 8, 1, 10, 1, 3, 8, 7, 1, 2, 2, 9, 4, 1, 3, 1, 11, 10, 1, 7, 9, 9, 12, 8, 1, 2, 8, 1, 2, 1, 1, 2, 10, 8, 1, 7, 3, 11, 2, 1, 9, 9, 4, 8, 9, 9, 2, 1, 1, 10, 9, 3, 4, 9, 1, 8, 8, 7, 1, 2, 9, 3, 2, 8, 9, 9, 1, 9, 3, 9, 8, 1, 1, 1, 9, 1, 1, 9, 9, 1, 9, 8, 3, 8, 9, 2, 1, 3, 7, 4, 0, 9, 8, 8, 2, 9, 8, 4, 4, 12, 1, 8, 9, 4, 7, 3, 5, 10, 2, 8, 9, 3, 9, 12, 7, 3, 9, 1, 8, 10, 8, 8, 9, 12, 1, 1, 10, 10, 1, 2, 9, 11, 12, 1, 9, 3, 9, 1, 9, 8, 8, 8, 9, 9, 8, 8, 9, 2, 9, 1, 1, 2, 10, 2, 2, 8, 9, 8, 2, 1, 7, 1, 9, 4, 2, 2, 1, 9, 1, 9, 2, 9, 1, 7, 3, 1, 8, 8, 12, 1, 1, 7, 8, 1, 2, 2, 2, 12, 1, 9, 3, 1, 9, 9, 2, 8, 5, 1, 2, 9, 1, 2, 8, 8, 8, 2, 9, 8, 2, 7, 1, 8, 9, 2, 3, 1, 12, 2, 8, 1, 1, 9, 12, 4, 3, 2, 7, 3, 1, 9, 2, 1, 5, 1, 3, 9, 1, 2, 1, 8, 8, 2, 1, 8, 12, 7, 1, 8, 5, 1, 1, 1, 8, 9, 3, 9, 9, 10, 4, 1, 9, 2, 10, 2, 1, 9, 5, 1, 1, 1, 3, 9, 8, 2, 8, 1, 8, 7, 3, 1, 10, 10, 1, 2, 8, 1, 3, 3, 8, 1, 2, 1, 1, 9, 8, 1, 12, 3, 1, 1, 8, 4, 3, 1, 2, 9, 1, 2, 2, 2, 3, 8, 9, 1, 1, 10, 8, 8, 10, 1, 9, 10, 9, 12, 1, 9, 8, 8, 7, 3, 8, 8, 8, 8, 1, 8, 2, 1, 3, 6, 2, 3, 3, 2, 2, 10, 3, 1, 3, 1, 9, 1, 7, 9, 1, 1, 2, 8, 12, 1, 1, 2, 8, 8, 1, 2, 3, 9, 1, 8, 9, 10, 1, 8, 3, 1, 2, 4, 7, 9, 1, 2, 9, 3, 7, 8, 8, 8, 0, 8, 1, 8, 1, 1, 6, 8, 8, 7, 1, 4, 1, 2, 1, 2, 6, 12, 1, 3, 1, 4, 3, 1, 8, 8, 1, 9, 9, 9, 9, 1, 9, 8, 4, 8, 9, 1, 3, 1, 2, 2, 1, 3, 1, 2, 9, 7, 8, 1, 4, 1, 1, 4, 1, 1, 9, 3, 3, 12, 10, 1, 1, 2, 8, 2, 3, 2, 5, 8, 1, 3, 7, 1, 1, 7, 1, 1, 9, 8, 2, 8, 8, 1, 8, 1, 8, 10, 6, 1, 9, 1, 1, 8, 8, 2, 10, 4, 8, 2, 1, 8, 9, 8, 2, 5, 4, 7, 2, 7, 9, 8, 12, 9, 8, 9, 1, 2, 1, 3, 8, 1, 10, 1, 1, 8, 9, 9, 8, 1, 1, 2, 2, 1, 8, 12, 9, 2, 7, 9, 1, 1, 9, 2, 8, 1, 8, 11, 1, 9, 1, 3, 1, 1, 1, 8, 2, 3, 8, 10, 7, 1, 10, 8, 9, 9, 2, 8, 9, 4, 3, 2, 2, 8, 4, 2, 1, 1, 9, 3, 8, 8, 3, 3, 9, 1, 1, 8, 1, 8, 2, 8, 1, 1, 9, 1, 8, 3, 3, 8, 9, 2, 3, 10, 3, 8, 3, 12, 1, 1, 12, 12, 1, 7, 8, 11, 1, 3, 2, 3, 1, 8, 7, 4, 8, 1, 8, 4, 8, 1, 2, 8, 8, 6, 10, 2, 9, 1, 7, 9, 9, 5, 8, 1, 1, 2, 8, 9, 3, 1, 2, 1, 8, 8, 1, 1, 1, 8, 4, 3, 8, 1, 1, 1, 8, 8, 2, 8, 8, 9, 8, 1, 7, 1, 2, 1, 8, 9, 10, 8, 4, 9, 9, 1, 1, 2, 1, 1, 8, 1, 2, 8, 10, 10, 8, 8, 2, 9, 1, 8, 2, 9, 7, 8, 1, 8, 1, 1, 3, 1, 1, 10, 1, 8, 1, 9, 1, 1, 2, 3, 1, 1, 9, 3, 1, 8, 7, 2, 8, 8, 1, 4, 9, 2, 1, 7, 1, 9, 9, 2, 12, 1, 3, 8, 9, 1, 8, 8, 1, 1, 2, 10, 1, 9, 1, 8, 2, 9, 2, 1, 1, 2, 2, 9, 10, 2, 9, 1, 1, 9, 9, 2, 1, 7, 9, 9, 1, 2, 12, 8, 9, 8, 1, 7, 12, 12, 8, 2, 1]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "final_pred_2=[]\n",
        "\n",
        "num_correct=0\n",
        "model_0_2=load_model(loss,accuracy,optimizer,'0-2')\n",
        "mod_labels_0=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_0:\n",
        "    pred_test_0_2=model_0_2.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_0_2 = pred_test_0_2.argmax(axis=1)\n",
        "    mod_labels_0.append(pred_labels_0_2[0])\n",
        "  else:\n",
        "    mod_labels_0.append(-1)\n",
        "\n",
        "model_1_2=load_model(loss,accuracy,optimizer,'1-2')\n",
        "mod_labels_1=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        " \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_1:\n",
        "    pred_test_1_2=model_1_2.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_1_2 = pred_test_1_2.argmax(axis=1)\n",
        "    mod_labels_1.append(pred_labels_1_2[0])\n",
        "  else:\n",
        "    mod_labels_1.append(-1)\n",
        "\n",
        "model_2_2=load_model(loss,accuracy,optimizer,'2-2')\n",
        "mod_labels_2=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_2:\n",
        "    pred_test_2_2=model_2_2.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_2_2 = pred_test_2_2.argmax(axis=1)\n",
        "    mod_labels_2.append(pred_labels_2_2[0])\n",
        "  else:\n",
        "    mod_labels_2.append(-1)\n",
        "\n",
        "model_3_2=load_model(loss,accuracy,optimizer,'3-2')\n",
        "mod_labels_3=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_3:\n",
        "    pred_test_3_2=model_3_2.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_3_2 = pred_test_3_2.argmax(axis=1)\n",
        "    mod_labels_3.append(pred_labels_3_2[0])\n",
        "  else:\n",
        "    mod_labels_3.append(-1)\n",
        "\n",
        "model_4_2=load_model(loss,accuracy,optimizer,'4-2')\n",
        "mod_labels_4=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_4:\n",
        "    pred_test_4_2=model_4_2.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_4_2 = pred_test_4_2.argmax(axis=1)\n",
        "    mod_labels_4.append(pred_labels_4_2[0])\n",
        "  else:\n",
        "    mod_labels_4.append(-1)\n",
        "\n",
        "model_5_2=load_model(loss,accuracy,optimizer,'5-2')\n",
        "mod_labels_5=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_5:\n",
        "    pred_test_5_2=model_5_2.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_5_2 = pred_test_5_2.argmax(axis=1)\n",
        "    mod_labels_5.append(pred_labels_5_2[0])\n",
        "  else:\n",
        "    mod_labels_5.append(-1)\n",
        "\n",
        "print(mod_labels_0)\n",
        "print(mod_labels_1)\n",
        "print(mod_labels_2)\n",
        "print(mod_labels_3)\n",
        "print(mod_labels_4)\n",
        "print(mod_labels_5)\n",
        "\n",
        "\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  fin_labels=[]\n",
        "  if mod_labels_0[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  if mod_labels_1[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  if mod_labels_2[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  if mod_labels_3[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  if mod_labels_4[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  if mod_labels_5[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  res=max(set(fin_labels), key = fin_labels.count)\n",
        "  final_pred_2.append(res)\n",
        "\n",
        "  if res==new_val_label_0[i]:\n",
        "    num_correct=num_correct+1\n",
        "\n",
        "accuracy=num_correct/len(new_val_label_0)\n",
        "print(accuracy)\n",
        "  \n",
        "print(final_pred_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snuZ1OsZn3cC",
        "outputId": "aaa3a27e-fc5d-4426-a7fc-157a204bde3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7600950118764845\n",
            "Weighted F1: 0.7589719001292172\n",
            "Micro F1: 0.7600950118764844\n",
            "Weighted Precision: 0.761443928900564\n",
            "Micro Precision: 0.7600950118764845\n",
            "Weighted Recall: 0.7600950118764845\n",
            "Micro Recall: 0.7600950118764845\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "\n",
        "accuracy=accuracy_score(new_val_label_0, final_pred_2)\n",
        "print(\"Accuracy: \"+str(accuracy))\n",
        "total_accuracy=total_accuracy+accuracy\n",
        "\n",
        "weighted_f1=f1_score(new_val_label_0,final_pred_2, average='weighted')\n",
        "print(\"Weighted F1: \"+ str(weighted_f1))\n",
        "total_weighted_f1=total_weighted_f1+weighted_f1\n",
        "micro_f1=f1_score(new_val_label_0,final_pred_2, average='micro')\n",
        "print(\"Micro F1: \"+ str(micro_f1))\n",
        "total_micro_f1=total_micro_f1+micro_f1\n",
        "\n",
        "weighted_precision=precision_score(new_val_label_0, final_pred_2, average='weighted')\n",
        "print(\"Weighted Precision: \" + str(weighted_precision))\n",
        "total_weighted_precision=total_weighted_precision+weighted_precision\n",
        "micro_precision=precision_score(new_val_label_0, final_pred_2, average='micro')\n",
        "print(\"Micro Precision: \" + str(micro_precision))\n",
        "total_micro_precision=total_micro_precision+micro_precision\n",
        "\n",
        "weighted_recall=recall_score(new_val_label_0, final_pred_2, average='weighted')\n",
        "print(\"Weighted Recall: \" + str(weighted_recall))\n",
        "total_weighted_recall=total_weighted_recall+weighted_recall\n",
        "micro_recall=recall_score(new_val_label_0, final_pred_2, average='micro')\n",
        "print(\"Micro Recall: \" + str(micro_recall))\n",
        "total_micro_recall=total_micro_recall+micro_recall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Z-lAlHvoBj6",
        "outputId": "c61d600b-77a4-46eb-f547-e361408488f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_20\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_41 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_42 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_41[0][0]',               \n",
            "                                thPoolingAndCrossAt               'input_42[0][0]']               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_20 (S  (None, 768)         0           ['tf_bert_model[20][0]']         \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " dense_40 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_20[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dropout_57 (Dropout)           (None, 512)          0           ['dense_40[0][0]']               \n",
            "                                                                                                  \n",
            " dense_41 (Dense)               (None, 15)           7695        ['dropout_57[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model: \"model_21\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_43 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_44 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_43[0][0]',               \n",
            "                                thPoolingAndCrossAt               'input_44[0][0]']               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_21 (S  (None, 768)         0           ['tf_bert_model[21][0]']         \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " dense_42 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_21[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dropout_58 (Dropout)           (None, 512)          0           ['dense_42[0][0]']               \n",
            "                                                                                                  \n",
            " dense_43 (Dense)               (None, 15)           7695        ['dropout_58[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model: \"model_22\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_45 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_46 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_45[0][0]',               \n",
            "                                thPoolingAndCrossAt               'input_46[0][0]']               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_22 (S  (None, 768)         0           ['tf_bert_model[22][0]']         \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " dense_44 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_22[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dropout_59 (Dropout)           (None, 512)          0           ['dense_44[0][0]']               \n",
            "                                                                                                  \n",
            " dense_45 (Dense)               (None, 15)           7695        ['dropout_59[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model: \"model_23\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_47 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_48 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_47[0][0]',               \n",
            "                                thPoolingAndCrossAt               'input_48[0][0]']               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_23 (S  (None, 768)         0           ['tf_bert_model[23][0]']         \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " dense_46 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_23[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dropout_60 (Dropout)           (None, 512)          0           ['dense_46[0][0]']               \n",
            "                                                                                                  \n",
            " dense_47 (Dense)               (None, 15)           7695        ['dropout_60[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model: \"model_24\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_49 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_50 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_49[0][0]',               \n",
            "                                thPoolingAndCrossAt               'input_50[0][0]']               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_24 (S  (None, 768)         0           ['tf_bert_model[24][0]']         \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " dense_48 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_24[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dropout_61 (Dropout)           (None, 512)          0           ['dense_48[0][0]']               \n",
            "                                                                                                  \n",
            " dense_49 (Dense)               (None, 15)           7695        ['dropout_61[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model: \"model_25\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_51 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_52 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_51[0][0]',               \n",
            "                                thPoolingAndCrossAt               'input_52[0][0]']               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_25 (S  (None, 768)         0           ['tf_bert_model[25][0]']         \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " dense_50 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_25[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dropout_62 (Dropout)           (None, 512)          0           ['dense_50[0][0]']               \n",
            "                                                                                                  \n",
            " dense_51 (Dense)               (None, 15)           7695        ['dropout_62[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "[1, 8, 3, 1, 2, 8, 3, 6, 2, 1, 8, 4, 1, 4, 8, 7, 2, 4, 1, 2, 8, 9, 9, 4, 2, 10, 2, 8, 1, 8, 8, 1, 8, 1, 3, 1, 1, 2, 2, 3, 2, 1, 10, 2, 2, 8, 1, 3, 4, 8, 3, 12, 1, 7, 2, 9, 9, 1, 9, 9, 3, 2, 6, 2, 1, 1, 8, 1, 8, 1, 3, 8, 7, 1, 2, 2, 9, 10, 1, 3, 1, 11, 10, 1, 7, 9, 9, 12, 8, 1, 2, 8, 1, 2, 1, 1, 2, 10, 8, 2, 7, 3, 11, 1, 1, 9, 10, 4, 8, 9, 10, 2, 1, 1, 10, 9, 3, 3, 9, 1, 8, 8, 7, 1, 2, 9, 3, 2, 8, 2, 9, 1, 8, 3, 9, 8, 1, 1, 1, 2, 1, 1, 8, 9, 1, 9, 8, 3, 8, 9, 2, 1, 3, 7, 4, 11, 3, 8, 8, 2, 9, 8, 8, 4, 12, 1, 8, 9, 8, 7, 3, 5, 10, 2, 8, 9, 3, 9, 12, 7, 2, 1, 9, 3, 9, 8, 9, 9, 12, 1, 1, 10, 10, 1, 2, 9, 11, 12, 1, 9, 3, 2, 1, 3, 8, 8, 8, 9, 2, 8, 8, 2, 2, 9, 1, 1, 2, 8, 2, 2, 8, 9, 8, 2, 1, 7, 1, 2, 1, 2, 2, 1, 9, 1, 9, 2, 9, 1, 7, 3, 1, 9, 8, 12, 1, 1, 7, 8, 1, 2, 2, 2, 12, 1, 9, 3, 1, 1, 9, 2, 8, 1, 1, 3, 2, 8, 2, 8, 8, 8, 2, 2, 8, 2, 7, 1, 8, 2, 2, 3, 9, 12, 2, 12, 1, 1, 9, 12, 1, 3, 2, 7, 3, 9, 8, 2, 1, 5, 1, 3, 2, 1, 2, 1, 8, 8, 2, 2, 8, 12, 10, 1, 10, 5, 1, 1, 1, 8, 9, 3, 9, 9, 10, 2, 1, 9, 1, 8, 2, 1, 9, 5, 1, 1, 1, 3, 1, 8, 2, 8, 1, 7, 7, 1, 1, 8, 8, 1, 2, 8, 1, 3, 3, 8, 1, 2, 1, 1, 9, 8, 1, 12, 3, 1, 1, 8, 4, 3, 1, 2, 9, 1, 2, 2, 2, 3, 6, 2, 1, 1, 10, 8, 8, 8, 1, 9, 8, 9, 12, 1, 2, 8, 9, 7, 3, 8, 8, 8, 8, 1, 8, 2, 1, 3, 2, 2, 3, 2, 2, 2, 8, 2, 1, 9, 1, 9, 1, 3, 9, 1, 8, 1, 8, 12, 1, 1, 2, 8, 8, 1, 2, 3, 2, 12, 8, 9, 10, 1, 12, 3, 1, 2, 8, 7, 9, 1, 2, 9, 2, 10, 8, 8, 8, 2, 8, 1, 8, 1, 1, 2, 8, 8, 7, 1, 4, 1, 2, 2, 2, 6, 12, 1, 3, 3, 2, 3, 1, 8, 8, 1, 9, 3, 9, 2, 2, 9, 9, 9, 8, 9, 1, 3, 1, 5, 2, 1, 3, 1, 2, 9, 10, 8, 1, 4, 1, 1, 4, 1, 9, 9, 3, 3, 12, 8, 1, 1, 2, 8, 1, 3, 2, 5, 1, 1, 3, 9, 1, 1, 7, 1, 1, 9, 8, 2, 8, 8, 1, 8, 1, 8, 8, 6, 1, 10, 1, 1, 8, 9, 3, 10, 4, 8, 2, 1, 8, 9, 8, 10, 5, 4, 7, 2, 8, 9, 8, 12, 8, 8, 9, 1, 2, 1, 3, 8, 1, 10, 1, 1, 8, 9, 2, 8, 1, 1, 2, 2, 12, 8, 12, 2, 2, 7, 2, 8, 1, 8, 2, 9, 1, 8, 11, 1, 1, 1, 3, 1, 1, 1, 8, 2, 3, 9, 9, 10, 1, 8, 8, 9, 2, 2, 8, 9, 3, 3, 2, 2, 8, 4, 2, 9, 1, 9, 1, 12, 8, 9, 3, 2, 1, 1, 3, 1, 8, 2, 9, 1, 1, 8, 1, 9, 3, 3, 8, 9, 2, 3, 10, 2, 8, 3, 12, 1, 1, 12, 12, 1, 7, 8, 11, 1, 3, 2, 3, 1, 8, 2, 4, 9, 2, 8, 2, 8, 1, 2, 8, 8, 6, 8, 2, 9, 1, 7, 9, 9, 5, 8, 1, 1, 2, 8, 7, 3, 1, 2, 1, 8, 3, 1, 1, 2, 8, 8, 3, 8, 1, 1, 1, 9, 8, 2, 8, 8, 9, 8, 1, 7, 1, 2, 1, 8, 9, 10, 8, 2, 9, 10, 8, 1, 2, 1, 1, 9, 1, 2, 8, 10, 10, 8, 8, 2, 9, 1, 8, 2, 8, 7, 8, 1, 8, 1, 1, 3, 1, 1, 8, 1, 8, 2, 10, 1, 1, 2, 3, 1, 1, 3, 2, 1, 8, 7, 2, 8, 12, 1, 12, 8, 2, 1, 7, 1, 8, 2, 2, 12, 1, 3, 8, 9, 1, 8, 8, 1, 1, 2, 10, 1, 9, 2, 8, 8, 10, 9, 1, 1, 2, 2, 9, 10, 2, 1, 1, 1, 12, 4, 8, 1, 10, 9, 8, 1, 9, 12, 8, 9, 8, 1, 7, 12, 12, 8, 2, 1]\n",
            "[1, 9, -1, 1, 2, 8, 3, 6, 2, 1, 8, 4, 1, 4, 1, -1, 2, 4, 1, 2, 8, -1, -1, 10, 2, 8, 2, 8, 1, 8, 8, 1, 8, 1, -1, 1, 1, 2, 2, 3, 2, 1, 10, 2, 10, 8, 9, 3, 4, 8, 3, 12, 1, 7, 2, 9, 9, -1, 9, 9, -1, 2, 6, 2, 1, 1, 8, 1, 10, 1, 3, 8, 7, 1, 2, 2, -1, 10, 1, 3, 1, 11, 10, 1, 7, -1, -1, 12, 8, 1, 2, 8, 1, 2, 1, 1, 2, 10, 8, -1, 7, 3, 10, 2, 1, 9, 7, 1, 8, 9, -1, 2, 1, 1, 10, 9, 3, 3, 9, 1, 8, 8, 7, 1, 2, 9, 3, 2, 8, 2, 9, 1, 8, 9, 9, -1, 1, 1, 1, -1, 1, 1, 9, 9, 1, -1, 8, 3, 8, 9, 1, 1, 3, 7, 2, -1, -1, 8, 8, 2, 4, 9, 8, 4, -1, 1, 8, 9, 4, 7, 3, 5, 7, 2, 8, 9, -1, 9, 12, 7, 2, 1, 9, 8, -1, -1, 9, -1, 12, 1, 1, 8, 10, 1, -1, 9, 10, 12, 1, 9, 8, -1, 1, 3, 8, 8, 8, 9, -1, 8, 8, 2, 2, 9, 1, 1, -1, 10, 2, 2, 8, 9, 8, 2, 1, 9, 1, 2, 1, 2, -1, 1, -1, -1, 9, 2, 9, 1, 7, 3, 1, 8, 8, 12, 1, 1, 7, 8, 1, -1, 2, 2, 12, -1, 9, 2, 1, -1, -1, 2, 8, 1, 1, 2, 9, 2, 2, 8, 8, 8, 2, 2, 8, 2, 7, 1, 8, 2, 2, 3, 7, 12, 2, 12, 1, 1, 9, 12, 1, 3, -1, 7, -1, 1, 9, 2, 1, 5, 1, 3, 9, 1, 2, 1, -1, 8, 2, 2, 8, 12, 7, 1, 10, 5, 1, 1, 1, 8, -1, 3, -1, 9, 10, 1, 1, 9, 1, 10, 2, 1, 7, 5, 1, 1, 1, 3, 1, 8, 2, 8, 1, 8, 7, 3, 1, 9, 10, 1, 2, 8, 1, 3, 3, 9, 1, 2, 1, 1, 9, -1, -1, 12, 3, 1, -1, 8, 4, 3, 1, 2, -1, 1, 2, 2, 2, 3, 6, -1, 1, 1, 10, -1, 8, 8, 1, 9, 8, 9, 12, 1, -1, 8, -1, 7, 9, -1, 8, 8, 8, 1, 12, 2, 1, 3, 2, 2, -1, 1, 2, 2, 10, 3, 1, -1, 1, -1, 1, 7, 9, 1, 1, 1, 10, 12, -1, 1, 2, 8, 8, 1, 2, -1, -1, 1, 8, 9, 10, 1, 12, 3, 1, -1, 8, 7, 9, 1, -1, 9, 2, 8, 8, 8, 8, -1, -1, 1, 8, 1, -1, 2, 8, 8, 7, 1, 4, 1, 2, 9, 3, 6, -1, 1, 3, 3, 2, -1, -1, 8, 8, 1, -1, -1, 9, 9, 1, -1, 9, 4, 8, 9, 9, -1, 1, 5, 2, 1, 1, 1, 2, 2, 7, 8, 1, 1, 1, 1, 4, 1, 2, 9, 3, 3, 12, 10, 1, 1, 2, 8, -1, 3, 2, 5, 1, 1, -1, 7, 1, 1, 8, 1, -1, 9, 8, -1, 8, 8, 1, 8, -1, -1, -1, 6, 1, 9, -1, 1, 8, 9, 9, -1, 8, 8, 2, 1, 8, 9, 8, 9, 5, 4, 7, 2, 7, 9, 8, 12, 8, 10, -1, 1, -1, 1, 3, 8, 1, 8, 1, 1, 8, 9, 9, 8, 1, -1, 2, 2, 1, 8, 12, -1, 2, 7, 9, 8, 1, 8, -1, 8, 1, 8, -1, 1, 9, 1, 3, 1, 1, 1, 8, 2, 3, 9, -1, 7, 1, 4, 8, 9, 9, 2, 8, 9, 3, 3, 2, 2, 8, 4, -1, 1, 1, -1, 3, 8, 8, 9, -1, -1, 1, 1, 9, 1, 8, 1, 9, 1, 1, 8, 1, 9, 3, 3, 8, 9, 2, 3, 10, 2, 8, 3, 12, 1, 1, 12, 12, 1, 7, 8, 10, 1, 3, 2, 3, 1, 8, 2, 4, -1, 2, 8, 2, 8, 2, 2, 8, -1, 6, 8, 2, 9, 1, 7, -1, 9, 5, 8, -1, 1, 2, 8, 7, 9, 1, 2, 1, 8, 8, 1, 1, -1, 8, 4, 3, 8, 1, 1, 1, 9, 8, 2, 8, 8, 9, 8, -1, 7, 1, 2, 1, 8, 9, 10, 8, 9, 9, 10, 1, 1, 2, 1, 1, 8, 1, -1, 8, 10, 10, 8, 8, 2, 9, 1, 8, -1, 9, 7, 8, 1, -1, 1, -1, -1, 1, -1, 8, 1, -1, -1, 10, 1, -1, 2, -1, 1, 1, 1, 3, 1, 8, 7, 2, 8, 8, 1, 12, 9, 2, 1, -1, 1, 8, -1, 2, 12, 1, 3, -1, 9, 1, 8, -1, 1, 1, 2, 10, 1, 9, 1, 8, -1, 9, 2, 1, 1, 2, 2, 9, -1, 2, 9, 1, 1, -1, 9, -1, 1, 10, 9, 9, 1, 2, -1, 8, 9, 8, -1, 7, 12, 12, 8, 2, 1]\n",
            "[1, 8, -1, 1, 2, 8, 3, 6, 2, 1, 8, 4, 1, -1, 8, -1, 2, -1, 1, 2, 8, -1, -1, 4, 2, 8, 2, 8, -1, 10, 8, 1, 8, 1, -1, 1, 1, 2, 2, 3, 2, 1, 8, 2, 2, 8, 9, 3, -1, 8, 3, 12, 3, 7, 1, 8, 9, -1, 9, 9, -1, 3, 6, 2, 1, 1, 10, -1, 9, 1, 3, 8, 7, 1, 2, -1, -1, 10, 1, 3, 1, 11, 10, 1, 7, -1, -1, 12, 8, -1, 2, 8, -1, 2, 1, 12, 2, 3, 8, -1, 7, 3, -1, 2, 1, 9, 7, 4, 8, 3, -1, 2, 1, 1, 8, 8, -1, 3, 9, 1, 8, 8, 7, -1, 2, 9, 3, 2, 8, 2, 9, 4, 10, 3, 9, -1, 4, 1, 1, -1, 1, 1, 10, 9, -1, -1, 8, 3, 8, 9, 2, 1, -1, 7, 4, -1, -1, 10, -1, 2, 4, 8, 8, 4, -1, 1, 8, 9, 4, 7, 3, 5, -1, 10, -1, 9, -1, 9, -1, 7, 3, 1, 1, 3, -1, -1, 8, -1, 12, -1, 1, 10, 9, -1, -1, 9, 10, 12, 1, 9, 3, -1, 1, 3, 8, 8, 8, 9, -1, 8, 8, 2, 2, 9, 1, 8, -1, 8, 2, 2, 8, 9, 4, 2, 1, 7, 3, 2, 4, 2, -1, 1, -1, -1, 8, 2, 9, 1, 8, 3, 1, 9, 8, -1, 1, 1, 7, 8, 1, -1, 3, 2, 12, -1, 9, 3, 1, -1, -1, 2, 8, 3, 1, 3, 3, 2, 2, 8, 8, 8, 2, 2, 8, 2, 3, -1, 9, 2, -1, 3, 7, 12, 2, 8, 1, 1, 9, 12, -1, 3, -1, 7, -1, 1, 8, 2, 1, 5, 1, 3, 2, 1, 2, 9, -1, 8, 2, 3, 8, 12, 7, 1, 8, 5, 1, 1, 1, 8, -1, -1, -1, 9, 2, 4, 1, 8, 1, 9, 9, 1, 3, 5, 1, 1, 1, 3, 1, 8, 2, 8, -1, 8, -1, -1, 1, 8, 8, 1, 2, 8, 1, 3, 3, 8, 1, 2, 1, 1, 9, -1, -1, 12, 3, 4, -1, 8, 4, -1, 3, 2, -1, 1, 2, 2, 2, 3, 4, -1, 1, 1, 10, -1, 8, -1, -1, 9, 8, 1, 12, 1, -1, 8, -1, 7, 3, -1, 8, 8, 8, 1, 9, 2, 1, 3, 6, 2, -1, -1, 2, 2, 8, 3, 1, -1, 1, -1, 1, 7, 3, 4, 8, 2, 10, 12, -1, 3, -1, 8, 10, 1, 2, -1, -1, 3, 8, 9, 10, 4, 12, 3, 1, -1, 8, 8, 9, 1, -1, 8, 2, 9, 3, 8, 8, -1, -1, 1, 8, -1, -1, 2, 8, 8, 7, 1, 4, 1, 2, -1, -1, 6, -1, 1, 3, 3, 2, -1, -1, 8, 8, 1, -1, -1, 3, 8, 1, -1, 9, 8, -1, 4, 3, -1, 1, 5, -1, 1, 3, 1, 2, 9, 7, -1, 3, 8, -1, 1, 4, 1, 1, 9, 3, 3, 12, 8, 1, 1, 2, 8, -1, 3, 2, 5, -1, 1, -1, 9, 1, -1, 7, 1, -1, 9, 8, -1, 8, 8, 1, 8, -1, -1, -1, 6, 1, -1, -1, 1, 8, 9, 3, -1, 10, 8, 2, 1, 8, 9, 8, 9, 5, 4, 7, 2, 8, 9, 8, 8, 8, 8, -1, 1, -1, 1, 3, 8, 1, -1, 1, 1, 8, 9, 9, 8, 1, -1, 2, 9, 1, 8, 12, -1, 4, 6, 2, 8, 1, -1, -1, 8, 1, 8, -1, 1, 3, 1, 3, 1, 8, 1, 8, 2, 3, -1, -1, 7, 1, 8, 10, 9, 2, 2, 8, 9, 3, 3, 3, -1, 8, 4, -1, 1, 1, -1, 3, 8, 8, -1, -1, -1, 1, 1, -1, 1, 8, 2, 8, 2, 1, 11, 1, 9, 3, -1, 8, 9, 2, 3, 10, 2, 8, 3, 12, 1, 8, 12, -1, 1, 7, 8, 10, 1, 3, 2, 3, 1, 8, 2, 4, -1, -1, 8, 2, 8, 4, -1, 8, -1, 6, 10, 2, 9, 1, 7, -1, 9, 5, 8, -1, 1, 2, 8, 7, 3, 1, 2, 1, 8, 3, 1, 1, -1, 8, 4, 3, 8, 1, 1, 1, 3, 8, 2, 8, 8, 9, 8, -1, 7, -1, 2, 3, 8, 9, 10, 8, 4, -1, 9, -1, 1, 2, 1, 1, -1, 1, -1, 8, 10, 10, 8, 8, 2, -1, 1, 8, -1, 9, 7, 8, 1, -1, -1, -1, -1, 1, -1, 4, 1, -1, -1, 10, -1, -1, 2, -1, 1, 1, 3, 3, 4, -1, 7, 2, 8, 8, 8, 4, 8, 2, 1, -1, 1, 9, -1, 2, -1, 3, 3, -1, 9, 1, 8, -1, 1, 1, -1, 10, 3, -1, 1, 8, -1, 9, 9, 1, 1, 2, 2, 8, -1, 2, 4, 1, 1, -1, 8, -1, 1, 10, 9, 8, 1, 2, -1, 8, 9, 9, -1, 7, 12, 12, 8, 2, 1]\n",
            "[1, 9, -1, 1, 2, 10, 3, 6, 2, 1, -1, 4, 1, -1, 8, -1, 2, -1, 1, 2, 8, -1, -1, 4, 9, 8, 2, 8, -1, 8, 8, 1, 8, -1, -1, 1, 9, 2, 2, 3, -1, 1, 9, 1, 10, 8, -1, 3, -1, 8, 3, 12, 1, 7, 2, 9, 9, -1, 9, 9, -1, 3, 2, 2, 1, -1, 2, -1, 9, 1, 3, -1, 7, 1, 2, -1, -1, 8, 1, 3, 1, -1, 10, -1, 7, -1, -1, -1, 8, -1, 2, 8, -1, -1, -1, 12, 2, 10, 8, -1, -1, 3, -1, 4, 1, -1, 10, 4, 8, 9, -1, 2, 1, 1, 9, 9, -1, 3, 9, -1, 8, -1, 7, -1, 2, 9, 3, 2, 8, 2, 9, 1, 8, -1, 9, -1, 1, 1, 1, -1, 1, 1, 9, 9, -1, -1, 8, 3, -1, 9, 1, 1, -1, 7, 4, -1, -1, -1, -1, 1, 4, 8, 4, 4, -1, -1, 8, 9, 4, 7, 3, 5, -1, 10, -1, -1, -1, 9, -1, -1, 3, -1, 9, 3, -1, -1, 8, -1, 12, -1, -1, 8, 10, -1, -1, 9, -1, 12, 1, 9, 3, -1, 1, -1, 8, 8, 9, 9, -1, 8, -1, 2, 2, 9, 1, 8, -1, 8, 2, 2, 8, 9, 4, 2, 1, 7, 3, 2, 4, 2, -1, 1, -1, -1, 8, -1, 9, -1, 7, 3, 1, 9, -1, -1, 1, 1, 7, 8, 1, -1, 9, 2, 12, -1, 9, 3, 1, -1, -1, 2, 8, 1, 1, 3, 9, 8, 2, 8, -1, 8, 1, 9, 8, -1, 7, -1, 9, 2, -1, 3, 7, 12, 2, 12, 1, 1, 9, -1, -1, 3, -1, 7, -1, 1, 9, 2, 1, 5, 1, 3, 2, 1, 2, 1, -1, 8, 2, 1, 8, 12, 7, 1, 8, 5, 1, 1, 1, 8, -1, -1, -1, 9, 11, 4, 1, -1, 1, 9, 2, 1, -1, 5, 1, 1, 1, 3, -1, 8, 2, 9, -1, 8, -1, -1, -1, 9, 8, -1, 2, 8, 1, 3, 3, 9, 1, -1, 1, 1, 9, -1, -1, 12, 3, 1, -1, 8, 4, -1, -1, 2, -1, 1, -1, 2, 8, 3, 9, -1, 9, 1, 10, -1, -1, -1, -1, 9, 8, -1, 12, 1, -1, -1, -1, 7, 9, -1, 8, 8, 8, 1, 12, 2, 1, 7, 2, 9, -1, -1, 2, 8, 10, 3, 1, -1, 1, -1, 1, 7, 4, -1, 1, 2, 10, 12, -1, 3, -1, 8, 10, 1, 2, -1, -1, 1, 8, 9, 10, 1, 12, 3, 1, -1, 8, 7, 9, 1, -1, -1, 1, 9, 8, 8, 8, -1, -1, 1, 8, -1, -1, 2, -1, 8, 7, 1, 4, 1, 2, -1, -1, 6, -1, 1, 3, 3, 2, -1, -1, 8, -1, 1, -1, -1, 9, 2, 1, -1, 9, 8, -1, -1, 1, -1, 1, 5, -1, 1, 3, 1, 2, 2, 10, -1, 1, -1, -1, 1, -1, 1, 1, 9, 10, 3, 12, 10, 1, 1, 2, 8, -1, 3, 2, 5, -1, 1, -1, 7, 1, -1, 7, 1, -1, 9, -1, -1, 8, 8, 1, 8, -1, -1, -1, 6, 1, -1, -1, 1, 8, 9, 3, -1, 4, -1, 4, 1, 8, 9, 8, 9, 5, 4, 7, 2, 7, 9, 8, -1, 9, 10, -1, 1, -1, 1, 3, 8, 1, -1, 1, -1, 8, 9, 9, 8, 1, -1, 2, -1, 1, 8, 12, -1, 4, 7, 2, 10, 1, -1, -1, 9, 1, 9, -1, 1, -1, 1, 10, 1, 1, 1, 8, 2, 3, -1, -1, 7, 1, 8, 10, 9, 9, 2, 8, 9, 3, 3, 4, -1, 8, 8, -1, 1, 1, -1, -1, 8, 8, -1, -1, -1, 1, 1, -1, 1, -1, 2, 8, -1, 1, 9, 1, 9, 3, -1, 8, 9, 2, 3, 10, 2, 9, 3, 12, 1, 10, 12, -1, 1, 7, 9, 11, 1, 3, 8, 3, 1, 8, 7, -1, -1, -1, 8, 4, -1, 1, -1, 8, -1, 6, 8, 2, 9, 1, 7, -1, 9, 5, 8, -1, 1, 2, 8, -1, 9, 1, -1, 1, 9, 9, 1, 1, -1, 8, 4, 3, 8, 1, 1, 1, 9, 8, 2, 8, 8, 9, 8, -1, 7, -1, 2, 1, -1, 9, 10, 8, 4, -1, 9, -1, 1, 2, 1, 1, -1, 1, -1, 8, 10, 10, 8, 9, 2, -1, 1, 8, -1, 9, 7, 8, -1, -1, -1, -1, -1, 1, -1, 10, 1, -1, -1, 9, -1, -1, 2, -1, 1, 1, 7, 3, 1, -1, 7, -1, -1, 12, -1, -1, 9, 2, -1, -1, 1, 8, -1, 2, -1, 1, 3, -1, 9, -1, 8, -1, 1, 1, -1, 10, 1, -1, 1, 8, -1, 9, 4, 1, 1, 2, 2, -1, -1, 2, 1, 1, 1, -1, 8, -1, 1, 10, 9, 8, 1, 2, -1, 8, 9, 9, -1, 4, 12, 12, 8, -1, 1]\n",
            "[1, 9, -1, 1, 2, 9, 3, 6, -1, 1, -1, 4, 1, -1, 1, -1, 2, -1, 1, 2, -1, -1, -1, 4, 2, 8, 2, 8, -1, 8, 8, 1, 8, -1, -1, 1, 1, 2, 2, 3, -1, 1, 9, -1, 6, 8, -1, 3, -1, -1, 3, 12, 1, 7, 2, -1, -1, -1, 9, 9, -1, 2, 6, 2, 1, -1, 2, -1, -1, -1, 3, -1, -1, 1, 2, -1, -1, -1, 1, 3, 1, -1, 10, -1, 7, -1, -1, -1, -1, -1, 2, 8, -1, -1, -1, 1, -1, 3, 8, -1, -1, 3, -1, 2, 1, -1, -1, 4, 8, 9, -1, 2, 1, 1, 10, 9, -1, 3, 2, -1, 8, -1, -1, -1, 2, 1, 3, 2, 8, 2, 9, 1, 9, -1, 9, -1, 1, 1, 1, -1, -1, 1, 9, 9, -1, -1, 8, 3, -1, 9, 2, 1, -1, -1, -1, -1, -1, -1, -1, 2, -1, 8, 4, -1, -1, -1, 8, 9, 4, 7, 3, 5, -1, 2, -1, -1, -1, 9, -1, -1, 2, -1, 1, 4, -1, -1, 9, -1, -1, -1, -1, 8, 9, -1, -1, -1, -1, 12, 1, 9, 3, -1, 1, -1, -1, -1, 8, 9, -1, 8, -1, 2, 2, 9, 1, 1, -1, 8, 2, 2, 8, 9, 8, 2, 1, 7, 3, -1, -1, 2, -1, 1, -1, -1, 9, -1, 9, -1, 7, 3, 1, 8, -1, -1, 1, 1, 7, 8, 1, -1, 2, 2, 3, -1, 9, 2, -1, -1, -1, 2, 8, 1, 1, 2, 9, -1, 2, 8, -1, 8, -1, 2, 8, -1, 7, -1, 8, 2, -1, -1, 7, 1, 2, 12, 1, -1, 9, -1, -1, 3, -1, 7, -1, 1, 9, 2, 1, 5, 1, 3, -1, 1, 2, 1, -1, 8, -1, 1, 8, 12, 7, 1, -1, 5, 1, 1, -1, 8, -1, -1, -1, 1, 11, -1, 1, -1, -1, 9, 2, -1, -1, 5, 1, 1, -1, -1, -1, 8, 2, 8, -1, 8, -1, -1, -1, 9, 9, -1, 2, 8, 1, 3, 3, 8, 1, -1, 1, 1, 9, -1, -1, 12, 3, 1, -1, 8, -1, -1, -1, 2, -1, 1, -1, 2, 2, 3, 4, -1, 1, 1, 10, -1, -1, -1, -1, 9, 8, -1, 12, 1, -1, -1, -1, 7, 3, -1, 8, 8, 8, 1, 8, 2, 1, 3, 6, 2, -1, -1, 2, 2, 9, 3, 1, -1, 1, -1, 1, 3, 4, -1, 1, 1, 10, 12, -1, 1, -1, 8, 10, 1, 2, -1, -1, 1, 8, 9, 8, 1, 12, -1, 1, -1, 4, 7, 9, 1, -1, -1, -1, 9, 3, 8, 8, -1, -1, 1, -1, -1, -1, 6, -1, 8, 7, 1, 4, -1, 2, -1, -1, 6, -1, -1, 3, 3, 2, -1, -1, 8, -1, 1, -1, -1, 9, 2, 1, -1, 9, 4, -1, -1, 1, -1, 1, 5, -1, 1, 1, 1, 2, -1, 7, -1, 1, -1, -1, 1, -1, 1, -1, 9, 3, 3, 12, 8, 1, 1, 2, -1, -1, 3, 2, 5, -1, 1, -1, 9, 1, -1, 7, 1, -1, 9, -1, -1, 8, 8, -1, -1, -1, -1, -1, 6, 1, -1, -1, 1, 8, 9, 2, -1, 8, -1, 2, -1, 9, 9, 8, 2, 5, 4, 7, 2, 9, 9, 8, -1, -1, 9, -1, 1, -1, 1, 3, 8, 1, -1, 1, -1, 8, 9, 9, 1, 1, -1, 2, -1, 1, 11, 12, -1, 4, -1, 2, 1, 1, -1, -1, 8, 1, 8, -1, -1, -1, 1, 3, 1, 1, 1, 1, 2, 3, -1, -1, -1, -1, 8, 10, 9, 2, 2, 8, -1, 3, 3, 1, -1, 8, 4, -1, 1, -1, -1, -1, 8, 8, -1, -1, -1, 1, 1, -1, -1, -1, 2, 8, -1, 1, -1, 1, 9, 3, -1, 8, 9, 2, 3, 2, -1, 9, 3, 12, 1, 1, -1, -1, 1, 7, 8, 11, 1, 3, 4, 3, 1, 8, 2, -1, -1, -1, 8, 4, -1, 1, -1, 9, -1, 6, -1, 2, -1, 1, 7, -1, -1, 5, 8, -1, 1, 2, 8, -1, 3, 1, -1, 1, -1, 3, 1, 1, -1, 8, 4, 3, 9, 1, 1, 1, 9, 8, 2, 8, 8, 9, 8, -1, -1, -1, 2, 1, -1, 9, 10, 8, -1, -1, 9, -1, 1, 2, 1, 1, -1, 1, -1, 8, 10, 10, 8, 9, 2, -1, 1, 8, -1, 9, -1, 8, -1, -1, -1, -1, -1, 1, -1, -1, 1, -1, -1, 9, -1, -1, 2, -1, 1, -1, 3, 2, -1, -1, 7, -1, -1, 8, -1, -1, 9, 2, -1, -1, 1, -1, -1, 2, -1, 1, 3, -1, -1, -1, 8, -1, 1, 1, -1, 10, 1, -1, -1, 9, -1, 9, 9, 1, 1, -1, 2, -1, -1, 2, 1, 1, 1, -1, 4, -1, 1, 3, 9, 8, 1, 9, -1, 8, 9, 8, -1, 7, 12, 12, -1, -1, -1]\n",
            "[1, 8, -1, 1, 2, 9, 3, 6, -1, 1, -1, 4, 1, -1, 1, -1, 2, -1, 1, 2, -1, -1, -1, 4, 2, 8, 2, 8, -1, 8, 8, 1, 8, -1, -1, -1, -1, 2, 2, 3, -1, 1, 9, -1, -1, -1, -1, 3, -1, -1, 3, 12, 1, 7, 1, -1, -1, -1, 9, 9, -1, 2, -1, 2, 1, -1, 2, -1, -1, -1, 3, -1, -1, 1, 2, -1, -1, -1, -1, 6, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 2, 8, -1, -1, -1, 1, -1, 10, 8, -1, -1, -1, -1, 1, 1, -1, -1, 1, 8, -1, -1, 2, 1, 1, 8, 8, -1, 3, -1, -1, 8, -1, -1, -1, 2, 1, 3, -1, 8, 2, 9, 1, -1, -1, 9, -1, 1, 1, 1, -1, -1, -1, 9, 9, -1, -1, 8, 3, -1, 9, 1, 1, -1, -1, -1, -1, -1, -1, -1, 2, -1, 8, -1, -1, -1, -1, 8, 9, 9, 7, 3, 5, -1, 10, -1, -1, -1, 9, -1, -1, 2, -1, 9, -1, -1, -1, 8, -1, -1, -1, -1, 8, 10, -1, -1, -1, -1, -1, 1, -1, 3, -1, 1, -1, -1, -1, -1, 9, -1, 8, -1, -1, 8, 9, 1, 1, -1, 8, -1, 2, 8, 9, 9, 2, -1, 7, 1, -1, -1, 2, -1, 1, -1, -1, 9, -1, -1, -1, -1, 3, 1, 8, -1, -1, 1, -1, 7, 8, 1, -1, 2, 6, 3, -1, -1, 3, -1, -1, -1, 2, 8, 1, 1, 2, 9, -1, 2, 10, -1, -1, -1, 2, 8, -1, -1, -1, 8, 2, -1, -1, -1, 12, 2, 12, 1, -1, 9, -1, -1, 3, -1, 7, -1, -1, 8, 2, 1, 5, -1, -1, -1, 1, 2, 1, -1, 8, -1, 1, 8, -1, -1, 1, -1, 5, 1, 1, -1, 8, -1, -1, -1, 9, 2, -1, 1, -1, -1, 9, 9, -1, -1, 5, 1, 1, -1, -1, -1, 8, 2, 8, -1, 8, -1, -1, -1, -1, 9, -1, -1, -1, -1, 3, 3, 9, 1, -1, 1, 1, 9, -1, -1, 12, 3, -1, -1, 8, -1, -1, -1, 2, -1, 1, -1, -1, -1, 3, 4, -1, 1, 1, 10, -1, -1, -1, -1, 9, 10, -1, 12, 1, -1, -1, -1, 7, -1, -1, 8, 8, -1, 1, 8, 2, -1, 3, 6, 2, -1, -1, 2, 2, 8, 3, 1, -1, 1, -1, 1, -1, 1, -1, 8, 1, 10, 12, -1, -1, -1, 8, 10, -1, 2, -1, -1, 1, -1, 9, 10, 1, 8, -1, 1, -1, 8, -1, -1, -1, -1, -1, -1, 10, 8, 8, 8, -1, -1, 1, -1, -1, -1, 2, -1, -1, 7, -1, 9, -1, 2, -1, -1, 6, -1, -1, 3, 1, 1, -1, -1, 8, -1, 1, -1, -1, 9, 9, 1, -1, -1, 8, -1, -1, 1, -1, 1, 5, -1, 1, -1, 1, 2, -1, 10, -1, 1, -1, -1, 1, -1, 1, -1, 9, 10, 3, 12, 10, 1, 1, 3, -1, -1, 3, 2, 5, -1, 1, -1, 9, 1, -1, 8, -1, -1, 8, -1, -1, 8, 8, -1, -1, -1, -1, -1, 6, 1, -1, -1, 1, 9, 9, 2, -1, 4, -1, 2, -1, -1, -1, 8, 9, 5, 4, 7, 2, 9, -1, 8, -1, -1, 10, -1, 1, -1, 1, 3, 8, 1, -1, 1, -1, 8, 9, 9, 8, 1, -1, 2, -1, 1, 11, 12, -1, 4, -1, 3, 8, 1, -1, -1, 8, 1, 8, -1, -1, -1, 1, 3, 1, 1, 1, 8, 2, 3, -1, -1, -1, -1, 8, -1, -1, -1, 2, 8, -1, 12, 3, -1, -1, 8, 11, -1, 1, -1, -1, -1, 10, 8, -1, -1, -1, 1, 1, -1, -1, -1, 1, 8, -1, 1, -1, 1, 9, 3, -1, 8, 9, 1, 3, 10, -1, 8, 3, 12, 1, 1, -1, -1, 1, 7, 8, 11, 1, 3, -1, 3, 1, 8, 7, -1, -1, -1, -1, -1, -1, 1, -1, 8, -1, 6, -1, 2, -1, 1, -1, -1, -1, 5, 8, -1, -1, 2, 8, -1, 9, 1, -1, 1, -1, 9, 1, 1, -1, 8, 4, 3, 8, 1, 1, 1, 3, 8, 2, 8, -1, 9, 8, -1, -1, -1, 2, 1, -1, 9, -1, 8, -1, -1, 9, -1, 1, 1, 1, 1, -1, 1, -1, 8, 10, -1, 8, 9, 2, -1, -1, 8, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, 1, -1, -1, 10, -1, -1, 2, -1, 1, -1, 9, 3, -1, -1, 7, -1, -1, -1, -1, -1, 8, 2, -1, -1, 1, -1, -1, 2, -1, 1, 3, -1, -1, -1, 8, -1, 1, 1, -1, 10, 1, -1, -1, 9, -1, 9, 9, 1, 1, -1, 2, -1, -1, 2, 1, -1, -1, -1, 8, -1, 1, 10, 9, 7, 1, 9, -1, 8, 9, 8, -1, 7, -1, 12, -1, -1, -1]\n",
            "0.7482185273159145\n",
            "[1, 8, 3, 1, 2, 8, 3, 6, 2, 1, 8, 4, 1, 4, 8, 7, 2, 4, 1, 2, 8, 9, 9, 4, 2, 10, 2, 8, 1, 8, 8, 1, 8, 1, 3, 1, 1, 2, 2, 3, 2, 1, 10, 2, 2, 8, 1, 3, 4, 8, 3, 12, 1, 7, 2, 9, 9, 1, 9, 9, 3, 2, 6, 2, 1, 1, 8, 1, 8, 1, 3, 8, 7, 1, 2, 2, 9, 10, 1, 3, 1, 11, 10, 1, 7, 9, 9, 12, 8, 1, 2, 8, 1, 2, 1, 1, 2, 10, 8, 2, 7, 3, 11, 1, 1, 9, 10, 4, 8, 9, 10, 2, 1, 1, 10, 9, 3, 3, 9, 1, 8, 8, 7, 1, 2, 9, 3, 2, 8, 2, 9, 1, 8, 3, 9, 8, 1, 1, 1, 2, 1, 1, 8, 9, 1, 9, 8, 3, 8, 9, 2, 1, 3, 7, 4, 11, 3, 8, 8, 2, 9, 8, 8, 4, 12, 1, 8, 9, 8, 7, 3, 5, 10, 2, 8, 9, 3, 9, 12, 7, 2, 1, 9, 3, 9, 8, 9, 9, 12, 1, 1, 10, 10, 1, 2, 9, 11, 12, 1, 9, 3, 2, 1, 3, 8, 8, 8, 9, 2, 8, 8, 2, 2, 9, 1, 1, 2, 8, 2, 2, 8, 9, 8, 2, 1, 7, 1, 2, 1, 2, 2, 1, 9, 1, 9, 2, 9, 1, 7, 3, 1, 9, 8, 12, 1, 1, 7, 8, 1, 2, 2, 2, 12, 1, 9, 3, 1, 1, 9, 2, 8, 1, 1, 3, 2, 8, 2, 8, 8, 8, 2, 2, 8, 2, 7, 1, 8, 2, 2, 3, 9, 12, 2, 12, 1, 1, 9, 12, 1, 3, 2, 7, 3, 9, 8, 2, 1, 5, 1, 3, 2, 1, 2, 1, 8, 8, 2, 2, 8, 12, 10, 1, 10, 5, 1, 1, 1, 8, 9, 3, 9, 9, 10, 2, 1, 9, 1, 8, 2, 1, 9, 5, 1, 1, 1, 3, 1, 8, 2, 8, 1, 7, 7, 1, 1, 8, 8, 1, 2, 8, 1, 3, 3, 8, 1, 2, 1, 1, 9, 8, 1, 12, 3, 1, 1, 8, 4, 3, 1, 2, 9, 1, 2, 2, 2, 3, 6, 2, 1, 1, 10, 8, 8, 8, 1, 9, 8, 9, 12, 1, 2, 8, 9, 7, 3, 8, 8, 8, 8, 1, 8, 2, 1, 3, 2, 2, 3, 2, 2, 2, 8, 2, 1, 9, 1, 9, 1, 3, 9, 1, 8, 1, 8, 12, 1, 1, 2, 8, 8, 1, 2, 3, 2, 12, 8, 9, 10, 1, 12, 3, 1, 2, 8, 7, 9, 1, 2, 9, 2, 10, 8, 8, 8, 2, 8, 1, 8, 1, 1, 2, 8, 8, 7, 1, 4, 1, 2, 2, 2, 6, 12, 1, 3, 3, 2, 3, 1, 8, 8, 1, 9, 3, 9, 2, 2, 9, 9, 9, 8, 9, 1, 3, 1, 5, 2, 1, 3, 1, 2, 9, 10, 8, 1, 4, 1, 1, 4, 1, 9, 9, 3, 3, 12, 8, 1, 1, 2, 8, 1, 3, 2, 5, 1, 1, 3, 9, 1, 1, 7, 1, 1, 9, 8, 2, 8, 8, 1, 8, 1, 8, 8, 6, 1, 10, 1, 1, 8, 9, 3, 10, 4, 8, 2, 1, 8, 9, 8, 10, 5, 4, 7, 2, 8, 9, 8, 12, 8, 8, 9, 1, 2, 1, 3, 8, 1, 10, 1, 1, 8, 9, 2, 8, 1, 1, 2, 2, 12, 8, 12, 2, 2, 7, 2, 8, 1, 8, 2, 9, 1, 8, 11, 1, 1, 1, 3, 1, 1, 1, 8, 2, 3, 9, 9, 10, 1, 8, 8, 9, 2, 2, 8, 9, 3, 3, 2, 2, 8, 4, 2, 9, 1, 9, 1, 12, 8, 9, 3, 2, 1, 1, 3, 1, 8, 2, 9, 1, 1, 8, 1, 9, 3, 3, 8, 9, 2, 3, 10, 2, 8, 3, 12, 1, 1, 12, 12, 1, 7, 8, 11, 1, 3, 2, 3, 1, 8, 2, 4, 9, 2, 8, 2, 8, 1, 2, 8, 8, 6, 8, 2, 9, 1, 7, 9, 9, 5, 8, 1, 1, 2, 8, 7, 3, 1, 2, 1, 8, 3, 1, 1, 2, 8, 8, 3, 8, 1, 1, 1, 9, 8, 2, 8, 8, 9, 8, 1, 7, 1, 2, 1, 8, 9, 10, 8, 2, 9, 10, 8, 1, 2, 1, 1, 9, 1, 2, 8, 10, 10, 8, 8, 2, 9, 1, 8, 2, 8, 7, 8, 1, 8, 1, 1, 3, 1, 1, 8, 1, 8, 2, 10, 1, 1, 2, 3, 1, 1, 3, 2, 1, 8, 7, 2, 8, 12, 1, 12, 8, 2, 1, 7, 1, 8, 2, 2, 12, 1, 3, 8, 9, 1, 8, 8, 1, 1, 2, 10, 1, 9, 2, 8, 8, 10, 9, 1, 1, 2, 2, 9, 10, 2, 1, 1, 1, 12, 4, 8, 1, 10, 9, 8, 1, 9, 12, 8, 9, 8, 1, 7, 12, 12, 8, 2, 1]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "final_pred_3=[]\n",
        "\n",
        "num_correct=0\n",
        "model_0_3=load_model(loss,accuracy,optimizer,'0-3')\n",
        "mod_labels_0=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_0:\n",
        "    pred_test_0_3=model_0_3.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_0_3 = pred_test_0_3.argmax(axis=1)\n",
        "    mod_labels_0.append(pred_labels_0_3[0])\n",
        "  else:\n",
        "    mod_labels_0.append(-1)\n",
        "\n",
        "model_1_3=load_model(loss,accuracy,optimizer,'1-3')\n",
        "mod_labels_1=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        " \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_1:\n",
        "    pred_test_1_3=model_1_3.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_1_3 = pred_test_1_3.argmax(axis=1)\n",
        "    mod_labels_1.append(pred_labels_1_3[0])\n",
        "  else:\n",
        "    mod_labels_1.append(-1)\n",
        "\n",
        "model_2_3=load_model(loss,accuracy,optimizer,'2-3')\n",
        "mod_labels_2=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_2:\n",
        "    pred_test_2_3=model_2_3.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_2_3 = pred_test_2_3.argmax(axis=1)\n",
        "    mod_labels_2.append(pred_labels_2_3[0])\n",
        "  else:\n",
        "    mod_labels_2.append(-1)\n",
        "\n",
        "model_3_3=load_model(loss,accuracy,optimizer,'3-3')\n",
        "mod_labels_3=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_3:\n",
        "    pred_test_3_3=model_3_3.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_3_3 = pred_test_3_3.argmax(axis=1)\n",
        "    mod_labels_3.append(pred_labels_3_3[0])\n",
        "  else:\n",
        "    mod_labels_3.append(-1)\n",
        "\n",
        "model_4_3=load_model(loss,accuracy,optimizer,'4-3')\n",
        "mod_labels_4=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_4:\n",
        "    pred_test_4_3=model_4_3.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_4_3 = pred_test_4_3.argmax(axis=1)\n",
        "    mod_labels_4.append(pred_labels_4_3[0])\n",
        "  else:\n",
        "    mod_labels_4.append(-1)\n",
        "\n",
        "model_5_3=load_model(loss,accuracy,optimizer,'5-3')\n",
        "mod_labels_5=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_5:\n",
        "    pred_test_5_3=model_5_3.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_5_3 = pred_test_5_3.argmax(axis=1)\n",
        "    mod_labels_5.append(pred_labels_5_3[0])\n",
        "  else:\n",
        "    mod_labels_5.append(-1)\n",
        "\n",
        "print(mod_labels_0)\n",
        "print(mod_labels_1)\n",
        "print(mod_labels_2)\n",
        "print(mod_labels_3)\n",
        "print(mod_labels_4)\n",
        "print(mod_labels_5)\n",
        "\n",
        "\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  fin_labels=[]\n",
        "  if mod_labels_0[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  if mod_labels_1[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  if mod_labels_2[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  if mod_labels_3[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  if mod_labels_4[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  if mod_labels_5[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  res=max(set(fin_labels), key = fin_labels.count)\n",
        "  final_pred_3.append(res)\n",
        "\n",
        "  if res==new_val_label_0[i]:\n",
        "    num_correct=num_correct+1\n",
        "\n",
        "accuracy=num_correct/len(new_val_label_0)\n",
        "print(accuracy)\n",
        "  \n",
        "print(final_pred_3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGmPqjsrofw9",
        "outputId": "6d63eab1-e163-4cc3-ae41-e3691349ddb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7482185273159145\n",
            "Weighted F1: 0.7408346225270779\n",
            "Micro F1: 0.7482185273159146\n",
            "Weighted Precision: 0.740785406186586\n",
            "Micro Precision: 0.7482185273159145\n",
            "Weighted Recall: 0.7482185273159145\n",
            "Micro Recall: 0.7482185273159145\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "\n",
        "accuracy=accuracy_score(new_val_label_0, final_pred_3)\n",
        "print(\"Accuracy: \"+str(accuracy))\n",
        "total_accuracy=total_accuracy+accuracy\n",
        "\n",
        "weighted_f1=f1_score(new_val_label_0,final_pred_3, average='weighted')\n",
        "print(\"Weighted F1: \"+ str(weighted_f1))\n",
        "total_weighted_f1=total_weighted_f1+weighted_f1\n",
        "micro_f1=f1_score(new_val_label_0,final_pred_3, average='micro')\n",
        "print(\"Micro F1: \"+ str(micro_f1))\n",
        "total_micro_f1=total_micro_f1+micro_f1\n",
        "\n",
        "weighted_precision=precision_score(new_val_label_0, final_pred_3, average='weighted')\n",
        "print(\"Weighted Precision: \" + str(weighted_precision))\n",
        "total_weighted_precision=total_weighted_precision+weighted_precision\n",
        "micro_precision=precision_score(new_val_label_0, final_pred_3, average='micro')\n",
        "print(\"Micro Precision: \" + str(micro_precision))\n",
        "total_micro_precision=total_micro_precision+micro_precision\n",
        "\n",
        "weighted_recall=recall_score(new_val_label_0, final_pred_3, average='weighted')\n",
        "print(\"Weighted Recall: \" + str(weighted_recall))\n",
        "total_weighted_recall=total_weighted_recall+weighted_recall\n",
        "micro_recall=recall_score(new_val_label_0, final_pred_3, average='micro')\n",
        "print(\"Micro Recall: \" + str(micro_recall))\n",
        "total_micro_recall=total_micro_recall+micro_recall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcnIY-AfoqM9",
        "outputId": "6aee8fdc-bd2a-4053-ea0d-3ea48dfbc098"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_26\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_53 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_54 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_53[0][0]',               \n",
            "                                thPoolingAndCrossAt               'input_54[0][0]']               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_26 (S  (None, 768)         0           ['tf_bert_model[26][0]']         \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " dense_52 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_26[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dropout_63 (Dropout)           (None, 512)          0           ['dense_52[0][0]']               \n",
            "                                                                                                  \n",
            " dense_53 (Dense)               (None, 15)           7695        ['dropout_63[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model: \"model_27\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_55 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_56 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_55[0][0]',               \n",
            "                                thPoolingAndCrossAt               'input_56[0][0]']               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_27 (S  (None, 768)         0           ['tf_bert_model[27][0]']         \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " dense_54 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_27[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dropout_64 (Dropout)           (None, 512)          0           ['dense_54[0][0]']               \n",
            "                                                                                                  \n",
            " dense_55 (Dense)               (None, 15)           7695        ['dropout_64[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model: \"model_28\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_57 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_58 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_57[0][0]',               \n",
            "                                thPoolingAndCrossAt               'input_58[0][0]']               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_28 (S  (None, 768)         0           ['tf_bert_model[28][0]']         \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " dense_56 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_28[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dropout_65 (Dropout)           (None, 512)          0           ['dense_56[0][0]']               \n",
            "                                                                                                  \n",
            " dense_57 (Dense)               (None, 15)           7695        ['dropout_65[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model: \"model_29\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_59 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_60 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_59[0][0]',               \n",
            "                                thPoolingAndCrossAt               'input_60[0][0]']               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_29 (S  (None, 768)         0           ['tf_bert_model[29][0]']         \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " dense_58 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_29[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dropout_66 (Dropout)           (None, 512)          0           ['dense_58[0][0]']               \n",
            "                                                                                                  \n",
            " dense_59 (Dense)               (None, 15)           7695        ['dropout_66[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model: \"model_30\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_61 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_62 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_61[0][0]',               \n",
            "                                thPoolingAndCrossAt               'input_62[0][0]']               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_30 (S  (None, 768)         0           ['tf_bert_model[30][0]']         \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " dense_60 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_30[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dropout_67 (Dropout)           (None, 512)          0           ['dense_60[0][0]']               \n",
            "                                                                                                  \n",
            " dense_61 (Dense)               (None, 15)           7695        ['dropout_67[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model: \"model_31\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_63 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_64 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_63[0][0]',               \n",
            "                                thPoolingAndCrossAt               'input_64[0][0]']               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_31 (S  (None, 768)         0           ['tf_bert_model[31][0]']         \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " dense_62 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_31[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dropout_68 (Dropout)           (None, 512)          0           ['dense_62[0][0]']               \n",
            "                                                                                                  \n",
            " dense_63 (Dense)               (None, 15)           7695        ['dropout_68[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "[1, 8, 3, 1, 2, 10, 3, 6, 4, 1, 8, 4, 1, 4, 3, 7, 2, 9, 1, 2, 8, 9, 9, 4, 2, 10, 2, 8, 1, 8, 12, 1, 8, 1, 3, 1, 9, 2, 2, 3, 2, 1, 1, 2, 2, 8, 1, 3, 4, 8, 3, 12, 1, 7, 2, 8, 9, 1, 9, 9, 3, 3, 6, 2, 1, 1, 10, 1, 10, 1, 3, 8, 7, 1, 2, 2, 9, 10, 1, 3, 1, 11, 10, 1, 7, 9, 9, 12, 8, 1, 2, 8, 1, 2, 1, 1, 2, 10, 8, 2, 7, 3, 10, 2, 1, 9, 7, 4, 8, 9, 10, 2, 1, 1, 10, 9, 3, 3, 9, 1, 8, 8, 7, 1, 2, 9, 3, 2, 8, 2, 9, 1, 8, 3, 9, 8, 1, 1, 1, 2, 9, 1, 10, 8, 1, 9, 8, 3, 8, 9, 2, 1, 3, 7, 2, 0, 3, 10, 8, 10, 9, 8, 12, 4, 12, 1, 8, 9, 4, 7, 3, 5, 7, 2, 8, 9, 3, 9, 12, 7, 2, 9, 9, 3, 10, 8, 8, 9, 12, 1, 1, 10, 10, 1, 2, 9, 10, 12, 1, 9, 3, 2, 1, 3, 10, 8, 8, 9, 9, 8, 8, 2, 2, 9, 1, 1, 2, 8, 3, 2, 8, 9, 8, 2, 1, 7, 3, 2, 4, 2, 2, 1, 9, 1, 9, 2, 9, 1, 8, 3, 1, 9, 8, 12, 1, 1, 7, 8, 1, 2, 9, 2, 12, 1, 9, 3, 1, 9, 9, 2, 8, 5, 1, 2, 9, 2, 2, 10, 8, 8, 2, 2, 8, 2, 7, 1, 9, 2, 2, 3, 7, 12, 2, 12, 1, 1, 9, 12, 1, 3, 2, 7, 3, 9, 8, 2, 1, 5, 1, 3, 9, 1, 2, 9, 8, 8, 2, 3, 8, 12, 10, 1, 7, 5, 1, 1, 1, 8, 9, 3, 9, 9, 2, 1, 1, 9, 1, 9, 2, 1, 7, 5, 1, 1, 1, 3, 1, 8, 2, 8, 1, 7, 7, 3, 1, 10, 8, 1, 2, 8, 1, 3, 3, 9, 1, 2, 1, 1, 9, 8, 1, 12, 3, 1, 1, 8, 4, 3, 3, 2, 9, 1, 2, 2, 2, 3, 6, 9, 1, 1, 10, 12, 8, 8, 1, 9, 10, 1, 12, 1, 9, 8, 9, 7, 3, 8, 8, 8, 8, 1, 12, 2, 1, 3, 6, 2, 3, 2, 2, 8, 10, 3, 1, 3, 1, 9, 1, 7, 9, 4, 1, 2, 10, 12, 9, 3, 2, 8, 10, 1, 2, 3, 9, 5, 8, 9, 10, 1, 12, 3, 1, 2, 8, 7, 9, 1, 2, 9, 2, 9, 3, 8, 8, 2, 8, 1, 8, 1, 1, 6, 8, 8, 7, 1, 4, 1, 2, 9, 3, 6, 12, 1, 3, 3, 2, 3, 1, 8, 8, 1, 9, 9, 9, 9, 1, 9, 9, 9, 10, 9, 1, 3, 1, 5, 2, 1, 1, 1, 2, 9, 7, 8, 1, 8, 1, 1, 4, 1, 1, 9, 10, 3, 12, 10, 1, 1, 2, 8, 2, 3, 2, 5, 3, 1, 3, 9, 1, 1, 7, 1, 1, 9, 8, 2, 8, 8, 1, 8, 1, 8, 10, 6, 1, 9, 1, 1, 8, 9, 3, 10, 8, 8, 2, 1, 8, 9, 8, 9, 5, 4, 7, 2, 8, 9, 8, 12, 8, 10, 9, 1, 2, 1, 3, 8, 1, 10, 1, 9, 8, 9, 2, 8, 1, 1, 2, 2, 1, 4, 12, 2, 2, 7, 9, 8, 1, 10, 9, 9, 1, 8, 11, 1, 9, 1, 3, 1, 1, 1, 8, 2, 3, 9, 10, 10, 1, 8, 8, 9, 9, 2, 8, 9, 12, 3, 4, 2, 8, 4, 2, 1, 1, 9, 3, 10, 8, 3, 3, 9, 1, 1, 9, 1, 8, 2, 8, 1, 1, 11, 1, 9, 3, 3, 8, 9, 2, 3, 10, 2, 8, 3, 12, 4, 1, 12, 12, 1, 7, 8, 11, 1, 3, 9, 3, 1, 8, 7, 4, 8, 2, 8, 4, 8, 2, 2, 8, 8, 6, 10, 2, 9, 1, 7, 9, 9, 5, 8, 1, 1, 2, 8, 9, 9, 1, 2, 1, 8, 8, 1, 1, 2, 8, 4, 3, 9, 1, 1, 1, 9, 8, 2, 8, 8, 9, 8, 1, 7, 1, 2, 1, 8, 9, 10, 8, 10, 9, 9, 1, 1, 2, 1, 1, 9, 1, 2, 8, 10, 10, 8, 8, 2, 9, 1, 8, 2, 9, 7, 8, 1, 8, 1, 1, 3, 1, 1, 4, 1, 8, 2, 10, 1, 1, 2, 3, 1, 1, 9, 3, 1, 8, 7, 2, 8, 8, 1, 9, 8, 2, 1, 7, 1, 8, 2, 2, 12, 1, 3, 8, 9, 1, 8, 8, 1, 1, 2, 10, 1, 9, 1, 10, 10, 10, 9, 1, 1, 2, 2, 9, 10, 2, 9, 1, 1, 9, 9, 8, 1, 3, 9, 7, 1, 2, 12, 8, 9, 8, 9, 7, 12, 12, 8, 2, 1]\n",
            "[1, 8, -1, 1, 2, 8, 3, 6, 2, 1, 8, 4, 1, 4, 1, -1, 2, 4, 1, 2, 8, -1, -1, 4, 2, 8, 2, 8, 1, 8, 8, 1, 8, 1, -1, 1, 1, 2, 2, 3, 2, 1, 8, 2, 2, 8, 1, 3, 4, 8, 3, 12, 1, 7, 2, 8, 9, -1, 9, 9, -1, 3, 6, 2, 1, 1, 8, 1, 8, 1, 3, 8, 7, 1, 2, 2, -1, 2, 1, 3, 1, 11, 10, 1, 7, -1, -1, 12, 8, 1, 2, 8, 1, 2, 1, 1, 2, 10, 8, -1, 7, 3, 11, 2, 1, 9, 10, 4, 8, 9, -1, 1, 1, 1, 8, 9, 3, 3, 9, 1, 8, 8, 7, 1, 2, 8, 3, 2, 8, 2, 9, 1, 8, 1, 9, -1, 1, 1, 1, -1, 1, 1, 9, 9, 1, -1, 8, 3, 8, 9, 2, 1, 3, 8, 2, -1, -1, 8, 8, 2, 1, 8, 4, 4, -1, 1, 8, 9, 8, 7, 1, 5, 7, 2, 8, 9, -1, 9, 12, 7, 2, 1, 8, 3, -1, -1, 8, -1, 12, 1, 1, 8, 8, 1, -1, 9, 10, 12, 1, 1, 3, -1, 1, 3, 8, 8, 8, 9, -1, 8, 8, 2, 2, 1, 1, 1, -1, 8, 2, 2, 8, 9, 8, 2, 1, 7, 1, 2, 1, 2, -1, 1, -1, -1, 9, 2, 9, 1, 7, 3, 1, 8, 8, 12, 1, 1, 7, 8, 1, -1, 2, 2, 12, -1, 9, 3, 1, -1, -1, 2, 8, 1, 1, 3, 2, 2, 2, 8, 8, 8, 2, 2, 8, 2, 7, 1, 8, 2, 2, 3, 8, 12, 2, 12, 1, 1, 9, 12, 1, 3, -1, 7, -1, 1, 8, 2, 1, 5, 1, 3, 2, 1, 2, 1, -1, 8, 2, 1, 8, 12, 7, 1, 8, 5, 1, 1, 1, 8, -1, 3, -1, 9, 4, 1, 1, 9, 1, 8, 2, 1, 3, 5, 1, 1, 1, 3, 1, 8, 2, 8, 1, 8, 7, 1, 1, 8, 8, 1, 2, 8, 1, 3, 8, 8, 1, 2, 1, 1, 1, -1, -1, 12, 3, 1, -1, 8, 2, 3, 3, 2, -1, 1, 2, 2, 2, 3, 8, -1, 1, 1, 8, -1, 8, 8, 1, 9, 8, 1, 12, 1, -1, 8, -1, 7, 3, -1, 8, 8, 8, 1, 8, 2, 1, 3, 6, 2, -1, 2, 2, 2, 8, 2, 1, -1, 1, -1, 1, 7, 5, 1, 1, 1, 8, 12, -1, 3, 2, 8, 8, 1, 2, -1, -1, 1, 8, 9, 8, 1, 12, 3, 1, -1, 8, 8, 9, 1, -1, 8, 2, 8, 3, 8, 8, -1, -1, 1, 8, 1, -1, 2, 8, 8, 7, 1, 8, 1, 2, 9, 3, 6, -1, 1, 3, 1, 2, -1, -1, 8, 8, 1, -1, -1, 9, 9, 1, -1, 8, 8, 8, 9, 1, -1, 1, 5, 2, 1, 1, 1, 2, 2, 10, 8, 1, 1, 1, 1, 8, 1, 1, 9, 3, 3, 12, 8, 1, 1, 2, 8, -1, 3, 2, 5, 1, 1, -1, 9, 1, 1, 7, 1, -1, 8, 8, -1, 8, 8, 1, 8, -1, -1, -1, 6, 1, 10, -1, 1, 8, 9, 2, -1, 8, 8, 2, 1, 8, 9, 8, 2, 5, 4, 7, 2, 8, 2, 8, 12, 8, 8, -1, 1, -1, 1, 3, 8, 1, 8, 1, 1, 8, 9, 2, 8, 1, -1, 2, 2, 1, 8, 12, -1, 2, 7, 2, 8, 1, 8, -1, 8, 1, 8, -1, 1, 3, 1, 3, 1, 1, 1, 8, 2, 3, 9, -1, 7, 1, 8, 8, 9, 2, 2, 8, 9, 1, 3, 2, 2, 8, 4, -1, 1, 1, -1, 1, 12, 8, 9, -1, -1, 1, 1, 8, 1, 8, 1, 8, 2, 1, 8, 1, 8, 3, 3, 8, 9, 2, 3, 10, 2, 8, 3, 12, 1, 1, 12, 12, 1, 7, 8, 11, 1, 3, 2, 3, 1, 8, 2, 1, -1, 2, 8, 2, 8, 1, 2, 8, -1, 6, 8, 2, 9, 1, 7, -1, 9, 5, 8, -1, 1, 2, 8, 7, 3, 1, 2, 1, 8, 8, 1, 1, -1, 8, 8, 3, 8, 1, 1, 1, 8, 8, 2, 8, 8, 8, 8, -1, 7, 1, 2, 1, 8, 9, 10, 8, 2, 9, 10, 1, 1, 2, 1, 1, 8, 1, -1, 8, 8, 8, 8, 8, 2, 9, 1, 8, -1, 8, 7, 8, 1, -1, 1, -1, -1, 1, -1, 4, 1, -1, -1, 8, 1, -1, 2, -1, 1, 1, 1, 3, 1, 8, 7, 2, 8, 12, 1, 4, 8, 2, 1, -1, 1, 8, -1, 2, 12, 1, 3, -1, 9, 1, 8, -1, 1, 1, 2, 10, 1, 8, 1, 8, -1, 10, 9, 1, 1, 2, 2, 8, -1, 2, 1, 1, 1, -1, 8, -1, 1, 3, 9, 8, 1, 2, -1, 8, 9, 8, -1, 7, 12, 12, 8, 2, 1]\n",
            "[1, 8, -1, 1, 2, 8, 3, 6, 2, 1, 8, 4, 1, -1, 10, -1, 2, -1, 1, 2, 8, -1, -1, 4, 9, 10, 2, 8, -1, 8, 8, 1, 8, 1, -1, 1, 1, 8, 2, 3, 2, 1, 8, 2, 2, 8, 9, 3, -1, 8, 3, 12, 3, 7, 2, 8, 9, -1, 9, 9, -1, 3, 6, 2, 1, 1, 8, -1, 8, 1, 3, 8, 7, 1, 2, -1, -1, 2, 1, 3, 1, 11, 10, 1, 7, -1, -1, 12, 8, -1, 2, 8, -1, 2, 1, 1, 2, 10, 8, -1, 7, 3, -1, 2, 1, 9, 7, 2, 8, 9, -1, 2, 1, 1, 8, 8, -1, 3, 9, 1, 8, 8, 7, -1, 2, 9, 3, 2, 8, 2, 9, 1, 8, 3, 9, -1, 1, 1, 1, -1, 1, 1, 10, 9, -1, -1, 8, 3, 8, 9, 2, 1, -1, 7, 2, -1, -1, 8, -1, 2, 9, 8, 8, 4, -1, 1, 8, 9, 8, 7, 3, 5, -1, 2, -1, 9, -1, 9, -1, 7, 3, 1, 8, 8, -1, -1, 8, -1, 12, -1, 1, 8, 10, -1, -1, 9, 10, 12, 1, 9, 3, -1, 1, 3, 8, 8, 9, 9, -1, 8, 8, 2, 2, 9, 1, 8, -1, 8, 2, 2, 8, 9, 4, 2, 1, 9, 1, 2, 2, 2, -1, 1, -1, -1, 9, 2, 9, 1, 7, 3, 1, 8, 8, -1, 1, 1, 7, 8, 1, -1, 9, 2, 12, -1, 9, 3, 1, -1, -1, 2, 8, 5, 1, 2, 8, 2, 2, 10, 8, 8, 2, 2, 8, 2, 7, -1, 8, 2, -1, 3, 7, 12, 2, 8, 1, 1, 9, 8, -1, 3, -1, 7, -1, 1, 9, 2, 1, 5, 1, 3, 2, 1, 2, 1, -1, 8, 2, 1, 8, 12, 7, 1, 8, 5, 1, 2, 1, 8, -1, -1, -1, 9, 2, 2, 1, 7, 1, 8, 2, 1, 7, 5, 1, 1, 1, 3, 1, 8, 2, 8, -1, 8, -1, -1, 1, 9, 8, 1, 2, 8, 1, 3, 3, 8, 1, 2, 1, 1, 9, -1, -1, 12, 3, 1, -1, 8, 4, -1, 3, 2, -1, 1, 2, 2, 2, 3, 8, -1, 1, 1, 10, -1, 8, -1, -1, 9, 8, 9, 12, 1, -1, 8, -1, 7, 3, -1, 8, 8, 8, 1, 8, 2, 1, 3, 6, 2, -1, -1, 2, 2, 8, 3, 1, -1, 1, -1, 1, 7, 2, 4, 8, 2, 10, 12, -1, 1, -1, 8, 10, 1, 2, -1, -1, 5, 8, 9, 10, 1, 8, 3, 2, -1, 8, 8, 9, 1, -1, 8, 2, 8, 8, 8, 8, -1, -1, 1, 8, -1, -1, 6, 8, 8, 7, 1, 4, 1, 2, -1, -1, 6, -1, 1, 3, 3, 2, -1, -1, 8, 8, 1, -1, -1, 9, 8, 1, -1, 9, 8, -1, 9, 9, -1, 8, 2, -1, 1, 3, 1, 2, 2, 7, -1, 1, 4, -1, 1, 8, 1, 9, 9, 7, 3, 12, 8, 1, 1, 2, 8, -1, 3, 2, 5, -1, 1, -1, 9, 1, -1, 8, 1, -1, 8, 8, -1, 8, 8, 1, 8, -1, -1, -1, 6, 1, -1, -1, 1, 8, 9, 8, -1, 10, 8, 2, 1, 8, 9, 8, 2, 5, 2, 7, 2, 8, 2, 8, 8, 8, 8, -1, 1, -1, 1, 3, 8, 1, -1, 1, 1, 8, 9, 9, 8, 1, -1, 2, 2, 8, 8, 12, -1, 2, 7, 2, 10, 1, -1, -1, 8, 1, 8, -1, 1, 3, 1, 3, 1, 8, 1, 8, 2, 3, -1, -1, 10, 1, 8, 8, 9, 9, 2, 8, 9, 8, 3, 4, -1, 8, 8, -1, 1, 1, -1, 3, 8, 8, -1, -1, -1, 1, 1, -1, 1, 8, 2, 8, 2, 1, 8, 1, 9, 3, -1, 8, 10, 2, 3, 10, 2, 8, 3, 12, 1, 8, 12, -1, 1, 7, 8, 11, 1, 3, 2, 3, 1, 8, 7, 4, -1, -1, 8, 2, 8, 4, -1, 8, -1, 6, 8, 2, 9, 1, 8, -1, 9, 2, 8, -1, 1, 2, 8, 7, 3, 1, 2, 1, 8, 8, 1, 1, -1, 8, 4, 3, 8, 1, 1, 2, 3, 8, 2, 8, 8, 9, 8, -1, 7, -1, 2, 1, 8, 9, 8, 8, 2, -1, 2, -1, 2, 2, 1, 1, -1, 1, -1, 8, 10, 8, 8, 8, 2, -1, 1, 8, -1, 9, 7, 8, 1, -1, -1, -1, -1, 1, -1, 8, 1, -1, -1, 10, -1, -1, 2, -1, 2, 1, 8, 3, 1, -1, 7, 2, 8, 8, 8, 8, 8, 2, 1, -1, 1, 8, -1, 2, -1, 1, 3, -1, 9, 1, 8, -1, 1, 1, -1, 10, 1, -1, 1, 8, -1, 10, 2, 1, 1, 2, 2, 8, -1, 2, 1, 1, 1, -1, 8, -1, 1, 10, 9, 8, 1, 2, -1, 8, 9, 8, -1, 7, 12, 12, 8, 2, 1]\n",
            "[1, 8, -1, 1, 2, 10, 3, 6, 2, 1, -1, 4, 1, -1, 8, -1, 2, -1, 1, 2, 8, -1, -1, 4, 2, 10, 2, 8, -1, 8, 8, 1, 9, -1, -1, 4, 9, 2, 2, 3, -1, 1, 10, 2, 2, 8, -1, 3, -1, 8, 3, 12, 1, 7, 2, 8, 9, -1, 2, 7, -1, 2, 6, 2, 1, -1, 7, -1, 8, 1, 3, -1, 7, 1, 2, -1, -1, 8, 1, 3, 1, -1, 10, -1, 7, -1, -1, -1, 10, -1, 2, 8, -1, -1, -1, 1, 2, 10, 8, -1, -1, 3, -1, 2, 1, -1, 7, 4, 8, 9, -1, 2, 1, 1, 10, 9, -1, 3, 10, -1, 8, -1, 7, -1, 2, 9, 3, 2, 8, 2, 9, 1, 12, -1, 9, -1, 4, 1, 1, -1, 1, 1, 9, 9, -1, -1, 8, 3, -1, 9, 2, 1, -1, 7, 2, -1, -1, -1, -1, 2, 4, 8, 4, 4, -1, -1, 8, 9, 4, 7, 3, 5, -1, 2, -1, -1, -1, 9, -1, -1, 2, -1, 9, 4, -1, -1, 8, -1, 12, -1, -1, 8, 10, -1, -1, 9, -1, 12, 1, 9, 3, -1, 1, -1, 8, 8, 8, 9, -1, 8, -1, 2, 2, 9, 1, 1, -1, 8, 2, 2, 8, 9, 4, 2, 1, 7, 3, 2, 4, 2, -1, 1, -1, -1, 9, -1, 9, -1, 7, 3, 1, 8, -1, -1, 1, 1, 7, 8, 1, -1, 2, 2, 12, -1, 9, 3, 1, -1, -1, 2, 8, 3, 1, 2, 2, 2, 2, 8, -1, 8, 2, 2, 8, -1, 7, -1, 8, 2, -1, 3, 7, 12, 2, 12, 1, 1, 9, -1, -1, 3, -1, 7, -1, 1, 8, 2, 4, 5, 1, 3, 9, 1, 2, 9, -1, 8, 2, 2, 8, 12, 7, 1, 7, 5, 1, 1, 1, 8, -1, -1, -1, 9, 2, 4, 9, -1, 1, 8, 2, 1, -1, 5, 1, 1, 4, 3, -1, 8, 2, 8, -1, 8, -1, -1, -1, 9, 10, -1, 2, 8, 12, 3, 8, 9, 1, -1, 1, 1, 9, -1, -1, 12, 3, 4, -1, 8, 4, -1, -1, 2, -1, 1, -1, 2, 8, 3, 6, -1, 8, 1, 10, -1, -1, -1, -1, 9, 10, -1, 12, 1, -1, -1, -1, 7, 3, -1, 8, 8, 8, 9, 12, 2, 1, 3, 6, 2, -1, -1, 2, 2, 10, 3, 1, -1, 1, -1, 1, 7, 4, -1, 1, 2, 10, 12, -1, 3, -1, 8, 10, 1, 2, -1, -1, 1, 8, 9, 8, 4, 12, 3, 1, -1, 8, 7, 9, 1, -1, -1, 2, 8, 8, 8, 8, -1, -1, 1, 8, -1, -1, 6, -1, 8, 7, 1, 4, 1, 2, -1, -1, 6, -1, 1, 3, 3, 4, -1, -1, 8, -1, 1, -1, -1, 9, 2, 1, -1, 9, 4, -1, -1, 9, -1, 4, 5, -1, 1, 3, 1, 2, 7, 7, -1, 1, -1, -1, 1, -1, 1, 1, 9, 10, 3, 12, 10, 1, 1, 2, 8, -1, 3, 2, 5, -1, 1, -1, 7, 1, -1, 7, 1, -1, 8, -1, -1, 8, 8, 1, 8, -1, -1, -1, 6, 1, -1, -1, 1, 8, 9, 4, -1, 4, -1, 4, 1, 8, 9, 8, 2, 5, 4, 7, 2, 7, 9, 8, -1, 9, 7, -1, 1, -1, 1, 3, 8, 1, -1, 1, -1, 8, 9, 9, 8, 1, -1, 2, -1, 1, 8, 12, -1, 2, 7, 2, 10, 1, -1, -1, 8, 1, 8, -1, 1, -1, 1, 8, 1, 4, 1, 8, 2, 3, -1, -1, 7, 1, 8, 10, 9, 9, 2, 8, 9, 3, 3, 2, -1, 8, 8, -1, 4, 3, -1, -1, 12, 8, -1, -1, -1, 1, 1, -1, 1, -1, 2, 8, -1, 1, 8, 1, 9, 3, -1, 8, 9, 2, 3, 10, 2, 8, 3, 12, 4, 1, 12, -1, 1, 7, 8, 11, 1, 3, 8, 3, 1, 8, 2, -1, -1, -1, 8, 2, -1, 4, -1, 8, -1, 6, 10, 2, 9, 1, 7, -1, 9, 5, 8, -1, 1, 2, 8, -1, 3, 1, -1, 1, 8, 9, 1, 1, -1, 8, 4, 3, 9, 1, 1, 1, 3, 8, 2, 8, 8, 8, 8, -1, 7, -1, 2, 3, -1, 9, 10, 8, 10, -1, 9, -1, 9, 2, 1, 1, -1, 1, -1, 8, 10, 10, 8, 8, 2, -1, 1, 8, -1, 2, 7, 8, -1, -1, -1, -1, -1, 1, -1, 8, 1, -1, -1, 10, -1, -1, 2, -1, 1, 1, 7, 2, 1, -1, 7, -1, -1, 12, -1, -1, 8, 2, -1, -1, 1, 8, -1, 2, -1, 1, 3, -1, 9, -1, 8, -1, 1, 1, -1, 10, 1, -1, 3, 9, -1, 9, 4, 1, 1, 2, 2, -1, -1, 2, 9, 1, 1, -1, 8, -1, 4, 7, 9, 7, 1, 9, -1, 8, 10, 8, -1, 7, 12, 12, 8, -1, 1]\n",
            "[1, 8, -1, 1, 2, 8, 3, 6, -1, 1, -1, 4, 1, -1, 8, -1, 2, -1, 1, 2, -1, -1, -1, 4, 2, 8, 2, 8, -1, 8, 8, 1, 8, -1, -1, 1, 1, 2, 2, 3, -1, 1, 10, -1, 2, 8, -1, 3, -1, -1, 3, 12, 1, 7, 2, -1, -1, -1, 9, 9, -1, 3, 6, 2, 1, -1, 8, -1, -1, -1, 3, -1, -1, 1, 2, -1, -1, -1, 1, 6, 1, -1, 10, -1, 7, -1, -1, -1, -1, -1, 2, 8, -1, -1, -1, 1, -1, 10, 8, -1, -1, 3, -1, 4, 1, -1, -1, 4, 8, 9, -1, 2, 1, 1, 8, 8, -1, 3, 2, -1, 8, -1, -1, -1, 2, 9, 3, 2, 8, 2, 9, 1, 8, -1, 9, -1, 4, 1, 1, -1, -1, 1, 9, 9, -1, -1, 8, 3, -1, 9, 2, 1, -1, -1, -1, -1, -1, -1, -1, 2, -1, 8, 12, -1, -1, -1, 8, 9, 4, 7, 3, 5, -1, 2, -1, -1, -1, 9, -1, -1, 2, -1, 9, 8, -1, -1, 8, -1, -1, -1, -1, 8, 8, -1, -1, -1, -1, 12, 1, 9, 8, -1, 1, -1, -1, -1, 8, 9, -1, 8, -1, 2, 7, 9, 1, 1, -1, 8, 2, 2, 8, 9, 8, 2, 1, 7, 1, -1, -1, 2, -1, 1, -1, -1, 8, -1, 9, -1, 7, 3, 1, 8, -1, -1, 1, 1, 7, 8, 1, -1, 2, 2, 3, -1, 9, 2, -1, -1, -1, 2, 8, 1, 1, 2, 8, -1, 2, 8, -1, 8, -1, 9, 8, -1, 7, -1, 8, 2, -1, -1, 7, 1, 2, 12, 1, -1, 9, -1, -1, 3, -1, 7, -1, 1, 8, 2, 1, 5, 1, 3, -1, 1, 2, 1, -1, 8, -1, 1, 8, 12, 7, 1, -1, 5, 1, 1, -1, 8, -1, -1, -1, 9, 2, -1, 1, -1, -1, 8, 2, -1, -1, 5, 1, 1, -1, -1, -1, 8, 2, 8, -1, 8, -1, -1, -1, 9, 6, -1, 2, 8, 1, 3, 8, 8, 1, -1, 1, 1, 9, -1, -1, 12, 3, 1, -1, 8, -1, -1, -1, 2, -1, 1, -1, 2, 4, 3, 6, -1, 1, 1, 8, -1, -1, -1, -1, 9, 8, -1, 12, 1, -1, -1, -1, 7, 3, -1, 8, 8, 8, 1, 9, 2, 1, 3, 6, 2, -1, -1, 2, 8, 8, 4, 1, -1, 1, -1, 1, 7, 4, -1, 1, 1, 8, 12, -1, 1, -1, 8, 10, 1, 2, -1, -1, 1, 8, 9, 10, 4, 12, -1, 1, -1, 8, 7, 9, 1, -1, -1, -1, 8, 8, 8, 8, -1, -1, 1, -1, -1, -1, 6, -1, 8, 7, 1, 4, -1, 2, -1, -1, 6, -1, -1, 3, 3, 4, -1, -1, 8, -1, 1, -1, -1, 9, 9, 1, -1, 8, 4, -1, -1, 1, -1, 1, 5, -1, 1, 3, 1, 8, -1, 7, -1, 1, -1, -1, 1, -1, 1, -1, 9, 10, 3, 12, 8, 1, 1, 8, -1, -1, 3, 2, 5, -1, 1, -1, 9, 1, -1, 8, 1, -1, 9, -1, -1, 8, 8, -1, -1, -1, -1, -1, 6, 1, -1, -1, 1, 8, 9, 2, -1, 8, -1, 2, -1, 8, 9, 8, 2, 5, 4, 7, 2, 8, 9, 8, -1, -1, 8, -1, 1, -1, 1, 3, 8, 1, -1, 1, -1, 8, 9, 9, 8, 1, -1, 2, -1, 1, 8, 12, -1, 2, -1, 2, 8, 1, -1, -1, 8, 1, 8, -1, -1, -1, 1, 3, 1, 8, 1, 8, 2, 3, -1, -1, -1, -1, 8, 10, 9, 2, 2, 8, -1, 8, 3, 4, -1, 8, 8, -1, 4, -1, -1, -1, 8, 8, -1, -1, -1, 1, 1, -1, -1, -1, 2, 8, -1, 1, -1, 1, 8, 3, -1, 8, 9, 2, 3, 10, -1, 8, 3, 12, 4, 1, -1, -1, 1, 7, 8, 10, 1, 3, 4, 3, 1, 8, 7, -1, -1, -1, 8, 4, -1, 1, -1, 9, -1, 6, -1, 2, -1, 1, 7, -1, -1, 5, 8, -1, 1, 2, 8, -1, 3, 1, -1, 1, -1, 8, 1, 1, -1, 8, 4, 3, 8, 1, 1, 1, 8, 8, 2, 8, 8, 9, 8, -1, -1, -1, 2, 1, -1, 9, 10, 8, -1, -1, 2, -1, 1, 1, 1, 1, -1, 1, -1, 8, 10, 10, 8, 8, 2, -1, 1, 8, -1, 8, -1, 8, -1, -1, -1, -1, -1, 1, -1, -1, 1, -1, -1, 10, -1, -1, 2, -1, 1, -1, 3, 3, -1, -1, 7, -1, -1, 8, -1, -1, 8, 2, -1, -1, 1, -1, -1, 2, -1, 1, 3, -1, -1, -1, 8, -1, 1, 1, -1, 10, 1, -1, -1, 8, -1, 9, 9, 1, 1, -1, 2, -1, -1, 2, 1, 1, 1, -1, 8, -1, 1, 7, 9, 7, 1, 2, -1, 8, 9, 8, -1, 7, 12, 12, -1, -1, -1]\n",
            "[1, 9, -1, 1, 2, 8, 3, 6, -1, 1, -1, 4, 1, -1, 8, -1, 2, -1, 1, 2, -1, -1, -1, 4, 2, 10, 2, 8, -1, 8, 8, 1, 8, -1, -1, -1, -1, 2, 2, 3, -1, 1, 1, -1, -1, -1, -1, 3, -1, -1, 3, 12, 3, 7, 2, -1, -1, -1, 9, 9, -1, 1, -1, 2, 1, -1, 9, -1, -1, -1, 3, -1, -1, 1, 2, -1, -1, -1, -1, 3, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 2, 8, -1, -1, -1, 1, -1, 10, 8, -1, -1, -1, -1, 1, 1, -1, -1, 4, 8, -1, -1, 2, 1, 1, 10, 9, -1, 3, -1, -1, 8, -1, -1, -1, 2, 1, 3, -1, 8, 9, 4, 1, -1, -1, 9, -1, 1, 1, 1, -1, -1, -1, 9, 9, -1, -1, 8, 3, -1, 9, 2, 1, -1, -1, -1, -1, -1, -1, -1, 2, -1, 8, -1, -1, -1, -1, 8, 9, 4, 7, 3, 5, -1, 2, -1, -1, -1, 9, -1, -1, 3, -1, 9, -1, -1, -1, 8, -1, -1, -1, -1, 10, 9, -1, -1, -1, -1, -1, 1, -1, 8, -1, 1, -1, -1, -1, -1, 9, -1, 8, -1, -1, 2, 9, 1, 8, -1, 8, -1, 2, 8, 9, 4, 2, -1, 7, 1, -1, -1, 2, -1, 1, -1, -1, 9, -1, -1, -1, -1, 3, 1, 8, -1, -1, 1, -1, 7, 8, 1, -1, 2, 2, 12, -1, -1, 3, -1, -1, -1, 2, 8, 1, 1, 2, 9, -1, 2, 10, -1, -1, -1, 9, 8, -1, -1, -1, 8, 2, -1, -1, -1, 12, 2, 8, 1, -1, 9, -1, -1, 3, -1, 7, -1, -1, 9, 2, 4, 5, -1, -1, -1, 1, 2, 1, -1, 8, -1, 1, 8, -1, -1, 1, -1, 5, 1, 1, -1, 8, -1, -1, -1, 9, 2, -1, 1, -1, -1, 8, 2, -1, -1, 5, 1, 1, -1, -1, -1, 8, 2, 8, -1, 8, -1, -1, -1, -1, 8, -1, -1, -1, -1, 3, 3, 9, 1, -1, 1, 1, 9, -1, -1, 12, 3, -1, -1, 8, -1, -1, -1, 2, -1, 1, -1, -1, -1, 3, 9, -1, 1, 1, 10, -1, -1, -1, -1, 9, 8, -1, 12, 1, -1, -1, -1, 7, -1, -1, 8, 8, -1, 1, 8, 2, -1, 7, 2, 2, -1, -1, 2, 2, 8, 3, 1, -1, 1, -1, 1, -1, 5, -1, 1, 1, 10, 12, -1, -1, -1, 8, 10, -1, 2, -1, -1, 1, -1, 9, 10, 1, 8, -1, 1, -1, 8, -1, -1, -1, -1, -1, -1, 8, 8, 8, 8, -1, -1, 1, -1, -1, -1, 6, -1, -1, 7, -1, 4, -1, 2, -1, -1, 6, -1, -1, 3, 4, 2, -1, -1, 8, -1, 1, -1, -1, 9, 2, 1, -1, -1, 8, -1, -1, 9, -1, 1, 9, -1, 1, -1, 1, 2, -1, 10, -1, 1, -1, -1, 1, -1, 1, -1, 9, 10, 3, 12, 10, 1, 1, 2, -1, -1, 3, 2, 5, -1, 1, -1, 9, 1, -1, 8, -1, -1, 8, -1, -1, 8, 8, -1, -1, -1, -1, -1, 6, 1, -1, -1, 1, 8, 9, 10, -1, 10, -1, 2, -1, -1, -1, 8, 2, 5, 8, 7, 2, 8, -1, 8, -1, -1, 8, -1, 1, -1, 1, 3, 8, 1, -1, 1, -1, 8, 9, 9, 8, 1, -1, 2, -1, 1, 8, 12, -1, 2, -1, 2, 10, 1, -1, -1, 8, 1, 8, -1, -1, -1, 1, 1, 1, 8, 1, 8, 2, 3, -1, -1, -1, -1, 8, -1, -1, -1, 2, 8, -1, 8, 3, -1, -1, 8, 4, -1, 1, -1, -1, -1, 10, 8, -1, -1, -1, 1, 1, -1, -1, -1, 1, 8, -1, 1, -1, 1, 8, 3, -1, 8, 9, 2, 3, 10, -1, 8, 3, 12, 4, 1, -1, -1, 1, 7, 8, 10, 1, 3, -1, 3, 1, 8, 7, -1, -1, -1, -1, -1, -1, 1, -1, 8, -1, 6, -1, 2, -1, 1, -1, -1, -1, 5, 8, -1, -1, 2, 8, -1, 2, 1, -1, 1, -1, 8, 1, 1, -1, 8, 4, 3, 8, 1, 1, 1, 3, 8, 2, 8, -1, 9, 8, -1, -1, -1, 2, 1, -1, 9, -1, 8, -1, -1, 9, -1, 2, 2, 1, 1, -1, 1, -1, 8, 10, -1, 8, 10, 2, -1, -1, 8, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, 1, -1, -1, 9, -1, -1, 2, -1, 1, -1, 7, 3, -1, -1, 7, -1, -1, -1, -1, -1, 8, 2, -1, -1, 1, -1, -1, 2, -1, 1, 3, -1, -1, -1, 8, -1, 1, 1, -1, 10, 3, -1, -1, 8, -1, 10, 9, 1, 1, -1, 2, -1, -1, 2, 9, -1, -1, -1, 4, -1, 1, 10, 9, 8, 1, 1, -1, 8, 9, 8, -1, 7, -1, 12, -1, -1, -1]\n",
            "0.7565320665083135\n",
            "[1, 8, 3, 1, 2, 10, 3, 6, 4, 1, 8, 4, 1, 4, 3, 7, 2, 9, 1, 2, 8, 9, 9, 4, 2, 10, 2, 8, 1, 8, 12, 1, 8, 1, 3, 1, 9, 2, 2, 3, 2, 1, 1, 2, 2, 8, 1, 3, 4, 8, 3, 12, 1, 7, 2, 8, 9, 1, 9, 9, 3, 3, 6, 2, 1, 1, 10, 1, 10, 1, 3, 8, 7, 1, 2, 2, 9, 10, 1, 3, 1, 11, 10, 1, 7, 9, 9, 12, 8, 1, 2, 8, 1, 2, 1, 1, 2, 10, 8, 2, 7, 3, 10, 2, 1, 9, 7, 4, 8, 9, 10, 2, 1, 1, 10, 9, 3, 3, 9, 1, 8, 8, 7, 1, 2, 9, 3, 2, 8, 2, 9, 1, 8, 3, 9, 8, 1, 1, 1, 2, 9, 1, 10, 8, 1, 9, 8, 3, 8, 9, 2, 1, 3, 7, 2, 0, 3, 10, 8, 10, 9, 8, 12, 4, 12, 1, 8, 9, 4, 7, 3, 5, 7, 2, 8, 9, 3, 9, 12, 7, 2, 9, 9, 3, 10, 8, 8, 9, 12, 1, 1, 10, 10, 1, 2, 9, 10, 12, 1, 9, 3, 2, 1, 3, 10, 8, 8, 9, 9, 8, 8, 2, 2, 9, 1, 1, 2, 8, 3, 2, 8, 9, 8, 2, 1, 7, 3, 2, 4, 2, 2, 1, 9, 1, 9, 2, 9, 1, 8, 3, 1, 9, 8, 12, 1, 1, 7, 8, 1, 2, 9, 2, 12, 1, 9, 3, 1, 9, 9, 2, 8, 5, 1, 2, 9, 2, 2, 10, 8, 8, 2, 2, 8, 2, 7, 1, 9, 2, 2, 3, 7, 12, 2, 12, 1, 1, 9, 12, 1, 3, 2, 7, 3, 9, 8, 2, 1, 5, 1, 3, 9, 1, 2, 9, 8, 8, 2, 3, 8, 12, 10, 1, 7, 5, 1, 1, 1, 8, 9, 3, 9, 9, 2, 1, 1, 9, 1, 9, 2, 1, 7, 5, 1, 1, 1, 3, 1, 8, 2, 8, 1, 7, 7, 3, 1, 10, 8, 1, 2, 8, 1, 3, 3, 9, 1, 2, 1, 1, 9, 8, 1, 12, 3, 1, 1, 8, 4, 3, 3, 2, 9, 1, 2, 2, 2, 3, 6, 9, 1, 1, 10, 12, 8, 8, 1, 9, 10, 1, 12, 1, 9, 8, 9, 7, 3, 8, 8, 8, 8, 1, 12, 2, 1, 3, 6, 2, 3, 2, 2, 8, 10, 3, 1, 3, 1, 9, 1, 7, 9, 4, 1, 2, 10, 12, 9, 3, 2, 8, 10, 1, 2, 3, 9, 5, 8, 9, 10, 1, 12, 3, 1, 2, 8, 7, 9, 1, 2, 9, 2, 9, 3, 8, 8, 2, 8, 1, 8, 1, 1, 6, 8, 8, 7, 1, 4, 1, 2, 9, 3, 6, 12, 1, 3, 3, 2, 3, 1, 8, 8, 1, 9, 9, 9, 9, 1, 9, 9, 9, 10, 9, 1, 3, 1, 5, 2, 1, 1, 1, 2, 9, 7, 8, 1, 8, 1, 1, 4, 1, 1, 9, 10, 3, 12, 10, 1, 1, 2, 8, 2, 3, 2, 5, 3, 1, 3, 9, 1, 1, 7, 1, 1, 9, 8, 2, 8, 8, 1, 8, 1, 8, 10, 6, 1, 9, 1, 1, 8, 9, 3, 10, 8, 8, 2, 1, 8, 9, 8, 9, 5, 4, 7, 2, 8, 9, 8, 12, 8, 10, 9, 1, 2, 1, 3, 8, 1, 10, 1, 9, 8, 9, 2, 8, 1, 1, 2, 2, 1, 4, 12, 2, 2, 7, 9, 8, 1, 10, 9, 9, 1, 8, 11, 1, 9, 1, 3, 1, 1, 1, 8, 2, 3, 9, 10, 10, 1, 8, 8, 9, 9, 2, 8, 9, 12, 3, 4, 2, 8, 4, 2, 1, 1, 9, 3, 10, 8, 3, 3, 9, 1, 1, 9, 1, 8, 2, 8, 1, 1, 11, 1, 9, 3, 3, 8, 9, 2, 3, 10, 2, 8, 3, 12, 4, 1, 12, 12, 1, 7, 8, 11, 1, 3, 9, 3, 1, 8, 7, 4, 8, 2, 8, 4, 8, 2, 2, 8, 8, 6, 10, 2, 9, 1, 7, 9, 9, 5, 8, 1, 1, 2, 8, 9, 9, 1, 2, 1, 8, 8, 1, 1, 2, 8, 4, 3, 9, 1, 1, 1, 9, 8, 2, 8, 8, 9, 8, 1, 7, 1, 2, 1, 8, 9, 10, 8, 10, 9, 9, 1, 1, 2, 1, 1, 9, 1, 2, 8, 10, 10, 8, 8, 2, 9, 1, 8, 2, 9, 7, 8, 1, 8, 1, 1, 3, 1, 1, 4, 1, 8, 2, 10, 1, 1, 2, 3, 1, 1, 9, 3, 1, 8, 7, 2, 8, 8, 1, 9, 8, 2, 1, 7, 1, 8, 2, 2, 12, 1, 3, 8, 9, 1, 8, 8, 1, 1, 2, 10, 1, 9, 1, 10, 10, 10, 9, 1, 1, 2, 2, 9, 10, 2, 9, 1, 1, 9, 9, 8, 1, 3, 9, 7, 1, 2, 12, 8, 9, 8, 9, 7, 12, 12, 8, 2, 1]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "final_pred_4=[]\n",
        "\n",
        "num_correct=0\n",
        "model_0_4=load_model(loss,accuracy,optimizer,'0-4')\n",
        "mod_labels_0=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_0:\n",
        "    pred_test_0_4=model_0_4.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_0_4 = pred_test_0_4.argmax(axis=1)\n",
        "    mod_labels_0.append(pred_labels_0_4[0])\n",
        "  else:\n",
        "    mod_labels_0.append(-1)\n",
        "\n",
        "model_1_4=load_model(loss,accuracy,optimizer,'1-4')\n",
        "mod_labels_1=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        " \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_1:\n",
        "    pred_test_1_4=model_1_4.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_1_4 = pred_test_1_4.argmax(axis=1)\n",
        "    mod_labels_1.append(pred_labels_1_4[0])\n",
        "  else:\n",
        "    mod_labels_1.append(-1)\n",
        "\n",
        "model_2_4=load_model(loss,accuracy,optimizer,'2-4')\n",
        "mod_labels_2=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_2:\n",
        "    pred_test_2_4=model_2_4.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_2_4 = pred_test_2_4.argmax(axis=1)\n",
        "    mod_labels_2.append(pred_labels_2_4[0])\n",
        "  else:\n",
        "    mod_labels_2.append(-1)\n",
        "\n",
        "model_3_4=load_model(loss,accuracy,optimizer,'3-4')\n",
        "mod_labels_3=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_3:\n",
        "    pred_test_3_4=model_3_4.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_3_4 = pred_test_3_4.argmax(axis=1)\n",
        "    mod_labels_3.append(pred_labels_3_4[0])\n",
        "  else:\n",
        "    mod_labels_3.append(-1)\n",
        "\n",
        "model_4_4=load_model(loss,accuracy,optimizer,'4-4')\n",
        "mod_labels_4=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_4:\n",
        "    pred_test_4_4=model_4_4.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_4_4 = pred_test_4_4.argmax(axis=1)\n",
        "    mod_labels_4.append(pred_labels_4_4[0])\n",
        "  else:\n",
        "    mod_labels_4.append(-1)\n",
        "\n",
        "model_5_4=load_model(loss,accuracy,optimizer,'5-4')\n",
        "mod_labels_5=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_5:\n",
        "    pred_test_5_4=model_5_4.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_5_4 = pred_test_5_4.argmax(axis=1)\n",
        "    mod_labels_5.append(pred_labels_5_4[0])\n",
        "  else:\n",
        "    mod_labels_5.append(-1)\n",
        "\n",
        "print(mod_labels_0)\n",
        "print(mod_labels_1)\n",
        "print(mod_labels_2)\n",
        "print(mod_labels_3)\n",
        "print(mod_labels_4)\n",
        "print(mod_labels_5)\n",
        "\n",
        "\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  fin_labels=[]\n",
        "  if mod_labels_0[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  if mod_labels_1[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  if mod_labels_2[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  if mod_labels_3[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  if mod_labels_4[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  if mod_labels_5[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  res=max(set(fin_labels), key = fin_labels.count)\n",
        "  final_pred_4.append(res)\n",
        "\n",
        "  if res==new_val_label_0[i]:\n",
        "    num_correct=num_correct+1\n",
        "\n",
        "accuracy=num_correct/len(new_val_label_0)\n",
        "print(accuracy)\n",
        "  \n",
        "print(final_pred_4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhJg0uGdpPP6",
        "outputId": "0e6840e0-f34f-4dec-a604-bdb8fbc2385f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7565320665083135\n",
            "Weighted F1: 0.7553445868527437\n",
            "Micro F1: 0.7565320665083135\n",
            "Weighted Precision: 0.7593648221436535\n",
            "Micro Precision: 0.7565320665083135\n",
            "Weighted Recall: 0.7565320665083135\n",
            "Micro Recall: 0.7565320665083135\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "\n",
        "accuracy=accuracy_score(new_val_label_0, final_pred_4)\n",
        "print(\"Accuracy: \"+str(accuracy))\n",
        "total_accuracy=total_accuracy+accuracy\n",
        "\n",
        "weighted_f1=f1_score(new_val_label_0,final_pred_4, average='weighted')\n",
        "print(\"Weighted F1: \"+ str(weighted_f1))\n",
        "total_weighted_f1=total_weighted_f1+weighted_f1\n",
        "micro_f1=f1_score(new_val_label_0,final_pred_4, average='micro')\n",
        "print(\"Micro F1: \"+ str(micro_f1))\n",
        "total_micro_f1=total_micro_f1+micro_f1\n",
        "\n",
        "weighted_precision=precision_score(new_val_label_0, final_pred_4, average='weighted')\n",
        "print(\"Weighted Precision: \" + str(weighted_precision))\n",
        "total_weighted_precision=total_weighted_precision+weighted_precision\n",
        "micro_precision=precision_score(new_val_label_0, final_pred_4, average='micro')\n",
        "print(\"Micro Precision: \" + str(micro_precision))\n",
        "total_micro_precision=total_micro_precision+micro_precision\n",
        "\n",
        "weighted_recall=recall_score(new_val_label_0, final_pred_4, average='weighted')\n",
        "print(\"Weighted Recall: \" + str(weighted_recall))\n",
        "total_weighted_recall=total_weighted_recall+weighted_recall\n",
        "micro_recall=recall_score(new_val_label_0, final_pred_4, average='micro')\n",
        "print(\"Micro Recall: \" + str(micro_recall))\n",
        "total_micro_recall=total_micro_recall+micro_recall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UnV81NZ-paGM",
        "outputId": "0ab610d5-8418-4452-8b80-55f313ed27c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Accuracy: 0.7553444180522565\n",
            "Average Weighted F1: 0.7527085179362435\n",
            "Average Micro F1: 0.7553444180522566\n",
            "Average Weighted Precision: 0.7557849104325605\n",
            "Average Micro Precision: 0.7553444180522565\n",
            "Average Weighted Recall: 0.7553444180522565\n",
            "Average Micro Recall: 0.7553444180522565\n"
          ]
        }
      ],
      "source": [
        "print(\"Average Accuracy: \"+str(total_accuracy/5))\n",
        "print(\"Average Weighted F1: \"+str(total_weighted_f1/5))\n",
        "print(\"Average Micro F1: \"+str(total_micro_f1/5))\n",
        "print(\"Average Weighted Precision: \"+str(total_weighted_precision/5))\n",
        "print(\"Average Micro Precision: \"+str(total_micro_precision/5))\n",
        "print(\"Average Weighted Recall: \"+str(total_weighted_recall/5))\n",
        "print(\"Average Micro Recall: \"+str(total_micro_recall/5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wl_ZrlhOsMJz"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "BERT_Ensemble_15labels.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "52fbd984e497486495d47fc4eae74d51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2235f9c65a89430abcde7ef98e00ec57",
              "IPY_MODEL_91485b6e59e24168afe6ee8ba41e0eda",
              "IPY_MODEL_6c006c66bd1144fcb2ab98da512f47de"
            ],
            "layout": "IPY_MODEL_955453ec403041a58a61520714947e73"
          }
        },
        "2235f9c65a89430abcde7ef98e00ec57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9adb12a1141d448ba72cdadf735128c4",
            "placeholder": "​",
            "style": "IPY_MODEL_3496e8cb43614eb799e7903e9c7af680",
            "value": "Downloading: 100%"
          }
        },
        "91485b6e59e24168afe6ee8ba41e0eda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5edf3c347b7a4c0f9f6efd10af96c783",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e65a4429c4bc432daaada9f1e4e477ad",
            "value": 231508
          }
        },
        "6c006c66bd1144fcb2ab98da512f47de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c32dc3b482d7420380a7b13f262071b8",
            "placeholder": "​",
            "style": "IPY_MODEL_88f627dc3d424590b20557719e5880ec",
            "value": " 226k/226k [00:00&lt;00:00, 689kB/s]"
          }
        },
        "955453ec403041a58a61520714947e73": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9adb12a1141d448ba72cdadf735128c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3496e8cb43614eb799e7903e9c7af680": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5edf3c347b7a4c0f9f6efd10af96c783": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e65a4429c4bc432daaada9f1e4e477ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c32dc3b482d7420380a7b13f262071b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88f627dc3d424590b20557719e5880ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "13df4bb550f74a448c6aac4a0b5a9685": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_18edebaedfc546f1b4904998e8ef1253",
              "IPY_MODEL_fb26cf05409549dbb4a1481bc74ee842",
              "IPY_MODEL_8b08515646f349a7b74b5e5e051a0011"
            ],
            "layout": "IPY_MODEL_1cb807ccb00a478d9f2ee050e4bc03e3"
          }
        },
        "18edebaedfc546f1b4904998e8ef1253": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d19867d167fe43aeacb79c4c1ac8b9b9",
            "placeholder": "​",
            "style": "IPY_MODEL_292bb12525f74aafb81934a9699930ac",
            "value": "Downloading: 100%"
          }
        },
        "fb26cf05409549dbb4a1481bc74ee842": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d1617be4e72469da767be64fce1c706",
            "max": 28,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_53f90735189944a0b761c929066d6ab6",
            "value": 28
          }
        },
        "8b08515646f349a7b74b5e5e051a0011": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9e9f14e45fc740e8a18ba23f8662c643",
            "placeholder": "​",
            "style": "IPY_MODEL_2611f291dd664a75b0600579410456e8",
            "value": " 28.0/28.0 [00:00&lt;00:00, 1.00kB/s]"
          }
        },
        "1cb807ccb00a478d9f2ee050e4bc03e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d19867d167fe43aeacb79c4c1ac8b9b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "292bb12525f74aafb81934a9699930ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5d1617be4e72469da767be64fce1c706": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53f90735189944a0b761c929066d6ab6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9e9f14e45fc740e8a18ba23f8662c643": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2611f291dd664a75b0600579410456e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1da7d15a66c445f78b3bf1ba36bf434b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4948d8bac4c74da68127c8369baff037",
              "IPY_MODEL_802601a6a29045418d483721d33c5305",
              "IPY_MODEL_256a4ce646d047ca808e075134fbd1e8"
            ],
            "layout": "IPY_MODEL_4886d8802a434523a6a4bb95b6d0d814"
          }
        },
        "4948d8bac4c74da68127c8369baff037": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c60f79f709834c5694476df90f4659e6",
            "placeholder": "​",
            "style": "IPY_MODEL_079151fd9bc14228ae89603421bf11cc",
            "value": "Downloading: 100%"
          }
        },
        "802601a6a29045418d483721d33c5305": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_74fbc094d10a41efb1021730c2212a86",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_157a062954124a95aaef53e5f888f4f2",
            "value": 570
          }
        },
        "256a4ce646d047ca808e075134fbd1e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9c1ad36c4d06400e9b7cef714c4549fa",
            "placeholder": "​",
            "style": "IPY_MODEL_eaa69447e8f34d069ccc705bca9bbb15",
            "value": " 570/570 [00:00&lt;00:00, 18.1kB/s]"
          }
        },
        "4886d8802a434523a6a4bb95b6d0d814": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c60f79f709834c5694476df90f4659e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "079151fd9bc14228ae89603421bf11cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "74fbc094d10a41efb1021730c2212a86": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "157a062954124a95aaef53e5f888f4f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9c1ad36c4d06400e9b7cef714c4549fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eaa69447e8f34d069ccc705bca9bbb15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "740efe6ecf6044c3a241a55e3c4234eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c7eba4ff1d7d45f6837c502aaf104b5c",
              "IPY_MODEL_0a9cc6f9f86b467f91bc8687d5cd08d5",
              "IPY_MODEL_d92767977a2b4f85bb550f1f556e13c7"
            ],
            "layout": "IPY_MODEL_63e91923708946ce8c97f021cbfe47e9"
          }
        },
        "c7eba4ff1d7d45f6837c502aaf104b5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f4c768be8544465f9008e682c8a3942b",
            "placeholder": "​",
            "style": "IPY_MODEL_1629c98f99a0471ca3a502178b720bab",
            "value": "Downloading: 100%"
          }
        },
        "0a9cc6f9f86b467f91bc8687d5cd08d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4cd9a7b07e87483ba8732fa1596840ad",
            "max": 536063208,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d6cae863f8cd426ba66b5656b9ebe522",
            "value": 536063208
          }
        },
        "d92767977a2b4f85bb550f1f556e13c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5026e967f4bc4991903e342d18c947d4",
            "placeholder": "​",
            "style": "IPY_MODEL_9f1923688c174c53acbc2e20963ce68c",
            "value": " 511M/511M [00:08&lt;00:00, 61.7MB/s]"
          }
        },
        "63e91923708946ce8c97f021cbfe47e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4c768be8544465f9008e682c8a3942b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1629c98f99a0471ca3a502178b720bab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4cd9a7b07e87483ba8732fa1596840ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6cae863f8cd426ba66b5656b9ebe522": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5026e967f4bc4991903e342d18c947d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f1923688c174c53acbc2e20963ce68c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}