{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mj3gUkvvEZW2"
      },
      "source": [
        "### <font color='blue'>Import all packages</font> ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVqcG_Q_FX4T",
        "outputId": "7d891ab0-a4cf-4d75-bd6a-9073190f7f8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "#drive.flush_and_unmount()\n",
        "drive.mount('/content/drive')\n",
        "#drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "AwA6C92yJyaP",
        "outputId": "fd26bacd-b472-4200-a074-87ba0d0ad5bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.19.2)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.5.18.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow==2.7.0 in /usr/local/lib/python3.7/dist-packages (2.7.0+zzzcolab20220506150900)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (3.3.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (1.1.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (2.7.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (3.1.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (0.26.0)\n",
            "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (0.4.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (1.0.0)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (3.10.0.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (3.17.3)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (14.0.1)\n",
            "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (2.7.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (0.2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (1.1.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (1.46.3)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (1.21.6)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (2.8.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (1.6.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow==2.7.0) (1.5.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (0.6.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (2.23.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (3.3.7)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (1.35.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (1.0.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.7.0) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow==2.7.0) (4.11.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow==2.7.0) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0) (2022.5.18.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.7.0) (3.2.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: stanza in /usr/local/lib/python3.7/dist-packages (1.4.0)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.7/dist-packages (from stanza) (1.7.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (from stanza) (4.19.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from stanza) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from stanza) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from stanza) (4.64.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from stanza) (3.17.3)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from stanza) (1.11.0+cu113)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from stanza) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3.0->stanza) (3.10.0.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->stanza) (2022.5.18.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->stanza) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->stanza) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->stanza) (3.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers->stanza) (21.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers->stanza) (0.12.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers->stanza) (6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers->stanza) (0.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers->stanza) (3.7.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers->stanza) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers->stanza) (4.11.4)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers->stanza) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers->stanza) (3.8.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.19.2)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.5.18.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.7/dist-packages (0.17.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (21.3)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow-addons) (3.0.9)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: textacy in /usr/local/lib/python3.7/dist-packages (0.11.0)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from textacy) (2.6.3)\n",
            "Requirement already satisfied: cytoolz>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from textacy) (0.11.2)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from textacy) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.19.0 in /usr/local/lib/python3.7/dist-packages (from textacy) (1.0.2)\n",
            "Requirement already satisfied: spacy>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from textacy) (3.3.0)\n",
            "Requirement already satisfied: tqdm>=4.19.6 in /usr/local/lib/python3.7/dist-packages (from textacy) (4.64.0)\n",
            "Requirement already satisfied: cachetools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from textacy) (4.2.4)\n",
            "Requirement already satisfied: jellyfish>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from textacy) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from textacy) (1.21.6)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from textacy) (2.23.0)\n",
            "Requirement already satisfied: pyphen>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from textacy) (0.12.0)\n",
            "Requirement already satisfied: joblib>=0.13.0 in /usr/local/lib/python3.7/dist-packages (from textacy) (1.1.0)\n",
            "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from cytoolz>=0.10.1->textacy) (0.11.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->textacy) (2022.5.18.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->textacy) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->textacy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->textacy) (2.10)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.0->textacy) (3.1.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (2.11.3)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (3.10.0.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (3.0.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (21.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (0.9.1)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (0.6.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (2.0.6)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (3.0.6)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (0.4.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (1.0.7)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (3.3.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (1.8.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (2.4.3)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (1.0.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (2.0.7)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (8.0.17)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (57.4.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (0.4.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy>=3.0.0->textacy) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy>=3.0.0->textacy) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy>=3.0.0->textacy) (5.2.1)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy>=3.0.0->textacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy>=3.0.0->textacy) (2.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "!pip install tensorflow==2.7.0\n",
        "!pip install stanza\n",
        "!pip install transformers\n",
        "!pip install tensorflow-addons\n",
        "!pip install nltk\n",
        "!pip install textacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jV9qKCQ3qpl",
        "outputId": "1db131a8-3400-422e-9c9e-ad3c5122c383"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.7.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DC59AD4KEZW7"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from textacy.datasets.supreme_court import SupremeCourt\n",
        "import numpy as np\n",
        "import re\n",
        "import unicodedata\n",
        "import nltk\n",
        "#from transformers import pipeline\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Dense,Dropout, Input, BatchNormalization\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "from sklearn.metrics import confusion_matrix,f1_score,classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "from sklearn.utils import shuffle\n",
        "from tensorflow.keras import regularizers\n",
        "#from transformers import *\n",
        "from transformers import BertTokenizer, TFBertModel, BertConfig,TFDistilBertModel,DistilBertTokenizer,DistilBertConfig\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, TFAutoModel\n",
        "import numpy as np\n",
        "import gc\n",
        "import math\n",
        "import json\n",
        "import stanza\n",
        "from tensorflow.keras import *\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import *\n",
        "import tensorflow.keras.backend as K\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import classification_report\n",
        "from transformers import TFRobertaModel,RobertaTokenizer\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.initializers import RandomUniform\n",
        "\n",
        "from numpy.random import seed\n",
        "import random as python_random\n",
        "import os\n",
        "import sys\n",
        "\n",
        "np.random.seed(1)\n",
        "python_random.seed(1)\n",
        "tf.random.set_seed(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OzfozpOUBkOX"
      },
      "outputs": [],
      "source": [
        "!cp \"/content/drive/My Drive/summarized_sc.txt\" \"./summarized_sc.txt\"\n",
        "!cp \"/content/drive/My Drive/labels_sc.txt\" \"./labels_sc.txt\"\n",
        "!cp \"/content/drive/My Drive/labels_sc_279.txt\" \"./labels_sc_279.txt\"\n",
        "!cp \"/content/drive/My Drive/sc_model_0.txt\" \"./sc_model_0.txt\"\n",
        "!cp \"/content/drive/My Drive/sc_model_1.txt\" \"./sc_model_1.txt\"\n",
        "!cp \"/content/drive/My Drive/sc_model_2.txt\" \"./sc_model_2.txt\"\n",
        "!cp \"/content/drive/My Drive/sc_model_3.txt\" \"./sc_model_3.txt\"\n",
        "!cp \"/content/drive/My Drive/sc_model_4.txt\" \"./sc_model_4.txt\"\n",
        "!cp \"/content/drive/My Drive/sc_model_5.txt\" \"./sc_model_5.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cVKqFgEEWBz",
        "outputId": "426cbd08-42fc-4e1f-fe8a-fbfdcca68c47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8419\n",
            "8419\n",
            "Average Length 489.8335906877301\n",
            "['8', '1', '8', '2', '8', '8', '8', '9', '7', '8', '1', '2', '1', '1', '8', '4', '8', '8', '12', '3', '3', '7', '3', '12', '1', '8', '8', '8', '8', '8', '8', '8', '1', '9', '5', '9', '9', '9', '11', '8', '8', '8', '4', '8', '8', '8', '8', '1', '3', '9', '3', '7', '1', '2', '9', '9', '7', '8', '8', '1', '10', '7', '8', '8', '9', '8', '7', '9', '9', '12', '7', '2', '8', '1', '11', '11', '1', '7', '7', '12', '1', '9', '8', '10', '12', '7', '8', '2', '8', '9', '9', '1', '8', '9', '1', '7', '12', '10', '10', '10', '8', '3', '7', '9', '8', '9', '1', '8', '8', '2', '7', '2', '9', '9', '11', '8', '8', '12', '12', '7', '8', '12', '4', '9', '3', '1', '12', '1', '1', '8', '8', '3', '8', '8', '8', '8', '9', '1', '8', '8', '10', '1', '8', '2', '8', '8', '7', '3', '8', '2', '4', '4', '9', '8', '10', '12', '12', '1', '1', '9', '1', '1', '1', '8', '2', '2', '8', '1', '1', '2', '2', '1', '2', '8', '1', '9', '9', '8', '8', '4', '2', '9', '9', '8', '3', '4', '3', '1', '8', '8', '2', '1', '9', '7', '8', '8', '1', '12', '3', '8', '2', '4', '2', '9', '12', '1', '4', '1', '8', '8', '8', '2', '2', '8', '9', '8', '8', '8', '10', '9', '8', '7', '9', '1', '1', '9', '4', '2', '4', '2', '2', '1', '7', '8', '11', '11', '3', '9', '2', '4', '8', '9', '1', '8', '1', '1', '4', '9', '1', '1', '8', '8', '2', '1', '8', '4', '2', '8', '9', '8', '8', '2', '8', '8', '8', '7', '1', '1', '1', '2', '1', '1', '8', '7', '8', '8', '12', '2', '12', '12', '8', '10', '12', '8', '3', '3', '12', '10', '1', '8', '12', '1', '8', '8', '2', '8', '4', '7', '8', '7', '10', '8', '10', '9', '8', '12', '12', '1', '8', '8', '3', '8', '8', '8', '8', '8', '1', '9', '8', '11', '1', '1', '1', '9', '8', '1', '9', '2', '3', '11', '8', '9', '9', '9', '2', '1', '8', '8', '9', '7', '1', '4', '9', '4', '8', '8', '4', '8', '12', '9', '4', '8', '2', '10', '10', '10', '8', '9', '9', '8', '8', '12', '7', '1', '8', '8', '8', '4', '1', '1', '1', '1', '1', '1', '8', '1', '9', '8', '9', '9', '4', '8', '12', '9', '8', '8', '2', '8', '8', '8', '6', '9', '8', '3', '7', '8', '8', '4', '12', '8', '8', '9', '12', '12', '9', '8', '2', '9', '2', '3', '1', '12', '8', '10', '9', '9', '9', '10', '10', '3', '8', '12', '1', '4', '2', '1', '10', '8', '2', '8', '4', '8', '9', '1', '9', '9', '10', '10', '1', '4', '9', '2', '4', '9', '1', '1', '3', '10', '3', '3', '8', '7', '3', '8', '9', '9', '12', '4', '8', '12', '2', '2', '4', '1', '9', '9', '4', '1', '4', '2', '8', '12', '2', '3', '10', '10', '9', '8', '9', '9', '1', '12', '8', '8', '8', '12', '4', '1', '8', '8', '1', '9', '8', '8', '2', '1', '8', '9', '8', '3', '3', '3', '1', '8', '8', '9', '1', '10', '9', '9', '9', '9', '5', '9', '9', '8', '8', '8', '8', '8', '8', '8', '8', '11', '12', '8', '8', '1', '8', '9', '11', '2', '2', '2', '2', '3', '1', '2', '2', '8', '2', '4', '9', '1', '2', '9', '8', '2', '8', '9', '9', '3', '10', '9', '9', '2', '8', '9', '8', '12', '12', '1', '3', '8', '8', '8', '2', '7', '7', '7', '7', '3', '9', '1', '9', '8', '9', '9', '1', '1', '1', '2', '9', '9', '9', '11', '1', '8', '8', '9', '1', '9', '8', '8', '8', '1', '1', '8', '7', '1', '1', '8', '8', '9', '4', '4', '8', '2', '2', '8', '8', '8', '8', '8', '8', '11', '8', '2', '9', '4', '9', '3', '9', '9', '1', '3', '9', '3', '1', '12', '8', '9', '12', '1', '8', '4', '2', '1', '4', '8', '3', '3', '8', '2', '8', '9', '7', '8', '8', '8', '5', '8', '3', '9', '8', '8', '13', '12', '1', '1', '2', '8', '4', '1', '9', '9', '12', '8', '9', '12', '9', '1', '9', '9', '9', '9', '3', '2', '9', '9', '4', '8', '12', '2', '4', '9', '3', '1', '9', '7', '8', '9', '9', '8', '4', '8', '8', '7', '9', '10', '3', '8', '8', '8', '1', '1', '1', '1', '8', '8', '4', '1', '10', '1', '5', '7', '7', '1', '8', '9', '3', '7', '2', '7', '7', '2', '4', '8', '12', '7', '4', '2', '9', '9', '12', '6', '10', '8', '2', '4', '12', '9', '9', '3', '8', '8', '1', '2', '10', '9', '9', '8', '4', '12', '2', '1', '8', '8', '8', '12', '10', '10', '9', '3', '8', '8', '9', '2', '8', '10', '1', '1', '1', '1', '2', '1', '1', '1', '1', '9', '8', '12', '9', '4', '8', '8', '9', '1', '9', '3', '9', '8', '8', '1', '7', '7', '10', '1', '8', '8', '1', '9', '8', '10', '3', '1', '7', '1', '8', '8', '12', '8', '8', '1', '8', '7', '1', '7', '7', '8', '2', '1', '8', '8', '2', '10', '8', '8', '8', '8', '8', '10', '1', '8', '8', '12', '8', '3', '3', '2', '2', '2', '10', '8', '8', '8', '2', '9', '1', '8', '9', '3', '2', '8', '10', '8', '6', '1', '1', '8', '4', '1', '9', '10', '8', '1', '7', '1', '2', '8', '1', '1', '1', '12', '1', '9', '12', '8', '12', '12', '12', '8', '8', '12', '4', '8', '8', '8', '8', '9', '9', '1', '3', '3', '3', '3', '1', '12', '12', '9', '10', '8', '8', '1', '9', '2', '2', '13', '9', '8', '9', '2', '1', '9', '1', '8', '8', '8', '4', '8', '1', '1', '1', '12', '12', '7', '2', '2', '2', '8', '3', '8', '9', '2', '10', '7', '8', '9', '2', '1', '2', '12', '12', '8', '8', '9', '2', '2', '9', '11', '1', '8', '1', '10', '9', '2', '1', '4', '7', '7', '7', '7', '7', '12', '8', '8', '8', '1', '1', '10', '1', '12', '1', '8', '2', '1', '1', '12', '8', '7', '9', '12', '8', '9', '3', '9', '8', '8', '8', '8', '3', '11', '2', '2', '9', '8', '8', '10', '8', '2', '7', '3', '1', '4', '7', '8', '8', '1', '8', '3', '7', '12', '8', '10', '9', '9', '8', '8', '2', '8', '9', '9', '1', '2', '8', '8', '9', '8', '3', '8', '1', '8', '10', '9', '8', '9', '9', '12', '4', '4', '8', '9', '9', '8', '2', '10', '1', '2', '8', '9', '1', '9', '9', '9', '7', '12', '12', '8', '1', '1', '1', '1', '8', '3', '1', '1', '8', '1', '8', '8', '7', '8', '8', '8', '8', '3', '2', '2', '10', '10', '10', '7', '8', '1', '2', '12', '7', '9', '8', '7', '8', '12', '2', '8', '9', '2', '6', '6', '7', '9', '8', '1', '8', '9', '8', '1', '12', '1', '2', '8', '7', '7', '7', '8', '2', '2', '8', '1', '2', '2', '9', '9', '1', '8', '8', '4', '3', '3', '1', '6', '3', '3', '12', '3', '8', '9', '1', '4', '3', '1', '8', '3', '9', '2', '8', '2', '8', '8', '8', '1', '1', '1', '9', '9', '8', '1', '9', '8', '1', '1', '3', '10', '8', '1', '1', '3', '9', '1', '4', '4', '1', '8', '9', '9', '2', '0', '0', '1', '8', '3', '1', '8', '8', '9', '8', '8', '1', '1', '8', '9', '8', '8', '8', '7', '9', '8', '8', '8', '10', '9', '8', '1', '2', '6', '1', '9', '9', '8', '12', '12', '12', '8', '8', '2', '8', '1', '2', '2', '2', '1', '9', '8', '2', '12', '2', '8', '12', '8', '9', '8', '8', '9', '7', '1', '1', '1', '1', '1', '8', '8', '1', '8', '8', '1', '1', '3', '2', '8', '8', '9', '10', '10', '2', '2', '1', '9', '2', '9', '9', '4', '12', '12', '12', '10', '7', '3', '3', '4', '2', '2', '9', '2', '8', '4', '2', '4', '1', '10', '9', '7', '8', '7', '1', '1', '3', '3', '1', '1', '3', '3', '3', '1', '1', '1', '1', '8', '2', '3', '1', '1', '2', '8', '8', '12', '8', '8', '8', '8', '11', '9', '1', '8', '9', '2', '8', '8', '8', '3', '9', '1', '9', '2', '7', '2', '8', '2', '8', '10', '8', '1', '10', '1', '1', '9', '9', '8', '8', '1', '8', '8', '8', '12', '8', '8', '8', '1', '8', '8', '8', '1', '9', '1', '1', '8', '1', '8', '9', '8', '2', '12', '9', '9', '0', '1', '8', '8', '1', '8', '12', '8', '8', '10', '8', '8', '8', '7', '8', '1', '8', '7', '3', '10', '1', '8', '9', '1', '8', '8', '8', '10', '1', '10', '3', '9', '1', '8', '9', '2', '8', '3', '3', '9', '9', '7', '9', '1', '1', '9', '2', '1', '1', '1', '7', '1', '1', '8', '8', '1', '1', '8', '1', '8', '3', '12', '9', '3', '3', '8', '8', '8', '8', '3', '1', '3', '3', '1', '11', '0', '8', '8', '7', '8', '12', '1', '8', '9', '8', '9', '8', '8', '3', '8', '8', '1', '1', '1', '9', '2', '2', '2', '8', '7', '12', '8', '8', '9', '10', '10', '7', '8', '1', '9', '8', '7', '3', '1', '3', '8', '2', '2', '3', '9', '8', '4', '4', '8', '9', '2', '1', '1', '7', '8', '9', '9', '7', '8', '7', '7', '8', '2', '2', '8', '4', '9', '7', '10', '0', '9', '8', '3', '7', '8', '1', '1', '8', '9', '9', '2', '2', '10', '1', '9', '10', '10', '10', '8', '3', '2', '12', '9', '9', '10', '12', '9', '12', '12', '9', '1', '2', '4', '12', '12', '7', '8', '9', '7', '7', '7', '3', '9', '8', '9', '1', '12', '8', '9', '4', '1', '3', '12', '12', '12', '12', '8', '8', '2', '1', '1', '2', '1', '1', '1', '12', '12', '8', '12', '2', '2', '12', '3', '3', '12', '8', '2', '8', '8', '12', '2', '1', '10', '3', '2', '8', '7', '1', '8', '1', '3', '7', '8', '9', '8', '3', '1', '1', '7', '8', '8', '9', '8', '2', '9', '2', '2', '9', '8', '1', '8', '8', '1', '3', '3', '1', '1', '10', '1', '2', '8', '1', '1', '1', '1', '9', '1', '4', '1', '7', '7', '7', '7', '2', '2', '8', '8', '12', '1', '9', '1', '7', '3', '3', '1', '8', '8', '10', '8', '9', '2', '9', '1', '3', '8', '8', '3', '12', '2', '8', '12', '2', '9', '1', '3', '3', '3', '3', '2', '8', '7', '9', '8', '3', '3', '1', '7', '8', '3', '1', '1', '12', '8', '9', '1', '2', '3', '8', '1', '1', '3', '3', '9', '1', '1', '1', '12', '1', '7', '3', '3', '1', '8', '8', '8', '1', '2', '4', '8', '1', '10', '2', '5', '3', '3', '12', '10', '9', '9', '12', '9', '0', '2', '8', '8', '9', '9', '9', '8', '1', '3', '1', '1', '4', '8', '1', '10', '8', '7', '2', '8', '2', '8', '4', '7', '8', '1', '9', '1', '9', '8', '2', '8', '2', '7', '9', '2', '2', '9', '1', '8', '12', '1', '8', '1', '4', '1', '9', '9', '1', '10', '12', '4', '8', '1', '7', '3', '9', '2', '12', '7', '8', '8', '2', '1', '12', '9', '8', '1', '2', '2', '8', '10', '2', '1', '7', '7', '7', '12', '3', '3', '8', '3', '8', '8', '3', '9', '8', '9', '1', '1', '8', '7', '9', '3', '3', '8', '1', '0', '9', '9', '9', '1', '8', '9', '9', '10', '1', '8', '7', '8', '8', '8', '9', '9', '8', '9', '10', '4', '9', '3', '7', '12', '1', '9', '9', '8', '9', '1', '9', '3', '1', '8', '4', '12', '10', '9', '8', '7', '10', '8', '12', '12', '3', '10', '8', '12', '2', '1', '2', '3', '9', '8', '8', '7', '1', '1', '2', '2', '1', '2', '2', '7', '1', '3', '9', '9', '3', '8', '8', '8', '8', '8', '7', '8', '8', '10', '8', '1', '8', '2', '8', '2', '2', '2', '2', '2', '1', '7', '12', '10', '1', '2', '8', '1', '4', '7', '8', '1', '12', '8', '7', '9', '2', '2', '2', '8', '1', '8', '1', '1', '0', '2', '2', '6', '1', '8', '2', '1', '8', '1', '1', '2', '2', '3', '2', '2', '10', '10', '9', '1', '7', '7', '8', '8', '1', '12', '10', '12', '3', '8', '8', '8', '3', '8', '3', '10', '2', '2', '2', '1', '2', '2', '1', '0', '1', '8', '9', '1', '1', '1', '2', '2', '2', '2', '2', '1', '1', '1', '1', '12', '9', '9', '1', '10', '11', '12', '9', '1', '1', '8', '8', '1', '2', '12', '8', '3', '7', '2', '10', '9', '7', '8', '1', '2', '2', '7', '9', '12', '2', '2', '1', '8', '9', '2', '3', '2', '2', '2', '9', '9', '8', '9', '2', '3', '8', '8', '9', '7', '3', '10', '8', '8', '9', '1', '1', '2', '8', '1', '10', '8', '8', '9', '7', '1', '7', '12', '8', '1', '7', '2', '1', '3', '9', '2', '8', '8', '1', '2', '3', '9', '3', '8', '9', '7', '7', '1', '10', '2', '8', '8', '8', '1', '8', '2', '8', '8', '1', '2', '8', '10', '2', '1', '0', '8', '3', '8', '10', '10', '12', '3', '8', '9', '3', '10', '8', '8', '8', '1', '8', '8', '2', '2', '2', '2', '2', '2', '1', '1', '10', '1', '10', '8', '2', '1', '2', '2', '8', '3', '3', '2', '2', '1', '8', '1', '3', '1', '9', '2', '2', '1', '3', '2', '2', '3', '2', '2', '3', '2', '1', '1', '2', '1', '2', '7', '8', '2', '1', '12', '12', '3', '1', '9', '9', '7', '8', '7', '2', '7', '9', '2', '2', '2', '8', '9', '7', '9', '2', '8', '9', '9', '2', '9', '1', '1', '9', '8', '3', '3', '8', '2', '7', '8', '11', '2', '8', '9', '1', '8', '3', '1', '2', '1', '2', '2', '2', '2', '8', '3', '9', '2', '10', '9', '9', '2', '7', '7', '7', '1', '3', '8', '8', '1', '1', '8', '7', '1', '8', '8', '9', '3', '3', '3', '2', '4', '8', '12', '8', '1', '12', '9', '2', '8', '2', '12', '2', '1', '3', '8', '12', '12', '11', '8', '10', '9', '2', '9', '4', '10', '1', '12', '12', '9', '3', '8', '9', '12', '9', '9', '2', '2', '2', '3', '5', '1', '1', '1', '7', '7', '8', '2', '2', '8', '2', '1', '7', '3', '2', '9', '8', '9', '1', '1', '9', '8', '9', '3', '3', '2', '9', '4', '9', '8', '8', '1', '8', '10', '2', '0', '3', '8', '8', '8', '9', '12', '9', '8', '2', '2', '8', '10', '9', '8', '2', '2', '4', '1', '9', '9', '8', '8', '10', '3', '1', '1', '2', '8', '7', '8', '2', '1', '1', '8', '12', '2', '9', '4', '8', '3', '3', '3', '8', '2', '12', '12', '8', '9', '12', '8', '2', '12', '7', '9', '2', '2', '1', '3', '1', '3', '11', '9', '8', '1', '10', '2', '12', '3', '8', '4', '1', '2', '8', '8', '3', '9', '3', '8', '7', '1', '1', '3', '1', '8', '2', '9', '8', '12', '1', '1', '1', '9', '9', '1', '8', '8', '8', '2', '2', '12', '3', '1', '1', '1', '9', '9', '3', '2', '1', '1', '9', '9', '1', '9', '9', '1', '9', '3', '9', '1', '8', '1', '9', '8', '3', '8', '8', '9', '9', '8', '2', '8', '1', '2', '2', '10', '1', '2', '1', '1', '9', '1', '9', '3', '7', '7', '2', '10', '3', '3', '1', '1', '9', '1', '2', '1', '3', '2', '1', '1', '1', '1', '1', '1', '1', '1', '1', '2', '8', '8', '7', '1', '1', '8', '8', '2', '1', '12', '1', '8', '1', '9', '8', '8', '2', '1', '9', '9', '2', '2', '8', '1', '7', '7', '9', '8', '1', '6', '8', '8', '2', '2', '8', '3', '8', '3', '2', '3', '8', '1', '2', '2', '2', '8', '9', '9', '9', '12', '1', '9', '2', '1', '9', '2', '9', '9', '10', '8', '1', '1', '9', '2', '1', '7', '1', '3', '7', '1', '1', '1', '3', '8', '8', '8', '9', '9', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '1', '0', '1', '1', '1', '2', '2', '3', '1', '1', '2', '3', '3', '3', '7', '3', '1', '9', '3', '9', '1', '4', '1', '1', '9', '10', '9', '8', '2', '3', '10', '9', '3', '10', '12', '8', '9', '8', '2', '1', '9', '8', '1', '7', '8', '1', '9', '1', '2', '10', '7', '7', '8', '1', '3', '3', '9', '3', '3', '1', '1', '1', '9', '1', '3', '8', '8', '8', '1', '8', '1', '9', '7', '8', '8', '2', '1', '3', '8', '2', '1', '6', '1', '8', '8', '2', '8', '2', '2', '9', '8', '1', '1', '6', '8', '6', '10', '2', '1', '1', '8', '9', '3', '3', '3', '1', '3', '9', '6', '8', '1', '8', '3', '1', '2', '2', '12', '7', '1', '1', '1', '1', '12', '1', '8', '8', '3', '2', '1', '9', '2', '3', '3', '2', '2', '7', '2', '2', '2', '3', '1', '9', '8', '7', '1', '1', '3', '1', '9', '4', '3', '9', '2', '1', '1', '9', '8', '8', '7', '2', '1', '3', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '2', '10', '1', '1', '8', '2', '8', '1', '8', '1', '3', '1', '2', '1', '1', '3', '1', '2', '1', '1', '1', '1', '1', '1', '1', '2', '1', '8', '1', '9', '8', '8', '1', '10', '8', '8', '1', '3', '9', '8', '9', '8', '2', '1', '3', '4', '1', '12', '1', '9', '8', '2', '7', '2', '1', '2', '9', '2', '1', '3', '8', '1', '2', '3', '8', '2', '10', '10', '10', '9', '9', '9', '3', '8', '3', '1', '1', '1', '1', '1', '1', '2', '1', '9', '2', '7', '9', '8', '7', '9', '1', '1', '1', '1', '8', '2', '2', '3', '3', '2', '12', '8', '3', '1', '1', '12', '9', '8', '2', '2', '9', '1', '2', '2', '9', '1', '1', '2', '4', '8', '1', '8', '10', '2', '9', '9', '9', '1', '2', '1', '1', '2', '2', '2', '12', '1', '2', '8', '3', '9', '9', '3', '2', '9', '9', '7', '2', '8', '8', '9', '2', '9', '1', '1', '1', '1', '1', '2', '8', '2', '1', '2', '2', '8', '3', '1', '1', '9', '1', '3', '10', '7', '9', '8', '8', '2', '2', '9', '2', '2', '2', '2', '2', '2', '9', '1', '3', '6', '4', '2', '2', '9', '1', '9', '8', '1', '1', '1', '7', '3', '2', '9', '9', '1', '9', '7', '2', '8', '9', '9', '12', '10', '8', '10', '2', '3', '1', '2', '2', '12', '12', '3', '12', '1', '2', '1', '2', '1', '2', '8', '2', '3', '12', '12', '8', '2', '2', '9', '2', '9', '2', '3', '3', '1', '1', '1', '12', '3', '7', '1', '3', '1', '2', '9', '2', '7', '2', '1', '8', '3', '7', '8', '1', '1', '3', '8', '3', '2', '9', '3', '1', '1', '1', '1', '11', '1', '2', '6', '1', '2', '3', '8', '9', '9', '2', '9', '2', '1', '1', '3', '1', '12', '9', '1', '1', '9', '1', '9', '9', '8', '9', '8', '1', '2', '7', '1', '0', '9', '8', '2', '3', '2', '4', '1', '1', '1', '8', '1', '12', '1', '2', '3', '3', '9', '9', '9', '9', '9', '7', '9', '3', '9', '1', '7', '3', '3', '3', '3', '7', '8', '2', '2', '2', '8', '2', '3', '9', '9', '3', '1', '8', '9', '12', '8', '8', '3', '2', '1', '6', '1', '9', '1', '1', '1', '1', '1', '1', '2', '2', '1', '2', '2', '2', '2', '9', '2', '5', '3', '2', '2', '8', '2', '1', '8', '3', '3', '8', '3', '1', '3', '9', '2', '8', '9', '4', '4', '9', '8', '8', '7', '7', '3', '1', '8', '8', '3', '2', '2', '10', '3', '3', '2', '2', '2', '2', '12', '3', '1', '2', '7', '7', '12', '2', '1', '2', '1', '1', '2', '1', '3', '3', '3', '1', '3', '9', '4', '8', '8', '4', '2', '4', '2', '8', '9', '1', '1', '2', '2', '2', '8', '7', '2', '8', '2', '9', '1', '8', '1', '1', '2', '1', '1', '1', '2', '9', '9', '8', '1', '8', '1', '4', '8', '1', '2', '7', '9', '2', '1', '8', '9', '8', '9', '1', '1', '2', '3', '2', '12', '7', '1', '2', '4', '4', '4', '2', '2', '2', '9', '8', '8', '1', '12', '3', '3', '2', '2', '3', '12', '1', '5', '1', '1', '3', '2', '8', '8', '2', '2', '2', '3', '9', '8', '8', '9', '8', '8', '11', '8', '2', '9', '2', '3', '1', '7', '1', '8', '2', '8', '1', '1', '1', '8', '1', '1', '1', '7', '8', '8', '2', '3', '2', '1', '8', '8', '1', '8', '4', '8', '9', '8', '9', '8', '1', '2', '3', '2', '1', '2', '2', '3', '1', '9', '4', '8', '1', '1', '9', '1', '1', '8', '2', '2', '2', '1', '7', '3', '3', '1', '3', '3', '12', '3', '1', '1', '3', '1', '1', '4', '8', '4', '4', '3', '3', '3', '4', '8', '1', '9', '9', '9', '4', '1', '7', '4', '8', '4', '9', '8', '1', '1', '3', '8', '9', '1', '9', '7', '1', '1', '9', '1', '8', '8', '8', '1', '1', '8', '2', '2', '2', '9', '4', '4', '1', '2', '1', '2', '9', '1', '1', '8', '5', '5', '5', '9', '12', '4', '1', '2', '1', '8', '8', '1', '9', '9', '2', '12', '1', '1', '7', '8', '8', '4', '1', '2', '8', '11', '2', '3', '2', '2', '1', '11', '2', '2', '2', '2', '9', '9', '2', '2', '9', '3', '1', '9', '1', '1', '1', '7', '8', '1', '8', '9', '1', '2', '9', '8', '1', '2', '9', '8', '12', '2', '2', '10', '8', '2', '2', '8', '8', '1', '2', '6', '1', '1', '7', '7', '2', '3', '2', '1', '1', '8', '1', '10', '8', '12', '6', '4', '1', '4', '1', '2', '9', '8', '9', '4', '8', '8', '10', '8', '8', '8', '8', '8', '2', '2', '8', '4', '9', '3', '3', '3', '3', '3', '9', '2', '1', '1', '2', '3', '2', '1', '2', '3', '3', '3', '2', '2', '3', '3', '2', '1', '2', '3', '3', '3', '3', '12', '9', '3', '2', '2', '3', '1', '5', '1', '1', '3', '2', '2', '1', '2', '9', '3', '9', '8', '1', '8', '7', '7', '9', '1', '1', '7', '9', '10', '1', '7', '1', '4', '3', '8', '9', '4', '9', '9', '2', '9', '2', '8', '4', '2', '2', '9', '5', '2', '1', '10', '9', '3', '1', '1', '1', '2', '1', '2', '9', '1', '7', '9', '9', '8', '2', '2', '8', '9', '1', '9', '8', '2', '3', '2', '2', '1', '2', '2', '2', '1', '1', '8', '1', '9', '4', '12', '8', '2', '7', '2', '2', '1', '9', '3', '1', '8', '12', '1', '1', '2', '1', '10', '2', '6', '12', '12', '4', '9', '1', '9', '10', '1', '2', '8', '2', '1', '8', '6', '12', '9', '9', '2', '1', '7', '8', '9', '5', '8', '1', '8', '8', '3', '1', '8', '2', '8', '2', '2', '1', '2', '9', '2', '8', '1', '12', '8', '2', '7', '3', '3', '12', '2', '3', '3', '9', '9', '3', '3', '3', '3', '3', '2', '1', '4', '1', '4', '8', '8', '2', '1', '2', '9', '12', '1', '10', '12', '1', '2', '9', '4', '8', '10', '3', '2', '8', '7', '4', '8', '9', '2', '7', '1', '10', '2', '2', '2', '8', '4', '2', '2', '1', '1', '8', '7', '9', '2', '1', '9', '8', '9', '12', '4', '8', '2', '8', '7', '7', '1', '9', '2', '1', '1', '9', '1', '8', '2', '3', '9', '9', '1', '3', '2', '9', '2', '2', '8', '1', '1', '2', '1', '2', '1', '9', '8', '2', '2', '4', '8', '8', '9', '5', '5', '9', '8', '9', '6', '2', '1', '12', '2', '3', '8', '8', '9', '2', '2', '1', '3', '2', '10', '9', '10', '7', '2', '8', '4', '2', '8', '9', '8', '1', '3', '8', '8', '1', '8', '3', '8', '8', '1', '10', '3', '1', '5', '8', '8', '9', '2', '2', '9', '2', '1', '9', '1', '4', '1', '12', '8', '8', '2', '1', '1', '1', '1', '1', '9', '2', '9', '8', '9', '5', '9', '1', '8', '2', '4', '1', '1', '1', '9', '12', '4', '1', '1', '9', '8', '4', '1', '8', '9', '8', '8', '9', '9', '9', '9', '9', '9', '1', '3', '4', '2', '2', '8', '2', '9', '2', '3', '8', '3', '1', '7', '2', '1', '1', '4', '2', '9', '2', '1', '3', '2', '9', '2', '9', '3', '8', '1', '9', '1', '1', '2', '8', '8', '8', '2', '3', '9', '9', '8', '2', '4', '1', '5', '1', '1', '9', '9', '2', '1', '1', '1', '9', '9', '1', '3', '9', '2', '2', '9', '9', '8', '3', '9', '9', '8', '2', '2', '8', '9', '2', '2', '8', '9', '10', '10', '2', '8', '7', '9', '9', '4', '2', '4', '11', '2', '1', '7', '8', '8', '8', '9', '4', '9', '8', '8', '2', '1', '1', '9', '8', '3', '3', '8', '8', '7', '9', '1', '3', '4', '8', '10', '2', '4', '4', '8', '2', '8', '2', '8', '9', '2', '1', '2', '2', '3', '1', '9', '4', '5', '9', '9', '1', '1', '1', '1', '1', '1', '7', '1', '1', '1', '8', '1', '1', '1', '1', '1', '9', '1', '2', '1', '12', '9', '3', '9', '9', '4', '1', '9', '1', '1', '2', '11', '9', '3', '2', '2', '2', '4', '2', '3', '2', '7', '7', '8', '1', '10', '9', '5', '7', '1', '10', '8', '1', '2', '7', '1', '12', '5', '8', '4', '2', '9', '9', '8', '2', '9', '8', '8', '10', '2', '3', '2', '7', '2', '8', '10', '3', '2', '9', '9', '9', '1', '1', '1', '1', '8', '2', '10', '9', '8', '1', '2', '2', '2', '1', '2', '1', '3', '9', '12', '2', '2', '4', '8', '1', '3', '9', '4', '9', '1', '9', '9', '2', '1', '1', '9', '7', '10', '3', '10', '2', '9', '2', '9', '2', '8', '2', '1', '2', '3', '1', '1', '1', '8', '5', '8', '3', '1', '1', '4', '2', '2', '1', '3', '8', '2', '1', '1', '1', '2', '9', '4', '1', '8', '1', '2', '8', '2', '9', '2', '2', '5', '5', '2', '5', '2', '1', '8', '8', '1', '3', '12', '2', '4', '3', '2', '2', '2', '6', '2', '3', '3', '1', '8', '2', '2', '1', '1', '8', '8', '10', '1', '9', '2', '9', '12', '0', '2', '1', '3', '2', '2', '1', '2', '1', '9', '9', '2', '9', '8', '8', '1', '7', '8', '1', '2', '6', '9', '8', '10', '1', '12', '2', '2', '1', '8', '2', '1', '12', '7', '4', '2', '10', '2', '2', '1', '2', '1', '2', '1', '1', '2', '1', '9', '8', '10', '2', '10', '9', '9', '12', '5', '3', '2', '8', '2', '8', '3', '3', '1', '4', '9', '10', '2', '4', '8', '1', '2', '10', '12', '12', '3', '1', '1', '2', '9', '6', '6', '1', '9', '1', '8', '1', '8', '8', '8', '8', '2', '2', '2', '8', '8', '2', '1', '1', '1', '1', '1', '8', '8', '5', '8', '8', '1', '8', '9', '9', '1', '7', '12', '9', '9', '7', '2', '2', '7', '8', '9', '8', '2', '9', '6', '3', '4', '8', '4', '1', '2', '9', '8', '8', '8', '2', '1', '1', '8', '8', '3', '1', '9', '9', '7', '1', '2', '2', '2', '9', '4', '1', '2', '12', '1', '8', '2', '1', '9', '1', '8', '9', '9', '2', '5', '4', '2', '1', '2', '8', '12', '8', '10', '3', '8', '8', '8', '2', '2', '8', '2', '8', '2', '4', '8', '10', '2', '7', '2', '1', '8', '8', '9', '5', '8', '1', '12', '7', '10', '2', '2', '2', '1', '4', '8', '12', '1', '8', '2', '2', '9', '1', '8', '3', '1', '1', '5', '10', '2', '1', '2', '4', '8', '9', '3', '2', '8', '7', '4', '4', '9', '9', '2', '8', '1', '1', '2', '4', '7', '1', '9', '1', '1', '1', '8', '4', '1', '3', '1', '2', '2', '9', '1', '8', '8', '2', '2', '9', '9', '1', '8', '8', '1', '9', '8', '8', '2', '2', '2', '2', '1', '1', '1', '7', '4', '1', '1', '4', '2', '3', '3', '2', '3', '9', '2', '8', '2', '1', '5', '1', '2', '2', '1', '3', '5', '2', '9', '1', '1', '9', '8', '1', '4', '8', '1', '8', '2', '4', '8', '4', '7', '9', '8', '1', '2', '2', '4', '4', '11', '3', '11', '1', '3', '8', '1', '9', '6', '8', '9', '3', '9', '8', '9', '2', '3', '3', '7', '1', '7', '2', '9', '9', '1', '8', '8', '1', '5', '5', '8', '8', '1', '2', '1', '3', '9', '9', '5', '9', '8', '1', '4', '3', '8', '1', '1', '2', '1', '8', '9', '8', '2', '2', '2', '1', '2', '4', '10', '8', '1', '9', '1', '8', '1', '1', '9', '9', '8', '11', '1', '8', '2', '1', '2', '8', '2', '8', '8', '2', '9', '6', '10', '1', '8', '6', '4', '5', '11', '2', '2', '8', '1', '8', '1', '8', '1', '4', '8', '1', '1', '1', '8', '3', '7', '3', '3', '7', '1', '1', '4', '8', '1', '6', '4', '2', '2', '1', '1', '1', '1', '6', '2', '2', '8', '1', '8', '5', '5', '2', '1', '1', '2', '3', '7', '1', '1', '6', '2', '9', '3', '9', '8', '9', '2', '1', '4', '2', '9', '13', '9', '2', '8', '12', '4', '1', '1', '9', '5', '1', '1', '1', '1', '8', '2', '1', '1', '2', '9', '0', '12', '1', '1', '8', '9', '8', '3', '2', '2', '12', '8', '2', '2', '2', '1', '10', '1', '9', '8', '9', '5', '2', '2', '9', '2', '8', '4', '8', '3', '7', '9', '2', '2', '7', '2', '2', '8', '1', '1', '9', '10', '9', '8', '2', '9', '0', '7', '1', '1', '1', '9', '8', '2', '8', '12', '8', '3', '2', '8', '8', '7', '7', '8', '12', '2', '1', '3', '9', '2', '2', '8', '2', '8', '8', '12', '8', '8', '1', '8', '9', '2', '4', '4', '7', '2', '1', '1', '9', '3', '7', '1', '3', '9', '8', '2', '2', '3', '1', '3', '10', '2', '3', '7', '1', '8', '1', '1', '10', '3', '10', '8', '4', '1', '2', '4', '3', '10', '9', '3', '9', '5', '8', '7', '2', '9', '3', '3', '2', '11', '8', '1', '1', '7', '2', '7', '2', '9', '4', '8', '1', '12', '8', '7', '9', '1', '0', '2', '6', '4', '9', '3', '2', '4', '12', '9', '8', '5', '9', '2', '8', '4', '6', '8', '9', '3', '1', '8', '7', '3', '1', '1', '1', '8', '9', '1', '8', '7', '9', '8', '2', '1', '2', '2', '3', '2', '2', '1', '1', '4', '9', '7', '3', '2', '8', '8', '9', '1', '9', '7', '4', '2', '2', '8', '5', '9', '1', '5', '7', '1', '4', '6', '9', '10', '1', '8', '2', '9', '1', '9', '2', '9', '8', '2', '9', '9', '11', '2', '12', '2', '2', '9', '4', '8', '1', '9', '9', '2', '8', '9', '1', '2', '9', '3', '8', '8', '1', '7', '8', '8', '2', '3', '2', '2', '2', '2', '9', '8', '10', '2', '2', '1', '9', '11', '1', '8', '8', '2', '4', '2', '1', '2', '8', '8', '9', '2', '8', '9', '2', '3', '1', '2', '1', '3', '10', '8', '1', '2', '4', '1', '9', '8', '9', '3', '3', '9', '8', '2', '11', '3', '7', '1', '8', '7', '1', '8', '8', '8', '1', '4', '8', '8', '1', '9', '3', '1', '1', '2', '8', '8', '8', '2', '1', '8', '10', '2', '12', '8', '8', '9', '1', '8', '1', '3', '2', '2', '10', '7', '2', '2', '1', '1', '8', '2', '9', '1', '2', '9', '3', '3', '10', '1', '4', '7', '10', '12', '2', '4', '10', '8', '6', '2', '9', '1', '8', '3', '1', '2', '8', '8', '2', '12', '10', '2', '10', '9', '1', '2', '5', '8', '12', '8', '8', '7', '8', '8', '1', '8', '2', '2', '9', '1', '8', '10', '5', '5', '5', '8', '1', '9', '1', '13', '1', '2', '2', '1', '2', '4', '9', '2', '1', '9', '13', '11', '1', '9', '9', '3', '10', '8', '7', '8', '8', '4', '2', '1', '9', '4', '3', '1', '1', '10', '9', '2', '8', '6', '2', '1', '1', '3', '2', '8', '1', '1', '1', '1', '2', '4', '1', '1', '10', '1', '8', '1', '1', '9', '1', '7', '1', '1', '9', '9', '2', '12', '9', '1', '8', '1', '12', '1', '8', '3', '8', '1', '8', '1', '2', '10', '1', '1', '2', '2', '9', '3', '9', '12', '1', '9', '1', '2', '8', '7', '2', '1', '1', '3', '2', '8', '4', '4', '5', '12', '7', '8', '9', '6', '1', '8', '9', '2', '1', '9', '1', '4', '1', '8', '2', '1', '1', '8', '7', '4', '9', '1', '8', '4', '2', '7', '5', '1', '3', '9', '2', '8', '2', '1', '2', '9', '1', '1', '2', '3', '4', '3', '1', '2', '2', '10', '2', '2', '8', '1', '1', '2', '4', '9', '2', '11', '9', '8', '9', '2', '1', '10', '4', '1', '1', '10', '7', '2', '10', '8', '1', '10', '8', '8', '8', '8', '8', '8', '2', '2', '2', '8', '3', '4', '1', '1', '1', '2', '1', '8', '8', '2', '8', '3', '8', '3', '1', '2', '3', '1', '1', '10', '1', '4', '4', '3', '3', '1', '9', '1', '1', '2', '1', '1', '6', '1', '1', '1', '9', '1', '4', '1', '1', '1', '1', '1', '1', '2', '12', '8', '8', '1', '9', '12', '10', '8', '2', '12', '1', '1', '2', '1', '7', '10', '7', '1', '2', '1', '10', '8', '2', '2', '8', '2', '2', '1', '9', '8', '8', '8', '8', '3', '1', '4', '2', '8', '3', '10', '10', '1', '9', '1', '9', '1', '9', '1', '8', '3', '1', '1', '8', '8', '8', '6', '3', '4', '1', '1', '1', '9', '9', '5', '2', '10', '2', '6', '2', '2', '7', '1', '8', '2', '11', '1', '1', '1', '2', '1', '9', '4', '4', '2', '8', '4', '6', '8', '8', '10', '10', '2', '1', '2', '9', '1', '8', '8', '3', '4', '8', '8', '2', '8', '8', '1', '2', '1', '2', '9', '4', '1', '3', '3', '3', '8', '8', '8', '2', '6', '4', '3', '3', '12', '3', '4', '9', '6', '8', '7', '7', '8', '6', '4', '1', '10', '6', '3', '3', '8', '1', '1', '8', '1', '11', '8', '1', '1', '2', '3', '11', '7', '2', '1', '1', '1', '9', '1', '2', '1', '1', '9', '8', '1', '1', '8', '0', '4', '1', '1', '2', '1', '1', '2', '2', '9', '1', '10', '1', '3', '8', '9', '3', '3', '1', '10', '1', '9', '1', '1', '7', '4', '1', '8', '10', '1', '2', '10', '1', '1', '2', '3', '3', '9', '1', '8', '10', '1', '7', '9', '1', '9', '6', '3', '9', '2', '8', '4', '12', '12', '3', '1', '1', '9', '1', '9', '2', '1', '1', '1', '1', '1', '9', '2', '1', '2', '10', '9', '8', '8', '10', '8', '2', '3', '2', '1', '12', '8', '12', '2', '9', '1', '3', '2', '5', '2', '2', '8', '2', '2', '4', '10', '9', '10', '9', '4', '2', '4', '12', '8', '2', '1', '8', '3', '9', '2', '9', '4', '1', '1', '1', '1', '1', '9', '6', '2', '9', '2', '1', '3', '2', '2', '5', '8', '9', '1', '6', '3', '1', '2', '2', '2', '6', '1', '8', '10', '8', '3', '3', '13', '2', '9', '9', '10', '6', '12', '8', '2', '9', '2', '3', '8', '8', '1', '2', '2', '3', '2', '10', '1', '8', '9', '1', '8', '9', '8', '8', '2', '10', '2', '1', '1', '1', '9', '9', '12', '1', '1', '4', '8', '3', '2', '2', '4', '4', '2', '2', '1', '1', '1', '9', '2', '2', '4', '1', '1', '8', '8', '8', '9', '2', '13', '9', '9', '4', '1', '9', '7', '10', '10', '8', '1', '9', '1', '1', '1', '8', '12', '4', '1', '2', '1', '1', '1', '7', '8', '3', '3', '1', '2', '2', '7', '2', '2', '10', '1', '8', '4', '0', '1', '9', '4', '1', '1', '2', '10', '10', '7', '9', '3', '12', '11', '2', '8', '10', '8', '9', '4', '3', '4', '10', '1', '1', '9', '3', '10', '1', '5', '3', '3', '2', '2', '1', '1', '6', '1', '7', '1', '1', '12', '1', '1', '1', '10', '1', '8', '8', '1', '3', '8', '3', '1', '2', '8', '3', '4', '2', '8', '8', '6', '1', '1', '4', '1', '2', '9', '5', '1', '7', '8', '9', '2', '4', '9', '8', '5', '9', '9', '9', '1', '2', '1', '3', '8', '9', '2', '9', '8', '9', '1', '8', '13', '9', '3', '8', '1', '4', '1', '3', '1', '6', '4', '10', '2', '2', '7', '8', '8', '12', '8', '9', '9', '9', '10', '3', '12', '12', '2', '9', '10', '9', '7', '2', '3', '4', '10', '10', '2', '7', '7', '1', '7', '3', '9', '1', '9', '8', '2', '9', '1', '8', '1', '10', '8', '8', '2', '7', '8', '1', '8', '9', '8', '1', '4', '1', '8', '8', '8', '1', '1', '10', '3', '2', '2', '6', '8', '9', '8', '2', '1', '1', '3', '8', '8', '1', '4', '4', '3', '9', '10', '4', '2', '4', '2', '9', '2', '2', '9', '1', '1', '10', '1', '1', '2', '1', '1', '1', '9', '1', '8', '8', '8', '2', '3', '8', '1', '6', '7', '3', '1', '9', '1', '2', '1', '6', '1', '2', '10', '1', '4', '2', '3', '2', '9', '9', '4', '9', '9', '1', '2', '8', '4', '7', '8', '7', '13', '9', '1', '2', '1', '9', '3', '3', '6', '8', '9', '10', '9', '2', '2', '3', '12', '1', '1', '1', '13', '2', '1', '7', '10', '8', '9', '1', '2', '9', '1', '1', '1', '2', '1', '12', '5', '6', '9', '10', '3', '1', '10', '1', '3', '2', '8', '9', '2', '10', '8', '8', '9', '2', '8', '9', '2', '2', '2', '8', '8', '2', '3', '1', '4', '8', '1', '9', '9', '2', '8', '10', '9', '1', '2', '12', '8', '8', '0', '2', '1', '1', '9', '10', '1', '6', '2', '2', '10', '2', '2', '4', '2', '2', '12', '1', '6', '7', '8', '9', '1', '3', '5', '7', '3', '1', '3', '1', '1', '3', '2', '6', '3', '2', '1', '8', '3', '5', '2', '1', '1', '8', '1', '1', '0', '2', '3', '5', '3', '1', '1', '9', '8', '8', '8', '8', '4', '7', '2', '9', '12', '5', '9', '3', '12', '3', '9', '1', '9', '1', '7', '3', '9', '3', '10', '1', '2', '1', '4', '9', '8', '2', '9', '11', '2', '8', '9', '4', '1', '1', '1', '1', '1', '1', '1', '9', '1', '9', '1', '1', '1', '12', '3', '10', '3', '6', '1', '7', '10', '8', '3', '7', '1', '1', '2', '6', '1', '3', '9', '2', '8', '1', '8', '8', '1', '8', '10', '12', '1', '10', '12', '10', '1', '8', '8', '1', '4', '7', '9', '2', '1', '9', '3', '8', '12', '10', '6', '1', '6', '8', '3', '1', '3', '1', '10', '10', '9', '8', '1', '2', '9', '8', '8', '1', '9', '8', '8', '3', '1', '3', '9', '12', '1', '8', '1', '5', '8', '11', '5', '5', '1', '2', '1', '3', '1', '1', '1', '9', '8', '8', '1', '9', '10', '8', '4', '1', '1', '8', '10', '8', '9', '1', '10', '1', '7', '2', '1', '1', '9', '6', '2', '9', '8', '1', '1', '1', '9', '8', '8', '1', '9', '9', '1', '1', '2', '7', '9', '2', '9', '9', '8', '7', '6', '8', '7', '8', '2', '9', '2', '1', '8', '8', '8', '2', '6', '3', '1', '8', '12', '12', '8', '7', '1', '2', '2', '7', '1', '9', '8', '1', '4', '1', '1', '8', '5', '8', '1', '8', '8', '8', '8', '1', '2', '11', '1', '1', '1', '2', '7', '1', '9', '2', '2', '2', '9', '8', '9', '2', '9', '1', '8', '1', '7', '11', '13', '1', '9', '8', '2', '2', '1', '2', '3', '10', '3', '10', '4', '3', '9', '1', '8', '1', '1', '13', '1', '1', '3', '8', '1', '2', '2', '2', '9', '4', '1', '8', '1', '9', '3', '6', '8', '5', '2', '8', '2', '8', '1', '9', '2', '2', '8', '1', '1', '4', '8', '9', '2', '2', '7', '1', '8', '12', '2', '12', '8', '2', '9', '9', '3', '8', '1', '1', '9', '9', '9', '2', '8', '1', '9', '1', '8', '9', '8', '2', '2', '4', '1', '9', '10', '10', '8', '8', '1', '1', '2', '1', '4', '1', '2', '1', '1', '3', '12', '8', '8', '8', '8', '10', '2', '8', '12', '1', '9', '9', '8', '9', '1', '2', '1', '8', '8', '8', '2', '8', '8', '3', '10', '8', '9', '1', '1', '1', '3', '4', '8', '10', '6', '3', '1', '3', '2', '8', '9', '3', '5', '4', '1', '5', '2', '9', '9', '9', '1', '1', '1', '9', '9', '6', '10', '9', '8', '12', '2', '9', '1', '2', '1', '1', '1', '8', '1', '12', '1', '12', '1', '9', '1', '8', '1', '2', '1', '2', '8', '8', '10', '1', '9', '1', '2', '2', '8', '8', '3', '12', '1', '11', '2', '10', '12', '11', '2', '1', '10', '1', '1', '9', '3', '10', '2', '7', '1', '8', '1', '9', '9', '2', '1', '3', '12', '5', '9', '9', '8', '1', '8', '8', '8', '1', '1', '3', '9', '8', '9', '8', '3', '10', '3', '7', '8', '9', '2', '3', '1', '9', '10', '9', '2', '8', '2', '6', '4', '1', '1', '3', '8', '2', '3', '9', '1', '2', '1', '8', '1', '2', '2', '2', '2', '9', '9', '4', '8', '9', '1', '1', '2', '9', '1', '5', '2', '9', '8', '10', '1', '2', '10', '8', '5', '6', '4', '8', '1', '1', '1', '8', '9', '2', '8', '8', '8', '12', '2', '2', '1', '8', '1', '2', '1', '9', '8', '8', '9', '1', '1', '8', '7', '1', '8', '0', '2', '3', '8', '1', '1', '8', '1', '1', '12', '3', '2', '10', '8', '10', '6', '1', '8', '8', '10', '8', '8', '1', '4', '8', '1', '2', '9', '8', '1', '1', '3', '3', '5', '1', '1', '2', '1', '2', '1', '9', '8', '3', '9', '8', '2', '10', '9', '1', '8', '1', '10', '9', '10', '1', '2', '3', '8', '1', '3', '1', '9', '9', '8', '1', '9', '8', '8', '9', '1', '9', '2', '8', '8', '2', '13', '8', '10', '8', '9', '3', '2', '1', '3', '1', '12', '10', '10', '1', '11', '1', '2', '4', '2', '10', '1', '8', '11', '1', '1', '1', '2', '2', '8', '2', '9', '8', '9', '9', '12', '8', '1', '9', '2', '4', '1', '8', '3', '9', '1', '6', '5', '8', '9', '3', '3', '2', '1', '1', '1', '8', '11', '1', '8', '7', '1', '11', '9', '1', '7', '9', '9', '8', '8', '8', '12', '8', '1', '2', '9', '8', '8', '2', '10', '9', '8', '8', '4', '10', '9', '8', '2', '10', '10', '2', '1', '2', '1', '2', '1', '4', '1', '7', '1', '1', '3', '12', '9', '8', '5', '9', '10', '1', '9', '9', '2', '7', '4', '1', '1', '8', '12', '8', '2', '2', '9', '4', '1', '1', '5', '9', '1', '8', '12', '8', '1', '1', '4', '9', '10', '2', '3', '1', '3', '3', '3', '8', '1', '2', '1', '2', '1', '2', '9', '1', '2', '8', '9', '4', '8', '8', '10', '2', '12', '5', '5', '1', '8', '1', '7', '8', '1', '1', '8', '9', '12', '4', '9', '3', '1', '2', '5', '9', '2', '1', '2', '2', '1', '2', '8', '1', '8', '8', '2', '2', '1', '8', '4', '1', '2', '10', '9', '10', '8', '1', '10', '4', '8', '8', '10', '2', '8', '1', '1', '3', '10', '1', '4', '2', '8', '3', '3', '2', '9', '8', '5', '5', '9', '3', '10', '8', '1', '2', '1', '2', '9', '1', '1', '2', '9', '9', '8', '8', '11', '9', '4', '1', '8', '2', '7', '1', '2', '8', '2', '9', '9', '8', '2', '9', '9', '8', '8', '1', '2', '8', '1', '1', '2', '9', '8', '1', '4', '9', '1', '8', '12', '1', '12', '2', '8', '2', '9', '1', '12', '1', '2', '9', '1', '1', '9', '3', '2', '9', '9', '2', '11', '8', '7', '9', '9', '9', '8', '8', '1', '2', '8', '1', '1', '4', '1', '2', '10', '1', '9', '2', '1', '1', '1', '9', '1', '13', '4', '3', '2', '1', '1', '2', '2', '7', '8', '7', '1', '1', '2', '8', '8', '3', '4', '8', '2', '10', '13', '8', '8', '9', '2', '2', '1', '1', '10', '4', '2', '7', '9', '8', '2', '1', '9', '8', '1', '1', '9', '8', '9', '8', '2', '8', '2', '2', '9', '2', '1', '9', '1', '2', '4', '8', '2', '1', '1', '8', '1', '4', '1', '9', '3', '9', '2', '7', '1', '9', '6', '1', '10', '2', '2', '2', '2', '2', '10', '10', '10', '9', '9', '9', '3', '12', '10', '1', '1', '10', '1', '1', '2', '2', '1', '2', '3', '12', '8', '8', '1', '2', '9', '9', '1', '1', '10', '8', '9', '8', '3', '1', '1', '3', '1', '10', '1', '1', '1', '4', '1', '1', '1', '7', '10', '2', '1', '8', '1', '9', '3', '1', '10', '8', '8', '1', '5', '9', '1', '2', '1', '8', '8', '1', '3', '1', '10', '2', '1', '1', '2', '8', '2', '5', '3', '5', '2', '1', '1', '9', '1', '7', '9', '9', '2', '8', '1', '12', '4', '1', '4', '9', '9', '4', '8', '1', '10', '10', '8', '8', '8', '9', '2', '3', '5', '1', '8', '1', '1', '1', '7', '10', '1', '4', '12', '3', '2', '2', '9', '1', '1', '1', '2', '8', '4', '1', '7', '3', '12', '8', '6', '2', '2', '7', '9', '11', '1', '10', '9', '1', '8', '12', '2', '11', '1', '2', '3', '1', '1', '1', '8', '9', '2', '2', '2', '2', '3', '3', '8', '10', '4', '1', '2', '9', '8', '2', '2', '9', '1', '8', '11', '4', '2', '8', '7', '1', '1', '7', '3', '9', '9', '4', '5', '8', '10', '2', '9', '9', '9', '8', '1', '9', '2', '2', '2', '1', '2', '9', '4', '3', '12', '4', '3', '2', '3', '8', '3', '10', '1', '10', '1', '1', '8', '10', '6', '8', '9', '9', '1', '2', '9', '2', '9', '3', '8', '1', '1', '12', '1', '5', '1', '10', '8', '10', '2', '7', '1', '1', '1', '1', '3', '2', '3', '5', '9', '1', '2', '1', '9', '8', '10', '9', '8', '1', '8', '1', '8', '2', '8', '1', '2', '8', '1', '2', '5', '8', '12', '2', '1', '1', '1', '1', '1', '8', '8', '2', '1', '4', '2', '8', '8', '8', '2', '9', '9', '11', '1', '2', '9', '3', '1', '1', '8', '9', '2', '4', '10', '1', '9', '8', '4', '9', '8', '10', '8', '8', '9', '2', '8', '9', '3', '3', '3', '4', '3', '2', '2', '10', '8', '2', '1', '5', '1', '9', '1', '1', '2', '1', '2', '3', '1', '1', '8', '8', '1', '10', '8', '8', '1', '6', '4', '8', '2', '5', '8', '1', '3', '8', '2', '8', '9', '1', '1', '8', '12', '10', '1', '5', '8', '2', '8', '10', '9', '9', '1', '6', '9', '1', '10', '8', '2', '9', '10', '1', '2', '1', '1', '8', '8', '3', '9', '8', '1', '10', '2', '8', '1', '10', '1', '5', '1', '1', '1', '5', '1', '1', '1', '4', '1', '1', '1', '8', '2', '8', '1', '8', '3', '8', '9', '1', '8', '1', '8', '1', '1', '2', '2', '1', '12', '9', '9', '1', '8', '2', '1', '2', '8', '1', '9', '1', '1', '2', '1', '2', '2', '2', '1', '9', '9', '8', '8', '1', '1', '9', '10', '8', '4', '3', '3', '4', '9', '1', '3', '3', '10', '2', '2', '1', '8', '4', '1', '9', '4', '1', '1', '9', '8', '10', '4', '9', '9', '8', '2', '1', '3', '4', '9', '3', '8', '9', '1', '1', '2', '1', '7', '8', '2', '9', '1', '9', '2', '6', '9', '10', '8', '1', '1', '5', '9', '5', '1', '9', '10', '9', '3', '3', '8', '8', '2', '9', '2', '8', '2', '1', '8', '5', '8', '3', '10', '1', '1', '2', '1', '8', '1', '4', '3', '2', '9', '4', '9', '8', '8', '8', '1', '3', '9', '1', '9', '9', '1', '2', '1', '9', '8', '9', '8', '1', '1', '1', '4', '2', '2', '9', '9', '1', '1', '1', '3', '6', '1', '2', '3', '1', '4', '2', '9', '1', '9', '9', '1', '8', '2', '1', '9', '1', '8', '1', '8', '8', '2', '1', '9', '9', '6', '8', '8', '9', '8', '10', '8', '9', '5', '1', '1', '1', '1', '8', '1', '8', '12', '8', '1', '12', '9', '2', '8', '1', '1', '2', '1', '8', '6', '9', '9', '8', '1', '8', '9', '7', '7', '8', '9', '9', '1', '8', '3', '8', '1', '3', '3', '8', '9', '8', '2', '8', '1', '2', '9', '8', '1', '1', '1', '1', '9', '1', '9', '8', '12', '3', '8', '8', '1', '10', '10', '10', '2', '2', '1', '10', '2', '2', '10', '8', '12', '8', '1', '1', '1', '2', '1', '1', '3', '8', '1', '2', '2', '2', '1', '1', '6', '2', '8', '8', '8', '1', '1', '1', '9', '9', '2', '8', '10', '2', '8', '2', '1', '2', '9', '9', '2', '1', '1', '8', '8', '1', '3', '1', '8', '9', '1', '10', '9', '1', '1', '1', '1', '7', '2', '2', '1', '2', '8', '8', '1', '2', '1', '3', '2', '1', '8', '3', '9', '2', '10', '2', '10', '1', '2', '1', '1', '13', '9', '10', '8', '2', '2', '1', '1', '8', '2', '2', '1', '9', '1', '1', '8', '9', '9', '2', '9', '2', '2', '1', '1', '1', '8', '9', '4', '9', '9', '1', '8', '2', '1', '1', '8', '2', '2', '2', '8', '1', '1', '8', '2', '10', '2', '9', '9', '7', '1', '1', '2', '2', '8', '3', '1', '2', '1', '1', '8', '1', '1', '1', '9', '1', '1', '8', '8', '1', '9', '6', '1', '9', '1', '8', '1', '8', '8', '8', '1', '3', '8', '8', '2', '6', '3', '2', '1', '10', '1', '8', '2', '1', '6', '1', '1', '1', '8', '1', '10', '1', '1', '8', '9', '2', '6', '9', '2', '9', '4', '1', '8', '1', '3', '7', '8', '8', '5', '8', '7', '1', '1', '1', '13', '8', '3', '1', '1', '1', '1', '2', '8', '2', '12', '8', '1', '2', '5', '1', '8', '2', '10', '8', '2', '10', '1', '5', '2', '1', '1', '5', '2', '2', '7', '8', '2', '9', '9', '1', '10', '1', '8', '2', '5', '8', '1', '5', '1', '10', '1', '1', '9', '8', '6', '1', '8', '9', '8', '1', '8', '2', '2', '8', '3', '9', '1', '9', '3', '8', '4', '4', '3', '3', '1', '1', '1', '8', '9', '1', '1', '8', '3', '1', '2', '8', '2', '1', '1', '10', '1', '2', '2', '1', '1', '8', '2', '8', '2', '10', '2', '2', '10', '8', '8', '8', '13', '2', '2', '9', '2', '5', '9', '2', '1', '8', '8', '8', '1', '12', '12', '2', '9', '2', '1', '8', '2', '1', '8', '1', '9', '9', '2', '7', '1', '1', '7', '1', '4', '3', '1', '10', '3', '10', '9', '4', '6', '7', '5', '2', '1', '8', '8', '1', '4', '9', '2', '1', '8', '1', '1', '1', '1', '1', '8', '9', '8', '8', '8', '8', '9', '8', '10', '1', '8', '2', '7', '8', '9', '1', '2', '5', '9', '10', '8', '9', '12', '1', '6', '1', '2', '10', '9', '4', '1', '8', '10', '10', '1', '1', '1', '1', '1', '2', '3', '8', '1', '2', '10', '2', '1', '2', '4', '2', '2', '9', '1', '4', '2', '1', '1', '12', '12', '9', '9', '7', '1', '8', '4', '9', '9', '8', '1', '7', '3', '2', '1', '1', '4', '8', '1', '5', '8', '2', '8', '1', '14', '9', '12', '1', '10', '2', '1', '1', '8', '8', '6', '6', '3', '1', '2', '8', '1', '3', '2', '1', '8', '8', '1', '10', '8', '2', '8', '2', '5', '9', '1', '8', '8', '1', '1', '8', '8', '13', '1', '3', '8', '4', '1', '9', '7', '8', '8', '8', '3', '8', '1', '4', '2', '8', '8', '8', '7', '1', '1', '2', '8', '8', '9', '8', '8', '2', '3', '9', '9', '2', '1', '2', '2', '1', '8', '2', '6', '2', '1', '1', '9', '10', '3', '8', '2', '3', '2', '8', '8', '1', '8', '8', '8', '9', '1', '4', '1', '9', '2', '1', '8', '0', '1', '8', '1', '1', '8', '9', '9', '1', '3', '1', '2', '8', '8', '7', '1', '1', '2', '9', '2', '10', '2', '8', '2', '2', '2', '7', '1', '9', '8', '1', '3', '9', '2', '1', '10', '8', '1', '4', '1', '8', '5', '9', '8', '8', '1', '2', '2', '1', '8', '8', '6', '8', '1', '1', '8', '3', '2', '2', '1', '8', '1', '7', '6', '8', '1', '1', '1', '9', '1', '1', '1', '2', '1', '11', '10', '10', '11', '9', '']\n",
            "['8', '1', '8', '2', '8', '8', '8', '9', '7', '8', '1', '2', '1', '1', '8', '4', '8', '8', '12', '3', '3', '7', '3', '12', '1', '8', '8', '8', '8', '8', '8', '8', '1', '9', '5', '9', '9', '9', '11', '8', '8', '8', '4', '8', '8', '8', '8', '1', '3', '9', '3', '7', '1', '2', '9', '9', '7', '8', '8', '1', '10', '7', '8', '8', '9', '8', '7', '9', '9', '12', '7', '2', '8', '1', '11', '11', '1', '7', '7', '12', '1', '9', '8', '10', '12', '7', '8', '2', '8', '9', '9', '1', '8', '9', '1', '7', '12', '10', '10', '10', '8', '3', '7', '9', '8', '9', '1', '8', '8', '2', '7', '2', '9', '9', '11', '8', '8', '12', '12', '7', '8', '12', '4', '9', '3', '1', '12', '1', '1', '8', '8', '3', '8', '8', '8', '8', '9', '1', '8', '8', '10', '1', '8', '2', '8', '8', '7', '3', '8', '2', '4', '4', '9', '8', '10', '12', '12', '1', '1', '9', '1', '1', '1', '8', '2', '2', '8', '1', '1', '2', '2', '1', '2', '8', '1', '9', '9', '8', '8', '4', '2', '9', '9', '8', '3', '4', '3', '1', '8', '8', '2', '1', '9', '7', '8', '8', '1', '12', '3', '8', '2', '4', '2', '9', '12', '1', '4', '1', '8', '8', '8', '2', '2', '8', '9', '8', '8', '8', '10', '9', '8', '7', '9', '1', '1', '9', '4', '2', '4', '2', '2', '1', '7', '8', '11', '11', '3', '9', '2', '4', '8', '9', '1', '8', '1', '1', '4', '9', '1', '1', '8', '8', '2', '1', '8', '4', '2', '8', '9', '8', '8', '2', '8', '8', '8', '7', '1', '1', '1', '2', '1', '1', '8', '7', '8', '8', '12', '2', '12', '12', '8', '10', '12', '8', '3', '3', '12', '10', '1', '8', '12', '1', '8', '8', '2', '8', '4', '7', '8', '7', '10', '8', '10', '9', '8', '12', '12', '1', '8', '8', '3', '8', '8', '8', '8', '8', '1', '9', '8', '11', '1', '1', '1', '9', '8', '1', '9', '2', '3', '11', '8', '9', '9', '9', '2', '1', '8', '8', '9', '7', '1', '4', '9', '4', '8', '8', '4', '8', '12', '9', '4', '8', '2', '10', '10', '10', '8', '9', '9', '8', '8', '12', '7', '1', '8', '8', '8', '4', '1', '1', '1', '1', '1', '1', '8', '1', '9', '8', '9', '9', '4', '8', '12', '9', '8', '8', '2', '8', '8', '8', '6', '9', '8', '3', '7', '8', '8', '4', '12', '8', '8', '9', '12', '12', '9', '8', '2', '9', '2', '3', '1', '12', '8', '10', '9', '9', '9', '10', '10', '3', '8', '12', '1', '4', '2', '1', '10', '8', '2', '8', '4', '8', '9', '1', '9', '9', '10', '10', '1', '4', '9', '2', '4', '9', '1', '1', '3', '10', '3', '3', '8', '7', '3', '8', '9', '9', '12', '4', '8', '12', '2', '2', '4', '1', '9', '9', '4', '1', '4', '2', '8', '12', '2', '3', '10', '10', '9', '8', '9', '9', '1', '12', '8', '8', '8', '12', '4', '1', '8', '8', '1', '9', '8', '8', '2', '1', '8', '9', '8', '3', '3', '3', '1', '8', '8', '9', '1', '10', '9', '9', '9', '9', '5', '9', '9', '8', '8', '8', '8', '8', '8', '8', '8', '11', '12', '8', '8', '1', '8', '9', '11', '2', '2', '2', '2', '3', '1', '2', '2', '8', '2', '4', '9', '1', '2', '9', '8', '2', '8', '9', '9', '3', '10', '9', '9', '2', '8', '9', '8', '12', '12', '1', '3', '8', '8', '8', '2', '7', '7', '7', '7', '3', '9', '1', '9', '8', '9', '9', '1', '1', '1', '2', '9', '9', '9', '11', '1', '8', '8', '9', '1', '9', '8', '8', '8', '1', '1', '8', '7', '1', '1', '8', '8', '9', '4', '4', '8', '2', '2', '8', '8', '8', '8', '8', '8', '11', '8', '2', '9', '4', '9', '3', '9', '9', '1', '3', '9', '3', '1', '12', '8', '9', '12', '1', '8', '4', '2', '1', '4', '8', '3', '3', '8', '2', '8', '9', '7', '8', '8', '8', '5', '8', '3', '9', '8', '8', '13', '12', '1', '1', '2', '8', '4', '1', '9', '9', '12', '8', '9', '12', '9', '1', '9', '9', '9', '9', '3', '2', '9', '9', '4', '8', '12', '2', '4', '9', '3', '1', '9', '7', '8', '9', '9', '8', '4', '8', '8', '7', '9', '10', '3', '8', '8', '8', '1', '1', '1', '1', '8', '8', '4', '1', '10', '1', '5', '7', '7', '1', '8', '9', '3', '7', '2', '7', '7', '2', '4', '8', '12', '7', '4', '2', '9', '9', '12', '6', '10', '8', '2', '4', '12', '9', '9', '3', '8', '8', '1', '2', '10', '9', '9', '8', '4', '12', '2', '1', '8', '8', '8', '12', '10', '10', '9', '3', '8', '8', '9', '2', '8', '10', '1', '1', '1', '1', '2', '1', '1', '1', '1', '9', '8', '12', '9', '4', '8', '8', '9', '1', '9', '3', '9', '8', '8', '1', '7', '7', '10', '1', '8', '8', '1', '9', '8', '10', '3', '1', '7', '1', '8', '8', '12', '8', '8', '1', '8', '7', '1', '7', '7', '8', '2', '1', '8', '8', '2', '10', '8', '8', '8', '8', '8', '10', '1', '8', '8', '12', '8', '3', '3', '2', '2', '2', '10', '8', '8', '8', '2', '9', '1', '8', '9', '3', '2', '8', '10', '8', '6', '1', '1', '8', '4', '1', '9', '10', '8', '1', '7', '1', '2', '8', '1', '1', '1', '12', '1', '9', '12', '8', '12', '12', '12', '8', '8', '12', '4', '8', '8', '8', '8', '9', '9', '1', '3', '3', '3', '3', '1', '12', '12', '9', '10', '8', '8', '1', '9', '2', '2', '13', '9', '8', '9', '2', '1', '9', '1', '8', '8', '8', '4', '8', '1', '1', '1', '12', '12', '7', '2', '2', '2', '8', '3', '8', '9', '2', '10', '7', '8', '9', '2', '1', '2', '12', '12', '8', '8', '9', '2', '2', '9', '11', '1', '8', '1', '10', '9', '2', '1', '4', '7', '7', '7', '7', '7', '12', '8', '8', '8', '1', '1', '10', '1', '12', '1', '8', '2', '1', '1', '12', '8', '7', '9', '12', '8', '9', '3', '9', '8', '8', '8', '8', '3', '11', '2', '2', '9', '8', '8', '10', '8', '2', '7', '3', '1', '4', '7', '8', '8', '1', '8', '3', '7', '12', '8', '10', '9', '9', '8', '8', '2', '8', '9', '9', '1', '2', '8', '8', '9', '8', '3', '8', '1', '8', '10', '9', '8', '9', '9', '12', '4', '4', '8', '9', '9', '8', '2', '10', '1', '2', '8', '9', '1', '9', '9', '9', '7', '12', '12', '8', '1', '1', '1', '1', '8', '3', '1', '1', '8', '1', '8', '8', '7', '8', '8', '8', '8', '3', '2', '2', '10', '10', '10', '7', '8', '1', '2', '12', '7', '9', '8', '7', '8', '12', '2', '8', '9', '2', '6', '6', '7', '9', '8', '1', '8', '9', '8', '1', '12', '1', '2', '8', '7', '7', '7', '8', '2', '2', '8', '1', '2', '2', '9', '9', '1', '8', '8', '4', '3', '3', '1', '6', '3', '3', '12', '3', '8', '9', '1', '4', '3', '1', '8', '3', '9', '2', '8', '2', '8', '8', '8', '1', '1', '1', '9', '9', '8', '1', '9', '8', '1', '1', '3', '10', '8', '1', '1', '3', '9', '1', '4', '4', '1', '8', '9', '9', '2', '0', '0', '1', '8', '3', '1', '8', '8', '9', '8', '8', '1', '1', '8', '9', '8', '8', '8', '7', '9', '8', '8', '8', '10', '9', '8', '1', '2', '6', '1', '9', '9', '8', '12', '12', '12', '8', '8', '2', '8', '1', '2', '2', '2', '1', '9', '8', '2', '12', '2', '8', '12', '8', '9', '8', '8', '9', '7', '1', '1', '1', '1', '1', '8', '8', '1', '8', '8', '1', '1', '3', '2', '8', '8', '9', '10', '10', '2', '2', '1', '9', '2', '9', '9', '4', '12', '12', '12', '10', '7', '3', '3', '4', '2', '2', '9', '2', '8', '4', '2', '4', '1', '10', '9', '7', '8', '7', '1', '1', '3', '3', '1', '1', '3', '3', '3', '1', '1', '1', '1', '8', '2', '3', '1', '1', '2', '8', '8', '12', '8', '8', '8', '8', '11', '9', '1', '8', '9', '2', '8', '8', '8', '3', '9', '1', '9', '2', '7', '2', '8', '2', '8', '10', '8', '1', '10', '1', '1', '9', '9', '8', '8', '1', '8', '8', '8', '12', '8', '8', '8', '1', '8', '8', '8', '1', '9', '1', '1', '8', '1', '8', '9', '8', '2', '12', '9', '9', '0', '1', '8', '8', '1', '8', '12', '8', '8', '10', '8', '8', '8', '7', '8', '1', '8', '7', '3', '10', '1', '8', '9', '1', '8', '8', '8', '10', '1', '10', '3', '9', '1', '8', '9', '2', '8', '3', '3', '9', '9', '7', '9', '1', '1', '9', '2', '1', '1', '1', '7', '1', '1', '8', '8', '1', '1', '8', '1', '8', '3', '12', '9', '3', '3', '8', '8', '8', '8', '3', '1', '3', '3', '1', '11', '0', '8', '8', '7', '8', '12', '1', '8', '9', '8', '9', '8', '8', '3', '8', '8', '1', '1', '1', '9', '2', '2', '2', '8', '7', '12', '8', '8', '9', '10', '10', '7', '8', '1', '9', '8', '7', '3', '1', '3', '8', '2', '2', '3', '9', '8', '4', '4', '8', '9', '2', '1', '1', '7', '8', '9', '9', '7', '8', '7', '7', '8', '2', '2', '8', '4', '9', '7', '10', '0', '9', '8', '3', '7', '8', '1', '1', '8', '9', '9', '2', '2', '10', '1', '9', '10', '10', '10', '8', '3', '2', '12', '9', '9', '10', '12', '9', '12', '12', '9', '1', '2', '4', '12', '12', '7', '8', '9', '7', '7', '7', '3', '9', '8', '9', '1', '12', '8', '9', '4', '1', '3', '12', '12', '12', '12', '8', '8', '2', '1', '1', '2', '1', '1', '1', '12', '12', '8', '12', '2', '2', '12', '3', '3', '12', '8', '2', '8', '8', '12', '2', '1', '10', '3', '2', '8', '7', '1', '8', '1', '3', '7', '8', '9', '8', '3', '1', '1', '7', '8', '8', '9', '8', '2', '9', '2', '2', '9', '8', '1', '8', '8', '1', '3', '3', '1', '1', '10', '1', '2', '8', '1', '1', '1', '1', '9', '1', '4', '1', '7', '7', '7', '7', '2', '2', '8', '8', '12', '1', '9', '1', '7', '3', '3', '1', '8', '8', '10', '8', '9', '2', '9', '1', '3', '8', '8', '3', '12', '2', '8', '12', '2', '9', '1', '3', '3', '3', '3', '2', '8', '7', '9', '8', '3', '3', '1', '7', '8', '3', '1', '1', '12', '8', '9', '1', '2', '3', '8', '1', '1', '3', '3', '9', '1', '1', '1', '12', '1', '7', '3', '3', '1', '8', '8', '8', '1', '2', '4', '8', '1', '10', '2', '5', '3', '3', '12', '10', '9', '9', '12', '9', '0', '2', '8', '8', '9', '9', '9', '8', '1', '3', '1', '1', '4', '8', '1', '10', '8', '7', '2', '8', '2', '8', '4', '7', '8', '1', '9', '1', '9', '8', '2', '8', '2', '7', '9', '2', '2', '9', '1', '8', '12', '1', '8', '1', '4', '1', '9', '9', '1', '10', '12', '4', '8', '1', '7', '3', '9', '2', '12', '7', '8', '8', '2', '1', '12', '9', '8', '1', '2', '2', '8', '10', '2', '1', '7', '7', '7', '12', '3', '3', '8', '3', '8', '8', '3', '9', '8', '9', '1', '1', '8', '7', '9', '3', '3', '8', '1', '0', '9', '9', '9', '1', '8', '9', '9', '10', '1', '8', '7', '8', '8', '8', '9', '9', '8', '9', '10', '4', '9', '3', '7', '12', '1', '9', '9', '8', '9', '1', '9', '3', '1', '8', '4', '12', '10', '9', '8', '7', '10', '8', '12', '12', '3', '10', '8', '12', '2', '1', '2', '3', '9', '8', '8', '7', '1', '1', '2', '2', '1', '2', '2', '7', '1', '3', '9', '9', '3', '8', '8', '8', '8', '8', '7', '8', '8', '10', '8', '1', '8', '2', '8', '2', '2', '2', '2', '2', '1', '7', '12', '10', '1', '2', '8', '1', '4', '7', '8', '1', '12', '8', '7', '9', '2', '2', '2', '8', '1', '8', '1', '1', '0', '2', '2', '6', '1', '8', '2', '1', '8', '1', '1', '2', '2', '3', '2', '2', '10', '10', '9', '1', '7', '7', '8', '8', '1', '12', '10', '12', '3', '8', '8', '8', '3', '8', '3', '10', '2', '2', '2', '1', '2', '2', '1', '0', '1', '8', '9', '1', '1', '1', '2', '2', '2', '2', '2', '1', '1', '1', '1', '12', '9', '9', '1', '10', '11', '12', '9', '1', '1', '8', '8', '1', '2', '12', '8', '3', '7', '2', '10', '9', '7', '8', '1', '2', '2', '7', '9', '12', '2', '2', '1', '8', '9', '2', '3', '2', '2', '2', '9', '9', '8', '9', '2', '3', '8', '8', '9', '7', '3', '10', '8', '8', '9', '1', '1', '2', '8', '1', '10', '8', '8', '9', '7', '1', '7', '12', '8', '1', '7', '2', '1', '3', '9', '2', '8', '8', '1', '2', '3', '9', '3', '8', '9', '7', '7', '1', '10', '2', '8', '8', '8', '1', '8', '2', '8', '8', '1', '2', '8', '10', '2', '1', '0', '8', '3', '8', '10', '10', '12', '3', '8', '9', '3', '10', '8', '8', '8', '1', '8', '8', '2', '2', '2', '2', '2', '2', '1', '1', '10', '1', '10', '8', '2', '1', '2', '2', '8', '3', '3', '2', '2', '1', '8', '1', '3', '1', '9', '2', '2', '1', '3', '2', '2', '3', '2', '2', '3', '2', '1', '1', '2', '1', '2', '7', '8', '2', '1', '12', '12', '3', '1', '9', '9', '7', '8', '7', '2', '7', '9', '2', '2', '2', '8', '9', '7', '9', '2', '8', '9', '9', '2', '9', '1', '1', '9', '8', '3', '3', '8', '2', '7', '8', '11', '2', '8', '9', '1', '8', '3', '1', '2', '1', '2', '2', '2', '2', '8', '3', '9', '2', '10', '9', '9', '2', '7', '7', '7', '1', '3', '8', '8', '1', '1', '8', '7', '1', '8', '8', '9', '3', '3', '3', '2', '4', '8', '12', '8', '1', '12', '9', '2', '8', '2', '12', '2', '1', '3', '8', '12', '12', '11', '8', '10', '9', '2', '9', '4', '10', '1', '12', '12', '9', '3', '8', '9', '12', '9', '9', '2', '2', '2', '3', '5', '1', '1', '1', '7', '7', '8', '2', '2', '8', '2', '1', '7', '3', '2', '9', '8', '9', '1', '1', '9', '8', '9', '3', '3', '2', '9', '4', '9', '8', '8', '1', '8', '10', '2', '0', '3', '8', '8', '8', '9', '12', '9', '8', '2', '2', '8', '10', '9', '8', '2', '2', '4', '1', '9', '9', '8', '8', '10', '3', '1', '1', '2', '8', '7', '8', '2', '1', '1', '8', '12', '2', '9', '4', '8', '3', '3', '3', '8', '2', '12', '12', '8', '9', '12', '8', '2', '12', '7', '9', '2', '2', '1', '3', '1', '3', '11', '9', '8', '1', '10', '2', '12', '3', '8', '4', '1', '2', '8', '8', '3', '9', '3', '8', '7', '1', '1', '3', '1', '8', '2', '9', '8', '12', '1', '1', '1', '9', '9', '1', '8', '8', '8', '2', '2', '12', '3', '1', '1', '1', '9', '9', '3', '2', '1', '1', '9', '9', '1', '9', '9', '1', '9', '3', '9', '1', '8', '1', '9', '8', '3', '8', '8', '9', '9', '8', '2', '8', '1', '2', '2', '10', '1', '2', '1', '1', '9', '1', '9', '3', '7', '7', '2', '10', '3', '3', '1', '1', '9', '1', '2', '1', '3', '2', '1', '1', '1', '1', '1', '1', '1', '1', '1', '2', '8', '8', '7', '1', '1', '8', '8', '2', '1', '12', '1', '8', '1', '9', '8', '8', '2', '1', '9', '9', '2', '2', '8', '1', '7', '7', '9', '8', '1', '6', '8', '8', '2', '2', '8', '3', '8', '3', '2', '3', '8', '1', '2', '2', '2', '8', '9', '9', '9', '12', '1', '9', '2', '1', '9', '2', '9', '9', '10', '8', '1', '1', '9', '2', '1', '7', '1', '3', '7', '1', '1', '1', '3', '8', '8', '8', '9', '9', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '1', '0', '1', '1', '1', '2', '2', '3', '1', '1', '2', '3', '3', '3', '7', '3', '1', '9', '3', '9', '1', '4', '1', '1', '9', '10', '9', '8', '2', '3', '10', '9', '3', '10', '12', '8', '9', '8', '2', '1', '9', '8', '1', '7', '8', '1', '9', '1', '2', '10', '7', '7', '8', '1', '3', '3', '9', '3', '3', '1', '1', '1', '9', '1', '3', '8', '8', '8', '1', '8', '1', '9', '7', '8', '8', '2', '1', '3', '8', '2', '1', '6', '1', '8', '8', '2', '8', '2', '2', '9', '8', '1', '1', '6', '8', '6', '10', '2', '1', '1', '8', '9', '3', '3', '3', '1', '3', '9', '6', '8', '1', '8', '3', '1', '2', '2', '12', '7', '1', '1', '1', '1', '12', '1', '8', '8', '3', '2', '1', '9', '2', '3', '3', '2', '2', '7', '2', '2', '2', '3', '1', '9', '8', '7', '1', '1', '3', '1', '9', '4', '3', '9', '2', '1', '1', '9', '8', '8', '7', '2', '1', '3', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '2', '10', '1', '1', '8', '2', '8', '1', '8', '1', '3', '1', '2', '1', '1', '3', '1', '2', '1', '1', '1', '1', '1', '1', '1', '2', '1', '8', '1', '9', '8', '8', '1', '10', '8', '8', '1', '3', '9', '8', '9', '8', '2', '1', '3', '4', '1', '12', '1', '9', '8', '2', '7', '2', '1', '2', '9', '2', '1', '3', '8', '1', '2', '3', '8', '2', '10', '10', '10', '9', '9', '9', '3', '8', '3', '1', '1', '1', '1', '1', '1', '2', '1', '9', '2', '7', '9', '8', '7', '9', '1', '1', '1', '1', '8', '2', '2', '3', '3', '2', '12', '8', '3', '1', '1', '12', '9', '8', '2', '2', '9', '1', '2', '2', '9', '1', '1', '2', '4', '8', '1', '8', '10', '2', '9', '9', '9', '1', '2', '1', '1', '2', '2', '2', '12', '1', '2', '8', '3', '9', '9', '3', '2', '9', '9', '7', '2', '8', '8', '9', '2', '9', '1', '1', '1', '1', '1', '2', '8', '2', '1', '2', '2', '8', '3', '1', '1', '9', '1', '3', '10', '7', '9', '8', '8', '2', '2', '9', '2', '2', '2', '2', '2', '2', '9', '1', '3', '6', '4', '2', '2', '9', '1', '9', '8', '1', '1', '1', '7', '3', '2', '9', '9', '1', '9', '7', '2', '8', '9', '9', '12', '10', '8', '10', '2', '3', '1', '2', '2', '12', '12', '3', '12', '1', '2', '1', '2', '1', '2', '8', '2', '3', '12', '12', '8', '2', '2', '9', '2', '9', '2', '3', '3', '1', '1', '1', '12', '3', '7', '1', '3', '1', '2', '9', '2', '7', '2', '1', '8', '3', '7', '8', '1', '1', '3', '8', '3', '2', '9', '3', '1', '1', '1', '1', '11', '1', '2', '6', '1', '2', '3', '8', '9', '9', '2', '9', '2', '1', '1', '3', '1', '12', '9', '1', '1', '9', '1', '9', '9', '8', '9', '8', '1', '2', '7', '1', '0', '9', '8', '2', '3', '2', '4', '1', '1', '1', '8', '1', '12', '1', '2', '3', '3', '9', '9', '9', '9', '9', '7', '9', '3', '9', '1', '7', '3', '3', '3', '3', '7', '8', '2', '2', '2', '8', '2', '3', '9', '9', '3', '1', '8', '9', '12', '8', '8', '3', '2', '1', '6', '1', '9', '1', '1', '1', '1', '1', '1', '2', '2', '1', '2', '2', '2', '2', '9', '2', '5', '3', '2', '2', '8', '2', '1', '8', '3', '3', '8', '3', '1', '3', '9', '2', '8', '9', '4', '4', '9', '8', '8', '7', '7', '3', '1', '8', '8', '3', '2', '2', '10', '3', '3', '2', '2', '2', '2', '12', '3', '1', '2', '7', '7', '12', '2', '1', '2', '1', '1', '2', '1', '3', '3', '3', '1', '3', '9', '4', '8', '8', '4', '2', '4', '2', '8', '9', '1', '1', '2', '2', '2', '8', '7', '2', '8', '2', '9', '1', '8', '1', '1', '2', '1', '1', '1', '2', '9', '9', '8', '1', '8', '1', '4', '8', '1', '2', '7', '9', '2', '1', '8', '9', '8', '9', '1', '1', '2', '3', '2', '12', '7', '1', '2', '4', '4', '4', '2', '2', '2', '9', '8', '8', '1', '12', '3', '3', '2', '2', '3', '12', '1', '5', '1', '1', '3', '2', '8', '8', '2', '2', '2', '3', '9', '8', '8', '9', '8', '8', '11', '8', '2', '9', '2', '3', '1', '7', '1', '8', '2', '8', '1', '1', '1', '8', '1', '1', '1', '7', '8', '8', '2', '3', '2', '1', '8', '8', '1', '8', '4', '8', '9', '8', '9', '8', '1', '2', '3', '2', '1', '2', '2', '3', '1', '9', '4', '8', '1', '1', '9', '1', '1', '8', '2', '2', '2', '1', '7', '3', '3', '1', '3', '3', '12', '3', '1', '1', '3', '1', '1', '4', '8', '4', '4', '3', '3', '3', '4', '8', '1', '9', '9', '9', '4', '1', '7', '4', '8', '4', '9', '8', '1', '1', '3', '8', '9', '1', '9', '7', '1', '1', '9', '1', '8', '8', '8', '1', '1', '8', '2', '2', '2', '9', '4', '4', '1', '2', '1', '2', '9', '1', '1', '8', '5', '5', '5', '9', '12', '4', '1', '2', '1', '8', '8', '1', '9', '9', '2', '12', '1', '1', '7', '8', '8', '4', '1', '2', '8', '11', '2', '3', '2', '2', '1', '11', '2', '2', '2', '2', '9', '9', '2', '2', '9', '3', '1', '9', '1', '1', '1', '7', '8', '1', '8', '9', '1', '2', '9', '8', '1', '2', '9', '8', '12', '2', '2', '10', '8', '2', '2', '8', '8', '1', '2', '6', '1', '1', '7', '7', '2', '3', '2', '1', '1', '8', '1', '10', '8', '12', '6', '4', '1', '4', '1', '2', '9', '8', '9', '4', '8', '8', '10', '8', '8', '8', '8', '8', '2', '2', '8', '4', '9', '3', '3', '3', '3', '3', '9', '2', '1', '1', '2', '3', '2', '1', '2', '3', '3', '3', '2', '2', '3', '3', '2', '1', '2', '3', '3', '3', '3', '12', '9', '3', '2', '2', '3', '1', '5', '1', '1', '3', '2', '2', '1', '2', '9', '3', '9', '8', '1', '8', '7', '7', '9', '1', '1', '7', '9', '10', '1', '7', '1', '4', '3', '8', '9', '4', '9', '9', '2', '9', '2', '8', '4', '2', '2', '9', '5', '2', '1', '10', '9', '3', '1', '1', '1', '2', '1', '2', '9', '1', '7', '9', '9', '8', '2', '2', '8', '9', '1', '9', '8', '2', '3', '2', '2', '1', '2', '2', '2', '1', '1', '8', '1', '9', '4', '12', '8', '2', '7', '2', '2', '1', '9', '3', '1', '8', '12', '1', '1', '2', '1', '10', '2', '6', '12', '12', '4', '9', '1', '9', '10', '1', '2', '8', '2', '1', '8', '6', '12', '9', '9', '2', '1', '7', '8', '9', '5', '8', '1', '8', '8', '3', '1', '8', '2', '8', '2', '2', '1', '2', '9', '2', '8', '1', '12', '8', '2', '7', '3', '3', '12', '2', '3', '3', '9', '9', '3', '3', '3', '3', '3', '2', '1', '4', '1', '4', '8', '8', '2', '1', '2', '9', '12', '1', '10', '12', '1', '2', '9', '4', '8', '10', '3', '2', '8', '7', '4', '8', '9', '2', '7', '1', '10', '2', '2', '2', '8', '4', '2', '2', '1', '1', '8', '7', '9', '2', '1', '9', '8', '9', '12', '4', '8', '2', '8', '7', '7', '1', '9', '2', '1', '1', '9', '1', '8', '2', '3', '9', '9', '1', '3', '2', '9', '2', '2', '8', '1', '1', '2', '1', '2', '1', '9', '8', '2', '2', '4', '8', '8', '9', '5', '5', '9', '8', '9', '6', '2', '1', '12', '2', '3', '8', '8', '9', '2', '2', '1', '3', '2', '10', '9', '10', '7', '2', '8', '4', '2', '8', '9', '8', '1', '3', '8', '8', '1', '8', '3', '8', '8', '1', '10', '3', '1', '5', '8', '8', '9', '2', '2', '9', '2', '1', '9', '1', '4', '1', '12', '8', '8', '2', '1', '1', '1', '1', '1', '9', '2', '9', '8', '9', '5', '9', '1', '8', '2', '4', '1', '1', '1', '9', '12', '4', '1', '1', '9', '8', '4', '1', '8', '9', '8', '8', '9', '9', '9', '9', '9', '9', '1', '3', '4', '2', '2', '8', '2', '9', '2', '3', '8', '3', '1', '7', '2', '1', '1', '4', '2', '9', '2', '1', '3', '2', '9', '2', '9', '3', '8', '1', '9', '1', '1', '2', '8', '8', '8', '2', '3', '9', '9', '8', '2', '4', '1', '5', '1', '1', '9', '9', '2', '1', '1', '1', '9', '9', '1', '3', '9', '2', '2', '9', '9', '8', '3', '9', '9', '8', '2', '2', '8', '9', '2', '2', '8', '9', '10', '10', '2', '8', '7', '9', '9', '4', '2', '4', '11', '2', '1', '7', '8', '8', '8', '9', '4', '9', '8', '8', '2', '1', '1', '9', '8', '3', '3', '8', '8', '7', '9', '1', '3', '4', '8', '10', '2', '4', '4', '8', '2', '8', '2', '8', '9', '2', '1', '2', '2', '3', '1', '9', '4', '5', '9', '9', '1', '1', '1', '1', '1', '1', '7', '1', '1', '1', '8', '1', '1', '1', '1', '1', '9', '1', '2', '1', '12', '9', '3', '9', '9', '4', '1', '9', '1', '1', '2', '11', '9', '3', '2', '2', '2', '4', '2', '3', '2', '7', '7', '8', '1', '10', '9', '5', '7', '1', '10', '8', '1', '2', '7', '1', '12', '5', '8', '4', '2', '9', '9', '8', '2', '9', '8', '8', '10', '2', '3', '2', '7', '2', '8', '10', '3', '2', '9', '9', '9', '1', '1', '1', '1', '8', '2', '10', '9', '8', '1', '2', '2', '2', '1', '2', '1', '3', '9', '12', '2', '2', '4', '8', '1', '3', '9', '4', '9', '1', '9', '9', '2', '1', '1', '9', '7', '10', '3', '10', '2', '9', '2', '9', '2', '8', '2', '1', '2', '3', '1', '1', '1', '8', '5', '8', '3', '1', '1', '4', '2', '2', '1', '3', '8', '2', '1', '1', '1', '2', '9', '4', '1', '8', '1', '2', '8', '2', '9', '2', '2', '5', '5', '2', '5', '2', '1', '8', '8', '1', '3', '12', '2', '4', '3', '2', '2', '2', '6', '2', '3', '3', '1', '8', '2', '2', '1', '1', '8', '8', '10', '1', '9', '2', '9', '12', '0', '2', '1', '3', '2', '2', '1', '2', '1', '9', '9', '2', '9', '8', '8', '1', '7', '8', '1', '2', '6', '9', '8', '10', '1', '12', '2', '2', '1', '8', '2', '1', '12', '7', '4', '2', '10', '2', '2', '1', '2', '1', '2', '1', '1', '2', '1', '9', '8', '10', '2', '10', '9', '9', '12', '5', '3', '2', '8', '2', '8', '3', '3', '1', '4', '9', '10', '2', '4', '8', '1', '2', '10', '12', '12', '3', '1', '1', '2', '9', '6', '6', '1', '9', '1', '8', '1', '8', '8', '8', '8', '2', '2', '2', '8', '8', '2', '1', '1', '1', '1', '1', '8', '8', '5', '8', '8', '1', '8', '9', '9', '1', '7', '12', '9', '9', '7', '2', '2', '7', '8', '9', '8', '2', '9', '6', '3', '4', '8', '4', '1', '2', '9', '8', '8', '8', '2', '1', '1', '8', '8', '3', '1', '9', '9', '7', '1', '2', '2', '2', '9', '4', '1', '2', '12', '1', '8', '2', '1', '9', '1', '8', '9', '9', '2', '5', '4', '2', '1', '2', '8', '12', '8', '10', '3', '8', '8', '8', '2', '2', '8', '2', '8', '2', '4', '8', '10', '2', '7', '2', '1', '8', '8', '9', '5', '8', '1', '12', '7', '10', '2', '2', '2', '1', '4', '8', '12', '1', '8', '2', '2', '9', '1', '8', '3', '1', '1', '5', '10', '2', '1', '2', '4', '8', '9', '3', '2', '8', '7', '4', '4', '9', '9', '2', '8', '1', '1', '2', '4', '7', '1', '9', '1', '1', '1', '8', '4', '1', '3', '1', '2', '2', '9', '1', '8', '8', '2', '2', '9', '9', '1', '8', '8', '1', '9', '8', '8', '2', '2', '2', '2', '1', '1', '1', '7', '4', '1', '1', '4', '2', '3', '3', '2', '3', '9', '2', '8', '2', '1', '5', '1', '2', '2', '1', '3', '5', '2', '9', '1', '1', '9', '8', '1', '4', '8', '1', '8', '2', '4', '8', '4', '7', '9', '8', '1', '2', '2', '4', '4', '11', '3', '11', '1', '3', '8', '1', '9', '6', '8', '9', '3', '9', '8', '9', '2', '3', '3', '7', '1', '7', '2', '9', '9', '1', '8', '8', '1', '5', '5', '8', '8', '1', '2', '1', '3', '9', '9', '5', '9', '8', '1', '4', '3', '8', '1', '1', '2', '1', '8', '9', '8', '2', '2', '2', '1', '2', '4', '10', '8', '1', '9', '1', '8', '1', '1', '9', '9', '8', '11', '1', '8', '2', '1', '2', '8', '2', '8', '8', '2', '9', '6', '10', '1', '8', '6', '4', '5', '11', '2', '2', '8', '1', '8', '1', '8', '1', '4', '8', '1', '1', '1', '8', '3', '7', '3', '3', '7', '1', '1', '4', '8', '1', '6', '4', '2', '2', '1', '1', '1', '1', '6', '2', '2', '8', '1', '8', '5', '5', '2', '1', '1', '2', '3', '7', '1', '1', '6', '2', '9', '3', '9', '8', '9', '2', '1', '4', '2', '9', '13', '9', '2', '8', '12', '4', '1', '1', '9', '5', '1', '1', '1', '1', '8', '2', '1', '1', '2', '9', '0', '12', '1', '1', '8', '9', '8', '3', '2', '2', '12', '8', '2', '2', '2', '1', '10', '1', '9', '8', '9', '5', '2', '2', '9', '2', '8', '4', '8', '3', '7', '9', '2', '2', '7', '2', '2', '8', '1', '1', '9', '10', '9', '8', '2', '9', '0', '7', '1', '1', '1', '9', '8', '2', '8', '12', '8', '3', '2', '8', '8', '7', '7', '8', '12', '2', '1', '3', '9', '2', '2', '8', '2', '8', '8', '12', '8', '8', '1', '8', '9', '2', '4', '4', '7', '2', '1', '1', '9', '3', '7', '1', '3', '9', '8', '2', '2', '3', '1', '3', '10', '2', '3', '7', '1', '8', '1', '1', '10', '3', '10', '8', '4', '1', '2', '4', '3', '10', '9', '3', '9', '5', '8', '7', '2', '9', '3', '3', '2', '11', '8', '1', '1', '7', '2', '7', '2', '9', '4', '8', '1', '12', '8', '7', '9', '1', '0', '2', '6', '4', '9', '3', '2', '4', '12', '9', '8', '5', '9', '2', '8', '4', '6', '8', '9', '3', '1', '8', '7', '3', '1', '1', '1', '8', '9', '1', '8', '7', '9', '8', '2', '1', '2', '2', '3', '2', '2', '1', '1', '4', '9', '7', '3', '2', '8', '8', '9', '1', '9', '7', '4', '2', '2', '8', '5', '9', '1', '5', '7', '1', '4', '6', '9', '10', '1', '8', '2', '9', '1', '9', '2', '9', '8', '2', '9', '9', '11', '2', '12', '2', '2', '9', '4', '8', '1', '9', '9', '2', '8', '9', '1', '2', '9', '3', '8', '8', '1', '7', '8', '8', '2', '3', '2', '2', '2', '2', '9', '8', '10', '2', '2', '1', '9', '11', '1', '8', '8', '2', '4', '2', '1', '2', '8', '8', '9', '2', '8', '9', '2', '3', '1', '2', '1', '3', '10', '8', '1', '2', '4', '1', '9', '8', '9', '3', '3', '9', '8', '2', '11', '3', '7', '1', '8', '7', '1', '8', '8', '8', '1', '4', '8', '8', '1', '9', '3', '1', '1', '2', '8', '8', '8', '2', '1', '8', '10', '2', '12', '8', '8', '9', '1', '8', '1', '3', '2', '2', '10', '7', '2', '2', '1', '1', '8', '2', '9', '1', '2', '9', '3', '3', '10', '1', '4', '7', '10', '12', '2', '4', '10', '8', '6', '2', '9', '1', '8', '3', '1', '2', '8', '8', '2', '12', '10', '2', '10', '9', '1', '2', '5', '8', '12', '8', '8', '7', '8', '8', '1', '8', '2', '2', '9', '1', '8', '10', '5', '5', '5', '8', '1', '9', '1', '13', '1', '2', '2', '1', '2', '4', '9', '2', '1', '9', '13', '11', '1', '9', '9', '3', '10', '8', '7', '8', '8', '4', '2', '1', '9', '4', '3', '1', '1', '10', '9', '2', '8', '6', '2', '1', '1', '3', '2', '8', '1', '1', '1', '1', '2', '4', '1', '1', '10', '1', '8', '1', '1', '9', '1', '7', '1', '1', '9', '9', '2', '12', '9', '1', '8', '1', '12', '1', '8', '3', '8', '1', '8', '1', '2', '10', '1', '1', '2', '2', '9', '3', '9', '12', '1', '9', '1', '2', '8', '7', '2', '1', '1', '3', '2', '8', '4', '4', '5', '12', '7', '8', '9', '6', '1', '8', '9', '2', '1', '9', '1', '4', '1', '8', '2', '1', '1', '8', '7', '4', '9', '1', '8', '4', '2', '7', '5', '1', '3', '9', '2', '8', '2', '1', '2', '9', '1', '1', '2', '3', '4', '3', '1', '2', '2', '10', '2', '2', '8', '1', '1', '2', '4', '9', '2', '11', '9', '8', '9', '2', '1', '10', '4', '1', '1', '10', '7', '2', '10', '8', '1', '10', '8', '8', '8', '8', '8', '8', '2', '2', '2', '8', '3', '4', '1', '1', '1', '2', '1', '8', '8', '2', '8', '3', '8', '3', '1', '2', '3', '1', '1', '10', '1', '4', '4', '3', '3', '1', '9', '1', '1', '2', '1', '1', '6', '1', '1', '1', '9', '1', '4', '1', '1', '1', '1', '1', '1', '2', '12', '8', '8', '1', '9', '12', '10', '8', '2', '12', '1', '1', '2', '1', '7', '10', '7', '1', '2', '1', '10', '8', '2', '2', '8', '2', '2', '1', '9', '8', '8', '8', '8', '3', '1', '4', '2', '8', '3', '10', '10', '1', '9', '1', '9', '1', '9', '1', '8', '3', '1', '1', '8', '8', '8', '6', '3', '4', '1', '1', '1', '9', '9', '5', '2', '10', '2', '6', '2', '2', '7', '1', '8', '2', '11', '1', '1', '1', '2', '1', '9', '4', '4', '2', '8', '4', '6', '8', '8', '10', '10', '2', '1', '2', '9', '1', '8', '8', '3', '4', '8', '8', '2', '8', '8', '1', '2', '1', '2', '9', '4', '1', '3', '3', '3', '8', '8', '8', '2', '6', '4', '3', '3', '12', '3', '4', '9', '6', '8', '7', '7', '8', '6', '4', '1', '10', '6', '3', '3', '8', '1', '1', '8', '1', '11', '8', '1', '1', '2', '3', '11', '7', '2', '1', '1', '1', '9', '1', '2', '1', '1', '9', '8', '1', '1', '8', '0', '4', '1', '1', '2', '1', '1', '2', '2', '9', '1', '10', '1', '3', '8', '9', '3', '3', '1', '10', '1', '9', '1', '1', '7', '4', '1', '8', '10', '1', '2', '10', '1', '1', '2', '3', '3', '9', '1', '8', '10', '1', '7', '9', '1', '9', '6', '3', '9', '2', '8', '4', '12', '12', '3', '1', '1', '9', '1', '9', '2', '1', '1', '1', '1', '1', '9', '2', '1', '2', '10', '9', '8', '8', '10', '8', '2', '3', '2', '1', '12', '8', '12', '2', '9', '1', '3', '2', '5', '2', '2', '8', '2', '2', '4', '10', '9', '10', '9', '4', '2', '4', '12', '8', '2', '1', '8', '3', '9', '2', '9', '4', '1', '1', '1', '1', '1', '9', '6', '2', '9', '2', '1', '3', '2', '2', '5', '8', '9', '1', '6', '3', '1', '2', '2', '2', '6', '1', '8', '10', '8', '3', '3', '13', '2', '9', '9', '10', '6', '12', '8', '2', '9', '2', '3', '8', '8', '1', '2', '2', '3', '2', '10', '1', '8', '9', '1', '8', '9', '8', '8', '2', '10', '2', '1', '1', '1', '9', '9', '12', '1', '1', '4', '8', '3', '2', '2', '4', '4', '2', '2', '1', '1', '1', '9', '2', '2', '4', '1', '1', '8', '8', '8', '9', '2', '13', '9', '9', '4', '1', '9', '7', '10', '10', '8', '1', '9', '1', '1', '1', '8', '12', '4', '1', '2', '1', '1', '1', '7', '8', '3', '3', '1', '2', '2', '7', '2', '2', '10', '1', '8', '4', '0', '1', '9', '4', '1', '1', '2', '10', '10', '7', '9', '3', '12', '11', '2', '8', '10', '8', '9', '4', '3', '4', '10', '1', '1', '9', '3', '10', '1', '5', '3', '3', '2', '2', '1', '1', '6', '1', '7', '1', '1', '12', '1', '1', '1', '10', '1', '8', '8', '1', '3', '8', '3', '1', '2', '8', '3', '4', '2', '8', '8', '6', '1', '1', '4', '1', '2', '9', '5', '1', '7', '8', '9', '2', '4', '9', '8', '5', '9', '9', '9', '1', '2', '1', '3', '8', '9', '2', '9', '8', '9', '1', '8', '13', '9', '3', '8', '1', '4', '1', '3', '1', '6', '4', '10', '2', '2', '7', '8', '8', '12', '8', '9', '9', '9', '10', '3', '12', '12', '2', '9', '10', '9', '7', '2', '3', '4', '10', '10', '2', '7', '7', '1', '7', '3', '9', '1', '9', '8', '2', '9', '1', '8', '1', '10', '8', '8', '2', '7', '8', '1', '8', '9', '8', '1', '4', '1', '8', '8', '8', '1', '1', '10', '3', '2', '2', '6', '8', '9', '8', '2', '1', '1', '3', '8', '8', '1', '4', '4', '3', '9', '10', '4', '2', '4', '2', '9', '2', '2', '9', '1', '1', '10', '1', '1', '2', '1', '1', '1', '9', '1', '8', '8', '8', '2', '3', '8', '1', '6', '7', '3', '1', '9', '1', '2', '1', '6', '1', '2', '10', '1', '4', '2', '3', '2', '9', '9', '4', '9', '9', '1', '2', '8', '4', '7', '8', '7', '13', '9', '1', '2', '1', '9', '3', '3', '6', '8', '9', '10', '9', '2', '2', '3', '12', '1', '1', '1', '13', '2', '1', '7', '10', '8', '9', '1', '2', '9', '1', '1', '1', '2', '1', '12', '5', '6', '9', '10', '3', '1', '10', '1', '3', '2', '8', '9', '2', '10', '8', '8', '9', '2', '8', '9', '2', '2', '2', '8', '8', '2', '3', '1', '4', '8', '1', '9', '9', '2', '8', '10', '9', '1', '2', '12', '8', '8', '0', '2', '1', '1', '9', '10', '1', '6', '2', '2', '10', '2', '2', '4', '2', '2', '12', '1', '6', '7', '8', '9', '1', '3', '5', '7', '3', '1', '3', '1', '1', '3', '2', '6', '3', '2', '1', '8', '3', '5', '2', '1', '1', '8', '1', '1', '0', '2', '3', '5', '3', '1', '1', '9', '8', '8', '8', '8', '4', '7', '2', '9', '12', '5', '9', '3', '12', '3', '9', '1', '9', '1', '7', '3', '9', '3', '10', '1', '2', '1', '4', '9', '8', '2', '9', '11', '2', '8', '9', '4', '1', '1', '1', '1', '1', '1', '1', '9', '1', '9', '1', '1', '1', '12', '3', '10', '3', '6', '1', '7', '10', '8', '3', '7', '1', '1', '2', '6', '1', '3', '9', '2', '8', '1', '8', '8', '1', '8', '10', '12', '1', '10', '12', '10', '1', '8', '8', '1', '4', '7', '9', '2', '1', '9', '3', '8', '12', '10', '6', '1', '6', '8', '3', '1', '3', '1', '10', '10', '9', '8', '1', '2', '9', '8', '8', '1', '9', '8', '8', '3', '1', '3', '9', '12', '1', '8', '1', '5', '8', '11', '5', '5', '1', '2', '1', '3', '1', '1', '1', '9', '8', '8', '1', '9', '10', '8', '4', '1', '1', '8', '10', '8', '9', '1', '10', '1', '7', '2', '1', '1', '9', '6', '2', '9', '8', '1', '1', '1', '9', '8', '8', '1', '9', '9', '1', '1', '2', '7', '9', '2', '9', '9', '8', '7', '6', '8', '7', '8', '2', '9', '2', '1', '8', '8', '8', '2', '6', '3', '1', '8', '12', '12', '8', '7', '1', '2', '2', '7', '1', '9', '8', '1', '4', '1', '1', '8', '5', '8', '1', '8', '8', '8', '8', '1', '2', '11', '1', '1', '1', '2', '7', '1', '9', '2', '2', '2', '9', '8', '9', '2', '9', '1', '8', '1', '7', '11', '13', '1', '9', '8', '2', '2', '1', '2', '3', '10', '3', '10', '4', '3', '9', '1', '8', '1', '1', '13', '1', '1', '3', '8', '1', '2', '2', '2', '9', '4', '1', '8', '1', '9', '3', '6', '8', '5', '2', '8', '2', '8', '1', '9', '2', '2', '8', '1', '1', '4', '8', '9', '2', '2', '7', '1', '8', '12', '2', '12', '8', '2', '9', '9', '3', '8', '1', '1', '9', '9', '9', '2', '8', '1', '9', '1', '8', '9', '8', '2', '2', '4', '1', '9', '10', '10', '8', '8', '1', '1', '2', '1', '4', '1', '2', '1', '1', '3', '12', '8', '8', '8', '8', '10', '2', '8', '12', '1', '9', '9', '8', '9', '1', '2', '1', '8', '8', '8', '2', '8', '8', '3', '10', '8', '9', '1', '1', '1', '3', '4', '8', '10', '6', '3', '1', '3', '2', '8', '9', '3', '5', '4', '1', '5', '2', '9', '9', '9', '1', '1', '1', '9', '9', '6', '10', '9', '8', '12', '2', '9', '1', '2', '1', '1', '1', '8', '1', '12', '1', '12', '1', '9', '1', '8', '1', '2', '1', '2', '8', '8', '10', '1', '9', '1', '2', '2', '8', '8', '3', '12', '1', '11', '2', '10', '12', '11', '2', '1', '10', '1', '1', '9', '3', '10', '2', '7', '1', '8', '1', '9', '9', '2', '1', '3', '12', '5', '9', '9', '8', '1', '8', '8', '8', '1', '1', '3', '9', '8', '9', '8', '3', '10', '3', '7', '8', '9', '2', '3', '1', '9', '10', '9', '2', '8', '2', '6', '4', '1', '1', '3', '8', '2', '3', '9', '1', '2', '1', '8', '1', '2', '2', '2', '2', '9', '9', '4', '8', '9', '1', '1', '2', '9', '1', '5', '2', '9', '8', '10', '1', '2', '10', '8', '5', '6', '4', '8', '1', '1', '1', '8', '9', '2', '8', '8', '8', '12', '2', '2', '1', '8', '1', '2', '1', '9', '8', '8', '9', '1', '1', '8', '7', '1', '8', '0', '2', '3', '8', '1', '1', '8', '1', '1', '12', '3', '2', '10', '8', '10', '6', '1', '8', '8', '10', '8', '8', '1', '4', '8', '1', '2', '9', '8', '1', '1', '3', '3', '5', '1', '1', '2', '1', '2', '1', '9', '8', '3', '9', '8', '2', '10', '9', '1', '8', '1', '10', '9', '10', '1', '2', '3', '8', '1', '3', '1', '9', '9', '8', '1', '9', '8', '8', '9', '1', '9', '2', '8', '8', '2', '13', '8', '10', '8', '9', '3', '2', '1', '3', '1', '12', '10', '10', '1', '11', '1', '2', '4', '2', '10', '1', '8', '11', '1', '1', '1', '2', '2', '8', '2', '9', '8', '9', '9', '12', '8', '1', '9', '2', '4', '1', '8', '3', '9', '1', '6', '5', '8', '9', '3', '3', '2', '1', '1', '1', '8', '11', '1', '8', '7', '1', '11', '9', '1', '7', '9', '9', '8', '8', '8', '12', '8', '1', '2', '9', '8', '8', '2', '10', '9', '8', '8', '4', '10', '9', '8', '2', '10', '10', '2', '1', '2', '1', '2', '1', '4', '1', '7', '1', '1', '3', '12', '9', '8', '5', '9', '10', '1', '9', '9', '2', '7', '4', '1', '1', '8', '12', '8', '2', '2', '9', '4', '1', '1', '5', '9', '1', '8', '12', '8', '1', '1', '4', '9', '10', '2', '3', '1', '3', '3', '3', '8', '1', '2', '1', '2', '1', '2', '9', '1', '2', '8', '9', '4', '8', '8', '10', '2', '12', '5', '5', '1', '8', '1', '7', '8', '1', '1', '8', '9', '12', '4', '9', '3', '1', '2', '5', '9', '2', '1', '2', '2', '1', '2', '8', '1', '8', '8', '2', '2', '1', '8', '4', '1', '2', '10', '9', '10', '8', '1', '10', '4', '8', '8', '10', '2', '8', '1', '1', '3', '10', '1', '4', '2', '8', '3', '3', '2', '9', '8', '5', '5', '9', '3', '10', '8', '1', '2', '1', '2', '9', '1', '1', '2', '9', '9', '8', '8', '11', '9', '4', '1', '8', '2', '7', '1', '2', '8', '2', '9', '9', '8', '2', '9', '9', '8', '8', '1', '2', '8', '1', '1', '2', '9', '8', '1', '4', '9', '1', '8', '12', '1', '12', '2', '8', '2', '9', '1', '12', '1', '2', '9', '1', '1', '9', '3', '2', '9', '9', '2', '11', '8', '7', '9', '9', '9', '8', '8', '1', '2', '8', '1', '1', '4', '1', '2', '10', '1', '9', '2', '1', '1', '1', '9', '1', '13', '4', '3', '2', '1', '1', '2', '2', '7', '8', '7', '1', '1', '2', '8', '8', '3', '4', '8', '2', '10', '13', '8', '8', '9', '2', '2', '1', '1', '10', '4', '2', '7', '9', '8', '2', '1', '9', '8', '1', '1', '9', '8', '9', '8', '2', '8', '2', '2', '9', '2', '1', '9', '1', '2', '4', '8', '2', '1', '1', '8', '1', '4', '1', '9', '3', '9', '2', '7', '1', '9', '6', '1', '10', '2', '2', '2', '2', '2', '10', '10', '10', '9', '9', '9', '3', '12', '10', '1', '1', '10', '1', '1', '2', '2', '1', '2', '3', '12', '8', '8', '1', '2', '9', '9', '1', '1', '10', '8', '9', '8', '3', '1', '1', '3', '1', '10', '1', '1', '1', '4', '1', '1', '1', '7', '10', '2', '1', '8', '1', '9', '3', '1', '10', '8', '8', '1', '5', '9', '1', '2', '1', '8', '8', '1', '3', '1', '10', '2', '1', '1', '2', '8', '2', '5', '3', '5', '2', '1', '1', '9', '1', '7', '9', '9', '2', '8', '1', '12', '4', '1', '4', '9', '9', '4', '8', '1', '10', '10', '8', '8', '8', '9', '2', '3', '5', '1', '8', '1', '1', '1', '7', '10', '1', '4', '12', '3', '2', '2', '9', '1', '1', '1', '2', '8', '4', '1', '7', '3', '12', '8', '6', '2', '2', '7', '9', '11', '1', '10', '9', '1', '8', '12', '2', '11', '1', '2', '3', '1', '1', '1', '8', '9', '2', '2', '2', '2', '3', '3', '8', '10', '4', '1', '2', '9', '8', '2', '2', '9', '1', '8', '11', '4', '2', '8', '7', '1', '1', '7', '3', '9', '9', '4', '5', '8', '10', '2', '9', '9', '9', '8', '1', '9', '2', '2', '2', '1', '2', '9', '4', '3', '12', '4', '3', '2', '3', '8', '3', '10', '1', '10', '1', '1', '8', '10', '6', '8', '9', '9', '1', '2', '9', '2', '9', '3', '8', '1', '1', '12', '1', '5', '1', '10', '8', '10', '2', '7', '1', '1', '1', '1', '3', '2', '3', '5', '9', '1', '2', '1', '9', '8', '10', '9', '8', '1', '8', '1', '8', '2', '8', '1', '2', '8', '1', '2', '5', '8', '12', '2', '1', '1', '1', '1', '1', '8', '8', '2', '1', '4', '2', '8', '8', '8', '2', '9', '9', '11', '1', '2', '9', '3', '1', '1', '8', '9', '2', '4', '10', '1', '9', '8', '4', '9', '8', '10', '8', '8', '9', '2', '8', '9', '3', '3', '3', '4', '3', '2', '2', '10', '8', '2', '1', '5', '1', '9', '1', '1', '2', '1', '2', '3', '1', '1', '8', '8', '1', '10', '8', '8', '1', '6', '4', '8', '2', '5', '8', '1', '3', '8', '2', '8', '9', '1', '1', '8', '12', '10', '1', '5', '8', '2', '8', '10', '9', '9', '1', '6', '9', '1', '10', '8', '2', '9', '10', '1', '2', '1', '1', '8', '8', '3', '9', '8', '1', '10', '2', '8', '1', '10', '1', '5', '1', '1', '1', '5', '1', '1', '1', '4', '1', '1', '1', '8', '2', '8', '1', '8', '3', '8', '9', '1', '8', '1', '8', '1', '1', '2', '2', '1', '12', '9', '9', '1', '8', '2', '1', '2', '8', '1', '9', '1', '1', '2', '1', '2', '2', '2', '1', '9', '9', '8', '8', '1', '1', '9', '10', '8', '4', '3', '3', '4', '9', '1', '3', '3', '10', '2', '2', '1', '8', '4', '1', '9', '4', '1', '1', '9', '8', '10', '4', '9', '9', '8', '2', '1', '3', '4', '9', '3', '8', '9', '1', '1', '2', '1', '7', '8', '2', '9', '1', '9', '2', '6', '9', '10', '8', '1', '1', '5', '9', '5', '1', '9', '10', '9', '3', '3', '8', '8', '2', '9', '2', '8', '2', '1', '8', '5', '8', '3', '10', '1', '1', '2', '1', '8', '1', '4', '3', '2', '9', '4', '9', '8', '8', '8', '1', '3', '9', '1', '9', '9', '1', '2', '1', '9', '8', '9', '8', '1', '1', '1', '4', '2', '2', '9', '9', '1', '1', '1', '3', '6', '1', '2', '3', '1', '4', '2', '9', '1', '9', '9', '1', '8', '2', '1', '9', '1', '8', '1', '8', '8', '2', '1', '9', '9', '6', '8', '8', '9', '8', '10', '8', '9', '5', '1', '1', '1', '1', '8', '1', '8', '12', '8', '1', '12', '9', '2', '8', '1', '1', '2', '1', '8', '6', '9', '9', '8', '1', '8', '9', '7', '7', '8', '9', '9', '1', '8', '3', '8', '1', '3', '3', '8', '9', '8', '2', '8', '1', '2', '9', '8', '1', '1', '1', '1', '9', '1', '9', '8', '12', '3', '8', '8', '1', '10', '10', '10', '2', '2', '1', '10', '2', '2', '10', '8', '12', '8', '1', '1', '1', '2', '1', '1', '3', '8', '1', '2', '2', '2', '1', '1', '6', '2', '8', '8', '8', '1', '1', '1', '9', '9', '2', '8', '10', '2', '8', '2', '1', '2', '9', '9', '2', '1', '1', '8', '8', '1', '3', '1', '8', '9', '1', '10', '9', '1', '1', '1', '1', '7', '2', '2', '1', '2', '8', '8', '1', '2', '1', '3', '2', '1', '8', '3', '9', '2', '10', '2', '10', '1', '2', '1', '1', '13', '9', '10', '8', '2', '2', '1', '1', '8', '2', '2', '1', '9', '1', '1', '8', '9', '9', '2', '9', '2', '2', '1', '1', '1', '8', '9', '4', '9', '9', '1', '8', '2', '1', '1', '8', '2', '2', '2', '8', '1', '1', '8', '2', '10', '2', '9', '9', '7', '1', '1', '2', '2', '8', '3', '1', '2', '1', '1', '8', '1', '1', '1', '9', '1', '1', '8', '8', '1', '9', '6', '1', '9', '1', '8', '1', '8', '8', '8', '1', '3', '8', '8', '2', '6', '3', '2', '1', '10', '1', '8', '2', '1', '6', '1', '1', '1', '8', '1', '10', '1', '1', '8', '9', '2', '6', '9', '2', '9', '4', '1', '8', '1', '3', '7', '8', '8', '5', '8', '7', '1', '1', '1', '13', '8', '3', '1', '1', '1', '1', '2', '8', '2', '12', '8', '1', '2', '5', '1', '8', '2', '10', '8', '2', '10', '1', '5', '2', '1', '1', '5', '2', '2', '7', '8', '2', '9', '9', '1', '10', '1', '8', '2', '5', '8', '1', '5', '1', '10', '1', '1', '9', '8', '6', '1', '8', '9', '8', '1', '8', '2', '2', '8', '3', '9', '1', '9', '3', '8', '4', '4', '3', '3', '1', '1', '1', '8', '9', '1', '1', '8', '3', '1', '2', '8', '2', '1', '1', '10', '1', '2', '2', '1', '1', '8', '2', '8', '2', '10', '2', '2', '10', '8', '8', '8', '13', '2', '2', '9', '2', '5', '9', '2', '1', '8', '8', '8', '1', '12', '12', '2', '9', '2', '1', '8', '2', '1', '8', '1', '9', '9', '2', '7', '1', '1', '7', '1', '4', '3', '1', '10', '3', '10', '9', '4', '6', '7', '5', '2', '1', '8', '8', '1', '4', '9', '2', '1', '8', '1', '1', '1', '1', '1', '8', '9', '8', '8', '8', '8', '9', '8', '10', '1', '8', '2', '7', '8', '9', '1', '2', '5', '9', '10', '8', '9', '12', '1', '6', '1', '2', '10', '9', '4', '1', '8', '10', '10', '1', '1', '1', '1', '1', '2', '3', '8', '1', '2', '10', '2', '1', '2', '4', '2', '2', '9', '1', '4', '2', '1', '1', '12', '12', '9', '9', '7', '1', '8', '4', '9', '9', '8', '1', '7', '3', '2', '1', '1', '4', '8', '1', '5', '8', '2', '8', '1', '14', '9', '12', '1', '10', '2', '1', '1', '8', '8', '6', '6', '3', '1', '2', '8', '1', '3', '2', '1', '8', '8', '1', '10', '8', '2', '8', '2', '5', '9', '1', '8', '8', '1', '1', '8', '8', '13', '1', '3', '8', '4', '1', '9', '7', '8', '8', '8', '3', '8', '1', '4', '2', '8', '8', '8', '7', '1', '1', '2', '8', '8', '9', '8', '8', '2', '3', '9', '9', '2', '1', '2', '2', '1', '8', '2', '6', '2', '1', '1', '9', '10', '3', '8', '2', '3', '2', '8', '8', '1', '8', '8', '8', '9', '1', '4', '1', '9', '2', '1', '8', '0', '1', '8', '1', '1', '8', '9', '9', '1', '3', '1', '2', '8', '8', '7', '1', '1', '2', '9', '2', '10', '2', '8', '2', '2', '2', '7', '1', '9', '8', '1', '3', '9', '2', '1', '10', '8', '1', '4', '1', '8', '5', '9', '8', '8', '1', '2', '2', '1', '8', '8', '6', '8', '1', '1', '8', '3', '2', '2', '1', '8', '1', '7', '6', '8', '1', '1', '1', '9', '1', '1', '1', '2', '1', '11', '10', '10', '11', '9']\n",
            "     filenum                                               text  label\n",
            "0          1  [ Halliburton Oil Well Cementing Co. v. Walker...      8\n",
            "1          2  Rehearing Denied Dec. See . Mr.Claude T. Barne...      1\n",
            "2          3  Rehearing Denied Dec. See . Appeal from the Di...      8\n",
            "3          4  Mr. Walter J. Cummings, Jr., of Washington, D....      2\n",
            "4          5  Mr.A. Devitt Vaneck, of Washington, D.C., for ...      8\n",
            "...      ...                                                ...    ...\n",
            "8414    8415  Opinion reported: Ante, p. DECREE It is ordere...     11\n",
            "8415    8416  In this dispute between Utah and the United St...     10\n",
            "8416    8417  The United States, to the exclusion of defenda...     10\n",
            "8417    8418  Louisiana's exception to the portion of the Sp...     11\n",
            "8418    8419  To resolve a dispute over the ownership of cer...      9\n",
            "\n",
            "[8419 rows x 3 columns]\n"
          ]
        }
      ],
      "source": [
        "fh = open('sc_model_0.txt')\n",
        "filenums=[]\n",
        "records=[]\n",
        "for record in fh:\n",
        "    record=record.split(\"---\")\n",
        "    filenum=record[0]\n",
        "    modelnum=record[1]\n",
        "    text=record[2]\n",
        "    filenums.append(filenum)\n",
        "    records.append(text)\n",
        "    \n",
        "print(len(filenums))\n",
        "print(len(records))\n",
        "\n",
        "summarized_data = pd.DataFrame(list(zip(filenums, records)),columns =['filenum','text'])\n",
        "len_list = [len(ele.split()) for ele in records]\n",
        "res = 0 if len(len_list) == 0 else (float(sum(len_list)) / len(len_list))\n",
        "print(\"Average Length %s\" % res)\n",
        "\n",
        "temp_file = open(\"labels_sc.txt\", \"r\")\n",
        "#temp_file = open(\"labels_sc_279.txt\", \"r\")\n",
        "\n",
        "data = temp_file.read()\n",
        "\n",
        "label_list = data.split(\"\\n\")\n",
        "print(label_list)\n",
        "label_list = label_list[0:-1]\n",
        "print(label_list)\n",
        "label_list = [int(i) for i in label_list]\n",
        "temp_file.close()\n",
        "\n",
        "\n",
        "new_label_list=[]\n",
        "for num in filenums:\n",
        "  new_label_list.append(label_list[int(num)-1])\n",
        "summarized_data['label'] = new_label_list\n",
        "\n",
        "print(summarized_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kEIh_vBxF52U"
      },
      "outputs": [],
      "source": [
        "def create_model():\n",
        "    inps = Input(shape = (max_len,), dtype='int64')\n",
        "    masks= Input(shape = (max_len,), dtype='int64')\n",
        "    dbert_layer = dbert_model(inps, attention_mask=masks)[0][:,0,:]\n",
        "    dense_0 = Dense(512,activation='relu',kernel_regularizer=regularizers.l2(0.01))(dbert_layer)\n",
        "    dropout_0= Dropout(0.5)(dense_0)\n",
        "    pred = Dense(15, activation='softmax',kernel_regularizer=regularizers.l2(0.01))(dropout_0)\n",
        "    model = tf.keras.Model(inputs=[inps,masks], outputs=pred)\n",
        "    print(model.summary())\n",
        "    return model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILY_HaQbtgIj",
        "outputId": "7969e499-e0c9-435b-9bcc-ae2591a75dd9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_1[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_roberta_model[0][0]']       \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 15)           7695        ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 8  1  8 ... 10 11  9]\n",
            "[   1    2    3 ... 8417 8418 8419]\n"
          ]
        }
      ],
      "source": [
        "gc.collect()\n",
        "tf.keras.backend.clear_session()\n",
        "dbert_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "dbert_model = TFRobertaModel.from_pretrained('roberta-base')\n",
        "max_len=512\n",
        "sentences=summarized_data['text']\n",
        "labels=summarized_data['label']\n",
        "filenumbers=summarized_data['filenum']\n",
        "len(sentences),len(labels),len(filenumbers)\n",
        "\n",
        "model_0=create_model()\n",
        "input_ids=[]\n",
        "attention_masks=[]\n",
        "\n",
        "for sent in sentences:\n",
        "  dbert_inps=dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n",
        "  input_ids.append(dbert_inps['input_ids'])\n",
        "  attention_masks.append(dbert_inps['attention_mask'])\n",
        "input_ids=np.asarray(input_ids)\n",
        "\n",
        "attention_masks=np.array(attention_masks)\n",
        "labels=np.array(labels)\n",
        "filenumbers=[int(i) for i in filenumbers]\n",
        "filenumbers=np.array(filenumbers)\n",
        "print(labels)\n",
        "print(filenumbers)\n",
        "\n",
        "train_inp,val_inp,train_label,val_label,train_mask,val_mask,train_fnum,val_fnum=train_test_split(input_ids,labels,attention_masks,filenumbers,test_size=0.1,random_state=42)\n",
        "\n",
        "new_train_inp_0=[]\n",
        "new_val_inp_0=[]\n",
        "new_train_label_0=[]\n",
        "new_val_label_0=[]\n",
        "new_train_mask_0=[]\n",
        "new_train_fnum_0=[]\n",
        "new_val_fnum_0=[]\n",
        "new_val_mask_0=[]\n",
        "for i in range(len(train_fnum)):\n",
        "  if train_fnum[i] in filenumbers:\n",
        "    new_train_inp_0.append(train_inp[i])\n",
        "    #new_val_inp.append(val_inp[i])\n",
        "    new_train_label_0.append(train_label[i])\n",
        "    #new_val_label.append(val_label[i])\n",
        "    new_train_mask_0.append(train_mask[i])\n",
        "    #new_val_mask.append(val_mask[i])\n",
        "    new_train_fnum_0.append(train_fnum[i])\n",
        "\n",
        "for i in range(len(val_fnum)):\n",
        "  if val_fnum[i] in filenumbers:\n",
        "    #new_train_inp.append(train_inp[i])\n",
        "    new_val_inp_0.append(val_inp[i])\n",
        "    #new_train_label.append(train_label[i])\n",
        "    new_val_label_0.append(val_label[i])\n",
        "    #new_train_mask.append(train_mask[i])\n",
        "    new_val_mask_0.append(val_mask[i])\n",
        "    new_val_fnum_0.append(val_fnum[i])\n",
        "\n",
        "new_train_inp_0=np.array(new_train_inp_0)\n",
        "new_val_inp_0=np.array(new_val_inp_0)\n",
        "new_train_label_0=np.array(new_train_label_0)\n",
        "new_val_label_0=np.array(new_val_label_0)\n",
        "new_train_mask_0=np.array(new_train_mask_0)\n",
        "new_train_fnum_0=np.array(new_train_fnum_0)\n",
        "new_val_fnum_0=np.array(new_val_fnum_0)\n",
        "new_val_mask_0=np.array(new_val_mask_0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "8758d872f8fd4e3195bd0d6e68923ff8",
            "99fbe21a71b04d0d834792309af01c97",
            "a716e417a0684d0cbc9f63bf69b84ee8",
            "1ea34926896c4352a5b3237e0fefd353",
            "b135e3191d644c03821749babe145797",
            "f9ff88f5e7ae40a8aebe70ebfad969ef",
            "27584f7e1c8c4f7b95d457d4a8bd9171",
            "8278ca504261441eadefc4d2f72f693b",
            "0187e8e709324358b8e8adf86b4fef51",
            "9fc93fdb90f4427d910c43a23cd66e40",
            "bbb1c0accaa44221b2b8ccee0a5acdd3",
            "e2034e0c6fad45e8a5713c0da190a335",
            "5b15c45d641f40a4808ee408a09bea9a",
            "de60f30acd29404f813e8b78da5e35a8",
            "7e1d6d93f06d4a63b696f37c9cf93277",
            "543b38434a8540a0bda6af97aa636c5a",
            "414c1977a7244908bd111159ad9f8436",
            "1b00c933bf144e0e9de04e641a3c7bb1",
            "cd26a756126f4df2b70f85679c04c093",
            "4aeb02a1804e43deacab431ccb445e2a",
            "0bf30ea7782d4456b3857f41fb26f993",
            "4589b993751d46c5a22c0294b239e95b",
            "5c9b80ec960d4601bebc9609d5571f49",
            "c0a03d1b96eb44548a745e1f8c7dad70",
            "bc0adc16bea742d08428c61db660367a",
            "6d9592282d064475b821b3ecb467bdd8",
            "64781d94d65d4647b7b50a08bf0680fc",
            "a1c6a5379c4d4f148e0492bb7f600d13",
            "57d2978550fb4faa839010f3327ac3bd",
            "edd7e7645ddb4d15862ca653ea4ad995",
            "7a18f8605f624849b227f87398449795",
            "f2161407bc9f4587b439721cf10252db",
            "d037e93131ab4c1a9dacaad5e6c320a6",
            "625ee541d52a42349ca3dcd7782c191e",
            "67f175fae04e442fb3a5ff137bbee7e5",
            "bde15d420aa3447bbad113eb5054c855",
            "f537c8a0a0a9441394fcab519c3019b4",
            "9128b5729d9c416696046d2e7bb31cc4",
            "8125a3ce13be4916a43f62837b99e1fa",
            "0549cad61c314736991691edce9896cf",
            "086075ff60364174a005c02a20d16bc2",
            "9202f64e927348d29ba6d42a9c0b05fe",
            "73854b2f8bf04b3286555e91121ff6aa",
            "755d35cb5a58400cb2eec8804ab652f2"
          ]
        },
        "id": "a2RiZXaKGABO",
        "outputId": "5b9061a5-605a-48ab-c173-6dbf76d82da1"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8758d872f8fd4e3195bd0d6e68923ff8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e2034e0c6fad45e8a5713c0da190a335",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5c9b80ec960d4601bebc9609d5571f49",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/481 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "625ee541d52a42349ca3dcd7782c191e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/627M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_1[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_roberta_model[0][0]']       \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 15)           7695        ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 8  1  8 ... 10 11  9]\n",
            "[   1    2    3 ... 8417 8418 8419]\n",
            "Sun Jun  5 01:43:57 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P0    38W / 300W |   1651MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "948/948 [==============================] - 340s 342ms/step - loss: 7.4368 - accuracy: 0.5915 - val_loss: 6.7518 - val_accuracy: 0.7043\n",
            "Epoch 2/5\n",
            "948/948 [==============================] - 325s 343ms/step - loss: 6.4649 - accuracy: 0.7550 - val_loss: 6.2828 - val_accuracy: 0.7399\n",
            "Epoch 3/5\n",
            "948/948 [==============================] - 325s 343ms/step - loss: 5.8852 - accuracy: 0.8164 - val_loss: 5.9174 - val_accuracy: 0.7601\n",
            "Epoch 4/5\n",
            "948/948 [==============================] - 325s 343ms/step - loss: 5.3894 - accuracy: 0.8584 - val_loss: 5.6463 - val_accuracy: 0.7625\n",
            "Epoch 5/5\n",
            "948/948 [==============================] - 322s 340ms/step - loss: 4.9456 - accuracy: 0.8915 - val_loss: 5.2708 - val_accuracy: 0.7625\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_3[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_roberta_model[1][0]']       \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 15)           7695        ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.7624703087885986\n",
            "Weighted F1: 0.7540823069521467\n",
            "Micro F1: 0.7624703087885987\n",
            "Weighted Precision: 0.7642949996433699\n",
            "Micro Precision: 0.7624703087885986\n",
            "Weighted Recall: 0.7624703087885986\n",
            "Micro Recall: 0.7624703087885986\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_1[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_roberta_model[0][0]']       \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 15)           7695        ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 8  1  8 ... 10 11  9]\n",
            "[   1    2    3 ... 8417 8418 8419]\n",
            "Sun Jun  5 02:13:05 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    39W / 300W |  15237MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "948/948 [==============================] - 340s 343ms/step - loss: 7.4479 - accuracy: 0.5717 - val_loss: 6.6929 - val_accuracy: 0.7055\n",
            "Epoch 2/5\n",
            "948/948 [==============================] - 326s 343ms/step - loss: 6.3846 - accuracy: 0.7535 - val_loss: 6.1725 - val_accuracy: 0.7447\n",
            "Epoch 3/5\n",
            "948/948 [==============================] - 325s 343ms/step - loss: 5.8087 - accuracy: 0.8155 - val_loss: 5.7629 - val_accuracy: 0.7613\n",
            "Epoch 4/5\n",
            "948/948 [==============================] - 322s 339ms/step - loss: 5.3014 - accuracy: 0.8587 - val_loss: 5.4948 - val_accuracy: 0.7530\n",
            "Epoch 5/5\n",
            "948/948 [==============================] - 325s 343ms/step - loss: 4.8356 - accuracy: 0.8953 - val_loss: 5.1911 - val_accuracy: 0.7732\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_3[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_roberta_model[1][0]']       \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 15)           7695        ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.7731591448931117\n",
            "Weighted F1: 0.772323496451826\n",
            "Micro F1: 0.7731591448931117\n",
            "Weighted Precision: 0.7761444195979078\n",
            "Micro Precision: 0.7731591448931117\n",
            "Weighted Recall: 0.7731591448931117\n",
            "Micro Recall: 0.7731591448931117\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_1[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_roberta_model[0][0]']       \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 15)           7695        ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 8  1  8 ... 10 11  9]\n",
            "[   1    2    3 ... 8417 8418 8419]\n",
            "Sun Jun  5 02:42:13 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    39W / 300W |  15237MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "948/948 [==============================] - 341s 344ms/step - loss: 7.5011 - accuracy: 0.5490 - val_loss: 6.7390 - val_accuracy: 0.6876\n",
            "Epoch 2/5\n",
            "948/948 [==============================] - 326s 344ms/step - loss: 6.4100 - accuracy: 0.7486 - val_loss: 6.1624 - val_accuracy: 0.7589\n",
            "Epoch 3/5\n",
            "948/948 [==============================] - 322s 340ms/step - loss: 5.8253 - accuracy: 0.8113 - val_loss: 5.8067 - val_accuracy: 0.7565\n",
            "Epoch 4/5\n",
            "948/948 [==============================] - 326s 344ms/step - loss: 5.3376 - accuracy: 0.8522 - val_loss: 5.5046 - val_accuracy: 0.7625\n",
            "Epoch 5/5\n",
            "948/948 [==============================] - 322s 340ms/step - loss: 4.8748 - accuracy: 0.8885 - val_loss: 5.2188 - val_accuracy: 0.7589\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_3[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_roberta_model[1][0]']       \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 15)           7695        ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.7624703087885986\n",
            "Weighted F1: 0.7558265061959636\n",
            "Micro F1: 0.7624703087885987\n",
            "Weighted Precision: 0.7640838452701672\n",
            "Micro Precision: 0.7624703087885986\n",
            "Weighted Recall: 0.7624703087885986\n",
            "Micro Recall: 0.7624703087885986\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_1[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_roberta_model[0][0]']       \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 15)           7695        ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 8  1  8 ... 10 11  9]\n",
            "[   1    2    3 ... 8417 8418 8419]\n",
            "Sun Jun  5 03:11:25 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    39W / 300W |  15237MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "948/948 [==============================] - 341s 344ms/step - loss: 7.3734 - accuracy: 0.5915 - val_loss: 6.6658 - val_accuracy: 0.6865\n",
            "Epoch 2/5\n",
            "948/948 [==============================] - 326s 344ms/step - loss: 6.3719 - accuracy: 0.7455 - val_loss: 6.1691 - val_accuracy: 0.7482\n",
            "Epoch 3/5\n",
            "948/948 [==============================] - 326s 344ms/step - loss: 5.7945 - accuracy: 0.8023 - val_loss: 5.7464 - val_accuracy: 0.7613\n",
            "Epoch 4/5\n",
            "948/948 [==============================] - 322s 340ms/step - loss: 5.2783 - accuracy: 0.8453 - val_loss: 5.4462 - val_accuracy: 0.7530\n",
            "Epoch 5/5\n",
            "948/948 [==============================] - 326s 344ms/step - loss: 4.8334 - accuracy: 0.8841 - val_loss: 5.0737 - val_accuracy: 0.7696\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_3[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_roberta_model[1][0]']       \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 15)           7695        ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.7695961995249406\n",
            "Weighted F1: 0.7699003496423334\n",
            "Micro F1: 0.7695961995249406\n",
            "Weighted Precision: 0.772262805060756\n",
            "Micro Precision: 0.7695961995249406\n",
            "Weighted Recall: 0.7695961995249406\n",
            "Micro Recall: 0.7695961995249406\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_1[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_roberta_model[0][0]']       \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 15)           7695        ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 8  1  8 ... 10 11  9]\n",
            "[   1    2    3 ... 8417 8418 8419]\n",
            "Sun Jun  5 03:40:37 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    39W / 300W |  15237MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "948/948 [==============================] - 342s 344ms/step - loss: 7.4640 - accuracy: 0.5802 - val_loss: 6.8132 - val_accuracy: 0.6936\n",
            "Epoch 2/5\n",
            "948/948 [==============================] - 326s 344ms/step - loss: 6.5053 - accuracy: 0.7364 - val_loss: 6.2765 - val_accuracy: 0.7292\n",
            "Epoch 3/5\n",
            "948/948 [==============================] - 326s 344ms/step - loss: 5.9211 - accuracy: 0.8077 - val_loss: 5.8566 - val_accuracy: 0.7506\n",
            "Epoch 4/5\n",
            "948/948 [==============================] - 327s 345ms/step - loss: 5.4256 - accuracy: 0.8468 - val_loss: 5.5632 - val_accuracy: 0.7648\n",
            "Epoch 5/5\n",
            "948/948 [==============================] - 322s 340ms/step - loss: 4.9616 - accuracy: 0.8852 - val_loss: 5.3156 - val_accuracy: 0.7601\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_3[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_roberta_model[1][0]']       \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 15)           7695        ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.7648456057007126\n",
            "Weighted F1: 0.7646855594630132\n",
            "Micro F1: 0.7648456057007126\n",
            "Weighted Precision: 0.7762568156095959\n",
            "Micro Precision: 0.7648456057007126\n",
            "Weighted Recall: 0.7648456057007126\n",
            "Micro Recall: 0.7648456057007126\n",
            "Average Accuracy: 0.7665083135391924\n",
            "Average Weighted F1: 0.7633636437410567\n",
            "Average Micro F1: 0.7665083135391924\n",
            "Average Weighted Precision: 0.7706085770363593\n",
            "Average Micro Precision: 0.7665083135391924\n",
            "Average Weighted Recall: 0.7665083135391924\n",
            "Average Micro Recall: 0.7665083135391924\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "total_accuracy=0\n",
        "total_weighted_f1=0\n",
        "total_micro_f1=0\n",
        "total_weighted_precision=0\n",
        "total_micro_precision=0\n",
        "total_weighted_recall=0\n",
        "total_micro_recall=0\n",
        "\n",
        "for f in range(5):\n",
        "  gc.collect()\n",
        "  tf.keras.backend.clear_session()\n",
        "  dbert_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "  dbert_model = TFRobertaModel.from_pretrained('roberta-base')\n",
        "  max_len=512\n",
        "  sentences=summarized_data['text']\n",
        "  labels=summarized_data['label']\n",
        "  filenumbers=summarized_data['filenum']\n",
        "  len(sentences),len(labels),len(filenumbers)\n",
        "\n",
        "  model_0=create_model()\n",
        "  input_ids=[]\n",
        "  attention_masks=[]\n",
        "\n",
        "  for sent in sentences:\n",
        "    dbert_inps=dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n",
        "    input_ids.append(dbert_inps['input_ids'])\n",
        "    attention_masks.append(dbert_inps['attention_mask'])\n",
        "  input_ids=np.asarray(input_ids)\n",
        "\n",
        "  attention_masks=np.array(attention_masks)\n",
        "  labels=np.array(labels)\n",
        "  filenumbers=[int(i) for i in filenumbers]\n",
        "  filenumbers=np.array(filenumbers)\n",
        "  print(labels)\n",
        "  print(filenumbers)\n",
        "\n",
        "  train_inp,val_inp,train_label,val_label,train_mask,val_mask,train_fnum,val_fnum=train_test_split(input_ids,labels,attention_masks,filenumbers,test_size=0.1,random_state=42)\n",
        "\n",
        "  new_train_inp_0=[]\n",
        "  new_val_inp_0=[]\n",
        "  new_train_label_0=[]\n",
        "  new_val_label_0=[]\n",
        "  new_train_mask_0=[]\n",
        "  new_train_fnum_0=[]\n",
        "  new_val_fnum_0=[]\n",
        "  new_val_mask_0=[]\n",
        "  for i in range(len(train_fnum)):\n",
        "    if train_fnum[i] in filenumbers:\n",
        "      new_train_inp_0.append(train_inp[i])\n",
        "      #new_val_inp.append(val_inp[i])\n",
        "      new_train_label_0.append(train_label[i])\n",
        "      #new_val_label.append(val_label[i])\n",
        "      new_train_mask_0.append(train_mask[i])\n",
        "      #new_val_mask.append(val_mask[i])\n",
        "      new_train_fnum_0.append(train_fnum[i])\n",
        "\n",
        "  for i in range(len(val_fnum)):\n",
        "    if val_fnum[i] in filenumbers:\n",
        "      #new_train_inp.append(train_inp[i])\n",
        "      new_val_inp_0.append(val_inp[i])\n",
        "      #new_train_label.append(train_label[i])\n",
        "      new_val_label_0.append(val_label[i])\n",
        "      #new_train_mask.append(train_mask[i])\n",
        "      new_val_mask_0.append(val_mask[i])\n",
        "      new_val_fnum_0.append(val_fnum[i])\n",
        "\n",
        "  # print(new_train_label)\n",
        "  # print(len(new_train_label))\n",
        "  # print(new_val_label)\n",
        "  # print(len(new_val_label))\n",
        "  new_train_inp_0=np.array(new_train_inp_0)\n",
        "  new_val_inp_0=np.array(new_val_inp_0)\n",
        "  new_train_label_0=np.array(new_train_label_0)\n",
        "  new_val_label_0=np.array(new_val_label_0)\n",
        "  new_train_mask_0=np.array(new_train_mask_0)\n",
        "  new_train_fnum_0=np.array(new_train_fnum_0)\n",
        "  new_val_fnum_0=np.array(new_val_fnum_0)\n",
        "  new_val_mask_0=np.array(new_val_mask_0)\n",
        "  \n",
        "  log_dir='dbert_model'\n",
        "\n",
        "  model_save_path='./drive/MyDrive/Ensemble/roberta-ensemble-512-model_0-'+str(f)+'-15labels.h5'\n",
        "\n",
        "  loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "  accuracy = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
        "  callbacks= [tf.keras.callbacks.ModelCheckpoint(filepath=model_save_path,monitor='val_accuracy',mode='max',save_best_only=True,save_weights_only=True),keras.callbacks.TensorBoard(log_dir=log_dir)]\n",
        "  model_0.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])\n",
        "  gpu_info = !nvidia-smi\n",
        "  gpu_info = '\\n'.join(gpu_info)\n",
        "  if gpu_info.find('failed') >= 0:\n",
        "    print('Not connected to a GPU')\n",
        "  else:\n",
        "    print(gpu_info)\n",
        "  \n",
        "  history=model_0.fit([new_train_inp_0,new_train_mask_0],new_train_label_0,batch_size=8,epochs=5,validation_data=([new_val_inp_0,new_val_mask_0],new_val_label_0),callbacks=callbacks)\n",
        "\n",
        "  pred_labels=[]\n",
        "\n",
        "  model_saved_0= create_model()\n",
        "  model_saved_0.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])\n",
        "  model_saved_0.load_weights('./drive/MyDrive/Ensemble/roberta-ensemble-512-model_0-'+str(f)+'-15labels.h5')\n",
        "\n",
        "  for i in range(0,len(val_inp)):\n",
        "    pred=model_saved_0.predict([val_inp[i].reshape(1,512),val_mask[i].reshape(1,512)])\n",
        "    pred_label = pred.argmax(axis=1)\n",
        "    pred_labels.append(pred_label)\n",
        "  accuracy=accuracy_score(val_label, pred_labels)\n",
        "  print(\"Accuracy: \"+str(accuracy))\n",
        "  total_accuracy=total_accuracy+accuracy\n",
        "  \n",
        "  weighted_f1=f1_score(val_label,pred_labels, average='weighted')\n",
        "  print(\"Weighted F1: \"+ str(weighted_f1))\n",
        "  total_weighted_f1=total_weighted_f1+weighted_f1\n",
        "  micro_f1=f1_score(val_label,pred_labels, average='micro')\n",
        "  print(\"Micro F1: \"+ str(micro_f1))\n",
        "  total_micro_f1=total_micro_f1+micro_f1\n",
        "\n",
        "  weighted_precision=precision_score(val_label, pred_labels, average='weighted')\n",
        "  print(\"Weighted Precision: \" + str(weighted_precision))\n",
        "  total_weighted_precision=total_weighted_precision+weighted_precision\n",
        "  micro_precision=precision_score(val_label, pred_labels, average='micro')\n",
        "  print(\"Micro Precision: \" + str(micro_precision))\n",
        "  total_micro_precision=total_micro_precision+micro_precision\n",
        "\n",
        "  weighted_recall=recall_score(val_label, pred_labels, average='weighted')\n",
        "  print(\"Weighted Recall: \" + str(weighted_recall))\n",
        "  total_weighted_recall=total_weighted_recall+weighted_recall\n",
        "  micro_recall=recall_score(val_label, pred_labels, average='micro')\n",
        "  print(\"Micro Recall: \" + str(micro_recall))\n",
        "  total_micro_recall=total_micro_recall+micro_recall\n",
        "\n",
        "\n",
        "print(\"Average Accuracy: \"+str(total_accuracy/5))\n",
        "print(\"Average Weighted F1: \"+str(total_weighted_f1/5))\n",
        "print(\"Average Micro F1: \"+str(total_micro_f1/5))\n",
        "print(\"Average Weighted Precision: \"+str(total_weighted_precision/5))\n",
        "print(\"Average Micro Precision: \"+str(total_micro_precision/5))\n",
        "print(\"Average Weighted Recall: \"+str(total_weighted_recall/5))\n",
        "print(\"Average Micro Recall: \"+str(total_micro_recall/5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NuZhHmEL6We",
        "outputId": "75c322aa-be62-4012-a26a-1baecab839b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7332\n",
            "7332\n",
            "Average Length 510.89102564102564\n",
            "['8', '1', '8', '2', '8', '8', '8', '9', '7', '8', '1', '2', '1', '1', '8', '4', '8', '8', '12', '3', '3', '7', '3', '12', '1', '8', '8', '8', '8', '8', '8', '8', '1', '9', '5', '9', '9', '9', '11', '8', '8', '8', '4', '8', '8', '8', '8', '1', '3', '9', '3', '7', '1', '2', '9', '9', '7', '8', '8', '1', '10', '7', '8', '8', '9', '8', '7', '9', '9', '12', '7', '2', '8', '1', '11', '11', '1', '7', '7', '12', '1', '9', '8', '10', '12', '7', '8', '2', '8', '9', '9', '1', '8', '9', '1', '7', '12', '10', '10', '10', '8', '3', '7', '9', '8', '9', '1', '8', '8', '2', '7', '2', '9', '9', '11', '8', '8', '12', '12', '7', '8', '12', '4', '9', '3', '1', '12', '1', '1', '8', '8', '3', '8', '8', '8', '8', '9', '1', '8', '8', '10', '1', '8', '2', '8', '8', '7', '3', '8', '2', '4', '4', '9', '8', '10', '12', '12', '1', '1', '9', '1', '1', '1', '8', '2', '2', '8', '1', '1', '2', '2', '1', '2', '8', '1', '9', '9', '8', '8', '4', '2', '9', '9', '8', '3', '4', '3', '1', '8', '8', '2', '1', '9', '7', '8', '8', '1', '12', '3', '8', '2', '4', '2', '9', '12', '1', '4', '1', '8', '8', '8', '2', '2', '8', '9', '8', '8', '8', '10', '9', '8', '7', '9', '1', '1', '9', '4', '2', '4', '2', '2', '1', '7', '8', '11', '11', '3', '9', '2', '4', '8', '9', '1', '8', '1', '1', '4', '9', '1', '1', '8', '8', '2', '1', '8', '4', '2', '8', '9', '8', '8', '2', '8', '8', '8', '7', '1', '1', '1', '2', '1', '1', '8', '7', '8', '8', '12', '2', '12', '12', '8', '10', '12', '8', '3', '3', '12', '10', '1', '8', '12', '1', '8', '8', '2', '8', '4', '7', '8', '7', '10', '8', '10', '9', '8', '12', '12', '1', '8', '8', '3', '8', '8', '8', '8', '8', '1', '9', '8', '11', '1', '1', '1', '9', '8', '1', '9', '2', '3', '11', '8', '9', '9', '9', '2', '1', '8', '8', '9', '7', '1', '4', '9', '4', '8', '8', '4', '8', '12', '9', '4', '8', '2', '10', '10', '10', '8', '9', '9', '8', '8', '12', '7', '1', '8', '8', '8', '4', '1', '1', '1', '1', '1', '1', '8', '1', '9', '8', '9', '9', '4', '8', '12', '9', '8', '8', '2', '8', '8', '8', '6', '9', '8', '3', '7', '8', '8', '4', '12', '8', '8', '9', '12', '12', '9', '8', '2', '9', '2', '3', '1', '12', '8', '10', '9', '9', '9', '10', '10', '3', '8', '12', '1', '4', '2', '1', '10', '8', '2', '8', '4', '8', '9', '1', '9', '9', '10', '10', '1', '4', '9', '2', '4', '9', '1', '1', '3', '10', '3', '3', '8', '7', '3', '8', '9', '9', '12', '4', '8', '12', '2', '2', '4', '1', '9', '9', '4', '1', '4', '2', '8', '12', '2', '3', '10', '10', '9', '8', '9', '9', '1', '12', '8', '8', '8', '12', '4', '1', '8', '8', '1', '9', '8', '8', '2', '1', '8', '9', '8', '3', '3', '3', '1', '8', '8', '9', '1', '10', '9', '9', '9', '9', '5', '9', '9', '8', '8', '8', '8', '8', '8', '8', '8', '11', '12', '8', '8', '1', '8', '9', '11', '2', '2', '2', '2', '3', '1', '2', '2', '8', '2', '4', '9', '1', '2', '9', '8', '2', '8', '9', '9', '3', '10', '9', '9', '2', '8', '9', '8', '12', '12', '1', '3', '8', '8', '8', '2', '7', '7', '7', '7', '3', '9', '1', '9', '8', '9', '9', '1', '1', '1', '2', '9', '9', '9', '11', '1', '8', '8', '9', '1', '9', '8', '8', '8', '1', '1', '8', '7', '1', '1', '8', '8', '9', '4', '4', '8', '2', '2', '8', '8', '8', '8', '8', '8', '11', '8', '2', '9', '4', '9', '3', '9', '9', '1', '3', '9', '3', '1', '12', '8', '9', '12', '1', '8', '4', '2', '1', '4', '8', '3', '3', '8', '2', '8', '9', '7', '8', '8', '8', '5', '8', '3', '9', '8', '8', '13', '12', '1', '1', '2', '8', '4', '1', '9', '9', '12', '8', '9', '12', '9', '1', '9', '9', '9', '9', '3', '2', '9', '9', '4', '8', '12', '2', '4', '9', '3', '1', '9', '7', '8', '9', '9', '8', '4', '8', '8', '7', '9', '10', '3', '8', '8', '8', '1', '1', '1', '1', '8', '8', '4', '1', '10', '1', '5', '7', '7', '1', '8', '9', '3', '7', '2', '7', '7', '2', '4', '8', '12', '7', '4', '2', '9', '9', '12', '6', '10', '8', '2', '4', '12', '9', '9', '3', '8', '8', '1', '2', '10', '9', '9', '8', '4', '12', '2', '1', '8', '8', '8', '12', '10', '10', '9', '3', '8', '8', '9', '2', '8', '10', '1', '1', '1', '1', '2', '1', '1', '1', '1', '9', '8', '12', '9', '4', '8', '8', '9', '1', '9', '3', '9', '8', '8', '1', '7', '7', '10', '1', '8', '8', '1', '9', '8', '10', '3', '1', '7', '1', '8', '8', '12', '8', '8', '1', '8', '7', '1', '7', '7', '8', '2', '1', '8', '8', '2', '10', '8', '8', '8', '8', '8', '10', '1', '8', '8', '12', '8', '3', '3', '2', '2', '2', '10', '8', '8', '8', '2', '9', '1', '8', '9', '3', '2', '8', '10', '8', '6', '1', '1', '8', '4', '1', '9', '10', '8', '1', '7', '1', '2', '8', '1', '1', '1', '12', '1', '9', '12', '8', '12', '12', '12', '8', '8', '12', '4', '8', '8', '8', '8', '9', '9', '1', '3', '3', '3', '3', '1', '12', '12', '9', '10', '8', '8', '1', '9', '2', '2', '13', '9', '8', '9', '2', '1', '9', '1', '8', '8', '8', '4', '8', '1', '1', '1', '12', '12', '7', '2', '2', '2', '8', '3', '8', '9', '2', '10', '7', '8', '9', '2', '1', '2', '12', '12', '8', '8', '9', '2', '2', '9', '11', '1', '8', '1', '10', '9', '2', '1', '4', '7', '7', '7', '7', '7', '12', '8', '8', '8', '1', '1', '10', '1', '12', '1', '8', '2', '1', '1', '12', '8', '7', '9', '12', '8', '9', '3', '9', '8', '8', '8', '8', '3', '11', '2', '2', '9', '8', '8', '10', '8', '2', '7', '3', '1', '4', '7', '8', '8', '1', '8', '3', '7', '12', '8', '10', '9', '9', '8', '8', '2', '8', '9', '9', '1', '2', '8', '8', '9', '8', '3', '8', '1', '8', '10', '9', '8', '9', '9', '12', '4', '4', '8', '9', '9', '8', '2', '10', '1', '2', '8', '9', '1', '9', '9', '9', '7', '12', '12', '8', '1', '1', '1', '1', '8', '3', '1', '1', '8', '1', '8', '8', '7', '8', '8', '8', '8', '3', '2', '2', '10', '10', '10', '7', '8', '1', '2', '12', '7', '9', '8', '7', '8', '12', '2', '8', '9', '2', '6', '6', '7', '9', '8', '1', '8', '9', '8', '1', '12', '1', '2', '8', '7', '7', '7', '8', '2', '2', '8', '1', '2', '2', '9', '9', '1', '8', '8', '4', '3', '3', '1', '6', '3', '3', '12', '3', '8', '9', '1', '4', '3', '1', '8', '3', '9', '2', '8', '2', '8', '8', '8', '1', '1', '1', '9', '9', '8', '1', '9', '8', '1', '1', '3', '10', '8', '1', '1', '3', '9', '1', '4', '4', '1', '8', '9', '9', '2', '0', '0', '1', '8', '3', '1', '8', '8', '9', '8', '8', '1', '1', '8', '9', '8', '8', '8', '7', '9', '8', '8', '8', '10', '9', '8', '1', '2', '6', '1', '9', '9', '8', '12', '12', '12', '8', '8', '2', '8', '1', '2', '2', '2', '1', '9', '8', '2', '12', '2', '8', '12', '8', '9', '8', '8', '9', '7', '1', '1', '1', '1', '1', '8', '8', '1', '8', '8', '1', '1', '3', '2', '8', '8', '9', '10', '10', '2', '2', '1', '9', '2', '9', '9', '4', '12', '12', '12', '10', '7', '3', '3', '4', '2', '2', '9', '2', '8', '4', '2', '4', '1', '10', '9', '7', '8', '7', '1', '1', '3', '3', '1', '1', '3', '3', '3', '1', '1', '1', '1', '8', '2', '3', '1', '1', '2', '8', '8', '12', '8', '8', '8', '8', '11', '9', '1', '8', '9', '2', '8', '8', '8', '3', '9', '1', '9', '2', '7', '2', '8', '2', '8', '10', '8', '1', '10', '1', '1', '9', '9', '8', '8', '1', '8', '8', '8', '12', '8', '8', '8', '1', '8', '8', '8', '1', '9', '1', '1', '8', '1', '8', '9', '8', '2', '12', '9', '9', '0', '1', '8', '8', '1', '8', '12', '8', '8', '10', '8', '8', '8', '7', '8', '1', '8', '7', '3', '10', '1', '8', '9', '1', '8', '8', '8', '10', '1', '10', '3', '9', '1', '8', '9', '2', '8', '3', '3', '9', '9', '7', '9', '1', '1', '9', '2', '1', '1', '1', '7', '1', '1', '8', '8', '1', '1', '8', '1', '8', '3', '12', '9', '3', '3', '8', '8', '8', '8', '3', '1', '3', '3', '1', '11', '0', '8', '8', '7', '8', '12', '1', '8', '9', '8', '9', '8', '8', '3', '8', '8', '1', '1', '1', '9', '2', '2', '2', '8', '7', '12', '8', '8', '9', '10', '10', '7', '8', '1', '9', '8', '7', '3', '1', '3', '8', '2', '2', '3', '9', '8', '4', '4', '8', '9', '2', '1', '1', '7', '8', '9', '9', '7', '8', '7', '7', '8', '2', '2', '8', '4', '9', '7', '10', '0', '9', '8', '3', '7', '8', '1', '1', '8', '9', '9', '2', '2', '10', '1', '9', '10', '10', '10', '8', '3', '2', '12', '9', '9', '10', '12', '9', '12', '12', '9', '1', '2', '4', '12', '12', '7', '8', '9', '7', '7', '7', '3', '9', '8', '9', '1', '12', '8', '9', '4', '1', '3', '12', '12', '12', '12', '8', '8', '2', '1', '1', '2', '1', '1', '1', '12', '12', '8', '12', '2', '2', '12', '3', '3', '12', '8', '2', '8', '8', '12', '2', '1', '10', '3', '2', '8', '7', '1', '8', '1', '3', '7', '8', '9', '8', '3', '1', '1', '7', '8', '8', '9', '8', '2', '9', '2', '2', '9', '8', '1', '8', '8', '1', '3', '3', '1', '1', '10', '1', '2', '8', '1', '1', '1', '1', '9', '1', '4', '1', '7', '7', '7', '7', '2', '2', '8', '8', '12', '1', '9', '1', '7', '3', '3', '1', '8', '8', '10', '8', '9', '2', '9', '1', '3', '8', '8', '3', '12', '2', '8', '12', '2', '9', '1', '3', '3', '3', '3', '2', '8', '7', '9', '8', '3', '3', '1', '7', '8', '3', '1', '1', '12', '8', '9', '1', '2', '3', '8', '1', '1', '3', '3', '9', '1', '1', '1', '12', '1', '7', '3', '3', '1', '8', '8', '8', '1', '2', '4', '8', '1', '10', '2', '5', '3', '3', '12', '10', '9', '9', '12', '9', '0', '2', '8', '8', '9', '9', '9', '8', '1', '3', '1', '1', '4', '8', '1', '10', '8', '7', '2', '8', '2', '8', '4', '7', '8', '1', '9', '1', '9', '8', '2', '8', '2', '7', '9', '2', '2', '9', '1', '8', '12', '1', '8', '1', '4', '1', '9', '9', '1', '10', '12', '4', '8', '1', '7', '3', '9', '2', '12', '7', '8', '8', '2', '1', '12', '9', '8', '1', '2', '2', '8', '10', '2', '1', '7', '7', '7', '12', '3', '3', '8', '3', '8', '8', '3', '9', '8', '9', '1', '1', '8', '7', '9', '3', '3', '8', '1', '0', '9', '9', '9', '1', '8', '9', '9', '10', '1', '8', '7', '8', '8', '8', '9', '9', '8', '9', '10', '4', '9', '3', '7', '12', '1', '9', '9', '8', '9', '1', '9', '3', '1', '8', '4', '12', '10', '9', '8', '7', '10', '8', '12', '12', '3', '10', '8', '12', '2', '1', '2', '3', '9', '8', '8', '7', '1', '1', '2', '2', '1', '2', '2', '7', '1', '3', '9', '9', '3', '8', '8', '8', '8', '8', '7', '8', '8', '10', '8', '1', '8', '2', '8', '2', '2', '2', '2', '2', '1', '7', '12', '10', '1', '2', '8', '1', '4', '7', '8', '1', '12', '8', '7', '9', '2', '2', '2', '8', '1', '8', '1', '1', '0', '2', '2', '6', '1', '8', '2', '1', '8', '1', '1', '2', '2', '3', '2', '2', '10', '10', '9', '1', '7', '7', '8', '8', '1', '12', '10', '12', '3', '8', '8', '8', '3', '8', '3', '10', '2', '2', '2', '1', '2', '2', '1', '0', '1', '8', '9', '1', '1', '1', '2', '2', '2', '2', '2', '1', '1', '1', '1', '12', '9', '9', '1', '10', '11', '12', '9', '1', '1', '8', '8', '1', '2', '12', '8', '3', '7', '2', '10', '9', '7', '8', '1', '2', '2', '7', '9', '12', '2', '2', '1', '8', '9', '2', '3', '2', '2', '2', '9', '9', '8', '9', '2', '3', '8', '8', '9', '7', '3', '10', '8', '8', '9', '1', '1', '2', '8', '1', '10', '8', '8', '9', '7', '1', '7', '12', '8', '1', '7', '2', '1', '3', '9', '2', '8', '8', '1', '2', '3', '9', '3', '8', '9', '7', '7', '1', '10', '2', '8', '8', '8', '1', '8', '2', '8', '8', '1', '2', '8', '10', '2', '1', '0', '8', '3', '8', '10', '10', '12', '3', '8', '9', '3', '10', '8', '8', '8', '1', '8', '8', '2', '2', '2', '2', '2', '2', '1', '1', '10', '1', '10', '8', '2', '1', '2', '2', '8', '3', '3', '2', '2', '1', '8', '1', '3', '1', '9', '2', '2', '1', '3', '2', '2', '3', '2', '2', '3', '2', '1', '1', '2', '1', '2', '7', '8', '2', '1', '12', '12', '3', '1', '9', '9', '7', '8', '7', '2', '7', '9', '2', '2', '2', '8', '9', '7', '9', '2', '8', '9', '9', '2', '9', '1', '1', '9', '8', '3', '3', '8', '2', '7', '8', '11', '2', '8', '9', '1', '8', '3', '1', '2', '1', '2', '2', '2', '2', '8', '3', '9', '2', '10', '9', '9', '2', '7', '7', '7', '1', '3', '8', '8', '1', '1', '8', '7', '1', '8', '8', '9', '3', '3', '3', '2', '4', '8', '12', '8', '1', '12', '9', '2', '8', '2', '12', '2', '1', '3', '8', '12', '12', '11', '8', '10', '9', '2', '9', '4', '10', '1', '12', '12', '9', '3', '8', '9', '12', '9', '9', '2', '2', '2', '3', '5', '1', '1', '1', '7', '7', '8', '2', '2', '8', '2', '1', '7', '3', '2', '9', '8', '9', '1', '1', '9', '8', '9', '3', '3', '2', '9', '4', '9', '8', '8', '1', '8', '10', '2', '0', '3', '8', '8', '8', '9', '12', '9', '8', '2', '2', '8', '10', '9', '8', '2', '2', '4', '1', '9', '9', '8', '8', '10', '3', '1', '1', '2', '8', '7', '8', '2', '1', '1', '8', '12', '2', '9', '4', '8', '3', '3', '3', '8', '2', '12', '12', '8', '9', '12', '8', '2', '12', '7', '9', '2', '2', '1', '3', '1', '3', '11', '9', '8', '1', '10', '2', '12', '3', '8', '4', '1', '2', '8', '8', '3', '9', '3', '8', '7', '1', '1', '3', '1', '8', '2', '9', '8', '12', '1', '1', '1', '9', '9', '1', '8', '8', '8', '2', '2', '12', '3', '1', '1', '1', '9', '9', '3', '2', '1', '1', '9', '9', '1', '9', '9', '1', '9', '3', '9', '1', '8', '1', '9', '8', '3', '8', '8', '9', '9', '8', '2', '8', '1', '2', '2', '10', '1', '2', '1', '1', '9', '1', '9', '3', '7', '7', '2', '10', '3', '3', '1', '1', '9', '1', '2', '1', '3', '2', '1', '1', '1', '1', '1', '1', '1', '1', '1', '2', '8', '8', '7', '1', '1', '8', '8', '2', '1', '12', '1', '8', '1', '9', '8', '8', '2', '1', '9', '9', '2', '2', '8', '1', '7', '7', '9', '8', '1', '6', '8', '8', '2', '2', '8', '3', '8', '3', '2', '3', '8', '1', '2', '2', '2', '8', '9', '9', '9', '12', '1', '9', '2', '1', '9', '2', '9', '9', '10', '8', '1', '1', '9', '2', '1', '7', '1', '3', '7', '1', '1', '1', '3', '8', '8', '8', '9', '9', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '1', '0', '1', '1', '1', '2', '2', '3', '1', '1', '2', '3', '3', '3', '7', '3', '1', '9', '3', '9', '1', '4', '1', '1', '9', '10', '9', '8', '2', '3', '10', '9', '3', '10', '12', '8', '9', '8', '2', '1', '9', '8', '1', '7', '8', '1', '9', '1', '2', '10', '7', '7', '8', '1', '3', '3', '9', '3', '3', '1', '1', '1', '9', '1', '3', '8', '8', '8', '1', '8', '1', '9', '7', '8', '8', '2', '1', '3', '8', '2', '1', '6', '1', '8', '8', '2', '8', '2', '2', '9', '8', '1', '1', '6', '8', '6', '10', '2', '1', '1', '8', '9', '3', '3', '3', '1', '3', '9', '6', '8', '1', '8', '3', '1', '2', '2', '12', '7', '1', '1', '1', '1', '12', '1', '8', '8', '3', '2', '1', '9', '2', '3', '3', '2', '2', '7', '2', '2', '2', '3', '1', '9', '8', '7', '1', '1', '3', '1', '9', '4', '3', '9', '2', '1', '1', '9', '8', '8', '7', '2', '1', '3', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '2', '10', '1', '1', '8', '2', '8', '1', '8', '1', '3', '1', '2', '1', '1', '3', '1', '2', '1', '1', '1', '1', '1', '1', '1', '2', '1', '8', '1', '9', '8', '8', '1', '10', '8', '8', '1', '3', '9', '8', '9', '8', '2', '1', '3', '4', '1', '12', '1', '9', '8', '2', '7', '2', '1', '2', '9', '2', '1', '3', '8', '1', '2', '3', '8', '2', '10', '10', '10', '9', '9', '9', '3', '8', '3', '1', '1', '1', '1', '1', '1', '2', '1', '9', '2', '7', '9', '8', '7', '9', '1', '1', '1', '1', '8', '2', '2', '3', '3', '2', '12', '8', '3', '1', '1', '12', '9', '8', '2', '2', '9', '1', '2', '2', '9', '1', '1', '2', '4', '8', '1', '8', '10', '2', '9', '9', '9', '1', '2', '1', '1', '2', '2', '2', '12', '1', '2', '8', '3', '9', '9', '3', '2', '9', '9', '7', '2', '8', '8', '9', '2', '9', '1', '1', '1', '1', '1', '2', '8', '2', '1', '2', '2', '8', '3', '1', '1', '9', '1', '3', '10', '7', '9', '8', '8', '2', '2', '9', '2', '2', '2', '2', '2', '2', '9', '1', '3', '6', '4', '2', '2', '9', '1', '9', '8', '1', '1', '1', '7', '3', '2', '9', '9', '1', '9', '7', '2', '8', '9', '9', '12', '10', '8', '10', '2', '3', '1', '2', '2', '12', '12', '3', '12', '1', '2', '1', '2', '1', '2', '8', '2', '3', '12', '12', '8', '2', '2', '9', '2', '9', '2', '3', '3', '1', '1', '1', '12', '3', '7', '1', '3', '1', '2', '9', '2', '7', '2', '1', '8', '3', '7', '8', '1', '1', '3', '8', '3', '2', '9', '3', '1', '1', '1', '1', '11', '1', '2', '6', '1', '2', '3', '8', '9', '9', '2', '9', '2', '1', '1', '3', '1', '12', '9', '1', '1', '9', '1', '9', '9', '8', '9', '8', '1', '2', '7', '1', '0', '9', '8', '2', '3', '2', '4', '1', '1', '1', '8', '1', '12', '1', '2', '3', '3', '9', '9', '9', '9', '9', '7', '9', '3', '9', '1', '7', '3', '3', '3', '3', '7', '8', '2', '2', '2', '8', '2', '3', '9', '9', '3', '1', '8', '9', '12', '8', '8', '3', '2', '1', '6', '1', '9', '1', '1', '1', '1', '1', '1', '2', '2', '1', '2', '2', '2', '2', '9', '2', '5', '3', '2', '2', '8', '2', '1', '8', '3', '3', '8', '3', '1', '3', '9', '2', '8', '9', '4', '4', '9', '8', '8', '7', '7', '3', '1', '8', '8', '3', '2', '2', '10', '3', '3', '2', '2', '2', '2', '12', '3', '1', '2', '7', '7', '12', '2', '1', '2', '1', '1', '2', '1', '3', '3', '3', '1', '3', '9', '4', '8', '8', '4', '2', '4', '2', '8', '9', '1', '1', '2', '2', '2', '8', '7', '2', '8', '2', '9', '1', '8', '1', '1', '2', '1', '1', '1', '2', '9', '9', '8', '1', '8', '1', '4', '8', '1', '2', '7', '9', '2', '1', '8', '9', '8', '9', '1', '1', '2', '3', '2', '12', '7', '1', '2', '4', '4', '4', '2', '2', '2', '9', '8', '8', '1', '12', '3', '3', '2', '2', '3', '12', '1', '5', '1', '1', '3', '2', '8', '8', '2', '2', '2', '3', '9', '8', '8', '9', '8', '8', '11', '8', '2', '9', '2', '3', '1', '7', '1', '8', '2', '8', '1', '1', '1', '8', '1', '1', '1', '7', '8', '8', '2', '3', '2', '1', '8', '8', '1', '8', '4', '8', '9', '8', '9', '8', '1', '2', '3', '2', '1', '2', '2', '3', '1', '9', '4', '8', '1', '1', '9', '1', '1', '8', '2', '2', '2', '1', '7', '3', '3', '1', '3', '3', '12', '3', '1', '1', '3', '1', '1', '4', '8', '4', '4', '3', '3', '3', '4', '8', '1', '9', '9', '9', '4', '1', '7', '4', '8', '4', '9', '8', '1', '1', '3', '8', '9', '1', '9', '7', '1', '1', '9', '1', '8', '8', '8', '1', '1', '8', '2', '2', '2', '9', '4', '4', '1', '2', '1', '2', '9', '1', '1', '8', '5', '5', '5', '9', '12', '4', '1', '2', '1', '8', '8', '1', '9', '9', '2', '12', '1', '1', '7', '8', '8', '4', '1', '2', '8', '11', '2', '3', '2', '2', '1', '11', '2', '2', '2', '2', '9', '9', '2', '2', '9', '3', '1', '9', '1', '1', '1', '7', '8', '1', '8', '9', '1', '2', '9', '8', '1', '2', '9', '8', '12', '2', '2', '10', '8', '2', '2', '8', '8', '1', '2', '6', '1', '1', '7', '7', '2', '3', '2', '1', '1', '8', '1', '10', '8', '12', '6', '4', '1', '4', '1', '2', '9', '8', '9', '4', '8', '8', '10', '8', '8', '8', '8', '8', '2', '2', '8', '4', '9', '3', '3', '3', '3', '3', '9', '2', '1', '1', '2', '3', '2', '1', '2', '3', '3', '3', '2', '2', '3', '3', '2', '1', '2', '3', '3', '3', '3', '12', '9', '3', '2', '2', '3', '1', '5', '1', '1', '3', '2', '2', '1', '2', '9', '3', '9', '8', '1', '8', '7', '7', '9', '1', '1', '7', '9', '10', '1', '7', '1', '4', '3', '8', '9', '4', '9', '9', '2', '9', '2', '8', '4', '2', '2', '9', '5', '2', '1', '10', '9', '3', '1', '1', '1', '2', '1', '2', '9', '1', '7', '9', '9', '8', '2', '2', '8', '9', '1', '9', '8', '2', '3', '2', '2', '1', '2', '2', '2', '1', '1', '8', '1', '9', '4', '12', '8', '2', '7', '2', '2', '1', '9', '3', '1', '8', '12', '1', '1', '2', '1', '10', '2', '6', '12', '12', '4', '9', '1', '9', '10', '1', '2', '8', '2', '1', '8', '6', '12', '9', '9', '2', '1', '7', '8', '9', '5', '8', '1', '8', '8', '3', '1', '8', '2', '8', '2', '2', '1', '2', '9', '2', '8', '1', '12', '8', '2', '7', '3', '3', '12', '2', '3', '3', '9', '9', '3', '3', '3', '3', '3', '2', '1', '4', '1', '4', '8', '8', '2', '1', '2', '9', '12', '1', '10', '12', '1', '2', '9', '4', '8', '10', '3', '2', '8', '7', '4', '8', '9', '2', '7', '1', '10', '2', '2', '2', '8', '4', '2', '2', '1', '1', '8', '7', '9', '2', '1', '9', '8', '9', '12', '4', '8', '2', '8', '7', '7', '1', '9', '2', '1', '1', '9', '1', '8', '2', '3', '9', '9', '1', '3', '2', '9', '2', '2', '8', '1', '1', '2', '1', '2', '1', '9', '8', '2', '2', '4', '8', '8', '9', '5', '5', '9', '8', '9', '6', '2', '1', '12', '2', '3', '8', '8', '9', '2', '2', '1', '3', '2', '10', '9', '10', '7', '2', '8', '4', '2', '8', '9', '8', '1', '3', '8', '8', '1', '8', '3', '8', '8', '1', '10', '3', '1', '5', '8', '8', '9', '2', '2', '9', '2', '1', '9', '1', '4', '1', '12', '8', '8', '2', '1', '1', '1', '1', '1', '9', '2', '9', '8', '9', '5', '9', '1', '8', '2', '4', '1', '1', '1', '9', '12', '4', '1', '1', '9', '8', '4', '1', '8', '9', '8', '8', '9', '9', '9', '9', '9', '9', '1', '3', '4', '2', '2', '8', '2', '9', '2', '3', '8', '3', '1', '7', '2', '1', '1', '4', '2', '9', '2', '1', '3', '2', '9', '2', '9', '3', '8', '1', '9', '1', '1', '2', '8', '8', '8', '2', '3', '9', '9', '8', '2', '4', '1', '5', '1', '1', '9', '9', '2', '1', '1', '1', '9', '9', '1', '3', '9', '2', '2', '9', '9', '8', '3', '9', '9', '8', '2', '2', '8', '9', '2', '2', '8', '9', '10', '10', '2', '8', '7', '9', '9', '4', '2', '4', '11', '2', '1', '7', '8', '8', '8', '9', '4', '9', '8', '8', '2', '1', '1', '9', '8', '3', '3', '8', '8', '7', '9', '1', '3', '4', '8', '10', '2', '4', '4', '8', '2', '8', '2', '8', '9', '2', '1', '2', '2', '3', '1', '9', '4', '5', '9', '9', '1', '1', '1', '1', '1', '1', '7', '1', '1', '1', '8', '1', '1', '1', '1', '1', '9', '1', '2', '1', '12', '9', '3', '9', '9', '4', '1', '9', '1', '1', '2', '11', '9', '3', '2', '2', '2', '4', '2', '3', '2', '7', '7', '8', '1', '10', '9', '5', '7', '1', '10', '8', '1', '2', '7', '1', '12', '5', '8', '4', '2', '9', '9', '8', '2', '9', '8', '8', '10', '2', '3', '2', '7', '2', '8', '10', '3', '2', '9', '9', '9', '1', '1', '1', '1', '8', '2', '10', '9', '8', '1', '2', '2', '2', '1', '2', '1', '3', '9', '12', '2', '2', '4', '8', '1', '3', '9', '4', '9', '1', '9', '9', '2', '1', '1', '9', '7', '10', '3', '10', '2', '9', '2', '9', '2', '8', '2', '1', '2', '3', '1', '1', '1', '8', '5', '8', '3', '1', '1', '4', '2', '2', '1', '3', '8', '2', '1', '1', '1', '2', '9', '4', '1', '8', '1', '2', '8', '2', '9', '2', '2', '5', '5', '2', '5', '2', '1', '8', '8', '1', '3', '12', '2', '4', '3', '2', '2', '2', '6', '2', '3', '3', '1', '8', '2', '2', '1', '1', '8', '8', '10', '1', '9', '2', '9', '12', '0', '2', '1', '3', '2', '2', '1', '2', '1', '9', '9', '2', '9', '8', '8', '1', '7', '8', '1', '2', '6', '9', '8', '10', '1', '12', '2', '2', '1', '8', '2', '1', '12', '7', '4', '2', '10', '2', '2', '1', '2', '1', '2', '1', '1', '2', '1', '9', '8', '10', '2', '10', '9', '9', '12', '5', '3', '2', '8', '2', '8', '3', '3', '1', '4', '9', '10', '2', '4', '8', '1', '2', '10', '12', '12', '3', '1', '1', '2', '9', '6', '6', '1', '9', '1', '8', '1', '8', '8', '8', '8', '2', '2', '2', '8', '8', '2', '1', '1', '1', '1', '1', '8', '8', '5', '8', '8', '1', '8', '9', '9', '1', '7', '12', '9', '9', '7', '2', '2', '7', '8', '9', '8', '2', '9', '6', '3', '4', '8', '4', '1', '2', '9', '8', '8', '8', '2', '1', '1', '8', '8', '3', '1', '9', '9', '7', '1', '2', '2', '2', '9', '4', '1', '2', '12', '1', '8', '2', '1', '9', '1', '8', '9', '9', '2', '5', '4', '2', '1', '2', '8', '12', '8', '10', '3', '8', '8', '8', '2', '2', '8', '2', '8', '2', '4', '8', '10', '2', '7', '2', '1', '8', '8', '9', '5', '8', '1', '12', '7', '10', '2', '2', '2', '1', '4', '8', '12', '1', '8', '2', '2', '9', '1', '8', '3', '1', '1', '5', '10', '2', '1', '2', '4', '8', '9', '3', '2', '8', '7', '4', '4', '9', '9', '2', '8', '1', '1', '2', '4', '7', '1', '9', '1', '1', '1', '8', '4', '1', '3', '1', '2', '2', '9', '1', '8', '8', '2', '2', '9', '9', '1', '8', '8', '1', '9', '8', '8', '2', '2', '2', '2', '1', '1', '1', '7', '4', '1', '1', '4', '2', '3', '3', '2', '3', '9', '2', '8', '2', '1', '5', '1', '2', '2', '1', '3', '5', '2', '9', '1', '1', '9', '8', '1', '4', '8', '1', '8', '2', '4', '8', '4', '7', '9', '8', '1', '2', '2', '4', '4', '11', '3', '11', '1', '3', '8', '1', '9', '6', '8', '9', '3', '9', '8', '9', '2', '3', '3', '7', '1', '7', '2', '9', '9', '1', '8', '8', '1', '5', '5', '8', '8', '1', '2', '1', '3', '9', '9', '5', '9', '8', '1', '4', '3', '8', '1', '1', '2', '1', '8', '9', '8', '2', '2', '2', '1', '2', '4', '10', '8', '1', '9', '1', '8', '1', '1', '9', '9', '8', '11', '1', '8', '2', '1', '2', '8', '2', '8', '8', '2', '9', '6', '10', '1', '8', '6', '4', '5', '11', '2', '2', '8', '1', '8', '1', '8', '1', '4', '8', '1', '1', '1', '8', '3', '7', '3', '3', '7', '1', '1', '4', '8', '1', '6', '4', '2', '2', '1', '1', '1', '1', '6', '2', '2', '8', '1', '8', '5', '5', '2', '1', '1', '2', '3', '7', '1', '1', '6', '2', '9', '3', '9', '8', '9', '2', '1', '4', '2', '9', '13', '9', '2', '8', '12', '4', '1', '1', '9', '5', '1', '1', '1', '1', '8', '2', '1', '1', '2', '9', '0', '12', '1', '1', '8', '9', '8', '3', '2', '2', '12', '8', '2', '2', '2', '1', '10', '1', '9', '8', '9', '5', '2', '2', '9', '2', '8', '4', '8', '3', '7', '9', '2', '2', '7', '2', '2', '8', '1', '1', '9', '10', '9', '8', '2', '9', '0', '7', '1', '1', '1', '9', '8', '2', '8', '12', '8', '3', '2', '8', '8', '7', '7', '8', '12', '2', '1', '3', '9', '2', '2', '8', '2', '8', '8', '12', '8', '8', '1', '8', '9', '2', '4', '4', '7', '2', '1', '1', '9', '3', '7', '1', '3', '9', '8', '2', '2', '3', '1', '3', '10', '2', '3', '7', '1', '8', '1', '1', '10', '3', '10', '8', '4', '1', '2', '4', '3', '10', '9', '3', '9', '5', '8', '7', '2', '9', '3', '3', '2', '11', '8', '1', '1', '7', '2', '7', '2', '9', '4', '8', '1', '12', '8', '7', '9', '1', '0', '2', '6', '4', '9', '3', '2', '4', '12', '9', '8', '5', '9', '2', '8', '4', '6', '8', '9', '3', '1', '8', '7', '3', '1', '1', '1', '8', '9', '1', '8', '7', '9', '8', '2', '1', '2', '2', '3', '2', '2', '1', '1', '4', '9', '7', '3', '2', '8', '8', '9', '1', '9', '7', '4', '2', '2', '8', '5', '9', '1', '5', '7', '1', '4', '6', '9', '10', '1', '8', '2', '9', '1', '9', '2', '9', '8', '2', '9', '9', '11', '2', '12', '2', '2', '9', '4', '8', '1', '9', '9', '2', '8', '9', '1', '2', '9', '3', '8', '8', '1', '7', '8', '8', '2', '3', '2', '2', '2', '2', '9', '8', '10', '2', '2', '1', '9', '11', '1', '8', '8', '2', '4', '2', '1', '2', '8', '8', '9', '2', '8', '9', '2', '3', '1', '2', '1', '3', '10', '8', '1', '2', '4', '1', '9', '8', '9', '3', '3', '9', '8', '2', '11', '3', '7', '1', '8', '7', '1', '8', '8', '8', '1', '4', '8', '8', '1', '9', '3', '1', '1', '2', '8', '8', '8', '2', '1', '8', '10', '2', '12', '8', '8', '9', '1', '8', '1', '3', '2', '2', '10', '7', '2', '2', '1', '1', '8', '2', '9', '1', '2', '9', '3', '3', '10', '1', '4', '7', '10', '12', '2', '4', '10', '8', '6', '2', '9', '1', '8', '3', '1', '2', '8', '8', '2', '12', '10', '2', '10', '9', '1', '2', '5', '8', '12', '8', '8', '7', '8', '8', '1', '8', '2', '2', '9', '1', '8', '10', '5', '5', '5', '8', '1', '9', '1', '13', '1', '2', '2', '1', '2', '4', '9', '2', '1', '9', '13', '11', '1', '9', '9', '3', '10', '8', '7', '8', '8', '4', '2', '1', '9', '4', '3', '1', '1', '10', '9', '2', '8', '6', '2', '1', '1', '3', '2', '8', '1', '1', '1', '1', '2', '4', '1', '1', '10', '1', '8', '1', '1', '9', '1', '7', '1', '1', '9', '9', '2', '12', '9', '1', '8', '1', '12', '1', '8', '3', '8', '1', '8', '1', '2', '10', '1', '1', '2', '2', '9', '3', '9', '12', '1', '9', '1', '2', '8', '7', '2', '1', '1', '3', '2', '8', '4', '4', '5', '12', '7', '8', '9', '6', '1', '8', '9', '2', '1', '9', '1', '4', '1', '8', '2', '1', '1', '8', '7', '4', '9', '1', '8', '4', '2', '7', '5', '1', '3', '9', '2', '8', '2', '1', '2', '9', '1', '1', '2', '3', '4', '3', '1', '2', '2', '10', '2', '2', '8', '1', '1', '2', '4', '9', '2', '11', '9', '8', '9', '2', '1', '10', '4', '1', '1', '10', '7', '2', '10', '8', '1', '10', '8', '8', '8', '8', '8', '8', '2', '2', '2', '8', '3', '4', '1', '1', '1', '2', '1', '8', '8', '2', '8', '3', '8', '3', '1', '2', '3', '1', '1', '10', '1', '4', '4', '3', '3', '1', '9', '1', '1', '2', '1', '1', '6', '1', '1', '1', '9', '1', '4', '1', '1', '1', '1', '1', '1', '2', '12', '8', '8', '1', '9', '12', '10', '8', '2', '12', '1', '1', '2', '1', '7', '10', '7', '1', '2', '1', '10', '8', '2', '2', '8', '2', '2', '1', '9', '8', '8', '8', '8', '3', '1', '4', '2', '8', '3', '10', '10', '1', '9', '1', '9', '1', '9', '1', '8', '3', '1', '1', '8', '8', '8', '6', '3', '4', '1', '1', '1', '9', '9', '5', '2', '10', '2', '6', '2', '2', '7', '1', '8', '2', '11', '1', '1', '1', '2', '1', '9', '4', '4', '2', '8', '4', '6', '8', '8', '10', '10', '2', '1', '2', '9', '1', '8', '8', '3', '4', '8', '8', '2', '8', '8', '1', '2', '1', '2', '9', '4', '1', '3', '3', '3', '8', '8', '8', '2', '6', '4', '3', '3', '12', '3', '4', '9', '6', '8', '7', '7', '8', '6', '4', '1', '10', '6', '3', '3', '8', '1', '1', '8', '1', '11', '8', '1', '1', '2', '3', '11', '7', '2', '1', '1', '1', '9', '1', '2', '1', '1', '9', '8', '1', '1', '8', '0', '4', '1', '1', '2', '1', '1', '2', '2', '9', '1', '10', '1', '3', '8', '9', '3', '3', '1', '10', '1', '9', '1', '1', '7', '4', '1', '8', '10', '1', '2', '10', '1', '1', '2', '3', '3', '9', '1', '8', '10', '1', '7', '9', '1', '9', '6', '3', '9', '2', '8', '4', '12', '12', '3', '1', '1', '9', '1', '9', '2', '1', '1', '1', '1', '1', '9', '2', '1', '2', '10', '9', '8', '8', '10', '8', '2', '3', '2', '1', '12', '8', '12', '2', '9', '1', '3', '2', '5', '2', '2', '8', '2', '2', '4', '10', '9', '10', '9', '4', '2', '4', '12', '8', '2', '1', '8', '3', '9', '2', '9', '4', '1', '1', '1', '1', '1', '9', '6', '2', '9', '2', '1', '3', '2', '2', '5', '8', '9', '1', '6', '3', '1', '2', '2', '2', '6', '1', '8', '10', '8', '3', '3', '13', '2', '9', '9', '10', '6', '12', '8', '2', '9', '2', '3', '8', '8', '1', '2', '2', '3', '2', '10', '1', '8', '9', '1', '8', '9', '8', '8', '2', '10', '2', '1', '1', '1', '9', '9', '12', '1', '1', '4', '8', '3', '2', '2', '4', '4', '2', '2', '1', '1', '1', '9', '2', '2', '4', '1', '1', '8', '8', '8', '9', '2', '13', '9', '9', '4', '1', '9', '7', '10', '10', '8', '1', '9', '1', '1', '1', '8', '12', '4', '1', '2', '1', '1', '1', '7', '8', '3', '3', '1', '2', '2', '7', '2', '2', '10', '1', '8', '4', '0', '1', '9', '4', '1', '1', '2', '10', '10', '7', '9', '3', '12', '11', '2', '8', '10', '8', '9', '4', '3', '4', '10', '1', '1', '9', '3', '10', '1', '5', '3', '3', '2', '2', '1', '1', '6', '1', '7', '1', '1', '12', '1', '1', '1', '10', '1', '8', '8', '1', '3', '8', '3', '1', '2', '8', '3', '4', '2', '8', '8', '6', '1', '1', '4', '1', '2', '9', '5', '1', '7', '8', '9', '2', '4', '9', '8', '5', '9', '9', '9', '1', '2', '1', '3', '8', '9', '2', '9', '8', '9', '1', '8', '13', '9', '3', '8', '1', '4', '1', '3', '1', '6', '4', '10', '2', '2', '7', '8', '8', '12', '8', '9', '9', '9', '10', '3', '12', '12', '2', '9', '10', '9', '7', '2', '3', '4', '10', '10', '2', '7', '7', '1', '7', '3', '9', '1', '9', '8', '2', '9', '1', '8', '1', '10', '8', '8', '2', '7', '8', '1', '8', '9', '8', '1', '4', '1', '8', '8', '8', '1', '1', '10', '3', '2', '2', '6', '8', '9', '8', '2', '1', '1', '3', '8', '8', '1', '4', '4', '3', '9', '10', '4', '2', '4', '2', '9', '2', '2', '9', '1', '1', '10', '1', '1', '2', '1', '1', '1', '9', '1', '8', '8', '8', '2', '3', '8', '1', '6', '7', '3', '1', '9', '1', '2', '1', '6', '1', '2', '10', '1', '4', '2', '3', '2', '9', '9', '4', '9', '9', '1', '2', '8', '4', '7', '8', '7', '13', '9', '1', '2', '1', '9', '3', '3', '6', '8', '9', '10', '9', '2', '2', '3', '12', '1', '1', '1', '13', '2', '1', '7', '10', '8', '9', '1', '2', '9', '1', '1', '1', '2', '1', '12', '5', '6', '9', '10', '3', '1', '10', '1', '3', '2', '8', '9', '2', '10', '8', '8', '9', '2', '8', '9', '2', '2', '2', '8', '8', '2', '3', '1', '4', '8', '1', '9', '9', '2', '8', '10', '9', '1', '2', '12', '8', '8', '0', '2', '1', '1', '9', '10', '1', '6', '2', '2', '10', '2', '2', '4', '2', '2', '12', '1', '6', '7', '8', '9', '1', '3', '5', '7', '3', '1', '3', '1', '1', '3', '2', '6', '3', '2', '1', '8', '3', '5', '2', '1', '1', '8', '1', '1', '0', '2', '3', '5', '3', '1', '1', '9', '8', '8', '8', '8', '4', '7', '2', '9', '12', '5', '9', '3', '12', '3', '9', '1', '9', '1', '7', '3', '9', '3', '10', '1', '2', '1', '4', '9', '8', '2', '9', '11', '2', '8', '9', '4', '1', '1', '1', '1', '1', '1', '1', '9', '1', '9', '1', '1', '1', '12', '3', '10', '3', '6', '1', '7', '10', '8', '3', '7', '1', '1', '2', '6', '1', '3', '9', '2', '8', '1', '8', '8', '1', '8', '10', '12', '1', '10', '12', '10', '1', '8', '8', '1', '4', '7', '9', '2', '1', '9', '3', '8', '12', '10', '6', '1', '6', '8', '3', '1', '3', '1', '10', '10', '9', '8', '1', '2', '9', '8', '8', '1', '9', '8', '8', '3', '1', '3', '9', '12', '1', '8', '1', '5', '8', '11', '5', '5', '1', '2', '1', '3', '1', '1', '1', '9', '8', '8', '1', '9', '10', '8', '4', '1', '1', '8', '10', '8', '9', '1', '10', '1', '7', '2', '1', '1', '9', '6', '2', '9', '8', '1', '1', '1', '9', '8', '8', '1', '9', '9', '1', '1', '2', '7', '9', '2', '9', '9', '8', '7', '6', '8', '7', '8', '2', '9', '2', '1', '8', '8', '8', '2', '6', '3', '1', '8', '12', '12', '8', '7', '1', '2', '2', '7', '1', '9', '8', '1', '4', '1', '1', '8', '5', '8', '1', '8', '8', '8', '8', '1', '2', '11', '1', '1', '1', '2', '7', '1', '9', '2', '2', '2', '9', '8', '9', '2', '9', '1', '8', '1', '7', '11', '13', '1', '9', '8', '2', '2', '1', '2', '3', '10', '3', '10', '4', '3', '9', '1', '8', '1', '1', '13', '1', '1', '3', '8', '1', '2', '2', '2', '9', '4', '1', '8', '1', '9', '3', '6', '8', '5', '2', '8', '2', '8', '1', '9', '2', '2', '8', '1', '1', '4', '8', '9', '2', '2', '7', '1', '8', '12', '2', '12', '8', '2', '9', '9', '3', '8', '1', '1', '9', '9', '9', '2', '8', '1', '9', '1', '8', '9', '8', '2', '2', '4', '1', '9', '10', '10', '8', '8', '1', '1', '2', '1', '4', '1', '2', '1', '1', '3', '12', '8', '8', '8', '8', '10', '2', '8', '12', '1', '9', '9', '8', '9', '1', '2', '1', '8', '8', '8', '2', '8', '8', '3', '10', '8', '9', '1', '1', '1', '3', '4', '8', '10', '6', '3', '1', '3', '2', '8', '9', '3', '5', '4', '1', '5', '2', '9', '9', '9', '1', '1', '1', '9', '9', '6', '10', '9', '8', '12', '2', '9', '1', '2', '1', '1', '1', '8', '1', '12', '1', '12', '1', '9', '1', '8', '1', '2', '1', '2', '8', '8', '10', '1', '9', '1', '2', '2', '8', '8', '3', '12', '1', '11', '2', '10', '12', '11', '2', '1', '10', '1', '1', '9', '3', '10', '2', '7', '1', '8', '1', '9', '9', '2', '1', '3', '12', '5', '9', '9', '8', '1', '8', '8', '8', '1', '1', '3', '9', '8', '9', '8', '3', '10', '3', '7', '8', '9', '2', '3', '1', '9', '10', '9', '2', '8', '2', '6', '4', '1', '1', '3', '8', '2', '3', '9', '1', '2', '1', '8', '1', '2', '2', '2', '2', '9', '9', '4', '8', '9', '1', '1', '2', '9', '1', '5', '2', '9', '8', '10', '1', '2', '10', '8', '5', '6', '4', '8', '1', '1', '1', '8', '9', '2', '8', '8', '8', '12', '2', '2', '1', '8', '1', '2', '1', '9', '8', '8', '9', '1', '1', '8', '7', '1', '8', '0', '2', '3', '8', '1', '1', '8', '1', '1', '12', '3', '2', '10', '8', '10', '6', '1', '8', '8', '10', '8', '8', '1', '4', '8', '1', '2', '9', '8', '1', '1', '3', '3', '5', '1', '1', '2', '1', '2', '1', '9', '8', '3', '9', '8', '2', '10', '9', '1', '8', '1', '10', '9', '10', '1', '2', '3', '8', '1', '3', '1', '9', '9', '8', '1', '9', '8', '8', '9', '1', '9', '2', '8', '8', '2', '13', '8', '10', '8', '9', '3', '2', '1', '3', '1', '12', '10', '10', '1', '11', '1', '2', '4', '2', '10', '1', '8', '11', '1', '1', '1', '2', '2', '8', '2', '9', '8', '9', '9', '12', '8', '1', '9', '2', '4', '1', '8', '3', '9', '1', '6', '5', '8', '9', '3', '3', '2', '1', '1', '1', '8', '11', '1', '8', '7', '1', '11', '9', '1', '7', '9', '9', '8', '8', '8', '12', '8', '1', '2', '9', '8', '8', '2', '10', '9', '8', '8', '4', '10', '9', '8', '2', '10', '10', '2', '1', '2', '1', '2', '1', '4', '1', '7', '1', '1', '3', '12', '9', '8', '5', '9', '10', '1', '9', '9', '2', '7', '4', '1', '1', '8', '12', '8', '2', '2', '9', '4', '1', '1', '5', '9', '1', '8', '12', '8', '1', '1', '4', '9', '10', '2', '3', '1', '3', '3', '3', '8', '1', '2', '1', '2', '1', '2', '9', '1', '2', '8', '9', '4', '8', '8', '10', '2', '12', '5', '5', '1', '8', '1', '7', '8', '1', '1', '8', '9', '12', '4', '9', '3', '1', '2', '5', '9', '2', '1', '2', '2', '1', '2', '8', '1', '8', '8', '2', '2', '1', '8', '4', '1', '2', '10', '9', '10', '8', '1', '10', '4', '8', '8', '10', '2', '8', '1', '1', '3', '10', '1', '4', '2', '8', '3', '3', '2', '9', '8', '5', '5', '9', '3', '10', '8', '1', '2', '1', '2', '9', '1', '1', '2', '9', '9', '8', '8', '11', '9', '4', '1', '8', '2', '7', '1', '2', '8', '2', '9', '9', '8', '2', '9', '9', '8', '8', '1', '2', '8', '1', '1', '2', '9', '8', '1', '4', '9', '1', '8', '12', '1', '12', '2', '8', '2', '9', '1', '12', '1', '2', '9', '1', '1', '9', '3', '2', '9', '9', '2', '11', '8', '7', '9', '9', '9', '8', '8', '1', '2', '8', '1', '1', '4', '1', '2', '10', '1', '9', '2', '1', '1', '1', '9', '1', '13', '4', '3', '2', '1', '1', '2', '2', '7', '8', '7', '1', '1', '2', '8', '8', '3', '4', '8', '2', '10', '13', '8', '8', '9', '2', '2', '1', '1', '10', '4', '2', '7', '9', '8', '2', '1', '9', '8', '1', '1', '9', '8', '9', '8', '2', '8', '2', '2', '9', '2', '1', '9', '1', '2', '4', '8', '2', '1', '1', '8', '1', '4', '1', '9', '3', '9', '2', '7', '1', '9', '6', '1', '10', '2', '2', '2', '2', '2', '10', '10', '10', '9', '9', '9', '3', '12', '10', '1', '1', '10', '1', '1', '2', '2', '1', '2', '3', '12', '8', '8', '1', '2', '9', '9', '1', '1', '10', '8', '9', '8', '3', '1', '1', '3', '1', '10', '1', '1', '1', '4', '1', '1', '1', '7', '10', '2', '1', '8', '1', '9', '3', '1', '10', '8', '8', '1', '5', '9', '1', '2', '1', '8', '8', '1', '3', '1', '10', '2', '1', '1', '2', '8', '2', '5', '3', '5', '2', '1', '1', '9', '1', '7', '9', '9', '2', '8', '1', '12', '4', '1', '4', '9', '9', '4', '8', '1', '10', '10', '8', '8', '8', '9', '2', '3', '5', '1', '8', '1', '1', '1', '7', '10', '1', '4', '12', '3', '2', '2', '9', '1', '1', '1', '2', '8', '4', '1', '7', '3', '12', '8', '6', '2', '2', '7', '9', '11', '1', '10', '9', '1', '8', '12', '2', '11', '1', '2', '3', '1', '1', '1', '8', '9', '2', '2', '2', '2', '3', '3', '8', '10', '4', '1', '2', '9', '8', '2', '2', '9', '1', '8', '11', '4', '2', '8', '7', '1', '1', '7', '3', '9', '9', '4', '5', '8', '10', '2', '9', '9', '9', '8', '1', '9', '2', '2', '2', '1', '2', '9', '4', '3', '12', '4', '3', '2', '3', '8', '3', '10', '1', '10', '1', '1', '8', '10', '6', '8', '9', '9', '1', '2', '9', '2', '9', '3', '8', '1', '1', '12', '1', '5', '1', '10', '8', '10', '2', '7', '1', '1', '1', '1', '3', '2', '3', '5', '9', '1', '2', '1', '9', '8', '10', '9', '8', '1', '8', '1', '8', '2', '8', '1', '2', '8', '1', '2', '5', '8', '12', '2', '1', '1', '1', '1', '1', '8', '8', '2', '1', '4', '2', '8', '8', '8', '2', '9', '9', '11', '1', '2', '9', '3', '1', '1', '8', '9', '2', '4', '10', '1', '9', '8', '4', '9', '8', '10', '8', '8', '9', '2', '8', '9', '3', '3', '3', '4', '3', '2', '2', '10', '8', '2', '1', '5', '1', '9', '1', '1', '2', '1', '2', '3', '1', '1', '8', '8', '1', '10', '8', '8', '1', '6', '4', '8', '2', '5', '8', '1', '3', '8', '2', '8', '9', '1', '1', '8', '12', '10', '1', '5', '8', '2', '8', '10', '9', '9', '1', '6', '9', '1', '10', '8', '2', '9', '10', '1', '2', '1', '1', '8', '8', '3', '9', '8', '1', '10', '2', '8', '1', '10', '1', '5', '1', '1', '1', '5', '1', '1', '1', '4', '1', '1', '1', '8', '2', '8', '1', '8', '3', '8', '9', '1', '8', '1', '8', '1', '1', '2', '2', '1', '12', '9', '9', '1', '8', '2', '1', '2', '8', '1', '9', '1', '1', '2', '1', '2', '2', '2', '1', '9', '9', '8', '8', '1', '1', '9', '10', '8', '4', '3', '3', '4', '9', '1', '3', '3', '10', '2', '2', '1', '8', '4', '1', '9', '4', '1', '1', '9', '8', '10', '4', '9', '9', '8', '2', '1', '3', '4', '9', '3', '8', '9', '1', '1', '2', '1', '7', '8', '2', '9', '1', '9', '2', '6', '9', '10', '8', '1', '1', '5', '9', '5', '1', '9', '10', '9', '3', '3', '8', '8', '2', '9', '2', '8', '2', '1', '8', '5', '8', '3', '10', '1', '1', '2', '1', '8', '1', '4', '3', '2', '9', '4', '9', '8', '8', '8', '1', '3', '9', '1', '9', '9', '1', '2', '1', '9', '8', '9', '8', '1', '1', '1', '4', '2', '2', '9', '9', '1', '1', '1', '3', '6', '1', '2', '3', '1', '4', '2', '9', '1', '9', '9', '1', '8', '2', '1', '9', '1', '8', '1', '8', '8', '2', '1', '9', '9', '6', '8', '8', '9', '8', '10', '8', '9', '5', '1', '1', '1', '1', '8', '1', '8', '12', '8', '1', '12', '9', '2', '8', '1', '1', '2', '1', '8', '6', '9', '9', '8', '1', '8', '9', '7', '7', '8', '9', '9', '1', '8', '3', '8', '1', '3', '3', '8', '9', '8', '2', '8', '1', '2', '9', '8', '1', '1', '1', '1', '9', '1', '9', '8', '12', '3', '8', '8', '1', '10', '10', '10', '2', '2', '1', '10', '2', '2', '10', '8', '12', '8', '1', '1', '1', '2', '1', '1', '3', '8', '1', '2', '2', '2', '1', '1', '6', '2', '8', '8', '8', '1', '1', '1', '9', '9', '2', '8', '10', '2', '8', '2', '1', '2', '9', '9', '2', '1', '1', '8', '8', '1', '3', '1', '8', '9', '1', '10', '9', '1', '1', '1', '1', '7', '2', '2', '1', '2', '8', '8', '1', '2', '1', '3', '2', '1', '8', '3', '9', '2', '10', '2', '10', '1', '2', '1', '1', '13', '9', '10', '8', '2', '2', '1', '1', '8', '2', '2', '1', '9', '1', '1', '8', '9', '9', '2', '9', '2', '2', '1', '1', '1', '8', '9', '4', '9', '9', '1', '8', '2', '1', '1', '8', '2', '2', '2', '8', '1', '1', '8', '2', '10', '2', '9', '9', '7', '1', '1', '2', '2', '8', '3', '1', '2', '1', '1', '8', '1', '1', '1', '9', '1', '1', '8', '8', '1', '9', '6', '1', '9', '1', '8', '1', '8', '8', '8', '1', '3', '8', '8', '2', '6', '3', '2', '1', '10', '1', '8', '2', '1', '6', '1', '1', '1', '8', '1', '10', '1', '1', '8', '9', '2', '6', '9', '2', '9', '4', '1', '8', '1', '3', '7', '8', '8', '5', '8', '7', '1', '1', '1', '13', '8', '3', '1', '1', '1', '1', '2', '8', '2', '12', '8', '1', '2', '5', '1', '8', '2', '10', '8', '2', '10', '1', '5', '2', '1', '1', '5', '2', '2', '7', '8', '2', '9', '9', '1', '10', '1', '8', '2', '5', '8', '1', '5', '1', '10', '1', '1', '9', '8', '6', '1', '8', '9', '8', '1', '8', '2', '2', '8', '3', '9', '1', '9', '3', '8', '4', '4', '3', '3', '1', '1', '1', '8', '9', '1', '1', '8', '3', '1', '2', '8', '2', '1', '1', '10', '1', '2', '2', '1', '1', '8', '2', '8', '2', '10', '2', '2', '10', '8', '8', '8', '13', '2', '2', '9', '2', '5', '9', '2', '1', '8', '8', '8', '1', '12', '12', '2', '9', '2', '1', '8', '2', '1', '8', '1', '9', '9', '2', '7', '1', '1', '7', '1', '4', '3', '1', '10', '3', '10', '9', '4', '6', '7', '5', '2', '1', '8', '8', '1', '4', '9', '2', '1', '8', '1', '1', '1', '1', '1', '8', '9', '8', '8', '8', '8', '9', '8', '10', '1', '8', '2', '7', '8', '9', '1', '2', '5', '9', '10', '8', '9', '12', '1', '6', '1', '2', '10', '9', '4', '1', '8', '10', '10', '1', '1', '1', '1', '1', '2', '3', '8', '1', '2', '10', '2', '1', '2', '4', '2', '2', '9', '1', '4', '2', '1', '1', '12', '12', '9', '9', '7', '1', '8', '4', '9', '9', '8', '1', '7', '3', '2', '1', '1', '4', '8', '1', '5', '8', '2', '8', '1', '14', '9', '12', '1', '10', '2', '1', '1', '8', '8', '6', '6', '3', '1', '2', '8', '1', '3', '2', '1', '8', '8', '1', '10', '8', '2', '8', '2', '5', '9', '1', '8', '8', '1', '1', '8', '8', '13', '1', '3', '8', '4', '1', '9', '7', '8', '8', '8', '3', '8', '1', '4', '2', '8', '8', '8', '7', '1', '1', '2', '8', '8', '9', '8', '8', '2', '3', '9', '9', '2', '1', '2', '2', '1', '8', '2', '6', '2', '1', '1', '9', '10', '3', '8', '2', '3', '2', '8', '8', '1', '8', '8', '8', '9', '1', '4', '1', '9', '2', '1', '8', '0', '1', '8', '1', '1', '8', '9', '9', '1', '3', '1', '2', '8', '8', '7', '1', '1', '2', '9', '2', '10', '2', '8', '2', '2', '2', '7', '1', '9', '8', '1', '3', '9', '2', '1', '10', '8', '1', '4', '1', '8', '5', '9', '8', '8', '1', '2', '2', '1', '8', '8', '6', '8', '1', '1', '8', '3', '2', '2', '1', '8', '1', '7', '6', '8', '1', '1', '1', '9', '1', '1', '1', '2', '1', '11', '10', '10', '11', '9', '']\n",
            "['8', '1', '8', '2', '8', '8', '8', '9', '7', '8', '1', '2', '1', '1', '8', '4', '8', '8', '12', '3', '3', '7', '3', '12', '1', '8', '8', '8', '8', '8', '8', '8', '1', '9', '5', '9', '9', '9', '11', '8', '8', '8', '4', '8', '8', '8', '8', '1', '3', '9', '3', '7', '1', '2', '9', '9', '7', '8', '8', '1', '10', '7', '8', '8', '9', '8', '7', '9', '9', '12', '7', '2', '8', '1', '11', '11', '1', '7', '7', '12', '1', '9', '8', '10', '12', '7', '8', '2', '8', '9', '9', '1', '8', '9', '1', '7', '12', '10', '10', '10', '8', '3', '7', '9', '8', '9', '1', '8', '8', '2', '7', '2', '9', '9', '11', '8', '8', '12', '12', '7', '8', '12', '4', '9', '3', '1', '12', '1', '1', '8', '8', '3', '8', '8', '8', '8', '9', '1', '8', '8', '10', '1', '8', '2', '8', '8', '7', '3', '8', '2', '4', '4', '9', '8', '10', '12', '12', '1', '1', '9', '1', '1', '1', '8', '2', '2', '8', '1', '1', '2', '2', '1', '2', '8', '1', '9', '9', '8', '8', '4', '2', '9', '9', '8', '3', '4', '3', '1', '8', '8', '2', '1', '9', '7', '8', '8', '1', '12', '3', '8', '2', '4', '2', '9', '12', '1', '4', '1', '8', '8', '8', '2', '2', '8', '9', '8', '8', '8', '10', '9', '8', '7', '9', '1', '1', '9', '4', '2', '4', '2', '2', '1', '7', '8', '11', '11', '3', '9', '2', '4', '8', '9', '1', '8', '1', '1', '4', '9', '1', '1', '8', '8', '2', '1', '8', '4', '2', '8', '9', '8', '8', '2', '8', '8', '8', '7', '1', '1', '1', '2', '1', '1', '8', '7', '8', '8', '12', '2', '12', '12', '8', '10', '12', '8', '3', '3', '12', '10', '1', '8', '12', '1', '8', '8', '2', '8', '4', '7', '8', '7', '10', '8', '10', '9', '8', '12', '12', '1', '8', '8', '3', '8', '8', '8', '8', '8', '1', '9', '8', '11', '1', '1', '1', '9', '8', '1', '9', '2', '3', '11', '8', '9', '9', '9', '2', '1', '8', '8', '9', '7', '1', '4', '9', '4', '8', '8', '4', '8', '12', '9', '4', '8', '2', '10', '10', '10', '8', '9', '9', '8', '8', '12', '7', '1', '8', '8', '8', '4', '1', '1', '1', '1', '1', '1', '8', '1', '9', '8', '9', '9', '4', '8', '12', '9', '8', '8', '2', '8', '8', '8', '6', '9', '8', '3', '7', '8', '8', '4', '12', '8', '8', '9', '12', '12', '9', '8', '2', '9', '2', '3', '1', '12', '8', '10', '9', '9', '9', '10', '10', '3', '8', '12', '1', '4', '2', '1', '10', '8', '2', '8', '4', '8', '9', '1', '9', '9', '10', '10', '1', '4', '9', '2', '4', '9', '1', '1', '3', '10', '3', '3', '8', '7', '3', '8', '9', '9', '12', '4', '8', '12', '2', '2', '4', '1', '9', '9', '4', '1', '4', '2', '8', '12', '2', '3', '10', '10', '9', '8', '9', '9', '1', '12', '8', '8', '8', '12', '4', '1', '8', '8', '1', '9', '8', '8', '2', '1', '8', '9', '8', '3', '3', '3', '1', '8', '8', '9', '1', '10', '9', '9', '9', '9', '5', '9', '9', '8', '8', '8', '8', '8', '8', '8', '8', '11', '12', '8', '8', '1', '8', '9', '11', '2', '2', '2', '2', '3', '1', '2', '2', '8', '2', '4', '9', '1', '2', '9', '8', '2', '8', '9', '9', '3', '10', '9', '9', '2', '8', '9', '8', '12', '12', '1', '3', '8', '8', '8', '2', '7', '7', '7', '7', '3', '9', '1', '9', '8', '9', '9', '1', '1', '1', '2', '9', '9', '9', '11', '1', '8', '8', '9', '1', '9', '8', '8', '8', '1', '1', '8', '7', '1', '1', '8', '8', '9', '4', '4', '8', '2', '2', '8', '8', '8', '8', '8', '8', '11', '8', '2', '9', '4', '9', '3', '9', '9', '1', '3', '9', '3', '1', '12', '8', '9', '12', '1', '8', '4', '2', '1', '4', '8', '3', '3', '8', '2', '8', '9', '7', '8', '8', '8', '5', '8', '3', '9', '8', '8', '13', '12', '1', '1', '2', '8', '4', '1', '9', '9', '12', '8', '9', '12', '9', '1', '9', '9', '9', '9', '3', '2', '9', '9', '4', '8', '12', '2', '4', '9', '3', '1', '9', '7', '8', '9', '9', '8', '4', '8', '8', '7', '9', '10', '3', '8', '8', '8', '1', '1', '1', '1', '8', '8', '4', '1', '10', '1', '5', '7', '7', '1', '8', '9', '3', '7', '2', '7', '7', '2', '4', '8', '12', '7', '4', '2', '9', '9', '12', '6', '10', '8', '2', '4', '12', '9', '9', '3', '8', '8', '1', '2', '10', '9', '9', '8', '4', '12', '2', '1', '8', '8', '8', '12', '10', '10', '9', '3', '8', '8', '9', '2', '8', '10', '1', '1', '1', '1', '2', '1', '1', '1', '1', '9', '8', '12', '9', '4', '8', '8', '9', '1', '9', '3', '9', '8', '8', '1', '7', '7', '10', '1', '8', '8', '1', '9', '8', '10', '3', '1', '7', '1', '8', '8', '12', '8', '8', '1', '8', '7', '1', '7', '7', '8', '2', '1', '8', '8', '2', '10', '8', '8', '8', '8', '8', '10', '1', '8', '8', '12', '8', '3', '3', '2', '2', '2', '10', '8', '8', '8', '2', '9', '1', '8', '9', '3', '2', '8', '10', '8', '6', '1', '1', '8', '4', '1', '9', '10', '8', '1', '7', '1', '2', '8', '1', '1', '1', '12', '1', '9', '12', '8', '12', '12', '12', '8', '8', '12', '4', '8', '8', '8', '8', '9', '9', '1', '3', '3', '3', '3', '1', '12', '12', '9', '10', '8', '8', '1', '9', '2', '2', '13', '9', '8', '9', '2', '1', '9', '1', '8', '8', '8', '4', '8', '1', '1', '1', '12', '12', '7', '2', '2', '2', '8', '3', '8', '9', '2', '10', '7', '8', '9', '2', '1', '2', '12', '12', '8', '8', '9', '2', '2', '9', '11', '1', '8', '1', '10', '9', '2', '1', '4', '7', '7', '7', '7', '7', '12', '8', '8', '8', '1', '1', '10', '1', '12', '1', '8', '2', '1', '1', '12', '8', '7', '9', '12', '8', '9', '3', '9', '8', '8', '8', '8', '3', '11', '2', '2', '9', '8', '8', '10', '8', '2', '7', '3', '1', '4', '7', '8', '8', '1', '8', '3', '7', '12', '8', '10', '9', '9', '8', '8', '2', '8', '9', '9', '1', '2', '8', '8', '9', '8', '3', '8', '1', '8', '10', '9', '8', '9', '9', '12', '4', '4', '8', '9', '9', '8', '2', '10', '1', '2', '8', '9', '1', '9', '9', '9', '7', '12', '12', '8', '1', '1', '1', '1', '8', '3', '1', '1', '8', '1', '8', '8', '7', '8', '8', '8', '8', '3', '2', '2', '10', '10', '10', '7', '8', '1', '2', '12', '7', '9', '8', '7', '8', '12', '2', '8', '9', '2', '6', '6', '7', '9', '8', '1', '8', '9', '8', '1', '12', '1', '2', '8', '7', '7', '7', '8', '2', '2', '8', '1', '2', '2', '9', '9', '1', '8', '8', '4', '3', '3', '1', '6', '3', '3', '12', '3', '8', '9', '1', '4', '3', '1', '8', '3', '9', '2', '8', '2', '8', '8', '8', '1', '1', '1', '9', '9', '8', '1', '9', '8', '1', '1', '3', '10', '8', '1', '1', '3', '9', '1', '4', '4', '1', '8', '9', '9', '2', '0', '0', '1', '8', '3', '1', '8', '8', '9', '8', '8', '1', '1', '8', '9', '8', '8', '8', '7', '9', '8', '8', '8', '10', '9', '8', '1', '2', '6', '1', '9', '9', '8', '12', '12', '12', '8', '8', '2', '8', '1', '2', '2', '2', '1', '9', '8', '2', '12', '2', '8', '12', '8', '9', '8', '8', '9', '7', '1', '1', '1', '1', '1', '8', '8', '1', '8', '8', '1', '1', '3', '2', '8', '8', '9', '10', '10', '2', '2', '1', '9', '2', '9', '9', '4', '12', '12', '12', '10', '7', '3', '3', '4', '2', '2', '9', '2', '8', '4', '2', '4', '1', '10', '9', '7', '8', '7', '1', '1', '3', '3', '1', '1', '3', '3', '3', '1', '1', '1', '1', '8', '2', '3', '1', '1', '2', '8', '8', '12', '8', '8', '8', '8', '11', '9', '1', '8', '9', '2', '8', '8', '8', '3', '9', '1', '9', '2', '7', '2', '8', '2', '8', '10', '8', '1', '10', '1', '1', '9', '9', '8', '8', '1', '8', '8', '8', '12', '8', '8', '8', '1', '8', '8', '8', '1', '9', '1', '1', '8', '1', '8', '9', '8', '2', '12', '9', '9', '0', '1', '8', '8', '1', '8', '12', '8', '8', '10', '8', '8', '8', '7', '8', '1', '8', '7', '3', '10', '1', '8', '9', '1', '8', '8', '8', '10', '1', '10', '3', '9', '1', '8', '9', '2', '8', '3', '3', '9', '9', '7', '9', '1', '1', '9', '2', '1', '1', '1', '7', '1', '1', '8', '8', '1', '1', '8', '1', '8', '3', '12', '9', '3', '3', '8', '8', '8', '8', '3', '1', '3', '3', '1', '11', '0', '8', '8', '7', '8', '12', '1', '8', '9', '8', '9', '8', '8', '3', '8', '8', '1', '1', '1', '9', '2', '2', '2', '8', '7', '12', '8', '8', '9', '10', '10', '7', '8', '1', '9', '8', '7', '3', '1', '3', '8', '2', '2', '3', '9', '8', '4', '4', '8', '9', '2', '1', '1', '7', '8', '9', '9', '7', '8', '7', '7', '8', '2', '2', '8', '4', '9', '7', '10', '0', '9', '8', '3', '7', '8', '1', '1', '8', '9', '9', '2', '2', '10', '1', '9', '10', '10', '10', '8', '3', '2', '12', '9', '9', '10', '12', '9', '12', '12', '9', '1', '2', '4', '12', '12', '7', '8', '9', '7', '7', '7', '3', '9', '8', '9', '1', '12', '8', '9', '4', '1', '3', '12', '12', '12', '12', '8', '8', '2', '1', '1', '2', '1', '1', '1', '12', '12', '8', '12', '2', '2', '12', '3', '3', '12', '8', '2', '8', '8', '12', '2', '1', '10', '3', '2', '8', '7', '1', '8', '1', '3', '7', '8', '9', '8', '3', '1', '1', '7', '8', '8', '9', '8', '2', '9', '2', '2', '9', '8', '1', '8', '8', '1', '3', '3', '1', '1', '10', '1', '2', '8', '1', '1', '1', '1', '9', '1', '4', '1', '7', '7', '7', '7', '2', '2', '8', '8', '12', '1', '9', '1', '7', '3', '3', '1', '8', '8', '10', '8', '9', '2', '9', '1', '3', '8', '8', '3', '12', '2', '8', '12', '2', '9', '1', '3', '3', '3', '3', '2', '8', '7', '9', '8', '3', '3', '1', '7', '8', '3', '1', '1', '12', '8', '9', '1', '2', '3', '8', '1', '1', '3', '3', '9', '1', '1', '1', '12', '1', '7', '3', '3', '1', '8', '8', '8', '1', '2', '4', '8', '1', '10', '2', '5', '3', '3', '12', '10', '9', '9', '12', '9', '0', '2', '8', '8', '9', '9', '9', '8', '1', '3', '1', '1', '4', '8', '1', '10', '8', '7', '2', '8', '2', '8', '4', '7', '8', '1', '9', '1', '9', '8', '2', '8', '2', '7', '9', '2', '2', '9', '1', '8', '12', '1', '8', '1', '4', '1', '9', '9', '1', '10', '12', '4', '8', '1', '7', '3', '9', '2', '12', '7', '8', '8', '2', '1', '12', '9', '8', '1', '2', '2', '8', '10', '2', '1', '7', '7', '7', '12', '3', '3', '8', '3', '8', '8', '3', '9', '8', '9', '1', '1', '8', '7', '9', '3', '3', '8', '1', '0', '9', '9', '9', '1', '8', '9', '9', '10', '1', '8', '7', '8', '8', '8', '9', '9', '8', '9', '10', '4', '9', '3', '7', '12', '1', '9', '9', '8', '9', '1', '9', '3', '1', '8', '4', '12', '10', '9', '8', '7', '10', '8', '12', '12', '3', '10', '8', '12', '2', '1', '2', '3', '9', '8', '8', '7', '1', '1', '2', '2', '1', '2', '2', '7', '1', '3', '9', '9', '3', '8', '8', '8', '8', '8', '7', '8', '8', '10', '8', '1', '8', '2', '8', '2', '2', '2', '2', '2', '1', '7', '12', '10', '1', '2', '8', '1', '4', '7', '8', '1', '12', '8', '7', '9', '2', '2', '2', '8', '1', '8', '1', '1', '0', '2', '2', '6', '1', '8', '2', '1', '8', '1', '1', '2', '2', '3', '2', '2', '10', '10', '9', '1', '7', '7', '8', '8', '1', '12', '10', '12', '3', '8', '8', '8', '3', '8', '3', '10', '2', '2', '2', '1', '2', '2', '1', '0', '1', '8', '9', '1', '1', '1', '2', '2', '2', '2', '2', '1', '1', '1', '1', '12', '9', '9', '1', '10', '11', '12', '9', '1', '1', '8', '8', '1', '2', '12', '8', '3', '7', '2', '10', '9', '7', '8', '1', '2', '2', '7', '9', '12', '2', '2', '1', '8', '9', '2', '3', '2', '2', '2', '9', '9', '8', '9', '2', '3', '8', '8', '9', '7', '3', '10', '8', '8', '9', '1', '1', '2', '8', '1', '10', '8', '8', '9', '7', '1', '7', '12', '8', '1', '7', '2', '1', '3', '9', '2', '8', '8', '1', '2', '3', '9', '3', '8', '9', '7', '7', '1', '10', '2', '8', '8', '8', '1', '8', '2', '8', '8', '1', '2', '8', '10', '2', '1', '0', '8', '3', '8', '10', '10', '12', '3', '8', '9', '3', '10', '8', '8', '8', '1', '8', '8', '2', '2', '2', '2', '2', '2', '1', '1', '10', '1', '10', '8', '2', '1', '2', '2', '8', '3', '3', '2', '2', '1', '8', '1', '3', '1', '9', '2', '2', '1', '3', '2', '2', '3', '2', '2', '3', '2', '1', '1', '2', '1', '2', '7', '8', '2', '1', '12', '12', '3', '1', '9', '9', '7', '8', '7', '2', '7', '9', '2', '2', '2', '8', '9', '7', '9', '2', '8', '9', '9', '2', '9', '1', '1', '9', '8', '3', '3', '8', '2', '7', '8', '11', '2', '8', '9', '1', '8', '3', '1', '2', '1', '2', '2', '2', '2', '8', '3', '9', '2', '10', '9', '9', '2', '7', '7', '7', '1', '3', '8', '8', '1', '1', '8', '7', '1', '8', '8', '9', '3', '3', '3', '2', '4', '8', '12', '8', '1', '12', '9', '2', '8', '2', '12', '2', '1', '3', '8', '12', '12', '11', '8', '10', '9', '2', '9', '4', '10', '1', '12', '12', '9', '3', '8', '9', '12', '9', '9', '2', '2', '2', '3', '5', '1', '1', '1', '7', '7', '8', '2', '2', '8', '2', '1', '7', '3', '2', '9', '8', '9', '1', '1', '9', '8', '9', '3', '3', '2', '9', '4', '9', '8', '8', '1', '8', '10', '2', '0', '3', '8', '8', '8', '9', '12', '9', '8', '2', '2', '8', '10', '9', '8', '2', '2', '4', '1', '9', '9', '8', '8', '10', '3', '1', '1', '2', '8', '7', '8', '2', '1', '1', '8', '12', '2', '9', '4', '8', '3', '3', '3', '8', '2', '12', '12', '8', '9', '12', '8', '2', '12', '7', '9', '2', '2', '1', '3', '1', '3', '11', '9', '8', '1', '10', '2', '12', '3', '8', '4', '1', '2', '8', '8', '3', '9', '3', '8', '7', '1', '1', '3', '1', '8', '2', '9', '8', '12', '1', '1', '1', '9', '9', '1', '8', '8', '8', '2', '2', '12', '3', '1', '1', '1', '9', '9', '3', '2', '1', '1', '9', '9', '1', '9', '9', '1', '9', '3', '9', '1', '8', '1', '9', '8', '3', '8', '8', '9', '9', '8', '2', '8', '1', '2', '2', '10', '1', '2', '1', '1', '9', '1', '9', '3', '7', '7', '2', '10', '3', '3', '1', '1', '9', '1', '2', '1', '3', '2', '1', '1', '1', '1', '1', '1', '1', '1', '1', '2', '8', '8', '7', '1', '1', '8', '8', '2', '1', '12', '1', '8', '1', '9', '8', '8', '2', '1', '9', '9', '2', '2', '8', '1', '7', '7', '9', '8', '1', '6', '8', '8', '2', '2', '8', '3', '8', '3', '2', '3', '8', '1', '2', '2', '2', '8', '9', '9', '9', '12', '1', '9', '2', '1', '9', '2', '9', '9', '10', '8', '1', '1', '9', '2', '1', '7', '1', '3', '7', '1', '1', '1', '3', '8', '8', '8', '9', '9', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '1', '0', '1', '1', '1', '2', '2', '3', '1', '1', '2', '3', '3', '3', '7', '3', '1', '9', '3', '9', '1', '4', '1', '1', '9', '10', '9', '8', '2', '3', '10', '9', '3', '10', '12', '8', '9', '8', '2', '1', '9', '8', '1', '7', '8', '1', '9', '1', '2', '10', '7', '7', '8', '1', '3', '3', '9', '3', '3', '1', '1', '1', '9', '1', '3', '8', '8', '8', '1', '8', '1', '9', '7', '8', '8', '2', '1', '3', '8', '2', '1', '6', '1', '8', '8', '2', '8', '2', '2', '9', '8', '1', '1', '6', '8', '6', '10', '2', '1', '1', '8', '9', '3', '3', '3', '1', '3', '9', '6', '8', '1', '8', '3', '1', '2', '2', '12', '7', '1', '1', '1', '1', '12', '1', '8', '8', '3', '2', '1', '9', '2', '3', '3', '2', '2', '7', '2', '2', '2', '3', '1', '9', '8', '7', '1', '1', '3', '1', '9', '4', '3', '9', '2', '1', '1', '9', '8', '8', '7', '2', '1', '3', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '2', '10', '1', '1', '8', '2', '8', '1', '8', '1', '3', '1', '2', '1', '1', '3', '1', '2', '1', '1', '1', '1', '1', '1', '1', '2', '1', '8', '1', '9', '8', '8', '1', '10', '8', '8', '1', '3', '9', '8', '9', '8', '2', '1', '3', '4', '1', '12', '1', '9', '8', '2', '7', '2', '1', '2', '9', '2', '1', '3', '8', '1', '2', '3', '8', '2', '10', '10', '10', '9', '9', '9', '3', '8', '3', '1', '1', '1', '1', '1', '1', '2', '1', '9', '2', '7', '9', '8', '7', '9', '1', '1', '1', '1', '8', '2', '2', '3', '3', '2', '12', '8', '3', '1', '1', '12', '9', '8', '2', '2', '9', '1', '2', '2', '9', '1', '1', '2', '4', '8', '1', '8', '10', '2', '9', '9', '9', '1', '2', '1', '1', '2', '2', '2', '12', '1', '2', '8', '3', '9', '9', '3', '2', '9', '9', '7', '2', '8', '8', '9', '2', '9', '1', '1', '1', '1', '1', '2', '8', '2', '1', '2', '2', '8', '3', '1', '1', '9', '1', '3', '10', '7', '9', '8', '8', '2', '2', '9', '2', '2', '2', '2', '2', '2', '9', '1', '3', '6', '4', '2', '2', '9', '1', '9', '8', '1', '1', '1', '7', '3', '2', '9', '9', '1', '9', '7', '2', '8', '9', '9', '12', '10', '8', '10', '2', '3', '1', '2', '2', '12', '12', '3', '12', '1', '2', '1', '2', '1', '2', '8', '2', '3', '12', '12', '8', '2', '2', '9', '2', '9', '2', '3', '3', '1', '1', '1', '12', '3', '7', '1', '3', '1', '2', '9', '2', '7', '2', '1', '8', '3', '7', '8', '1', '1', '3', '8', '3', '2', '9', '3', '1', '1', '1', '1', '11', '1', '2', '6', '1', '2', '3', '8', '9', '9', '2', '9', '2', '1', '1', '3', '1', '12', '9', '1', '1', '9', '1', '9', '9', '8', '9', '8', '1', '2', '7', '1', '0', '9', '8', '2', '3', '2', '4', '1', '1', '1', '8', '1', '12', '1', '2', '3', '3', '9', '9', '9', '9', '9', '7', '9', '3', '9', '1', '7', '3', '3', '3', '3', '7', '8', '2', '2', '2', '8', '2', '3', '9', '9', '3', '1', '8', '9', '12', '8', '8', '3', '2', '1', '6', '1', '9', '1', '1', '1', '1', '1', '1', '2', '2', '1', '2', '2', '2', '2', '9', '2', '5', '3', '2', '2', '8', '2', '1', '8', '3', '3', '8', '3', '1', '3', '9', '2', '8', '9', '4', '4', '9', '8', '8', '7', '7', '3', '1', '8', '8', '3', '2', '2', '10', '3', '3', '2', '2', '2', '2', '12', '3', '1', '2', '7', '7', '12', '2', '1', '2', '1', '1', '2', '1', '3', '3', '3', '1', '3', '9', '4', '8', '8', '4', '2', '4', '2', '8', '9', '1', '1', '2', '2', '2', '8', '7', '2', '8', '2', '9', '1', '8', '1', '1', '2', '1', '1', '1', '2', '9', '9', '8', '1', '8', '1', '4', '8', '1', '2', '7', '9', '2', '1', '8', '9', '8', '9', '1', '1', '2', '3', '2', '12', '7', '1', '2', '4', '4', '4', '2', '2', '2', '9', '8', '8', '1', '12', '3', '3', '2', '2', '3', '12', '1', '5', '1', '1', '3', '2', '8', '8', '2', '2', '2', '3', '9', '8', '8', '9', '8', '8', '11', '8', '2', '9', '2', '3', '1', '7', '1', '8', '2', '8', '1', '1', '1', '8', '1', '1', '1', '7', '8', '8', '2', '3', '2', '1', '8', '8', '1', '8', '4', '8', '9', '8', '9', '8', '1', '2', '3', '2', '1', '2', '2', '3', '1', '9', '4', '8', '1', '1', '9', '1', '1', '8', '2', '2', '2', '1', '7', '3', '3', '1', '3', '3', '12', '3', '1', '1', '3', '1', '1', '4', '8', '4', '4', '3', '3', '3', '4', '8', '1', '9', '9', '9', '4', '1', '7', '4', '8', '4', '9', '8', '1', '1', '3', '8', '9', '1', '9', '7', '1', '1', '9', '1', '8', '8', '8', '1', '1', '8', '2', '2', '2', '9', '4', '4', '1', '2', '1', '2', '9', '1', '1', '8', '5', '5', '5', '9', '12', '4', '1', '2', '1', '8', '8', '1', '9', '9', '2', '12', '1', '1', '7', '8', '8', '4', '1', '2', '8', '11', '2', '3', '2', '2', '1', '11', '2', '2', '2', '2', '9', '9', '2', '2', '9', '3', '1', '9', '1', '1', '1', '7', '8', '1', '8', '9', '1', '2', '9', '8', '1', '2', '9', '8', '12', '2', '2', '10', '8', '2', '2', '8', '8', '1', '2', '6', '1', '1', '7', '7', '2', '3', '2', '1', '1', '8', '1', '10', '8', '12', '6', '4', '1', '4', '1', '2', '9', '8', '9', '4', '8', '8', '10', '8', '8', '8', '8', '8', '2', '2', '8', '4', '9', '3', '3', '3', '3', '3', '9', '2', '1', '1', '2', '3', '2', '1', '2', '3', '3', '3', '2', '2', '3', '3', '2', '1', '2', '3', '3', '3', '3', '12', '9', '3', '2', '2', '3', '1', '5', '1', '1', '3', '2', '2', '1', '2', '9', '3', '9', '8', '1', '8', '7', '7', '9', '1', '1', '7', '9', '10', '1', '7', '1', '4', '3', '8', '9', '4', '9', '9', '2', '9', '2', '8', '4', '2', '2', '9', '5', '2', '1', '10', '9', '3', '1', '1', '1', '2', '1', '2', '9', '1', '7', '9', '9', '8', '2', '2', '8', '9', '1', '9', '8', '2', '3', '2', '2', '1', '2', '2', '2', '1', '1', '8', '1', '9', '4', '12', '8', '2', '7', '2', '2', '1', '9', '3', '1', '8', '12', '1', '1', '2', '1', '10', '2', '6', '12', '12', '4', '9', '1', '9', '10', '1', '2', '8', '2', '1', '8', '6', '12', '9', '9', '2', '1', '7', '8', '9', '5', '8', '1', '8', '8', '3', '1', '8', '2', '8', '2', '2', '1', '2', '9', '2', '8', '1', '12', '8', '2', '7', '3', '3', '12', '2', '3', '3', '9', '9', '3', '3', '3', '3', '3', '2', '1', '4', '1', '4', '8', '8', '2', '1', '2', '9', '12', '1', '10', '12', '1', '2', '9', '4', '8', '10', '3', '2', '8', '7', '4', '8', '9', '2', '7', '1', '10', '2', '2', '2', '8', '4', '2', '2', '1', '1', '8', '7', '9', '2', '1', '9', '8', '9', '12', '4', '8', '2', '8', '7', '7', '1', '9', '2', '1', '1', '9', '1', '8', '2', '3', '9', '9', '1', '3', '2', '9', '2', '2', '8', '1', '1', '2', '1', '2', '1', '9', '8', '2', '2', '4', '8', '8', '9', '5', '5', '9', '8', '9', '6', '2', '1', '12', '2', '3', '8', '8', '9', '2', '2', '1', '3', '2', '10', '9', '10', '7', '2', '8', '4', '2', '8', '9', '8', '1', '3', '8', '8', '1', '8', '3', '8', '8', '1', '10', '3', '1', '5', '8', '8', '9', '2', '2', '9', '2', '1', '9', '1', '4', '1', '12', '8', '8', '2', '1', '1', '1', '1', '1', '9', '2', '9', '8', '9', '5', '9', '1', '8', '2', '4', '1', '1', '1', '9', '12', '4', '1', '1', '9', '8', '4', '1', '8', '9', '8', '8', '9', '9', '9', '9', '9', '9', '1', '3', '4', '2', '2', '8', '2', '9', '2', '3', '8', '3', '1', '7', '2', '1', '1', '4', '2', '9', '2', '1', '3', '2', '9', '2', '9', '3', '8', '1', '9', '1', '1', '2', '8', '8', '8', '2', '3', '9', '9', '8', '2', '4', '1', '5', '1', '1', '9', '9', '2', '1', '1', '1', '9', '9', '1', '3', '9', '2', '2', '9', '9', '8', '3', '9', '9', '8', '2', '2', '8', '9', '2', '2', '8', '9', '10', '10', '2', '8', '7', '9', '9', '4', '2', '4', '11', '2', '1', '7', '8', '8', '8', '9', '4', '9', '8', '8', '2', '1', '1', '9', '8', '3', '3', '8', '8', '7', '9', '1', '3', '4', '8', '10', '2', '4', '4', '8', '2', '8', '2', '8', '9', '2', '1', '2', '2', '3', '1', '9', '4', '5', '9', '9', '1', '1', '1', '1', '1', '1', '7', '1', '1', '1', '8', '1', '1', '1', '1', '1', '9', '1', '2', '1', '12', '9', '3', '9', '9', '4', '1', '9', '1', '1', '2', '11', '9', '3', '2', '2', '2', '4', '2', '3', '2', '7', '7', '8', '1', '10', '9', '5', '7', '1', '10', '8', '1', '2', '7', '1', '12', '5', '8', '4', '2', '9', '9', '8', '2', '9', '8', '8', '10', '2', '3', '2', '7', '2', '8', '10', '3', '2', '9', '9', '9', '1', '1', '1', '1', '8', '2', '10', '9', '8', '1', '2', '2', '2', '1', '2', '1', '3', '9', '12', '2', '2', '4', '8', '1', '3', '9', '4', '9', '1', '9', '9', '2', '1', '1', '9', '7', '10', '3', '10', '2', '9', '2', '9', '2', '8', '2', '1', '2', '3', '1', '1', '1', '8', '5', '8', '3', '1', '1', '4', '2', '2', '1', '3', '8', '2', '1', '1', '1', '2', '9', '4', '1', '8', '1', '2', '8', '2', '9', '2', '2', '5', '5', '2', '5', '2', '1', '8', '8', '1', '3', '12', '2', '4', '3', '2', '2', '2', '6', '2', '3', '3', '1', '8', '2', '2', '1', '1', '8', '8', '10', '1', '9', '2', '9', '12', '0', '2', '1', '3', '2', '2', '1', '2', '1', '9', '9', '2', '9', '8', '8', '1', '7', '8', '1', '2', '6', '9', '8', '10', '1', '12', '2', '2', '1', '8', '2', '1', '12', '7', '4', '2', '10', '2', '2', '1', '2', '1', '2', '1', '1', '2', '1', '9', '8', '10', '2', '10', '9', '9', '12', '5', '3', '2', '8', '2', '8', '3', '3', '1', '4', '9', '10', '2', '4', '8', '1', '2', '10', '12', '12', '3', '1', '1', '2', '9', '6', '6', '1', '9', '1', '8', '1', '8', '8', '8', '8', '2', '2', '2', '8', '8', '2', '1', '1', '1', '1', '1', '8', '8', '5', '8', '8', '1', '8', '9', '9', '1', '7', '12', '9', '9', '7', '2', '2', '7', '8', '9', '8', '2', '9', '6', '3', '4', '8', '4', '1', '2', '9', '8', '8', '8', '2', '1', '1', '8', '8', '3', '1', '9', '9', '7', '1', '2', '2', '2', '9', '4', '1', '2', '12', '1', '8', '2', '1', '9', '1', '8', '9', '9', '2', '5', '4', '2', '1', '2', '8', '12', '8', '10', '3', '8', '8', '8', '2', '2', '8', '2', '8', '2', '4', '8', '10', '2', '7', '2', '1', '8', '8', '9', '5', '8', '1', '12', '7', '10', '2', '2', '2', '1', '4', '8', '12', '1', '8', '2', '2', '9', '1', '8', '3', '1', '1', '5', '10', '2', '1', '2', '4', '8', '9', '3', '2', '8', '7', '4', '4', '9', '9', '2', '8', '1', '1', '2', '4', '7', '1', '9', '1', '1', '1', '8', '4', '1', '3', '1', '2', '2', '9', '1', '8', '8', '2', '2', '9', '9', '1', '8', '8', '1', '9', '8', '8', '2', '2', '2', '2', '1', '1', '1', '7', '4', '1', '1', '4', '2', '3', '3', '2', '3', '9', '2', '8', '2', '1', '5', '1', '2', '2', '1', '3', '5', '2', '9', '1', '1', '9', '8', '1', '4', '8', '1', '8', '2', '4', '8', '4', '7', '9', '8', '1', '2', '2', '4', '4', '11', '3', '11', '1', '3', '8', '1', '9', '6', '8', '9', '3', '9', '8', '9', '2', '3', '3', '7', '1', '7', '2', '9', '9', '1', '8', '8', '1', '5', '5', '8', '8', '1', '2', '1', '3', '9', '9', '5', '9', '8', '1', '4', '3', '8', '1', '1', '2', '1', '8', '9', '8', '2', '2', '2', '1', '2', '4', '10', '8', '1', '9', '1', '8', '1', '1', '9', '9', '8', '11', '1', '8', '2', '1', '2', '8', '2', '8', '8', '2', '9', '6', '10', '1', '8', '6', '4', '5', '11', '2', '2', '8', '1', '8', '1', '8', '1', '4', '8', '1', '1', '1', '8', '3', '7', '3', '3', '7', '1', '1', '4', '8', '1', '6', '4', '2', '2', '1', '1', '1', '1', '6', '2', '2', '8', '1', '8', '5', '5', '2', '1', '1', '2', '3', '7', '1', '1', '6', '2', '9', '3', '9', '8', '9', '2', '1', '4', '2', '9', '13', '9', '2', '8', '12', '4', '1', '1', '9', '5', '1', '1', '1', '1', '8', '2', '1', '1', '2', '9', '0', '12', '1', '1', '8', '9', '8', '3', '2', '2', '12', '8', '2', '2', '2', '1', '10', '1', '9', '8', '9', '5', '2', '2', '9', '2', '8', '4', '8', '3', '7', '9', '2', '2', '7', '2', '2', '8', '1', '1', '9', '10', '9', '8', '2', '9', '0', '7', '1', '1', '1', '9', '8', '2', '8', '12', '8', '3', '2', '8', '8', '7', '7', '8', '12', '2', '1', '3', '9', '2', '2', '8', '2', '8', '8', '12', '8', '8', '1', '8', '9', '2', '4', '4', '7', '2', '1', '1', '9', '3', '7', '1', '3', '9', '8', '2', '2', '3', '1', '3', '10', '2', '3', '7', '1', '8', '1', '1', '10', '3', '10', '8', '4', '1', '2', '4', '3', '10', '9', '3', '9', '5', '8', '7', '2', '9', '3', '3', '2', '11', '8', '1', '1', '7', '2', '7', '2', '9', '4', '8', '1', '12', '8', '7', '9', '1', '0', '2', '6', '4', '9', '3', '2', '4', '12', '9', '8', '5', '9', '2', '8', '4', '6', '8', '9', '3', '1', '8', '7', '3', '1', '1', '1', '8', '9', '1', '8', '7', '9', '8', '2', '1', '2', '2', '3', '2', '2', '1', '1', '4', '9', '7', '3', '2', '8', '8', '9', '1', '9', '7', '4', '2', '2', '8', '5', '9', '1', '5', '7', '1', '4', '6', '9', '10', '1', '8', '2', '9', '1', '9', '2', '9', '8', '2', '9', '9', '11', '2', '12', '2', '2', '9', '4', '8', '1', '9', '9', '2', '8', '9', '1', '2', '9', '3', '8', '8', '1', '7', '8', '8', '2', '3', '2', '2', '2', '2', '9', '8', '10', '2', '2', '1', '9', '11', '1', '8', '8', '2', '4', '2', '1', '2', '8', '8', '9', '2', '8', '9', '2', '3', '1', '2', '1', '3', '10', '8', '1', '2', '4', '1', '9', '8', '9', '3', '3', '9', '8', '2', '11', '3', '7', '1', '8', '7', '1', '8', '8', '8', '1', '4', '8', '8', '1', '9', '3', '1', '1', '2', '8', '8', '8', '2', '1', '8', '10', '2', '12', '8', '8', '9', '1', '8', '1', '3', '2', '2', '10', '7', '2', '2', '1', '1', '8', '2', '9', '1', '2', '9', '3', '3', '10', '1', '4', '7', '10', '12', '2', '4', '10', '8', '6', '2', '9', '1', '8', '3', '1', '2', '8', '8', '2', '12', '10', '2', '10', '9', '1', '2', '5', '8', '12', '8', '8', '7', '8', '8', '1', '8', '2', '2', '9', '1', '8', '10', '5', '5', '5', '8', '1', '9', '1', '13', '1', '2', '2', '1', '2', '4', '9', '2', '1', '9', '13', '11', '1', '9', '9', '3', '10', '8', '7', '8', '8', '4', '2', '1', '9', '4', '3', '1', '1', '10', '9', '2', '8', '6', '2', '1', '1', '3', '2', '8', '1', '1', '1', '1', '2', '4', '1', '1', '10', '1', '8', '1', '1', '9', '1', '7', '1', '1', '9', '9', '2', '12', '9', '1', '8', '1', '12', '1', '8', '3', '8', '1', '8', '1', '2', '10', '1', '1', '2', '2', '9', '3', '9', '12', '1', '9', '1', '2', '8', '7', '2', '1', '1', '3', '2', '8', '4', '4', '5', '12', '7', '8', '9', '6', '1', '8', '9', '2', '1', '9', '1', '4', '1', '8', '2', '1', '1', '8', '7', '4', '9', '1', '8', '4', '2', '7', '5', '1', '3', '9', '2', '8', '2', '1', '2', '9', '1', '1', '2', '3', '4', '3', '1', '2', '2', '10', '2', '2', '8', '1', '1', '2', '4', '9', '2', '11', '9', '8', '9', '2', '1', '10', '4', '1', '1', '10', '7', '2', '10', '8', '1', '10', '8', '8', '8', '8', '8', '8', '2', '2', '2', '8', '3', '4', '1', '1', '1', '2', '1', '8', '8', '2', '8', '3', '8', '3', '1', '2', '3', '1', '1', '10', '1', '4', '4', '3', '3', '1', '9', '1', '1', '2', '1', '1', '6', '1', '1', '1', '9', '1', '4', '1', '1', '1', '1', '1', '1', '2', '12', '8', '8', '1', '9', '12', '10', '8', '2', '12', '1', '1', '2', '1', '7', '10', '7', '1', '2', '1', '10', '8', '2', '2', '8', '2', '2', '1', '9', '8', '8', '8', '8', '3', '1', '4', '2', '8', '3', '10', '10', '1', '9', '1', '9', '1', '9', '1', '8', '3', '1', '1', '8', '8', '8', '6', '3', '4', '1', '1', '1', '9', '9', '5', '2', '10', '2', '6', '2', '2', '7', '1', '8', '2', '11', '1', '1', '1', '2', '1', '9', '4', '4', '2', '8', '4', '6', '8', '8', '10', '10', '2', '1', '2', '9', '1', '8', '8', '3', '4', '8', '8', '2', '8', '8', '1', '2', '1', '2', '9', '4', '1', '3', '3', '3', '8', '8', '8', '2', '6', '4', '3', '3', '12', '3', '4', '9', '6', '8', '7', '7', '8', '6', '4', '1', '10', '6', '3', '3', '8', '1', '1', '8', '1', '11', '8', '1', '1', '2', '3', '11', '7', '2', '1', '1', '1', '9', '1', '2', '1', '1', '9', '8', '1', '1', '8', '0', '4', '1', '1', '2', '1', '1', '2', '2', '9', '1', '10', '1', '3', '8', '9', '3', '3', '1', '10', '1', '9', '1', '1', '7', '4', '1', '8', '10', '1', '2', '10', '1', '1', '2', '3', '3', '9', '1', '8', '10', '1', '7', '9', '1', '9', '6', '3', '9', '2', '8', '4', '12', '12', '3', '1', '1', '9', '1', '9', '2', '1', '1', '1', '1', '1', '9', '2', '1', '2', '10', '9', '8', '8', '10', '8', '2', '3', '2', '1', '12', '8', '12', '2', '9', '1', '3', '2', '5', '2', '2', '8', '2', '2', '4', '10', '9', '10', '9', '4', '2', '4', '12', '8', '2', '1', '8', '3', '9', '2', '9', '4', '1', '1', '1', '1', '1', '9', '6', '2', '9', '2', '1', '3', '2', '2', '5', '8', '9', '1', '6', '3', '1', '2', '2', '2', '6', '1', '8', '10', '8', '3', '3', '13', '2', '9', '9', '10', '6', '12', '8', '2', '9', '2', '3', '8', '8', '1', '2', '2', '3', '2', '10', '1', '8', '9', '1', '8', '9', '8', '8', '2', '10', '2', '1', '1', '1', '9', '9', '12', '1', '1', '4', '8', '3', '2', '2', '4', '4', '2', '2', '1', '1', '1', '9', '2', '2', '4', '1', '1', '8', '8', '8', '9', '2', '13', '9', '9', '4', '1', '9', '7', '10', '10', '8', '1', '9', '1', '1', '1', '8', '12', '4', '1', '2', '1', '1', '1', '7', '8', '3', '3', '1', '2', '2', '7', '2', '2', '10', '1', '8', '4', '0', '1', '9', '4', '1', '1', '2', '10', '10', '7', '9', '3', '12', '11', '2', '8', '10', '8', '9', '4', '3', '4', '10', '1', '1', '9', '3', '10', '1', '5', '3', '3', '2', '2', '1', '1', '6', '1', '7', '1', '1', '12', '1', '1', '1', '10', '1', '8', '8', '1', '3', '8', '3', '1', '2', '8', '3', '4', '2', '8', '8', '6', '1', '1', '4', '1', '2', '9', '5', '1', '7', '8', '9', '2', '4', '9', '8', '5', '9', '9', '9', '1', '2', '1', '3', '8', '9', '2', '9', '8', '9', '1', '8', '13', '9', '3', '8', '1', '4', '1', '3', '1', '6', '4', '10', '2', '2', '7', '8', '8', '12', '8', '9', '9', '9', '10', '3', '12', '12', '2', '9', '10', '9', '7', '2', '3', '4', '10', '10', '2', '7', '7', '1', '7', '3', '9', '1', '9', '8', '2', '9', '1', '8', '1', '10', '8', '8', '2', '7', '8', '1', '8', '9', '8', '1', '4', '1', '8', '8', '8', '1', '1', '10', '3', '2', '2', '6', '8', '9', '8', '2', '1', '1', '3', '8', '8', '1', '4', '4', '3', '9', '10', '4', '2', '4', '2', '9', '2', '2', '9', '1', '1', '10', '1', '1', '2', '1', '1', '1', '9', '1', '8', '8', '8', '2', '3', '8', '1', '6', '7', '3', '1', '9', '1', '2', '1', '6', '1', '2', '10', '1', '4', '2', '3', '2', '9', '9', '4', '9', '9', '1', '2', '8', '4', '7', '8', '7', '13', '9', '1', '2', '1', '9', '3', '3', '6', '8', '9', '10', '9', '2', '2', '3', '12', '1', '1', '1', '13', '2', '1', '7', '10', '8', '9', '1', '2', '9', '1', '1', '1', '2', '1', '12', '5', '6', '9', '10', '3', '1', '10', '1', '3', '2', '8', '9', '2', '10', '8', '8', '9', '2', '8', '9', '2', '2', '2', '8', '8', '2', '3', '1', '4', '8', '1', '9', '9', '2', '8', '10', '9', '1', '2', '12', '8', '8', '0', '2', '1', '1', '9', '10', '1', '6', '2', '2', '10', '2', '2', '4', '2', '2', '12', '1', '6', '7', '8', '9', '1', '3', '5', '7', '3', '1', '3', '1', '1', '3', '2', '6', '3', '2', '1', '8', '3', '5', '2', '1', '1', '8', '1', '1', '0', '2', '3', '5', '3', '1', '1', '9', '8', '8', '8', '8', '4', '7', '2', '9', '12', '5', '9', '3', '12', '3', '9', '1', '9', '1', '7', '3', '9', '3', '10', '1', '2', '1', '4', '9', '8', '2', '9', '11', '2', '8', '9', '4', '1', '1', '1', '1', '1', '1', '1', '9', '1', '9', '1', '1', '1', '12', '3', '10', '3', '6', '1', '7', '10', '8', '3', '7', '1', '1', '2', '6', '1', '3', '9', '2', '8', '1', '8', '8', '1', '8', '10', '12', '1', '10', '12', '10', '1', '8', '8', '1', '4', '7', '9', '2', '1', '9', '3', '8', '12', '10', '6', '1', '6', '8', '3', '1', '3', '1', '10', '10', '9', '8', '1', '2', '9', '8', '8', '1', '9', '8', '8', '3', '1', '3', '9', '12', '1', '8', '1', '5', '8', '11', '5', '5', '1', '2', '1', '3', '1', '1', '1', '9', '8', '8', '1', '9', '10', '8', '4', '1', '1', '8', '10', '8', '9', '1', '10', '1', '7', '2', '1', '1', '9', '6', '2', '9', '8', '1', '1', '1', '9', '8', '8', '1', '9', '9', '1', '1', '2', '7', '9', '2', '9', '9', '8', '7', '6', '8', '7', '8', '2', '9', '2', '1', '8', '8', '8', '2', '6', '3', '1', '8', '12', '12', '8', '7', '1', '2', '2', '7', '1', '9', '8', '1', '4', '1', '1', '8', '5', '8', '1', '8', '8', '8', '8', '1', '2', '11', '1', '1', '1', '2', '7', '1', '9', '2', '2', '2', '9', '8', '9', '2', '9', '1', '8', '1', '7', '11', '13', '1', '9', '8', '2', '2', '1', '2', '3', '10', '3', '10', '4', '3', '9', '1', '8', '1', '1', '13', '1', '1', '3', '8', '1', '2', '2', '2', '9', '4', '1', '8', '1', '9', '3', '6', '8', '5', '2', '8', '2', '8', '1', '9', '2', '2', '8', '1', '1', '4', '8', '9', '2', '2', '7', '1', '8', '12', '2', '12', '8', '2', '9', '9', '3', '8', '1', '1', '9', '9', '9', '2', '8', '1', '9', '1', '8', '9', '8', '2', '2', '4', '1', '9', '10', '10', '8', '8', '1', '1', '2', '1', '4', '1', '2', '1', '1', '3', '12', '8', '8', '8', '8', '10', '2', '8', '12', '1', '9', '9', '8', '9', '1', '2', '1', '8', '8', '8', '2', '8', '8', '3', '10', '8', '9', '1', '1', '1', '3', '4', '8', '10', '6', '3', '1', '3', '2', '8', '9', '3', '5', '4', '1', '5', '2', '9', '9', '9', '1', '1', '1', '9', '9', '6', '10', '9', '8', '12', '2', '9', '1', '2', '1', '1', '1', '8', '1', '12', '1', '12', '1', '9', '1', '8', '1', '2', '1', '2', '8', '8', '10', '1', '9', '1', '2', '2', '8', '8', '3', '12', '1', '11', '2', '10', '12', '11', '2', '1', '10', '1', '1', '9', '3', '10', '2', '7', '1', '8', '1', '9', '9', '2', '1', '3', '12', '5', '9', '9', '8', '1', '8', '8', '8', '1', '1', '3', '9', '8', '9', '8', '3', '10', '3', '7', '8', '9', '2', '3', '1', '9', '10', '9', '2', '8', '2', '6', '4', '1', '1', '3', '8', '2', '3', '9', '1', '2', '1', '8', '1', '2', '2', '2', '2', '9', '9', '4', '8', '9', '1', '1', '2', '9', '1', '5', '2', '9', '8', '10', '1', '2', '10', '8', '5', '6', '4', '8', '1', '1', '1', '8', '9', '2', '8', '8', '8', '12', '2', '2', '1', '8', '1', '2', '1', '9', '8', '8', '9', '1', '1', '8', '7', '1', '8', '0', '2', '3', '8', '1', '1', '8', '1', '1', '12', '3', '2', '10', '8', '10', '6', '1', '8', '8', '10', '8', '8', '1', '4', '8', '1', '2', '9', '8', '1', '1', '3', '3', '5', '1', '1', '2', '1', '2', '1', '9', '8', '3', '9', '8', '2', '10', '9', '1', '8', '1', '10', '9', '10', '1', '2', '3', '8', '1', '3', '1', '9', '9', '8', '1', '9', '8', '8', '9', '1', '9', '2', '8', '8', '2', '13', '8', '10', '8', '9', '3', '2', '1', '3', '1', '12', '10', '10', '1', '11', '1', '2', '4', '2', '10', '1', '8', '11', '1', '1', '1', '2', '2', '8', '2', '9', '8', '9', '9', '12', '8', '1', '9', '2', '4', '1', '8', '3', '9', '1', '6', '5', '8', '9', '3', '3', '2', '1', '1', '1', '8', '11', '1', '8', '7', '1', '11', '9', '1', '7', '9', '9', '8', '8', '8', '12', '8', '1', '2', '9', '8', '8', '2', '10', '9', '8', '8', '4', '10', '9', '8', '2', '10', '10', '2', '1', '2', '1', '2', '1', '4', '1', '7', '1', '1', '3', '12', '9', '8', '5', '9', '10', '1', '9', '9', '2', '7', '4', '1', '1', '8', '12', '8', '2', '2', '9', '4', '1', '1', '5', '9', '1', '8', '12', '8', '1', '1', '4', '9', '10', '2', '3', '1', '3', '3', '3', '8', '1', '2', '1', '2', '1', '2', '9', '1', '2', '8', '9', '4', '8', '8', '10', '2', '12', '5', '5', '1', '8', '1', '7', '8', '1', '1', '8', '9', '12', '4', '9', '3', '1', '2', '5', '9', '2', '1', '2', '2', '1', '2', '8', '1', '8', '8', '2', '2', '1', '8', '4', '1', '2', '10', '9', '10', '8', '1', '10', '4', '8', '8', '10', '2', '8', '1', '1', '3', '10', '1', '4', '2', '8', '3', '3', '2', '9', '8', '5', '5', '9', '3', '10', '8', '1', '2', '1', '2', '9', '1', '1', '2', '9', '9', '8', '8', '11', '9', '4', '1', '8', '2', '7', '1', '2', '8', '2', '9', '9', '8', '2', '9', '9', '8', '8', '1', '2', '8', '1', '1', '2', '9', '8', '1', '4', '9', '1', '8', '12', '1', '12', '2', '8', '2', '9', '1', '12', '1', '2', '9', '1', '1', '9', '3', '2', '9', '9', '2', '11', '8', '7', '9', '9', '9', '8', '8', '1', '2', '8', '1', '1', '4', '1', '2', '10', '1', '9', '2', '1', '1', '1', '9', '1', '13', '4', '3', '2', '1', '1', '2', '2', '7', '8', '7', '1', '1', '2', '8', '8', '3', '4', '8', '2', '10', '13', '8', '8', '9', '2', '2', '1', '1', '10', '4', '2', '7', '9', '8', '2', '1', '9', '8', '1', '1', '9', '8', '9', '8', '2', '8', '2', '2', '9', '2', '1', '9', '1', '2', '4', '8', '2', '1', '1', '8', '1', '4', '1', '9', '3', '9', '2', '7', '1', '9', '6', '1', '10', '2', '2', '2', '2', '2', '10', '10', '10', '9', '9', '9', '3', '12', '10', '1', '1', '10', '1', '1', '2', '2', '1', '2', '3', '12', '8', '8', '1', '2', '9', '9', '1', '1', '10', '8', '9', '8', '3', '1', '1', '3', '1', '10', '1', '1', '1', '4', '1', '1', '1', '7', '10', '2', '1', '8', '1', '9', '3', '1', '10', '8', '8', '1', '5', '9', '1', '2', '1', '8', '8', '1', '3', '1', '10', '2', '1', '1', '2', '8', '2', '5', '3', '5', '2', '1', '1', '9', '1', '7', '9', '9', '2', '8', '1', '12', '4', '1', '4', '9', '9', '4', '8', '1', '10', '10', '8', '8', '8', '9', '2', '3', '5', '1', '8', '1', '1', '1', '7', '10', '1', '4', '12', '3', '2', '2', '9', '1', '1', '1', '2', '8', '4', '1', '7', '3', '12', '8', '6', '2', '2', '7', '9', '11', '1', '10', '9', '1', '8', '12', '2', '11', '1', '2', '3', '1', '1', '1', '8', '9', '2', '2', '2', '2', '3', '3', '8', '10', '4', '1', '2', '9', '8', '2', '2', '9', '1', '8', '11', '4', '2', '8', '7', '1', '1', '7', '3', '9', '9', '4', '5', '8', '10', '2', '9', '9', '9', '8', '1', '9', '2', '2', '2', '1', '2', '9', '4', '3', '12', '4', '3', '2', '3', '8', '3', '10', '1', '10', '1', '1', '8', '10', '6', '8', '9', '9', '1', '2', '9', '2', '9', '3', '8', '1', '1', '12', '1', '5', '1', '10', '8', '10', '2', '7', '1', '1', '1', '1', '3', '2', '3', '5', '9', '1', '2', '1', '9', '8', '10', '9', '8', '1', '8', '1', '8', '2', '8', '1', '2', '8', '1', '2', '5', '8', '12', '2', '1', '1', '1', '1', '1', '8', '8', '2', '1', '4', '2', '8', '8', '8', '2', '9', '9', '11', '1', '2', '9', '3', '1', '1', '8', '9', '2', '4', '10', '1', '9', '8', '4', '9', '8', '10', '8', '8', '9', '2', '8', '9', '3', '3', '3', '4', '3', '2', '2', '10', '8', '2', '1', '5', '1', '9', '1', '1', '2', '1', '2', '3', '1', '1', '8', '8', '1', '10', '8', '8', '1', '6', '4', '8', '2', '5', '8', '1', '3', '8', '2', '8', '9', '1', '1', '8', '12', '10', '1', '5', '8', '2', '8', '10', '9', '9', '1', '6', '9', '1', '10', '8', '2', '9', '10', '1', '2', '1', '1', '8', '8', '3', '9', '8', '1', '10', '2', '8', '1', '10', '1', '5', '1', '1', '1', '5', '1', '1', '1', '4', '1', '1', '1', '8', '2', '8', '1', '8', '3', '8', '9', '1', '8', '1', '8', '1', '1', '2', '2', '1', '12', '9', '9', '1', '8', '2', '1', '2', '8', '1', '9', '1', '1', '2', '1', '2', '2', '2', '1', '9', '9', '8', '8', '1', '1', '9', '10', '8', '4', '3', '3', '4', '9', '1', '3', '3', '10', '2', '2', '1', '8', '4', '1', '9', '4', '1', '1', '9', '8', '10', '4', '9', '9', '8', '2', '1', '3', '4', '9', '3', '8', '9', '1', '1', '2', '1', '7', '8', '2', '9', '1', '9', '2', '6', '9', '10', '8', '1', '1', '5', '9', '5', '1', '9', '10', '9', '3', '3', '8', '8', '2', '9', '2', '8', '2', '1', '8', '5', '8', '3', '10', '1', '1', '2', '1', '8', '1', '4', '3', '2', '9', '4', '9', '8', '8', '8', '1', '3', '9', '1', '9', '9', '1', '2', '1', '9', '8', '9', '8', '1', '1', '1', '4', '2', '2', '9', '9', '1', '1', '1', '3', '6', '1', '2', '3', '1', '4', '2', '9', '1', '9', '9', '1', '8', '2', '1', '9', '1', '8', '1', '8', '8', '2', '1', '9', '9', '6', '8', '8', '9', '8', '10', '8', '9', '5', '1', '1', '1', '1', '8', '1', '8', '12', '8', '1', '12', '9', '2', '8', '1', '1', '2', '1', '8', '6', '9', '9', '8', '1', '8', '9', '7', '7', '8', '9', '9', '1', '8', '3', '8', '1', '3', '3', '8', '9', '8', '2', '8', '1', '2', '9', '8', '1', '1', '1', '1', '9', '1', '9', '8', '12', '3', '8', '8', '1', '10', '10', '10', '2', '2', '1', '10', '2', '2', '10', '8', '12', '8', '1', '1', '1', '2', '1', '1', '3', '8', '1', '2', '2', '2', '1', '1', '6', '2', '8', '8', '8', '1', '1', '1', '9', '9', '2', '8', '10', '2', '8', '2', '1', '2', '9', '9', '2', '1', '1', '8', '8', '1', '3', '1', '8', '9', '1', '10', '9', '1', '1', '1', '1', '7', '2', '2', '1', '2', '8', '8', '1', '2', '1', '3', '2', '1', '8', '3', '9', '2', '10', '2', '10', '1', '2', '1', '1', '13', '9', '10', '8', '2', '2', '1', '1', '8', '2', '2', '1', '9', '1', '1', '8', '9', '9', '2', '9', '2', '2', '1', '1', '1', '8', '9', '4', '9', '9', '1', '8', '2', '1', '1', '8', '2', '2', '2', '8', '1', '1', '8', '2', '10', '2', '9', '9', '7', '1', '1', '2', '2', '8', '3', '1', '2', '1', '1', '8', '1', '1', '1', '9', '1', '1', '8', '8', '1', '9', '6', '1', '9', '1', '8', '1', '8', '8', '8', '1', '3', '8', '8', '2', '6', '3', '2', '1', '10', '1', '8', '2', '1', '6', '1', '1', '1', '8', '1', '10', '1', '1', '8', '9', '2', '6', '9', '2', '9', '4', '1', '8', '1', '3', '7', '8', '8', '5', '8', '7', '1', '1', '1', '13', '8', '3', '1', '1', '1', '1', '2', '8', '2', '12', '8', '1', '2', '5', '1', '8', '2', '10', '8', '2', '10', '1', '5', '2', '1', '1', '5', '2', '2', '7', '8', '2', '9', '9', '1', '10', '1', '8', '2', '5', '8', '1', '5', '1', '10', '1', '1', '9', '8', '6', '1', '8', '9', '8', '1', '8', '2', '2', '8', '3', '9', '1', '9', '3', '8', '4', '4', '3', '3', '1', '1', '1', '8', '9', '1', '1', '8', '3', '1', '2', '8', '2', '1', '1', '10', '1', '2', '2', '1', '1', '8', '2', '8', '2', '10', '2', '2', '10', '8', '8', '8', '13', '2', '2', '9', '2', '5', '9', '2', '1', '8', '8', '8', '1', '12', '12', '2', '9', '2', '1', '8', '2', '1', '8', '1', '9', '9', '2', '7', '1', '1', '7', '1', '4', '3', '1', '10', '3', '10', '9', '4', '6', '7', '5', '2', '1', '8', '8', '1', '4', '9', '2', '1', '8', '1', '1', '1', '1', '1', '8', '9', '8', '8', '8', '8', '9', '8', '10', '1', '8', '2', '7', '8', '9', '1', '2', '5', '9', '10', '8', '9', '12', '1', '6', '1', '2', '10', '9', '4', '1', '8', '10', '10', '1', '1', '1', '1', '1', '2', '3', '8', '1', '2', '10', '2', '1', '2', '4', '2', '2', '9', '1', '4', '2', '1', '1', '12', '12', '9', '9', '7', '1', '8', '4', '9', '9', '8', '1', '7', '3', '2', '1', '1', '4', '8', '1', '5', '8', '2', '8', '1', '14', '9', '12', '1', '10', '2', '1', '1', '8', '8', '6', '6', '3', '1', '2', '8', '1', '3', '2', '1', '8', '8', '1', '10', '8', '2', '8', '2', '5', '9', '1', '8', '8', '1', '1', '8', '8', '13', '1', '3', '8', '4', '1', '9', '7', '8', '8', '8', '3', '8', '1', '4', '2', '8', '8', '8', '7', '1', '1', '2', '8', '8', '9', '8', '8', '2', '3', '9', '9', '2', '1', '2', '2', '1', '8', '2', '6', '2', '1', '1', '9', '10', '3', '8', '2', '3', '2', '8', '8', '1', '8', '8', '8', '9', '1', '4', '1', '9', '2', '1', '8', '0', '1', '8', '1', '1', '8', '9', '9', '1', '3', '1', '2', '8', '8', '7', '1', '1', '2', '9', '2', '10', '2', '8', '2', '2', '2', '7', '1', '9', '8', '1', '3', '9', '2', '1', '10', '8', '1', '4', '1', '8', '5', '9', '8', '8', '1', '2', '2', '1', '8', '8', '6', '8', '1', '1', '8', '3', '2', '2', '1', '8', '1', '7', '6', '8', '1', '1', '1', '9', '1', '1', '1', '2', '1', '11', '10', '10', '11', '9']\n",
            "     filenum                                               text  label\n",
            "0          1  only had this sound-echo-time method been long...      8\n",
            "1          2  has no such implied limitation. In common unde...      1\n",
            "2          3  to apply its conclusion to Champlin. The contr...      8\n",
            "3          4  size of the reservation; in Congress by statut...      2\n",
            "4          5  to them.' Consequently, the Government cannot ...      8\n",
            "...      ...                                                ...    ...\n",
            "7327    8413  process of law secured by the Fourteenth Amend...      2\n",
            "7328    8414  other shootings and repeatedly expressed an in...      1\n",
            "7329    8417  U.S.C. (b), the United States in April asked l...     10\n",
            "7330    8418  AAA pursuant to agreement of the parties. That...     11\n",
            "7331    8419  Bd. v. Corvallis Sand & Gravel Co., . In the e...      9\n",
            "\n",
            "[7332 rows x 3 columns]\n",
            "0        8\n",
            "1        1\n",
            "2        8\n",
            "3        2\n",
            "4        8\n",
            "        ..\n",
            "7327     2\n",
            "7328     1\n",
            "7329    10\n",
            "7330    11\n",
            "7331     9\n",
            "Name: label, Length: 7332, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "fh = open('sc_model_1.txt')\n",
        "filenums=[]\n",
        "records=[]\n",
        "for record in fh:\n",
        "    record=record.split(\"---\")\n",
        "    filenum=record[0]\n",
        "    modelnum=record[1]\n",
        "    text=record[2]\n",
        "    filenums.append(filenum)\n",
        "    records.append(text)\n",
        "    \n",
        "print(len(filenums))\n",
        "print(len(records))\n",
        "\n",
        "summarized_data = pd.DataFrame(list(zip(filenums, records)),columns =['filenum','text'])\n",
        "len_list = [len(ele.split()) for ele in records]\n",
        "res = 0 if len(len_list) == 0 else (float(sum(len_list)) / len(len_list))\n",
        "print(\"Average Length %s\" % res)\n",
        "\n",
        "temp_file = open(\"labels_sc.txt\", \"r\")\n",
        "#temp_file = open(\"labels_sc_279.txt\", \"r\")\n",
        "\n",
        "data = temp_file.read()\n",
        "\n",
        "label_list = data.split(\"\\n\")\n",
        "print(label_list)\n",
        "label_list = label_list[0:-1]\n",
        "print(label_list)\n",
        "label_list = [int(i) for i in label_list]\n",
        "temp_file.close()\n",
        "\n",
        "\n",
        "new_label_list=[]\n",
        "for num in filenums:\n",
        "  new_label_list.append(label_list[int(num)-1])\n",
        "summarized_data['label'] = new_label_list\n",
        "\n",
        "print(summarized_data)\n",
        "print(summarized_data['label'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQbAYvIIvxo4",
        "outputId": "ec51f7a1-eb45-4a7f-fbc8-cd800567e80d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_1[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_roberta_model[0][0]']       \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 15)           7695        ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 8  1  8 ... 10 11  9]\n",
            "[   1    2    3 ... 8417 8418 8419]\n",
            "[6622   35 6713 4989 8233 7444 5061 3942 8344  454 5464  317 2883 4515\n",
            " 3001 3410 6991 2875 1125 3885 6647 4857 4504  214 3230 8340 2969 3847\n",
            " 4609 5092 1346 5993 7358 2752 7651 7301 8188 7073 4782 6069 2346 6445\n",
            " 6483  542 1663 4952 5072 1967 4071 7590  860 6418 5030 7297  626 5463\n",
            " 6029 6302 3819 7256 2950 5667 1235 3993 1439 1323 4263 3835 2134 2624\n",
            " 5801 4293 8129  320 5866 3247 5777   19 6059 6520 4837 3843 5695 1666\n",
            "  858 1418 5409 5274 6205 3167 2356 8418 5997 6685  223 3444 3310 7721\n",
            " 5928 2360 7197 4252 8343  909 4093 4354 4752 1421  649  829 1916 1003\n",
            " 3488 2496 5765 6612 4004 4052 1403 6697 5268 1010 6293 6380 4602 6057\n",
            " 1851 4577 4236 7978 1350 5083 6552 1058  361 7706 4186 2590 4452 4396\n",
            "  749  870 8351 3834  812 5108  463 1743 6600 8264 8064 7121 8279 5556\n",
            " 1845 5119 2516 4673 7947 8215 1882 3350 7617 7152 5199 4282 1092 3342\n",
            " 8003 5025 5051  535  933 3969 1927 3615 1221 8062 1489 3296 7442 5459\n",
            " 2031 7351  234 2313 8015 7416 7921 6574 6257 8396  941 4520 3300 5640\n",
            "  240 7810 7425 1321 6580  928 1170 5404 8273 6742 7005   94 7459    9\n",
            " 7861 7975 1018 5029 2322 2140 1415 7061 7767 4990 8124 5927 6051 3269\n",
            " 6916 5959 2846 6818 7052 7775 7582 8116 4240 4767 5248  430 1812 2175\n",
            " 6055 8331 3916  998 7271 1564 7326 1077 1559  292 3065 6764 6854 4915\n",
            " 5552 3583 1917  868 5902 4229 2144 7565 3712 7224 4502 1406 1651 6256\n",
            " 3827 6525 6318 2686 3616 5369 8235 1704 1011 3351 8089 8021  102  224\n",
            " 5671 8294 1510 7707 2985 6624 5146 1428 7381 3709 6824 6308  701 2281\n",
            " 6837 5436 1741 3520 5995 2649 4886  966 2863 5476 4058 1246 2406 1199\n",
            " 8387  513 5449 1028 2226 6166 4387  350 6123  678 5236 5041  734 3412\n",
            " 6085  587  489 5760 3163 3376 4982 5115 4655 5575 3839 6743 7173 5724\n",
            " 7689 7787  520  713 3214 8302 3150 5713 2247 8128 4607 6660 3505 4610\n",
            " 2866 4161 6905 7096  768 1062 4411 6087 2096   21 4903 7546 6630 2928\n",
            " 4640 7125 1792 2063 1700 2305 7928 8095  737 5849  922 3423 2359 6015\n",
            " 4489 4361 6081 6346 5014 5167 1023 8009 3335  268 4777 5221 4323 6168\n",
            "  759  711 5702 6129 7181 1860 7192 1347 1692 5467 3378 6579   67 2693\n",
            " 6134 4841 4270 3073 8397 4611 2943 6746 7696 3466 3950 4693 6125 8370\n",
            " 6142 6131  995 5548 2186 2112 1721 5067 4015 5587 7092 3156   25 1749\n",
            " 5554 4878 1197 8248  458 8035 6836 1006 5310 2698 6960 2339 1650 6227\n",
            " 4104 5130 7089 6993 2400 1289 4305 5585 2524 7933 6284 7057 5324 3807\n",
            " 5790 8347 2390 5280 2841 7220 1384 7693 8090 2671 4171 5260  133 7451\n",
            " 3957 6793 1719 7379 4743 4746 5435 6216 4858 4417 8355 4492 2648 5104\n",
            " 5278 6539  743 7308  158 7543 2168 5913 7111  864 3414 5788 7592   39\n",
            " 7157 5647 4555 3956 1045 3617 4630 4384 6596   96 4539 6749 7202 4710\n",
            " 2413 4749 5122 5052 1598 5929  252 5565  336 6008 4564 5580 7237 1833\n",
            " 1466 1568 5958 7657 4327 5091 6918 6487 4082 2989 7448 4715   81 3382\n",
            "  467 3784 4624  764 4419 1117 2962 7954 7122 1430 6392  505 3174 1534\n",
            "  323 4164 3659 7526  498 5944 3510 4572 2307 2929 7480 4412  731 4563\n",
            " 7692 1869 2472 6966 4238 4702 6549 1594 2276 7269 2545 2216  605 6587\n",
            " 6315 5297 5282 4816 3200 1619 5504 2536 6399 3459 3917 6342 5754 5721\n",
            " 7385 7909 6126 7382 8382 4383 5502 7130  485 5862 4936 7674 3575 5372\n",
            " 3893 7109 4605 5211 5462 8065   50 4080 3334 8167 8378 5595 1873 8207\n",
            " 7076 4106 3719 4627  395 2685  231 2405 5419 6973 3480 3246 5414 3593\n",
            " 5931 6252 8208 4825 6203 3019 1105 5822   63 5854 6543 5787 4764 6701\n",
            "  334 6562 3822 4861 3051 1315 4805  246 8318 1089 8169 8218 2756 7549\n",
            "  707 1284 1076 2465 3815  540 4636 3188  697   80 1455  381 7025 7127\n",
            " 3992 4026 2777 4961  383 7531  744 6640 1047 6684 4297 7939  150 7384\n",
            "  446 7738 3287 6675 7752 5279 8196 3020 1203 4192 1552 4253 2887 2556\n",
            " 5729 1898 3208 5973 7033 1431 5915 8312 5799 7105  511 6404 2852 1561\n",
            " 4150 4835 1178]\n"
          ]
        }
      ],
      "source": [
        "gc.collect()\n",
        "tf.keras.backend.clear_session()\n",
        "dbert_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "dbert_model = TFRobertaModel.from_pretrained('roberta-base')\n",
        "max_len=512\n",
        "sentences=summarized_data['text']\n",
        "labels=summarized_data['label']\n",
        "filenumbers=summarized_data['filenum']\n",
        "len(sentences),len(labels),len(filenumbers)\n",
        "\n",
        "model_1=create_model()\n",
        "input_ids=[]\n",
        "attention_masks=[]\n",
        "\n",
        "for sent in sentences:\n",
        "  dbert_inps=dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n",
        "  input_ids.append(dbert_inps['input_ids'])\n",
        "  attention_masks.append(dbert_inps['attention_mask'])\n",
        "input_ids=np.asarray(input_ids)\n",
        "\n",
        "attention_masks=np.array(attention_masks)\n",
        "labels=np.array(labels)\n",
        "filenumbers=[int(i) for i in filenumbers]\n",
        "filenumbers=np.array(filenumbers)\n",
        "print(labels)\n",
        "print(filenumbers)\n",
        "\n",
        "#train_inp,val_inp,train_label,val_label,train_mask,val_mask,train_fnum,val_fnum=train_test_split(input_ids,labels,attention_masks,filenumbers,test_size=0.1,random_state=42)\n",
        "\n",
        "new_train_inp_1=[]\n",
        "new_val_inp_1=[]\n",
        "new_train_label_1=[]\n",
        "new_val_label_1=[]\n",
        "new_train_mask_1=[]\n",
        "new_train_fnum_1=[]\n",
        "new_val_fnum_1=[]\n",
        "new_val_mask_1=[]\n",
        "for i in range(len(train_fnum)):\n",
        "  if train_fnum[i] in filenumbers:\n",
        "    new_train_inp_1.append(train_inp[i])\n",
        "    #new_val_inp.append(val_inp[i])\n",
        "    new_train_label_1.append(train_label[i])\n",
        "    #new_val_label.append(val_label[i])\n",
        "    new_train_mask_1.append(train_mask[i])\n",
        "    #new_val_mask.append(val_mask[i])\n",
        "    new_train_fnum_1.append(train_fnum[i])\n",
        "\n",
        "for i in range(len(val_fnum)):\n",
        "  if val_fnum[i] in filenumbers:\n",
        "    #new_train_inp.append(train_inp[i])\n",
        "    new_val_inp_1.append(val_inp[i])\n",
        "    #new_train_label.append(train_label[i])\n",
        "    new_val_label_1.append(val_label[i])\n",
        "    #new_train_mask.append(train_mask[i])\n",
        "    new_val_mask_1.append(val_mask[i])\n",
        "    new_val_fnum_1.append(val_fnum[i])\n",
        "\n",
        "# print(new_train_label)\n",
        "# print(len(new_train_label))\n",
        "# print(new_val_label)\n",
        "# print(len(new_val_label))\n",
        "new_train_inp_1=np.array(new_train_inp_1)\n",
        "new_val_inp_1=np.array(new_val_inp_1)\n",
        "new_train_label_1=np.array(new_train_label_1)\n",
        "new_val_label_1=np.array(new_val_label_1)\n",
        "new_train_mask_1=np.array(new_train_mask_1)\n",
        "new_train_fnum_1=np.array(new_train_fnum_1)\n",
        "new_val_fnum_1=np.array(new_val_fnum_1)\n",
        "new_val_mask_1=np.array(new_val_mask_1)\n",
        "\n",
        "print(new_val_fnum_1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5WPkKimLztr",
        "outputId": "94170725-5b4d-485a-82ef-77b958c8409e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_1[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_roberta_model[0][0]']       \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 15)           7695        ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 8  1  8 ... 10 11  9]\n",
            "[   1    2    3 ... 8417 8418 8419]\n",
            "[6622   35 6713 4989 8233 7444 5061 3942 8344  454 5464  317 2883 4515\n",
            " 3001 3410 6991 2875 1125 3885 6647 4857 4504  214 3230 8340 2969 3847\n",
            " 4609 5092 1346 5993 7358 2752 7651 7301 8188 7073 4782 6069 2346 6445\n",
            " 6483  542 1663 4952 5072 1967 4071 7590  860 6418 5030 7297  626 5463\n",
            " 6029 6302 3819 7256 2950 5667 1235 3993 1439 1323 4263 3835 2134 2624\n",
            " 5801 4293 8129  320 5866 3247 5777   19 6059 6520 4837 3843 5695 1666\n",
            "  858 1418 5409 5274 6205 3167 2356 8418 5997 6685  223 3444 3310 7721\n",
            " 5928 2360 7197 4252 8343  909 4093 4354 4752 1421  649  829 1916 1003\n",
            " 3488 2496 5765 6612 4004 4052 1403 6697 5268 1010 6293 6380 4602 6057\n",
            " 1851 4577 4236 7978 1350 5083 6552 1058  361 7706 4186 2590 4452 4396\n",
            "  749  870 8351 3834  812 5108  463 1743 6600 8264 8064 7121 8279 5556\n",
            " 1845 5119 2516 4673 7947 8215 1882 3350 7617 7152 5199 4282 1092 3342\n",
            " 8003 5025 5051  535  933 3969 1927 3615 1221 8062 1489 3296 7442 5459\n",
            " 2031 7351  234 2313 8015 7416 7921 6574 6257 8396  941 4520 3300 5640\n",
            "  240 7810 7425 1321 6580  928 1170 5404 8273 6742 7005   94 7459    9\n",
            " 7861 7975 1018 5029 2322 2140 1415 7061 7767 4990 8124 5927 6051 3269\n",
            " 6916 5959 2846 6818 7052 7775 7582 8116 4240 4767 5248  430 1812 2175\n",
            " 6055 8331 3916  998 7271 1564 7326 1077 1559  292 3065 6764 6854 4915\n",
            " 5552 3583 1917  868 5902 4229 2144 7565 3712 7224 4502 1406 1651 6256\n",
            " 3827 6525 6318 2686 3616 5369 8235 1704 1011 3351 8089 8021  102  224\n",
            " 5671 8294 1510 7707 2985 6624 5146 1428 7381 3709 6824 6308  701 2281\n",
            " 6837 5436 1741 3520 5995 2649 4886  966 2863 5476 4058 1246 2406 1199\n",
            " 8387  513 5449 1028 2226 6166 4387  350 6123  678 5236 5041  734 3412\n",
            " 6085  587  489 5760 3163 3376 4982 5115 4655 5575 3839 6743 7173 5724\n",
            " 7689 7787  520  713 3214 8302 3150 5713 2247 8128 4607 6660 3505 4610\n",
            " 2866 4161 6905 7096  768 1062 4411 6087 2096   21 4903 7546 6630 2928\n",
            " 4640 7125 1792 2063 1700 2305 7928 8095  737 5849  922 3423 2359 6015\n",
            " 4489 4361 6081 6346 5014 5167 1023 8009 3335  268 4777 5221 4323 6168\n",
            "  759  711 5702 6129 7181 1860 7192 1347 1692 5467 3378 6579   67 2693\n",
            " 6134 4841 4270 3073 8397 4611 2943 6746 7696 3466 3950 4693 6125 8370\n",
            " 6142 6131  995 5548 2186 2112 1721 5067 4015 5587 7092 3156   25 1749\n",
            " 5554 4878 1197 8248  458 8035 6836 1006 5310 2698 6960 2339 1650 6227\n",
            " 4104 5130 7089 6993 2400 1289 4305 5585 2524 7933 6284 7057 5324 3807\n",
            " 5790 8347 2390 5280 2841 7220 1384 7693 8090 2671 4171 5260  133 7451\n",
            " 3957 6793 1719 7379 4743 4746 5435 6216 4858 4417 8355 4492 2648 5104\n",
            " 5278 6539  743 7308  158 7543 2168 5913 7111  864 3414 5788 7592   39\n",
            " 7157 5647 4555 3956 1045 3617 4630 4384 6596   96 4539 6749 7202 4710\n",
            " 2413 4749 5122 5052 1598 5929  252 5565  336 6008 4564 5580 7237 1833\n",
            " 1466 1568 5958 7657 4327 5091 6918 6487 4082 2989 7448 4715   81 3382\n",
            "  467 3784 4624  764 4419 1117 2962 7954 7122 1430 6392  505 3174 1534\n",
            "  323 4164 3659 7526  498 5944 3510 4572 2307 2929 7480 4412  731 4563\n",
            " 7692 1869 2472 6966 4238 4702 6549 1594 2276 7269 2545 2216  605 6587\n",
            " 6315 5297 5282 4816 3200 1619 5504 2536 6399 3459 3917 6342 5754 5721\n",
            " 7385 7909 6126 7382 8382 4383 5502 7130  485 5862 4936 7674 3575 5372\n",
            " 3893 7109 4605 5211 5462 8065   50 4080 3334 8167 8378 5595 1873 8207\n",
            " 7076 4106 3719 4627  395 2685  231 2405 5419 6973 3480 3246 5414 3593\n",
            " 5931 6252 8208 4825 6203 3019 1105 5822   63 5854 6543 5787 4764 6701\n",
            "  334 6562 3822 4861 3051 1315 4805  246 8318 1089 8169 8218 2756 7549\n",
            "  707 1284 1076 2465 3815  540 4636 3188  697   80 1455  381 7025 7127\n",
            " 3992 4026 2777 4961  383 7531  744 6640 1047 6684 4297 7939  150 7384\n",
            "  446 7738 3287 6675 7752 5279 8196 3020 1203 4192 1552 4253 2887 2556\n",
            " 5729 1898 3208 5973 7033 1431 5915 8312 5799 7105  511 6404 2852 1561\n",
            " 4150 4835 1178]\n",
            "Sun Jun  5 04:09:52 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    39W / 300W |  15237MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "826/826 [==============================] - 300s 345ms/step - loss: 7.4050 - accuracy: 0.6054 - val_loss: 6.7461 - val_accuracy: 0.7387\n",
            "Epoch 2/5\n",
            "826/826 [==============================] - 281s 340ms/step - loss: 6.4521 - accuracy: 0.7650 - val_loss: 6.2621 - val_accuracy: 0.7387\n",
            "Epoch 3/5\n",
            "826/826 [==============================] - 285s 345ms/step - loss: 5.8989 - accuracy: 0.8212 - val_loss: 5.9721 - val_accuracy: 0.7565\n",
            "Epoch 4/5\n",
            "826/826 [==============================] - 285s 345ms/step - loss: 5.4136 - accuracy: 0.8688 - val_loss: 5.5064 - val_accuracy: 0.7743\n",
            "Epoch 5/5\n",
            "826/826 [==============================] - 281s 340ms/step - loss: 4.9715 - accuracy: 0.9017 - val_loss: 5.3730 - val_accuracy: 0.7606\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_3[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_roberta_model[1][0]']       \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 15)           7695        ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.7553444180522565\n",
            "Weighted F1: 0.7521814131778802\n",
            "Micro F1: 0.7553444180522564\n",
            "Weighted Precision: 0.7568265433199957\n",
            "Micro Precision: 0.7553444180522565\n",
            "Weighted Recall: 0.7553444180522565\n",
            "Micro Recall: 0.7553444180522565\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_1[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_roberta_model[0][0]']       \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 15)           7695        ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 8  1  8 ... 10 11  9]\n",
            "[   1    2    3 ... 8417 8418 8419]\n",
            "[6622   35 6713 4989 8233 7444 5061 3942 8344  454 5464  317 2883 4515\n",
            " 3001 3410 6991 2875 1125 3885 6647 4857 4504  214 3230 8340 2969 3847\n",
            " 4609 5092 1346 5993 7358 2752 7651 7301 8188 7073 4782 6069 2346 6445\n",
            " 6483  542 1663 4952 5072 1967 4071 7590  860 6418 5030 7297  626 5463\n",
            " 6029 6302 3819 7256 2950 5667 1235 3993 1439 1323 4263 3835 2134 2624\n",
            " 5801 4293 8129  320 5866 3247 5777   19 6059 6520 4837 3843 5695 1666\n",
            "  858 1418 5409 5274 6205 3167 2356 8418 5997 6685  223 3444 3310 7721\n",
            " 5928 2360 7197 4252 8343  909 4093 4354 4752 1421  649  829 1916 1003\n",
            " 3488 2496 5765 6612 4004 4052 1403 6697 5268 1010 6293 6380 4602 6057\n",
            " 1851 4577 4236 7978 1350 5083 6552 1058  361 7706 4186 2590 4452 4396\n",
            "  749  870 8351 3834  812 5108  463 1743 6600 8264 8064 7121 8279 5556\n",
            " 1845 5119 2516 4673 7947 8215 1882 3350 7617 7152 5199 4282 1092 3342\n",
            " 8003 5025 5051  535  933 3969 1927 3615 1221 8062 1489 3296 7442 5459\n",
            " 2031 7351  234 2313 8015 7416 7921 6574 6257 8396  941 4520 3300 5640\n",
            "  240 7810 7425 1321 6580  928 1170 5404 8273 6742 7005   94 7459    9\n",
            " 7861 7975 1018 5029 2322 2140 1415 7061 7767 4990 8124 5927 6051 3269\n",
            " 6916 5959 2846 6818 7052 7775 7582 8116 4240 4767 5248  430 1812 2175\n",
            " 6055 8331 3916  998 7271 1564 7326 1077 1559  292 3065 6764 6854 4915\n",
            " 5552 3583 1917  868 5902 4229 2144 7565 3712 7224 4502 1406 1651 6256\n",
            " 3827 6525 6318 2686 3616 5369 8235 1704 1011 3351 8089 8021  102  224\n",
            " 5671 8294 1510 7707 2985 6624 5146 1428 7381 3709 6824 6308  701 2281\n",
            " 6837 5436 1741 3520 5995 2649 4886  966 2863 5476 4058 1246 2406 1199\n",
            " 8387  513 5449 1028 2226 6166 4387  350 6123  678 5236 5041  734 3412\n",
            " 6085  587  489 5760 3163 3376 4982 5115 4655 5575 3839 6743 7173 5724\n",
            " 7689 7787  520  713 3214 8302 3150 5713 2247 8128 4607 6660 3505 4610\n",
            " 2866 4161 6905 7096  768 1062 4411 6087 2096   21 4903 7546 6630 2928\n",
            " 4640 7125 1792 2063 1700 2305 7928 8095  737 5849  922 3423 2359 6015\n",
            " 4489 4361 6081 6346 5014 5167 1023 8009 3335  268 4777 5221 4323 6168\n",
            "  759  711 5702 6129 7181 1860 7192 1347 1692 5467 3378 6579   67 2693\n",
            " 6134 4841 4270 3073 8397 4611 2943 6746 7696 3466 3950 4693 6125 8370\n",
            " 6142 6131  995 5548 2186 2112 1721 5067 4015 5587 7092 3156   25 1749\n",
            " 5554 4878 1197 8248  458 8035 6836 1006 5310 2698 6960 2339 1650 6227\n",
            " 4104 5130 7089 6993 2400 1289 4305 5585 2524 7933 6284 7057 5324 3807\n",
            " 5790 8347 2390 5280 2841 7220 1384 7693 8090 2671 4171 5260  133 7451\n",
            " 3957 6793 1719 7379 4743 4746 5435 6216 4858 4417 8355 4492 2648 5104\n",
            " 5278 6539  743 7308  158 7543 2168 5913 7111  864 3414 5788 7592   39\n",
            " 7157 5647 4555 3956 1045 3617 4630 4384 6596   96 4539 6749 7202 4710\n",
            " 2413 4749 5122 5052 1598 5929  252 5565  336 6008 4564 5580 7237 1833\n",
            " 1466 1568 5958 7657 4327 5091 6918 6487 4082 2989 7448 4715   81 3382\n",
            "  467 3784 4624  764 4419 1117 2962 7954 7122 1430 6392  505 3174 1534\n",
            "  323 4164 3659 7526  498 5944 3510 4572 2307 2929 7480 4412  731 4563\n",
            " 7692 1869 2472 6966 4238 4702 6549 1594 2276 7269 2545 2216  605 6587\n",
            " 6315 5297 5282 4816 3200 1619 5504 2536 6399 3459 3917 6342 5754 5721\n",
            " 7385 7909 6126 7382 8382 4383 5502 7130  485 5862 4936 7674 3575 5372\n",
            " 3893 7109 4605 5211 5462 8065   50 4080 3334 8167 8378 5595 1873 8207\n",
            " 7076 4106 3719 4627  395 2685  231 2405 5419 6973 3480 3246 5414 3593\n",
            " 5931 6252 8208 4825 6203 3019 1105 5822   63 5854 6543 5787 4764 6701\n",
            "  334 6562 3822 4861 3051 1315 4805  246 8318 1089 8169 8218 2756 7549\n",
            "  707 1284 1076 2465 3815  540 4636 3188  697   80 1455  381 7025 7127\n",
            " 3992 4026 2777 4961  383 7531  744 6640 1047 6684 4297 7939  150 7384\n",
            "  446 7738 3287 6675 7752 5279 8196 3020 1203 4192 1552 4253 2887 2556\n",
            " 5729 1898 3208 5973 7033 1431 5915 8312 5799 7105  511 6404 2852 1561\n",
            " 4150 4835 1178]\n",
            "Sun Jun  5 04:35:34 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    39W / 300W |  15237MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "826/826 [==============================] - 300s 345ms/step - loss: 7.4457 - accuracy: 0.6052 - val_loss: 6.7492 - val_accuracy: 0.7360\n",
            "Epoch 2/5\n",
            "826/826 [==============================] - 284s 344ms/step - loss: 6.5203 - accuracy: 0.7600 - val_loss: 6.3386 - val_accuracy: 0.7442\n",
            "Epoch 3/5\n",
            "826/826 [==============================] - 284s 344ms/step - loss: 5.9868 - accuracy: 0.8128 - val_loss: 6.0235 - val_accuracy: 0.7483\n",
            "Epoch 4/5\n",
            "826/826 [==============================] - 285s 345ms/step - loss: 5.5240 - accuracy: 0.8581 - val_loss: 5.6090 - val_accuracy: 0.7784\n",
            "Epoch 5/5\n",
            "826/826 [==============================] - 280s 340ms/step - loss: 5.0926 - accuracy: 0.8944 - val_loss: 5.4260 - val_accuracy: 0.7688\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_3[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_roberta_model[1][0]']       \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 15)           7695        ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.7529691211401425\n",
            "Weighted F1: 0.7487744873306905\n",
            "Micro F1: 0.7529691211401426\n",
            "Weighted Precision: 0.7506744360991452\n",
            "Micro Precision: 0.7529691211401425\n",
            "Weighted Recall: 0.7529691211401425\n",
            "Micro Recall: 0.7529691211401425\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_1[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_roberta_model[0][0]']       \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 15)           7695        ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 8  1  8 ... 10 11  9]\n",
            "[   1    2    3 ... 8417 8418 8419]\n",
            "[6622   35 6713 4989 8233 7444 5061 3942 8344  454 5464  317 2883 4515\n",
            " 3001 3410 6991 2875 1125 3885 6647 4857 4504  214 3230 8340 2969 3847\n",
            " 4609 5092 1346 5993 7358 2752 7651 7301 8188 7073 4782 6069 2346 6445\n",
            " 6483  542 1663 4952 5072 1967 4071 7590  860 6418 5030 7297  626 5463\n",
            " 6029 6302 3819 7256 2950 5667 1235 3993 1439 1323 4263 3835 2134 2624\n",
            " 5801 4293 8129  320 5866 3247 5777   19 6059 6520 4837 3843 5695 1666\n",
            "  858 1418 5409 5274 6205 3167 2356 8418 5997 6685  223 3444 3310 7721\n",
            " 5928 2360 7197 4252 8343  909 4093 4354 4752 1421  649  829 1916 1003\n",
            " 3488 2496 5765 6612 4004 4052 1403 6697 5268 1010 6293 6380 4602 6057\n",
            " 1851 4577 4236 7978 1350 5083 6552 1058  361 7706 4186 2590 4452 4396\n",
            "  749  870 8351 3834  812 5108  463 1743 6600 8264 8064 7121 8279 5556\n",
            " 1845 5119 2516 4673 7947 8215 1882 3350 7617 7152 5199 4282 1092 3342\n",
            " 8003 5025 5051  535  933 3969 1927 3615 1221 8062 1489 3296 7442 5459\n",
            " 2031 7351  234 2313 8015 7416 7921 6574 6257 8396  941 4520 3300 5640\n",
            "  240 7810 7425 1321 6580  928 1170 5404 8273 6742 7005   94 7459    9\n",
            " 7861 7975 1018 5029 2322 2140 1415 7061 7767 4990 8124 5927 6051 3269\n",
            " 6916 5959 2846 6818 7052 7775 7582 8116 4240 4767 5248  430 1812 2175\n",
            " 6055 8331 3916  998 7271 1564 7326 1077 1559  292 3065 6764 6854 4915\n",
            " 5552 3583 1917  868 5902 4229 2144 7565 3712 7224 4502 1406 1651 6256\n",
            " 3827 6525 6318 2686 3616 5369 8235 1704 1011 3351 8089 8021  102  224\n",
            " 5671 8294 1510 7707 2985 6624 5146 1428 7381 3709 6824 6308  701 2281\n",
            " 6837 5436 1741 3520 5995 2649 4886  966 2863 5476 4058 1246 2406 1199\n",
            " 8387  513 5449 1028 2226 6166 4387  350 6123  678 5236 5041  734 3412\n",
            " 6085  587  489 5760 3163 3376 4982 5115 4655 5575 3839 6743 7173 5724\n",
            " 7689 7787  520  713 3214 8302 3150 5713 2247 8128 4607 6660 3505 4610\n",
            " 2866 4161 6905 7096  768 1062 4411 6087 2096   21 4903 7546 6630 2928\n",
            " 4640 7125 1792 2063 1700 2305 7928 8095  737 5849  922 3423 2359 6015\n",
            " 4489 4361 6081 6346 5014 5167 1023 8009 3335  268 4777 5221 4323 6168\n",
            "  759  711 5702 6129 7181 1860 7192 1347 1692 5467 3378 6579   67 2693\n",
            " 6134 4841 4270 3073 8397 4611 2943 6746 7696 3466 3950 4693 6125 8370\n",
            " 6142 6131  995 5548 2186 2112 1721 5067 4015 5587 7092 3156   25 1749\n",
            " 5554 4878 1197 8248  458 8035 6836 1006 5310 2698 6960 2339 1650 6227\n",
            " 4104 5130 7089 6993 2400 1289 4305 5585 2524 7933 6284 7057 5324 3807\n",
            " 5790 8347 2390 5280 2841 7220 1384 7693 8090 2671 4171 5260  133 7451\n",
            " 3957 6793 1719 7379 4743 4746 5435 6216 4858 4417 8355 4492 2648 5104\n",
            " 5278 6539  743 7308  158 7543 2168 5913 7111  864 3414 5788 7592   39\n",
            " 7157 5647 4555 3956 1045 3617 4630 4384 6596   96 4539 6749 7202 4710\n",
            " 2413 4749 5122 5052 1598 5929  252 5565  336 6008 4564 5580 7237 1833\n",
            " 1466 1568 5958 7657 4327 5091 6918 6487 4082 2989 7448 4715   81 3382\n",
            "  467 3784 4624  764 4419 1117 2962 7954 7122 1430 6392  505 3174 1534\n",
            "  323 4164 3659 7526  498 5944 3510 4572 2307 2929 7480 4412  731 4563\n",
            " 7692 1869 2472 6966 4238 4702 6549 1594 2276 7269 2545 2216  605 6587\n",
            " 6315 5297 5282 4816 3200 1619 5504 2536 6399 3459 3917 6342 5754 5721\n",
            " 7385 7909 6126 7382 8382 4383 5502 7130  485 5862 4936 7674 3575 5372\n",
            " 3893 7109 4605 5211 5462 8065   50 4080 3334 8167 8378 5595 1873 8207\n",
            " 7076 4106 3719 4627  395 2685  231 2405 5419 6973 3480 3246 5414 3593\n",
            " 5931 6252 8208 4825 6203 3019 1105 5822   63 5854 6543 5787 4764 6701\n",
            "  334 6562 3822 4861 3051 1315 4805  246 8318 1089 8169 8218 2756 7549\n",
            "  707 1284 1076 2465 3815  540 4636 3188  697   80 1455  381 7025 7127\n",
            " 3992 4026 2777 4961  383 7531  744 6640 1047 6684 4297 7939  150 7384\n",
            "  446 7738 3287 6675 7752 5279 8196 3020 1203 4192 1552 4253 2887 2556\n",
            " 5729 1898 3208 5973 7033 1431 5915 8312 5799 7105  511 6404 2852 1561\n",
            " 4150 4835 1178]\n",
            "Sun Jun  5 05:01:18 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    39W / 300W |  15237MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "826/826 [==============================] - 300s 346ms/step - loss: 7.4403 - accuracy: 0.5999 - val_loss: 6.7549 - val_accuracy: 0.7415\n",
            "Epoch 2/5\n",
            "826/826 [==============================] - 285s 345ms/step - loss: 6.5093 - accuracy: 0.7552 - val_loss: 6.3078 - val_accuracy: 0.7442\n",
            "Epoch 3/5\n",
            "826/826 [==============================] - 281s 340ms/step - loss: 5.9579 - accuracy: 0.8214 - val_loss: 6.0841 - val_accuracy: 0.7319\n",
            "Epoch 4/5\n",
            "826/826 [==============================] - 285s 345ms/step - loss: 5.5045 - accuracy: 0.8564 - val_loss: 5.6593 - val_accuracy: 0.7606\n",
            "Epoch 5/5\n",
            "826/826 [==============================] - 285s 345ms/step - loss: 5.0766 - accuracy: 0.8929 - val_loss: 5.3982 - val_accuracy: 0.7852\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_3[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_roberta_model[1][0]']       \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 15)           7695        ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.7684085510688836\n",
            "Weighted F1: 0.7712155292395787\n",
            "Micro F1: 0.7684085510688835\n",
            "Weighted Precision: 0.7793634047297409\n",
            "Micro Precision: 0.7684085510688836\n",
            "Weighted Recall: 0.7684085510688836\n",
            "Micro Recall: 0.7684085510688836\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_1[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_roberta_model[0][0]']       \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 15)           7695        ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 8  1  8 ... 10 11  9]\n",
            "[   1    2    3 ... 8417 8418 8419]\n",
            "[6622   35 6713 4989 8233 7444 5061 3942 8344  454 5464  317 2883 4515\n",
            " 3001 3410 6991 2875 1125 3885 6647 4857 4504  214 3230 8340 2969 3847\n",
            " 4609 5092 1346 5993 7358 2752 7651 7301 8188 7073 4782 6069 2346 6445\n",
            " 6483  542 1663 4952 5072 1967 4071 7590  860 6418 5030 7297  626 5463\n",
            " 6029 6302 3819 7256 2950 5667 1235 3993 1439 1323 4263 3835 2134 2624\n",
            " 5801 4293 8129  320 5866 3247 5777   19 6059 6520 4837 3843 5695 1666\n",
            "  858 1418 5409 5274 6205 3167 2356 8418 5997 6685  223 3444 3310 7721\n",
            " 5928 2360 7197 4252 8343  909 4093 4354 4752 1421  649  829 1916 1003\n",
            " 3488 2496 5765 6612 4004 4052 1403 6697 5268 1010 6293 6380 4602 6057\n",
            " 1851 4577 4236 7978 1350 5083 6552 1058  361 7706 4186 2590 4452 4396\n",
            "  749  870 8351 3834  812 5108  463 1743 6600 8264 8064 7121 8279 5556\n",
            " 1845 5119 2516 4673 7947 8215 1882 3350 7617 7152 5199 4282 1092 3342\n",
            " 8003 5025 5051  535  933 3969 1927 3615 1221 8062 1489 3296 7442 5459\n",
            " 2031 7351  234 2313 8015 7416 7921 6574 6257 8396  941 4520 3300 5640\n",
            "  240 7810 7425 1321 6580  928 1170 5404 8273 6742 7005   94 7459    9\n",
            " 7861 7975 1018 5029 2322 2140 1415 7061 7767 4990 8124 5927 6051 3269\n",
            " 6916 5959 2846 6818 7052 7775 7582 8116 4240 4767 5248  430 1812 2175\n",
            " 6055 8331 3916  998 7271 1564 7326 1077 1559  292 3065 6764 6854 4915\n",
            " 5552 3583 1917  868 5902 4229 2144 7565 3712 7224 4502 1406 1651 6256\n",
            " 3827 6525 6318 2686 3616 5369 8235 1704 1011 3351 8089 8021  102  224\n",
            " 5671 8294 1510 7707 2985 6624 5146 1428 7381 3709 6824 6308  701 2281\n",
            " 6837 5436 1741 3520 5995 2649 4886  966 2863 5476 4058 1246 2406 1199\n",
            " 8387  513 5449 1028 2226 6166 4387  350 6123  678 5236 5041  734 3412\n",
            " 6085  587  489 5760 3163 3376 4982 5115 4655 5575 3839 6743 7173 5724\n",
            " 7689 7787  520  713 3214 8302 3150 5713 2247 8128 4607 6660 3505 4610\n",
            " 2866 4161 6905 7096  768 1062 4411 6087 2096   21 4903 7546 6630 2928\n",
            " 4640 7125 1792 2063 1700 2305 7928 8095  737 5849  922 3423 2359 6015\n",
            " 4489 4361 6081 6346 5014 5167 1023 8009 3335  268 4777 5221 4323 6168\n",
            "  759  711 5702 6129 7181 1860 7192 1347 1692 5467 3378 6579   67 2693\n",
            " 6134 4841 4270 3073 8397 4611 2943 6746 7696 3466 3950 4693 6125 8370\n",
            " 6142 6131  995 5548 2186 2112 1721 5067 4015 5587 7092 3156   25 1749\n",
            " 5554 4878 1197 8248  458 8035 6836 1006 5310 2698 6960 2339 1650 6227\n",
            " 4104 5130 7089 6993 2400 1289 4305 5585 2524 7933 6284 7057 5324 3807\n",
            " 5790 8347 2390 5280 2841 7220 1384 7693 8090 2671 4171 5260  133 7451\n",
            " 3957 6793 1719 7379 4743 4746 5435 6216 4858 4417 8355 4492 2648 5104\n",
            " 5278 6539  743 7308  158 7543 2168 5913 7111  864 3414 5788 7592   39\n",
            " 7157 5647 4555 3956 1045 3617 4630 4384 6596   96 4539 6749 7202 4710\n",
            " 2413 4749 5122 5052 1598 5929  252 5565  336 6008 4564 5580 7237 1833\n",
            " 1466 1568 5958 7657 4327 5091 6918 6487 4082 2989 7448 4715   81 3382\n",
            "  467 3784 4624  764 4419 1117 2962 7954 7122 1430 6392  505 3174 1534\n",
            "  323 4164 3659 7526  498 5944 3510 4572 2307 2929 7480 4412  731 4563\n",
            " 7692 1869 2472 6966 4238 4702 6549 1594 2276 7269 2545 2216  605 6587\n",
            " 6315 5297 5282 4816 3200 1619 5504 2536 6399 3459 3917 6342 5754 5721\n",
            " 7385 7909 6126 7382 8382 4383 5502 7130  485 5862 4936 7674 3575 5372\n",
            " 3893 7109 4605 5211 5462 8065   50 4080 3334 8167 8378 5595 1873 8207\n",
            " 7076 4106 3719 4627  395 2685  231 2405 5419 6973 3480 3246 5414 3593\n",
            " 5931 6252 8208 4825 6203 3019 1105 5822   63 5854 6543 5787 4764 6701\n",
            "  334 6562 3822 4861 3051 1315 4805  246 8318 1089 8169 8218 2756 7549\n",
            "  707 1284 1076 2465 3815  540 4636 3188  697   80 1455  381 7025 7127\n",
            " 3992 4026 2777 4961  383 7531  744 6640 1047 6684 4297 7939  150 7384\n",
            "  446 7738 3287 6675 7752 5279 8196 3020 1203 4192 1552 4253 2887 2556\n",
            " 5729 1898 3208 5973 7033 1431 5915 8312 5799 7105  511 6404 2852 1561\n",
            " 4150 4835 1178]\n",
            "Sun Jun  5 05:27:02 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    39W / 300W |  15237MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "826/826 [==============================] - 300s 346ms/step - loss: 7.4434 - accuracy: 0.5904 - val_loss: 6.6986 - val_accuracy: 0.7387\n",
            "Epoch 2/5\n",
            "826/826 [==============================] - 281s 340ms/step - loss: 6.4398 - accuracy: 0.7659 - val_loss: 6.3003 - val_accuracy: 0.7373\n",
            "Epoch 3/5\n",
            "826/826 [==============================] - 285s 345ms/step - loss: 5.9079 - accuracy: 0.8203 - val_loss: 5.9861 - val_accuracy: 0.7415\n",
            "Epoch 4/5\n",
            "826/826 [==============================] - 284s 344ms/step - loss: 5.4328 - accuracy: 0.8629 - val_loss: 5.5797 - val_accuracy: 0.7633\n",
            "Epoch 5/5\n",
            "826/826 [==============================] - 285s 345ms/step - loss: 5.0012 - accuracy: 0.8979 - val_loss: 5.2858 - val_accuracy: 0.7811\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_3[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_roberta_model[1][0]']       \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 15)           7695        ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.7648456057007126\n",
            "Weighted F1: 0.7678132485747801\n",
            "Micro F1: 0.7648456057007126\n",
            "Weighted Precision: 0.7762229665308458\n",
            "Micro Precision: 0.7648456057007126\n",
            "Weighted Recall: 0.7648456057007126\n",
            "Micro Recall: 0.7648456057007126\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_1[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_roberta_model[0][0]']       \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 15)           7695        ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 8  1  8 ... 10 11  9]\n",
            "[   1    2    3 ... 8417 8418 8419]\n",
            "[6622   35 6713 4989 8233 7444 5061 3942 8344  454 5464  317 2883 4515\n",
            " 3001 3410 6991 2875 1125 3885 6647 4857 4504  214 3230 8340 2969 3847\n",
            " 4609 5092 1346 5993 7358 2752 7651 7301 8188 7073 4782 6069 2346 6445\n",
            " 6483  542 1663 4952 5072 1967 4071 7590  860 6418 5030 7297  626 5463\n",
            " 6029 6302 3819 7256 2950 5667 1235 3993 1439 1323 4263 3835 2134 2624\n",
            " 5801 4293 8129  320 5866 3247 5777   19 6059 6520 4837 3843 5695 1666\n",
            "  858 1418 5409 5274 6205 3167 2356 8418 5997 6685  223 3444 3310 7721\n",
            " 5928 2360 7197 4252 8343  909 4093 4354 4752 1421  649  829 1916 1003\n",
            " 3488 2496 5765 6612 4004 4052 1403 6697 5268 1010 6293 6380 4602 6057\n",
            " 1851 4577 4236 7978 1350 5083 6552 1058  361 7706 4186 2590 4452 4396\n",
            "  749  870 8351 3834  812 5108  463 1743 6600 8264 8064 7121 8279 5556\n",
            " 1845 5119 2516 4673 7947 8215 1882 3350 7617 7152 5199 4282 1092 3342\n",
            " 8003 5025 5051  535  933 3969 1927 3615 1221 8062 1489 3296 7442 5459\n",
            " 2031 7351  234 2313 8015 7416 7921 6574 6257 8396  941 4520 3300 5640\n",
            "  240 7810 7425 1321 6580  928 1170 5404 8273 6742 7005   94 7459    9\n",
            " 7861 7975 1018 5029 2322 2140 1415 7061 7767 4990 8124 5927 6051 3269\n",
            " 6916 5959 2846 6818 7052 7775 7582 8116 4240 4767 5248  430 1812 2175\n",
            " 6055 8331 3916  998 7271 1564 7326 1077 1559  292 3065 6764 6854 4915\n",
            " 5552 3583 1917  868 5902 4229 2144 7565 3712 7224 4502 1406 1651 6256\n",
            " 3827 6525 6318 2686 3616 5369 8235 1704 1011 3351 8089 8021  102  224\n",
            " 5671 8294 1510 7707 2985 6624 5146 1428 7381 3709 6824 6308  701 2281\n",
            " 6837 5436 1741 3520 5995 2649 4886  966 2863 5476 4058 1246 2406 1199\n",
            " 8387  513 5449 1028 2226 6166 4387  350 6123  678 5236 5041  734 3412\n",
            " 6085  587  489 5760 3163 3376 4982 5115 4655 5575 3839 6743 7173 5724\n",
            " 7689 7787  520  713 3214 8302 3150 5713 2247 8128 4607 6660 3505 4610\n",
            " 2866 4161 6905 7096  768 1062 4411 6087 2096   21 4903 7546 6630 2928\n",
            " 4640 7125 1792 2063 1700 2305 7928 8095  737 5849  922 3423 2359 6015\n",
            " 4489 4361 6081 6346 5014 5167 1023 8009 3335  268 4777 5221 4323 6168\n",
            "  759  711 5702 6129 7181 1860 7192 1347 1692 5467 3378 6579   67 2693\n",
            " 6134 4841 4270 3073 8397 4611 2943 6746 7696 3466 3950 4693 6125 8370\n",
            " 6142 6131  995 5548 2186 2112 1721 5067 4015 5587 7092 3156   25 1749\n",
            " 5554 4878 1197 8248  458 8035 6836 1006 5310 2698 6960 2339 1650 6227\n",
            " 4104 5130 7089 6993 2400 1289 4305 5585 2524 7933 6284 7057 5324 3807\n",
            " 5790 8347 2390 5280 2841 7220 1384 7693 8090 2671 4171 5260  133 7451\n",
            " 3957 6793 1719 7379 4743 4746 5435 6216 4858 4417 8355 4492 2648 5104\n",
            " 5278 6539  743 7308  158 7543 2168 5913 7111  864 3414 5788 7592   39\n",
            " 7157 5647 4555 3956 1045 3617 4630 4384 6596   96 4539 6749 7202 4710\n",
            " 2413 4749 5122 5052 1598 5929  252 5565  336 6008 4564 5580 7237 1833\n",
            " 1466 1568 5958 7657 4327 5091 6918 6487 4082 2989 7448 4715   81 3382\n",
            "  467 3784 4624  764 4419 1117 2962 7954 7122 1430 6392  505 3174 1534\n",
            "  323 4164 3659 7526  498 5944 3510 4572 2307 2929 7480 4412  731 4563\n",
            " 7692 1869 2472 6966 4238 4702 6549 1594 2276 7269 2545 2216  605 6587\n",
            " 6315 5297 5282 4816 3200 1619 5504 2536 6399 3459 3917 6342 5754 5721\n",
            " 7385 7909 6126 7382 8382 4383 5502 7130  485 5862 4936 7674 3575 5372\n",
            " 3893 7109 4605 5211 5462 8065   50 4080 3334 8167 8378 5595 1873 8207\n",
            " 7076 4106 3719 4627  395 2685  231 2405 5419 6973 3480 3246 5414 3593\n",
            " 5931 6252 8208 4825 6203 3019 1105 5822   63 5854 6543 5787 4764 6701\n",
            "  334 6562 3822 4861 3051 1315 4805  246 8318 1089 8169 8218 2756 7549\n",
            "  707 1284 1076 2465 3815  540 4636 3188  697   80 1455  381 7025 7127\n",
            " 3992 4026 2777 4961  383 7531  744 6640 1047 6684 4297 7939  150 7384\n",
            "  446 7738 3287 6675 7752 5279 8196 3020 1203 4192 1552 4253 2887 2556\n",
            " 5729 1898 3208 5973 7033 1431 5915 8312 5799 7105  511 6404 2852 1561\n",
            " 4150 4835 1178]\n",
            "Sun Jun  5 05:52:44 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0    39W / 300W |  15237MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "826/826 [==============================] - 300s 345ms/step - loss: 7.5090 - accuracy: 0.5760 - val_loss: 6.7729 - val_accuracy: 0.7237\n",
            "Epoch 2/5\n",
            "826/826 [==============================] - 285s 345ms/step - loss: 6.4990 - accuracy: 0.7687 - val_loss: 6.3471 - val_accuracy: 0.7319\n",
            "Epoch 3/5\n",
            "826/826 [==============================] - 285s 345ms/step - loss: 5.9730 - accuracy: 0.8244 - val_loss: 6.0792 - val_accuracy: 0.7483\n",
            "Epoch 4/5\n",
            "826/826 [==============================] - 286s 347ms/step - loss: 5.5164 - accuracy: 0.8650 - val_loss: 5.6781 - val_accuracy: 0.7702\n",
            "Epoch 5/5\n",
            "826/826 [==============================] - 285s 345ms/step - loss: 5.0832 - accuracy: 0.8982 - val_loss: 5.4248 - val_accuracy: 0.7743\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_3[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_roberta_model[1][0]']       \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 15)           7695        ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.7589073634204275\n",
            "Weighted F1: 0.7604899885238907\n",
            "Micro F1: 0.7589073634204275\n",
            "Weighted Precision: 0.7696632531925779\n",
            "Micro Precision: 0.7589073634204275\n",
            "Weighted Recall: 0.7589073634204275\n",
            "Micro Recall: 0.7589073634204275\n",
            "Average Accuracy: 0.7600950118764844\n",
            "Average Weighted F1: 0.760094933369364\n",
            "Average Micro F1: 0.7600950118764844\n",
            "Average Weighted Precision: 0.766550120774461\n",
            "Average Micro Precision: 0.7600950118764844\n",
            "Average Weighted Recall: 0.7600950118764844\n",
            "Average Micro Recall: 0.7600950118764844\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "total_accuracy=0\n",
        "total_weighted_f1=0\n",
        "total_micro_f1=0\n",
        "total_weighted_precision=0\n",
        "total_micro_precision=0\n",
        "total_weighted_recall=0\n",
        "total_micro_recall=0\n",
        "\n",
        "for f in range(5):\n",
        "  gc.collect()\n",
        "  tf.keras.backend.clear_session()\n",
        "  dbert_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "  dbert_model = TFRobertaModel.from_pretrained('roberta-base')\n",
        "  max_len=512\n",
        "  sentences=summarized_data['text']\n",
        "  labels=summarized_data['label']\n",
        "  filenumbers=summarized_data['filenum']\n",
        "  len(sentences),len(labels),len(filenumbers)\n",
        "  \n",
        "  model_1=create_model()\n",
        "  input_ids=[]\n",
        "  attention_masks=[]\n",
        "\n",
        "  for sent in sentences:\n",
        "    dbert_inps=dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n",
        "    input_ids.append(dbert_inps['input_ids'])\n",
        "    attention_masks.append(dbert_inps['attention_mask'])\n",
        "  input_ids=np.asarray(input_ids)\n",
        "\n",
        "  attention_masks=np.array(attention_masks)\n",
        "  labels=np.array(labels)\n",
        "  filenumbers=[int(i) for i in filenumbers]\n",
        "  filenumbers=np.array(filenumbers)\n",
        "  print(labels)\n",
        "  print(filenumbers)\n",
        "\n",
        "  #train_inp,val_inp,train_label,val_label,train_mask,val_mask,train_fnum,val_fnum=train_test_split(input_ids,labels,attention_masks,filenumbers,test_size=0.1,random_state=42)\n",
        "\n",
        "  new_train_inp_1=[]\n",
        "  new_val_inp_1=[]\n",
        "  new_train_label_1=[]\n",
        "  new_val_label_1=[]\n",
        "  new_train_mask_1=[]\n",
        "  new_train_fnum_1=[]\n",
        "  new_val_fnum_1=[]\n",
        "  new_val_mask_1=[]\n",
        "  for i in range(len(train_fnum)):\n",
        "    if train_fnum[i] in filenumbers:\n",
        "      new_train_inp_1.append(train_inp[i])\n",
        "      #new_val_inp.append(val_inp[i])\n",
        "      new_train_label_1.append(train_label[i])\n",
        "      #new_val_label.append(val_label[i])\n",
        "      new_train_mask_1.append(train_mask[i])\n",
        "      #new_val_mask.append(val_mask[i])\n",
        "      new_train_fnum_1.append(train_fnum[i])\n",
        "\n",
        "  for i in range(len(val_fnum)):\n",
        "    if val_fnum[i] in filenumbers:\n",
        "      #new_train_inp.append(train_inp[i])\n",
        "      new_val_inp_1.append(val_inp[i])\n",
        "      #new_train_label.append(train_label[i])\n",
        "      new_val_label_1.append(val_label[i])\n",
        "      #new_train_mask.append(train_mask[i])\n",
        "      new_val_mask_1.append(val_mask[i])\n",
        "      new_val_fnum_1.append(val_fnum[i])\n",
        "\n",
        "  # print(new_train_label)\n",
        "  # print(len(new_train_label))\n",
        "  # print(new_val_label)\n",
        "  # print(len(new_val_label))\n",
        "  new_train_inp_1=np.array(new_train_inp_1)\n",
        "  new_val_inp_1=np.array(new_val_inp_1)\n",
        "  new_train_label_1=np.array(new_train_label_1)\n",
        "  new_val_label_1=np.array(new_val_label_1)\n",
        "  new_train_mask_1=np.array(new_train_mask_1)\n",
        "  new_train_fnum_1=np.array(new_train_fnum_1)\n",
        "  new_val_fnum_1=np.array(new_val_fnum_1)\n",
        "  new_val_mask_1=np.array(new_val_mask_1)\n",
        "\n",
        "  print(new_val_fnum_1)\n",
        "  \n",
        "  log_dir='dbert_model'\n",
        "\n",
        "  model_save_path='./drive/MyDrive/Ensemble/roberta-ensemble-512-model_1-'+str(f)+'-15labels.h5'\n",
        "\n",
        "  loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "  accuracy = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
        "  callbacks= [tf.keras.callbacks.ModelCheckpoint(filepath=model_save_path,monitor='val_accuracy',mode='max',save_best_only=True,save_weights_only=True),keras.callbacks.TensorBoard(log_dir=log_dir)]\n",
        "  model_1.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])\n",
        "  gpu_info = !nvidia-smi\n",
        "  gpu_info = '\\n'.join(gpu_info)\n",
        "  if gpu_info.find('failed') >= 0:\n",
        "    print('Not connected to a GPU')\n",
        "  else:\n",
        "    print(gpu_info)\n",
        "  \n",
        "  history=model_1.fit([new_train_inp_1,new_train_mask_1],new_train_label_1,batch_size=8,epochs=5,validation_data=([new_val_inp_1,new_val_mask_1],new_val_label_1),callbacks=callbacks)\n",
        "\n",
        "  pred_labels=[]\n",
        "\n",
        "  model_saved_1= create_model()\n",
        "  model_saved_1.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])\n",
        "  model_saved_1.load_weights('./drive/MyDrive/Ensemble/roberta-ensemble-512-model_1-'+str(f)+'-15labels.h5')\n",
        "\n",
        "  for i in range(0,len(val_inp)):\n",
        "    pred=model_saved_1.predict([val_inp[i].reshape(1,512),val_mask[i].reshape(1,512)])\n",
        "    pred_label = pred.argmax(axis=1)\n",
        "    pred_labels.append(pred_label)\n",
        "  accuracy=accuracy_score(val_label, pred_labels)\n",
        "  print(\"Accuracy: \"+str(accuracy))\n",
        "  total_accuracy=total_accuracy+accuracy\n",
        "  \n",
        "  weighted_f1=f1_score(val_label,pred_labels, average='weighted')\n",
        "  print(\"Weighted F1: \"+ str(weighted_f1))\n",
        "  total_weighted_f1=total_weighted_f1+weighted_f1\n",
        "  micro_f1=f1_score(val_label,pred_labels, average='micro')\n",
        "  print(\"Micro F1: \"+ str(micro_f1))\n",
        "  total_micro_f1=total_micro_f1+micro_f1\n",
        "\n",
        "  weighted_precision=precision_score(val_label, pred_labels, average='weighted')\n",
        "  print(\"Weighted Precision: \" + str(weighted_precision))\n",
        "  total_weighted_precision=total_weighted_precision+weighted_precision\n",
        "  micro_precision=precision_score(val_label, pred_labels, average='micro')\n",
        "  print(\"Micro Precision: \" + str(micro_precision))\n",
        "  total_micro_precision=total_micro_precision+micro_precision\n",
        "\n",
        "  weighted_recall=recall_score(val_label, pred_labels, average='weighted')\n",
        "  print(\"Weighted Recall: \" + str(weighted_recall))\n",
        "  total_weighted_recall=total_weighted_recall+weighted_recall\n",
        "  micro_recall=recall_score(val_label, pred_labels, average='micro')\n",
        "  print(\"Micro Recall: \" + str(micro_recall))\n",
        "  total_micro_recall=total_micro_recall+micro_recall\n",
        "\n",
        "\n",
        "print(\"Average Accuracy: \"+str(total_accuracy/5))\n",
        "print(\"Average Weighted F1: \"+str(total_weighted_f1/5))\n",
        "print(\"Average Micro F1: \"+str(total_micro_f1/5))\n",
        "print(\"Average Weighted Precision: \"+str(total_weighted_precision/5))\n",
        "print(\"Average Micro Precision: \"+str(total_micro_precision/5))\n",
        "print(\"Average Weighted Recall: \"+str(total_weighted_recall/5))\n",
        "print(\"Average Micro Recall: \"+str(total_micro_recall/5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4Yijn8aNjtR",
        "outputId": "eb0142e9-4650-437c-8288-a905682648c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6722\n",
            "6722\n",
            "Average Length 511.5861350788456\n",
            "['8', '1', '8', '2', '8', '8', '8', '9', '7', '8', '1', '2', '1', '1', '8', '4', '8', '8', '12', '3', '3', '7', '3', '12', '1', '8', '8', '8', '8', '8', '8', '8', '1', '9', '5', '9', '9', '9', '11', '8', '8', '8', '4', '8', '8', '8', '8', '1', '3', '9', '3', '7', '1', '2', '9', '9', '7', '8', '8', '1', '10', '7', '8', '8', '9', '8', '7', '9', '9', '12', '7', '2', '8', '1', '11', '11', '1', '7', '7', '12', '1', '9', '8', '10', '12', '7', '8', '2', '8', '9', '9', '1', '8', '9', '1', '7', '12', '10', '10', '10', '8', '3', '7', '9', '8', '9', '1', '8', '8', '2', '7', '2', '9', '9', '11', '8', '8', '12', '12', '7', '8', '12', '4', '9', '3', '1', '12', '1', '1', '8', '8', '3', '8', '8', '8', '8', '9', '1', '8', '8', '10', '1', '8', '2', '8', '8', '7', '3', '8', '2', '4', '4', '9', '8', '10', '12', '12', '1', '1', '9', '1', '1', '1', '8', '2', '2', '8', '1', '1', '2', '2', '1', '2', '8', '1', '9', '9', '8', '8', '4', '2', '9', '9', '8', '3', '4', '3', '1', '8', '8', '2', '1', '9', '7', '8', '8', '1', '12', '3', '8', '2', '4', '2', '9', '12', '1', '4', '1', '8', '8', '8', '2', '2', '8', '9', '8', '8', '8', '10', '9', '8', '7', '9', '1', '1', '9', '4', '2', '4', '2', '2', '1', '7', '8', '11', '11', '3', '9', '2', '4', '8', '9', '1', '8', '1', '1', '4', '9', '1', '1', '8', '8', '2', '1', '8', '4', '2', '8', '9', '8', '8', '2', '8', '8', '8', '7', '1', '1', '1', '2', '1', '1', '8', '7', '8', '8', '12', '2', '12', '12', '8', '10', '12', '8', '3', '3', '12', '10', '1', '8', '12', '1', '8', '8', '2', '8', '4', '7', '8', '7', '10', '8', '10', '9', '8', '12', '12', '1', '8', '8', '3', '8', '8', '8', '8', '8', '1', '9', '8', '11', '1', '1', '1', '9', '8', '1', '9', '2', '3', '11', '8', '9', '9', '9', '2', '1', '8', '8', '9', '7', '1', '4', '9', '4', '8', '8', '4', '8', '12', '9', '4', '8', '2', '10', '10', '10', '8', '9', '9', '8', '8', '12', '7', '1', '8', '8', '8', '4', '1', '1', '1', '1', '1', '1', '8', '1', '9', '8', '9', '9', '4', '8', '12', '9', '8', '8', '2', '8', '8', '8', '6', '9', '8', '3', '7', '8', '8', '4', '12', '8', '8', '9', '12', '12', '9', '8', '2', '9', '2', '3', '1', '12', '8', '10', '9', '9', '9', '10', '10', '3', '8', '12', '1', '4', '2', '1', '10', '8', '2', '8', '4', '8', '9', '1', '9', '9', '10', '10', '1', '4', '9', '2', '4', '9', '1', '1', '3', '10', '3', '3', '8', '7', '3', '8', '9', '9', '12', '4', '8', '12', '2', '2', '4', '1', '9', '9', '4', '1', '4', '2', '8', '12', '2', '3', '10', '10', '9', '8', '9', '9', '1', '12', '8', '8', '8', '12', '4', '1', '8', '8', '1', '9', '8', '8', '2', '1', '8', '9', '8', '3', '3', '3', '1', '8', '8', '9', '1', '10', '9', '9', '9', '9', '5', '9', '9', '8', '8', '8', '8', '8', '8', '8', '8', '11', '12', '8', '8', '1', '8', '9', '11', '2', '2', '2', '2', '3', '1', '2', '2', '8', '2', '4', '9', '1', '2', '9', '8', '2', '8', '9', '9', '3', '10', '9', '9', '2', '8', '9', '8', '12', '12', '1', '3', '8', '8', '8', '2', '7', '7', '7', '7', '3', '9', '1', '9', '8', '9', '9', '1', '1', '1', '2', '9', '9', '9', '11', '1', '8', '8', '9', '1', '9', '8', '8', '8', '1', '1', '8', '7', '1', '1', '8', '8', '9', '4', '4', '8', '2', '2', '8', '8', '8', '8', '8', '8', '11', '8', '2', '9', '4', '9', '3', '9', '9', '1', '3', '9', '3', '1', '12', '8', '9', '12', '1', '8', '4', '2', '1', '4', '8', '3', '3', '8', '2', '8', '9', '7', '8', '8', '8', '5', '8', '3', '9', '8', '8', '13', '12', '1', '1', '2', '8', '4', '1', '9', '9', '12', '8', '9', '12', '9', '1', '9', '9', '9', '9', '3', '2', '9', '9', '4', '8', '12', '2', '4', '9', '3', '1', '9', '7', '8', '9', '9', '8', '4', '8', '8', '7', '9', '10', '3', '8', '8', '8', '1', '1', '1', '1', '8', '8', '4', '1', '10', '1', '5', '7', '7', '1', '8', '9', '3', '7', '2', '7', '7', '2', '4', '8', '12', '7', '4', '2', '9', '9', '12', '6', '10', '8', '2', '4', '12', '9', '9', '3', '8', '8', '1', '2', '10', '9', '9', '8', '4', '12', '2', '1', '8', '8', '8', '12', '10', '10', '9', '3', '8', '8', '9', '2', '8', '10', '1', '1', '1', '1', '2', '1', '1', '1', '1', '9', '8', '12', '9', '4', '8', '8', '9', '1', '9', '3', '9', '8', '8', '1', '7', '7', '10', '1', '8', '8', '1', '9', '8', '10', '3', '1', '7', '1', '8', '8', '12', '8', '8', '1', '8', '7', '1', '7', '7', '8', '2', '1', '8', '8', '2', '10', '8', '8', '8', '8', '8', '10', '1', '8', '8', '12', '8', '3', '3', '2', '2', '2', '10', '8', '8', '8', '2', '9', '1', '8', '9', '3', '2', '8', '10', '8', '6', '1', '1', '8', '4', '1', '9', '10', '8', '1', '7', '1', '2', '8', '1', '1', '1', '12', '1', '9', '12', '8', '12', '12', '12', '8', '8', '12', '4', '8', '8', '8', '8', '9', '9', '1', '3', '3', '3', '3', '1', '12', '12', '9', '10', '8', '8', '1', '9', '2', '2', '13', '9', '8', '9', '2', '1', '9', '1', '8', '8', '8', '4', '8', '1', '1', '1', '12', '12', '7', '2', '2', '2', '8', '3', '8', '9', '2', '10', '7', '8', '9', '2', '1', '2', '12', '12', '8', '8', '9', '2', '2', '9', '11', '1', '8', '1', '10', '9', '2', '1', '4', '7', '7', '7', '7', '7', '12', '8', '8', '8', '1', '1', '10', '1', '12', '1', '8', '2', '1', '1', '12', '8', '7', '9', '12', '8', '9', '3', '9', '8', '8', '8', '8', '3', '11', '2', '2', '9', '8', '8', '10', '8', '2', '7', '3', '1', '4', '7', '8', '8', '1', '8', '3', '7', '12', '8', '10', '9', '9', '8', '8', '2', '8', '9', '9', '1', '2', '8', '8', '9', '8', '3', '8', '1', '8', '10', '9', '8', '9', '9', '12', '4', '4', '8', '9', '9', '8', '2', '10', '1', '2', '8', '9', '1', '9', '9', '9', '7', '12', '12', '8', '1', '1', '1', '1', '8', '3', '1', '1', '8', '1', '8', '8', '7', '8', '8', '8', '8', '3', '2', '2', '10', '10', '10', '7', '8', '1', '2', '12', '7', '9', '8', '7', '8', '12', '2', '8', '9', '2', '6', '6', '7', '9', '8', '1', '8', '9', '8', '1', '12', '1', '2', '8', '7', '7', '7', '8', '2', '2', '8', '1', '2', '2', '9', '9', '1', '8', '8', '4', '3', '3', '1', '6', '3', '3', '12', '3', '8', '9', '1', '4', '3', '1', '8', '3', '9', '2', '8', '2', '8', '8', '8', '1', '1', '1', '9', '9', '8', '1', '9', '8', '1', '1', '3', '10', '8', '1', '1', '3', '9', '1', '4', '4', '1', '8', '9', '9', '2', '0', '0', '1', '8', '3', '1', '8', '8', '9', '8', '8', '1', '1', '8', '9', '8', '8', '8', '7', '9', '8', '8', '8', '10', '9', '8', '1', '2', '6', '1', '9', '9', '8', '12', '12', '12', '8', '8', '2', '8', '1', '2', '2', '2', '1', '9', '8', '2', '12', '2', '8', '12', '8', '9', '8', '8', '9', '7', '1', '1', '1', '1', '1', '8', '8', '1', '8', '8', '1', '1', '3', '2', '8', '8', '9', '10', '10', '2', '2', '1', '9', '2', '9', '9', '4', '12', '12', '12', '10', '7', '3', '3', '4', '2', '2', '9', '2', '8', '4', '2', '4', '1', '10', '9', '7', '8', '7', '1', '1', '3', '3', '1', '1', '3', '3', '3', '1', '1', '1', '1', '8', '2', '3', '1', '1', '2', '8', '8', '12', '8', '8', '8', '8', '11', '9', '1', '8', '9', '2', '8', '8', '8', '3', '9', '1', '9', '2', '7', '2', '8', '2', '8', '10', '8', '1', '10', '1', '1', '9', '9', '8', '8', '1', '8', '8', '8', '12', '8', '8', '8', '1', '8', '8', '8', '1', '9', '1', '1', '8', '1', '8', '9', '8', '2', '12', '9', '9', '0', '1', '8', '8', '1', '8', '12', '8', '8', '10', '8', '8', '8', '7', '8', '1', '8', '7', '3', '10', '1', '8', '9', '1', '8', '8', '8', '10', '1', '10', '3', '9', '1', '8', '9', '2', '8', '3', '3', '9', '9', '7', '9', '1', '1', '9', '2', '1', '1', '1', '7', '1', '1', '8', '8', '1', '1', '8', '1', '8', '3', '12', '9', '3', '3', '8', '8', '8', '8', '3', '1', '3', '3', '1', '11', '0', '8', '8', '7', '8', '12', '1', '8', '9', '8', '9', '8', '8', '3', '8', '8', '1', '1', '1', '9', '2', '2', '2', '8', '7', '12', '8', '8', '9', '10', '10', '7', '8', '1', '9', '8', '7', '3', '1', '3', '8', '2', '2', '3', '9', '8', '4', '4', '8', '9', '2', '1', '1', '7', '8', '9', '9', '7', '8', '7', '7', '8', '2', '2', '8', '4', '9', '7', '10', '0', '9', '8', '3', '7', '8', '1', '1', '8', '9', '9', '2', '2', '10', '1', '9', '10', '10', '10', '8', '3', '2', '12', '9', '9', '10', '12', '9', '12', '12', '9', '1', '2', '4', '12', '12', '7', '8', '9', '7', '7', '7', '3', '9', '8', '9', '1', '12', '8', '9', '4', '1', '3', '12', '12', '12', '12', '8', '8', '2', '1', '1', '2', '1', '1', '1', '12', '12', '8', '12', '2', '2', '12', '3', '3', '12', '8', '2', '8', '8', '12', '2', '1', '10', '3', '2', '8', '7', '1', '8', '1', '3', '7', '8', '9', '8', '3', '1', '1', '7', '8', '8', '9', '8', '2', '9', '2', '2', '9', '8', '1', '8', '8', '1', '3', '3', '1', '1', '10', '1', '2', '8', '1', '1', '1', '1', '9', '1', '4', '1', '7', '7', '7', '7', '2', '2', '8', '8', '12', '1', '9', '1', '7', '3', '3', '1', '8', '8', '10', '8', '9', '2', '9', '1', '3', '8', '8', '3', '12', '2', '8', '12', '2', '9', '1', '3', '3', '3', '3', '2', '8', '7', '9', '8', '3', '3', '1', '7', '8', '3', '1', '1', '12', '8', '9', '1', '2', '3', '8', '1', '1', '3', '3', '9', '1', '1', '1', '12', '1', '7', '3', '3', '1', '8', '8', '8', '1', '2', '4', '8', '1', '10', '2', '5', '3', '3', '12', '10', '9', '9', '12', '9', '0', '2', '8', '8', '9', '9', '9', '8', '1', '3', '1', '1', '4', '8', '1', '10', '8', '7', '2', '8', '2', '8', '4', '7', '8', '1', '9', '1', '9', '8', '2', '8', '2', '7', '9', '2', '2', '9', '1', '8', '12', '1', '8', '1', '4', '1', '9', '9', '1', '10', '12', '4', '8', '1', '7', '3', '9', '2', '12', '7', '8', '8', '2', '1', '12', '9', '8', '1', '2', '2', '8', '10', '2', '1', '7', '7', '7', '12', '3', '3', '8', '3', '8', '8', '3', '9', '8', '9', '1', '1', '8', '7', '9', '3', '3', '8', '1', '0', '9', '9', '9', '1', '8', '9', '9', '10', '1', '8', '7', '8', '8', '8', '9', '9', '8', '9', '10', '4', '9', '3', '7', '12', '1', '9', '9', '8', '9', '1', '9', '3', '1', '8', '4', '12', '10', '9', '8', '7', '10', '8', '12', '12', '3', '10', '8', '12', '2', '1', '2', '3', '9', '8', '8', '7', '1', '1', '2', '2', '1', '2', '2', '7', '1', '3', '9', '9', '3', '8', '8', '8', '8', '8', '7', '8', '8', '10', '8', '1', '8', '2', '8', '2', '2', '2', '2', '2', '1', '7', '12', '10', '1', '2', '8', '1', '4', '7', '8', '1', '12', '8', '7', '9', '2', '2', '2', '8', '1', '8', '1', '1', '0', '2', '2', '6', '1', '8', '2', '1', '8', '1', '1', '2', '2', '3', '2', '2', '10', '10', '9', '1', '7', '7', '8', '8', '1', '12', '10', '12', '3', '8', '8', '8', '3', '8', '3', '10', '2', '2', '2', '1', '2', '2', '1', '0', '1', '8', '9', '1', '1', '1', '2', '2', '2', '2', '2', '1', '1', '1', '1', '12', '9', '9', '1', '10', '11', '12', '9', '1', '1', '8', '8', '1', '2', '12', '8', '3', '7', '2', '10', '9', '7', '8', '1', '2', '2', '7', '9', '12', '2', '2', '1', '8', '9', '2', '3', '2', '2', '2', '9', '9', '8', '9', '2', '3', '8', '8', '9', '7', '3', '10', '8', '8', '9', '1', '1', '2', '8', '1', '10', '8', '8', '9', '7', '1', '7', '12', '8', '1', '7', '2', '1', '3', '9', '2', '8', '8', '1', '2', '3', '9', '3', '8', '9', '7', '7', '1', '10', '2', '8', '8', '8', '1', '8', '2', '8', '8', '1', '2', '8', '10', '2', '1', '0', '8', '3', '8', '10', '10', '12', '3', '8', '9', '3', '10', '8', '8', '8', '1', '8', '8', '2', '2', '2', '2', '2', '2', '1', '1', '10', '1', '10', '8', '2', '1', '2', '2', '8', '3', '3', '2', '2', '1', '8', '1', '3', '1', '9', '2', '2', '1', '3', '2', '2', '3', '2', '2', '3', '2', '1', '1', '2', '1', '2', '7', '8', '2', '1', '12', '12', '3', '1', '9', '9', '7', '8', '7', '2', '7', '9', '2', '2', '2', '8', '9', '7', '9', '2', '8', '9', '9', '2', '9', '1', '1', '9', '8', '3', '3', '8', '2', '7', '8', '11', '2', '8', '9', '1', '8', '3', '1', '2', '1', '2', '2', '2', '2', '8', '3', '9', '2', '10', '9', '9', '2', '7', '7', '7', '1', '3', '8', '8', '1', '1', '8', '7', '1', '8', '8', '9', '3', '3', '3', '2', '4', '8', '12', '8', '1', '12', '9', '2', '8', '2', '12', '2', '1', '3', '8', '12', '12', '11', '8', '10', '9', '2', '9', '4', '10', '1', '12', '12', '9', '3', '8', '9', '12', '9', '9', '2', '2', '2', '3', '5', '1', '1', '1', '7', '7', '8', '2', '2', '8', '2', '1', '7', '3', '2', '9', '8', '9', '1', '1', '9', '8', '9', '3', '3', '2', '9', '4', '9', '8', '8', '1', '8', '10', '2', '0', '3', '8', '8', '8', '9', '12', '9', '8', '2', '2', '8', '10', '9', '8', '2', '2', '4', '1', '9', '9', '8', '8', '10', '3', '1', '1', '2', '8', '7', '8', '2', '1', '1', '8', '12', '2', '9', '4', '8', '3', '3', '3', '8', '2', '12', '12', '8', '9', '12', '8', '2', '12', '7', '9', '2', '2', '1', '3', '1', '3', '11', '9', '8', '1', '10', '2', '12', '3', '8', '4', '1', '2', '8', '8', '3', '9', '3', '8', '7', '1', '1', '3', '1', '8', '2', '9', '8', '12', '1', '1', '1', '9', '9', '1', '8', '8', '8', '2', '2', '12', '3', '1', '1', '1', '9', '9', '3', '2', '1', '1', '9', '9', '1', '9', '9', '1', '9', '3', '9', '1', '8', '1', '9', '8', '3', '8', '8', '9', '9', '8', '2', '8', '1', '2', '2', '10', '1', '2', '1', '1', '9', '1', '9', '3', '7', '7', '2', '10', '3', '3', '1', '1', '9', '1', '2', '1', '3', '2', '1', '1', '1', '1', '1', '1', '1', '1', '1', '2', '8', '8', '7', '1', '1', '8', '8', '2', '1', '12', '1', '8', '1', '9', '8', '8', '2', '1', '9', '9', '2', '2', '8', '1', '7', '7', '9', '8', '1', '6', '8', '8', '2', '2', '8', '3', '8', '3', '2', '3', '8', '1', '2', '2', '2', '8', '9', '9', '9', '12', '1', '9', '2', '1', '9', '2', '9', '9', '10', '8', '1', '1', '9', '2', '1', '7', '1', '3', '7', '1', '1', '1', '3', '8', '8', '8', '9', '9', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '1', '0', '1', '1', '1', '2', '2', '3', '1', '1', '2', '3', '3', '3', '7', '3', '1', '9', '3', '9', '1', '4', '1', '1', '9', '10', '9', '8', '2', '3', '10', '9', '3', '10', '12', '8', '9', '8', '2', '1', '9', '8', '1', '7', '8', '1', '9', '1', '2', '10', '7', '7', '8', '1', '3', '3', '9', '3', '3', '1', '1', '1', '9', '1', '3', '8', '8', '8', '1', '8', '1', '9', '7', '8', '8', '2', '1', '3', '8', '2', '1', '6', '1', '8', '8', '2', '8', '2', '2', '9', '8', '1', '1', '6', '8', '6', '10', '2', '1', '1', '8', '9', '3', '3', '3', '1', '3', '9', '6', '8', '1', '8', '3', '1', '2', '2', '12', '7', '1', '1', '1', '1', '12', '1', '8', '8', '3', '2', '1', '9', '2', '3', '3', '2', '2', '7', '2', '2', '2', '3', '1', '9', '8', '7', '1', '1', '3', '1', '9', '4', '3', '9', '2', '1', '1', '9', '8', '8', '7', '2', '1', '3', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '2', '10', '1', '1', '8', '2', '8', '1', '8', '1', '3', '1', '2', '1', '1', '3', '1', '2', '1', '1', '1', '1', '1', '1', '1', '2', '1', '8', '1', '9', '8', '8', '1', '10', '8', '8', '1', '3', '9', '8', '9', '8', '2', '1', '3', '4', '1', '12', '1', '9', '8', '2', '7', '2', '1', '2', '9', '2', '1', '3', '8', '1', '2', '3', '8', '2', '10', '10', '10', '9', '9', '9', '3', '8', '3', '1', '1', '1', '1', '1', '1', '2', '1', '9', '2', '7', '9', '8', '7', '9', '1', '1', '1', '1', '8', '2', '2', '3', '3', '2', '12', '8', '3', '1', '1', '12', '9', '8', '2', '2', '9', '1', '2', '2', '9', '1', '1', '2', '4', '8', '1', '8', '10', '2', '9', '9', '9', '1', '2', '1', '1', '2', '2', '2', '12', '1', '2', '8', '3', '9', '9', '3', '2', '9', '9', '7', '2', '8', '8', '9', '2', '9', '1', '1', '1', '1', '1', '2', '8', '2', '1', '2', '2', '8', '3', '1', '1', '9', '1', '3', '10', '7', '9', '8', '8', '2', '2', '9', '2', '2', '2', '2', '2', '2', '9', '1', '3', '6', '4', '2', '2', '9', '1', '9', '8', '1', '1', '1', '7', '3', '2', '9', '9', '1', '9', '7', '2', '8', '9', '9', '12', '10', '8', '10', '2', '3', '1', '2', '2', '12', '12', '3', '12', '1', '2', '1', '2', '1', '2', '8', '2', '3', '12', '12', '8', '2', '2', '9', '2', '9', '2', '3', '3', '1', '1', '1', '12', '3', '7', '1', '3', '1', '2', '9', '2', '7', '2', '1', '8', '3', '7', '8', '1', '1', '3', '8', '3', '2', '9', '3', '1', '1', '1', '1', '11', '1', '2', '6', '1', '2', '3', '8', '9', '9', '2', '9', '2', '1', '1', '3', '1', '12', '9', '1', '1', '9', '1', '9', '9', '8', '9', '8', '1', '2', '7', '1', '0', '9', '8', '2', '3', '2', '4', '1', '1', '1', '8', '1', '12', '1', '2', '3', '3', '9', '9', '9', '9', '9', '7', '9', '3', '9', '1', '7', '3', '3', '3', '3', '7', '8', '2', '2', '2', '8', '2', '3', '9', '9', '3', '1', '8', '9', '12', '8', '8', '3', '2', '1', '6', '1', '9', '1', '1', '1', '1', '1', '1', '2', '2', '1', '2', '2', '2', '2', '9', '2', '5', '3', '2', '2', '8', '2', '1', '8', '3', '3', '8', '3', '1', '3', '9', '2', '8', '9', '4', '4', '9', '8', '8', '7', '7', '3', '1', '8', '8', '3', '2', '2', '10', '3', '3', '2', '2', '2', '2', '12', '3', '1', '2', '7', '7', '12', '2', '1', '2', '1', '1', '2', '1', '3', '3', '3', '1', '3', '9', '4', '8', '8', '4', '2', '4', '2', '8', '9', '1', '1', '2', '2', '2', '8', '7', '2', '8', '2', '9', '1', '8', '1', '1', '2', '1', '1', '1', '2', '9', '9', '8', '1', '8', '1', '4', '8', '1', '2', '7', '9', '2', '1', '8', '9', '8', '9', '1', '1', '2', '3', '2', '12', '7', '1', '2', '4', '4', '4', '2', '2', '2', '9', '8', '8', '1', '12', '3', '3', '2', '2', '3', '12', '1', '5', '1', '1', '3', '2', '8', '8', '2', '2', '2', '3', '9', '8', '8', '9', '8', '8', '11', '8', '2', '9', '2', '3', '1', '7', '1', '8', '2', '8', '1', '1', '1', '8', '1', '1', '1', '7', '8', '8', '2', '3', '2', '1', '8', '8', '1', '8', '4', '8', '9', '8', '9', '8', '1', '2', '3', '2', '1', '2', '2', '3', '1', '9', '4', '8', '1', '1', '9', '1', '1', '8', '2', '2', '2', '1', '7', '3', '3', '1', '3', '3', '12', '3', '1', '1', '3', '1', '1', '4', '8', '4', '4', '3', '3', '3', '4', '8', '1', '9', '9', '9', '4', '1', '7', '4', '8', '4', '9', '8', '1', '1', '3', '8', '9', '1', '9', '7', '1', '1', '9', '1', '8', '8', '8', '1', '1', '8', '2', '2', '2', '9', '4', '4', '1', '2', '1', '2', '9', '1', '1', '8', '5', '5', '5', '9', '12', '4', '1', '2', '1', '8', '8', '1', '9', '9', '2', '12', '1', '1', '7', '8', '8', '4', '1', '2', '8', '11', '2', '3', '2', '2', '1', '11', '2', '2', '2', '2', '9', '9', '2', '2', '9', '3', '1', '9', '1', '1', '1', '7', '8', '1', '8', '9', '1', '2', '9', '8', '1', '2', '9', '8', '12', '2', '2', '10', '8', '2', '2', '8', '8', '1', '2', '6', '1', '1', '7', '7', '2', '3', '2', '1', '1', '8', '1', '10', '8', '12', '6', '4', '1', '4', '1', '2', '9', '8', '9', '4', '8', '8', '10', '8', '8', '8', '8', '8', '2', '2', '8', '4', '9', '3', '3', '3', '3', '3', '9', '2', '1', '1', '2', '3', '2', '1', '2', '3', '3', '3', '2', '2', '3', '3', '2', '1', '2', '3', '3', '3', '3', '12', '9', '3', '2', '2', '3', '1', '5', '1', '1', '3', '2', '2', '1', '2', '9', '3', '9', '8', '1', '8', '7', '7', '9', '1', '1', '7', '9', '10', '1', '7', '1', '4', '3', '8', '9', '4', '9', '9', '2', '9', '2', '8', '4', '2', '2', '9', '5', '2', '1', '10', '9', '3', '1', '1', '1', '2', '1', '2', '9', '1', '7', '9', '9', '8', '2', '2', '8', '9', '1', '9', '8', '2', '3', '2', '2', '1', '2', '2', '2', '1', '1', '8', '1', '9', '4', '12', '8', '2', '7', '2', '2', '1', '9', '3', '1', '8', '12', '1', '1', '2', '1', '10', '2', '6', '12', '12', '4', '9', '1', '9', '10', '1', '2', '8', '2', '1', '8', '6', '12', '9', '9', '2', '1', '7', '8', '9', '5', '8', '1', '8', '8', '3', '1', '8', '2', '8', '2', '2', '1', '2', '9', '2', '8', '1', '12', '8', '2', '7', '3', '3', '12', '2', '3', '3', '9', '9', '3', '3', '3', '3', '3', '2', '1', '4', '1', '4', '8', '8', '2', '1', '2', '9', '12', '1', '10', '12', '1', '2', '9', '4', '8', '10', '3', '2', '8', '7', '4', '8', '9', '2', '7', '1', '10', '2', '2', '2', '8', '4', '2', '2', '1', '1', '8', '7', '9', '2', '1', '9', '8', '9', '12', '4', '8', '2', '8', '7', '7', '1', '9', '2', '1', '1', '9', '1', '8', '2', '3', '9', '9', '1', '3', '2', '9', '2', '2', '8', '1', '1', '2', '1', '2', '1', '9', '8', '2', '2', '4', '8', '8', '9', '5', '5', '9', '8', '9', '6', '2', '1', '12', '2', '3', '8', '8', '9', '2', '2', '1', '3', '2', '10', '9', '10', '7', '2', '8', '4', '2', '8', '9', '8', '1', '3', '8', '8', '1', '8', '3', '8', '8', '1', '10', '3', '1', '5', '8', '8', '9', '2', '2', '9', '2', '1', '9', '1', '4', '1', '12', '8', '8', '2', '1', '1', '1', '1', '1', '9', '2', '9', '8', '9', '5', '9', '1', '8', '2', '4', '1', '1', '1', '9', '12', '4', '1', '1', '9', '8', '4', '1', '8', '9', '8', '8', '9', '9', '9', '9', '9', '9', '1', '3', '4', '2', '2', '8', '2', '9', '2', '3', '8', '3', '1', '7', '2', '1', '1', '4', '2', '9', '2', '1', '3', '2', '9', '2', '9', '3', '8', '1', '9', '1', '1', '2', '8', '8', '8', '2', '3', '9', '9', '8', '2', '4', '1', '5', '1', '1', '9', '9', '2', '1', '1', '1', '9', '9', '1', '3', '9', '2', '2', '9', '9', '8', '3', '9', '9', '8', '2', '2', '8', '9', '2', '2', '8', '9', '10', '10', '2', '8', '7', '9', '9', '4', '2', '4', '11', '2', '1', '7', '8', '8', '8', '9', '4', '9', '8', '8', '2', '1', '1', '9', '8', '3', '3', '8', '8', '7', '9', '1', '3', '4', '8', '10', '2', '4', '4', '8', '2', '8', '2', '8', '9', '2', '1', '2', '2', '3', '1', '9', '4', '5', '9', '9', '1', '1', '1', '1', '1', '1', '7', '1', '1', '1', '8', '1', '1', '1', '1', '1', '9', '1', '2', '1', '12', '9', '3', '9', '9', '4', '1', '9', '1', '1', '2', '11', '9', '3', '2', '2', '2', '4', '2', '3', '2', '7', '7', '8', '1', '10', '9', '5', '7', '1', '10', '8', '1', '2', '7', '1', '12', '5', '8', '4', '2', '9', '9', '8', '2', '9', '8', '8', '10', '2', '3', '2', '7', '2', '8', '10', '3', '2', '9', '9', '9', '1', '1', '1', '1', '8', '2', '10', '9', '8', '1', '2', '2', '2', '1', '2', '1', '3', '9', '12', '2', '2', '4', '8', '1', '3', '9', '4', '9', '1', '9', '9', '2', '1', '1', '9', '7', '10', '3', '10', '2', '9', '2', '9', '2', '8', '2', '1', '2', '3', '1', '1', '1', '8', '5', '8', '3', '1', '1', '4', '2', '2', '1', '3', '8', '2', '1', '1', '1', '2', '9', '4', '1', '8', '1', '2', '8', '2', '9', '2', '2', '5', '5', '2', '5', '2', '1', '8', '8', '1', '3', '12', '2', '4', '3', '2', '2', '2', '6', '2', '3', '3', '1', '8', '2', '2', '1', '1', '8', '8', '10', '1', '9', '2', '9', '12', '0', '2', '1', '3', '2', '2', '1', '2', '1', '9', '9', '2', '9', '8', '8', '1', '7', '8', '1', '2', '6', '9', '8', '10', '1', '12', '2', '2', '1', '8', '2', '1', '12', '7', '4', '2', '10', '2', '2', '1', '2', '1', '2', '1', '1', '2', '1', '9', '8', '10', '2', '10', '9', '9', '12', '5', '3', '2', '8', '2', '8', '3', '3', '1', '4', '9', '10', '2', '4', '8', '1', '2', '10', '12', '12', '3', '1', '1', '2', '9', '6', '6', '1', '9', '1', '8', '1', '8', '8', '8', '8', '2', '2', '2', '8', '8', '2', '1', '1', '1', '1', '1', '8', '8', '5', '8', '8', '1', '8', '9', '9', '1', '7', '12', '9', '9', '7', '2', '2', '7', '8', '9', '8', '2', '9', '6', '3', '4', '8', '4', '1', '2', '9', '8', '8', '8', '2', '1', '1', '8', '8', '3', '1', '9', '9', '7', '1', '2', '2', '2', '9', '4', '1', '2', '12', '1', '8', '2', '1', '9', '1', '8', '9', '9', '2', '5', '4', '2', '1', '2', '8', '12', '8', '10', '3', '8', '8', '8', '2', '2', '8', '2', '8', '2', '4', '8', '10', '2', '7', '2', '1', '8', '8', '9', '5', '8', '1', '12', '7', '10', '2', '2', '2', '1', '4', '8', '12', '1', '8', '2', '2', '9', '1', '8', '3', '1', '1', '5', '10', '2', '1', '2', '4', '8', '9', '3', '2', '8', '7', '4', '4', '9', '9', '2', '8', '1', '1', '2', '4', '7', '1', '9', '1', '1', '1', '8', '4', '1', '3', '1', '2', '2', '9', '1', '8', '8', '2', '2', '9', '9', '1', '8', '8', '1', '9', '8', '8', '2', '2', '2', '2', '1', '1', '1', '7', '4', '1', '1', '4', '2', '3', '3', '2', '3', '9', '2', '8', '2', '1', '5', '1', '2', '2', '1', '3', '5', '2', '9', '1', '1', '9', '8', '1', '4', '8', '1', '8', '2', '4', '8', '4', '7', '9', '8', '1', '2', '2', '4', '4', '11', '3', '11', '1', '3', '8', '1', '9', '6', '8', '9', '3', '9', '8', '9', '2', '3', '3', '7', '1', '7', '2', '9', '9', '1', '8', '8', '1', '5', '5', '8', '8', '1', '2', '1', '3', '9', '9', '5', '9', '8', '1', '4', '3', '8', '1', '1', '2', '1', '8', '9', '8', '2', '2', '2', '1', '2', '4', '10', '8', '1', '9', '1', '8', '1', '1', '9', '9', '8', '11', '1', '8', '2', '1', '2', '8', '2', '8', '8', '2', '9', '6', '10', '1', '8', '6', '4', '5', '11', '2', '2', '8', '1', '8', '1', '8', '1', '4', '8', '1', '1', '1', '8', '3', '7', '3', '3', '7', '1', '1', '4', '8', '1', '6', '4', '2', '2', '1', '1', '1', '1', '6', '2', '2', '8', '1', '8', '5', '5', '2', '1', '1', '2', '3', '7', '1', '1', '6', '2', '9', '3', '9', '8', '9', '2', '1', '4', '2', '9', '13', '9', '2', '8', '12', '4', '1', '1', '9', '5', '1', '1', '1', '1', '8', '2', '1', '1', '2', '9', '0', '12', '1', '1', '8', '9', '8', '3', '2', '2', '12', '8', '2', '2', '2', '1', '10', '1', '9', '8', '9', '5', '2', '2', '9', '2', '8', '4', '8', '3', '7', '9', '2', '2', '7', '2', '2', '8', '1', '1', '9', '10', '9', '8', '2', '9', '0', '7', '1', '1', '1', '9', '8', '2', '8', '12', '8', '3', '2', '8', '8', '7', '7', '8', '12', '2', '1', '3', '9', '2', '2', '8', '2', '8', '8', '12', '8', '8', '1', '8', '9', '2', '4', '4', '7', '2', '1', '1', '9', '3', '7', '1', '3', '9', '8', '2', '2', '3', '1', '3', '10', '2', '3', '7', '1', '8', '1', '1', '10', '3', '10', '8', '4', '1', '2', '4', '3', '10', '9', '3', '9', '5', '8', '7', '2', '9', '3', '3', '2', '11', '8', '1', '1', '7', '2', '7', '2', '9', '4', '8', '1', '12', '8', '7', '9', '1', '0', '2', '6', '4', '9', '3', '2', '4', '12', '9', '8', '5', '9', '2', '8', '4', '6', '8', '9', '3', '1', '8', '7', '3', '1', '1', '1', '8', '9', '1', '8', '7', '9', '8', '2', '1', '2', '2', '3', '2', '2', '1', '1', '4', '9', '7', '3', '2', '8', '8', '9', '1', '9', '7', '4', '2', '2', '8', '5', '9', '1', '5', '7', '1', '4', '6', '9', '10', '1', '8', '2', '9', '1', '9', '2', '9', '8', '2', '9', '9', '11', '2', '12', '2', '2', '9', '4', '8', '1', '9', '9', '2', '8', '9', '1', '2', '9', '3', '8', '8', '1', '7', '8', '8', '2', '3', '2', '2', '2', '2', '9', '8', '10', '2', '2', '1', '9', '11', '1', '8', '8', '2', '4', '2', '1', '2', '8', '8', '9', '2', '8', '9', '2', '3', '1', '2', '1', '3', '10', '8', '1', '2', '4', '1', '9', '8', '9', '3', '3', '9', '8', '2', '11', '3', '7', '1', '8', '7', '1', '8', '8', '8', '1', '4', '8', '8', '1', '9', '3', '1', '1', '2', '8', '8', '8', '2', '1', '8', '10', '2', '12', '8', '8', '9', '1', '8', '1', '3', '2', '2', '10', '7', '2', '2', '1', '1', '8', '2', '9', '1', '2', '9', '3', '3', '10', '1', '4', '7', '10', '12', '2', '4', '10', '8', '6', '2', '9', '1', '8', '3', '1', '2', '8', '8', '2', '12', '10', '2', '10', '9', '1', '2', '5', '8', '12', '8', '8', '7', '8', '8', '1', '8', '2', '2', '9', '1', '8', '10', '5', '5', '5', '8', '1', '9', '1', '13', '1', '2', '2', '1', '2', '4', '9', '2', '1', '9', '13', '11', '1', '9', '9', '3', '10', '8', '7', '8', '8', '4', '2', '1', '9', '4', '3', '1', '1', '10', '9', '2', '8', '6', '2', '1', '1', '3', '2', '8', '1', '1', '1', '1', '2', '4', '1', '1', '10', '1', '8', '1', '1', '9', '1', '7', '1', '1', '9', '9', '2', '12', '9', '1', '8', '1', '12', '1', '8', '3', '8', '1', '8', '1', '2', '10', '1', '1', '2', '2', '9', '3', '9', '12', '1', '9', '1', '2', '8', '7', '2', '1', '1', '3', '2', '8', '4', '4', '5', '12', '7', '8', '9', '6', '1', '8', '9', '2', '1', '9', '1', '4', '1', '8', '2', '1', '1', '8', '7', '4', '9', '1', '8', '4', '2', '7', '5', '1', '3', '9', '2', '8', '2', '1', '2', '9', '1', '1', '2', '3', '4', '3', '1', '2', '2', '10', '2', '2', '8', '1', '1', '2', '4', '9', '2', '11', '9', '8', '9', '2', '1', '10', '4', '1', '1', '10', '7', '2', '10', '8', '1', '10', '8', '8', '8', '8', '8', '8', '2', '2', '2', '8', '3', '4', '1', '1', '1', '2', '1', '8', '8', '2', '8', '3', '8', '3', '1', '2', '3', '1', '1', '10', '1', '4', '4', '3', '3', '1', '9', '1', '1', '2', '1', '1', '6', '1', '1', '1', '9', '1', '4', '1', '1', '1', '1', '1', '1', '2', '12', '8', '8', '1', '9', '12', '10', '8', '2', '12', '1', '1', '2', '1', '7', '10', '7', '1', '2', '1', '10', '8', '2', '2', '8', '2', '2', '1', '9', '8', '8', '8', '8', '3', '1', '4', '2', '8', '3', '10', '10', '1', '9', '1', '9', '1', '9', '1', '8', '3', '1', '1', '8', '8', '8', '6', '3', '4', '1', '1', '1', '9', '9', '5', '2', '10', '2', '6', '2', '2', '7', '1', '8', '2', '11', '1', '1', '1', '2', '1', '9', '4', '4', '2', '8', '4', '6', '8', '8', '10', '10', '2', '1', '2', '9', '1', '8', '8', '3', '4', '8', '8', '2', '8', '8', '1', '2', '1', '2', '9', '4', '1', '3', '3', '3', '8', '8', '8', '2', '6', '4', '3', '3', '12', '3', '4', '9', '6', '8', '7', '7', '8', '6', '4', '1', '10', '6', '3', '3', '8', '1', '1', '8', '1', '11', '8', '1', '1', '2', '3', '11', '7', '2', '1', '1', '1', '9', '1', '2', '1', '1', '9', '8', '1', '1', '8', '0', '4', '1', '1', '2', '1', '1', '2', '2', '9', '1', '10', '1', '3', '8', '9', '3', '3', '1', '10', '1', '9', '1', '1', '7', '4', '1', '8', '10', '1', '2', '10', '1', '1', '2', '3', '3', '9', '1', '8', '10', '1', '7', '9', '1', '9', '6', '3', '9', '2', '8', '4', '12', '12', '3', '1', '1', '9', '1', '9', '2', '1', '1', '1', '1', '1', '9', '2', '1', '2', '10', '9', '8', '8', '10', '8', '2', '3', '2', '1', '12', '8', '12', '2', '9', '1', '3', '2', '5', '2', '2', '8', '2', '2', '4', '10', '9', '10', '9', '4', '2', '4', '12', '8', '2', '1', '8', '3', '9', '2', '9', '4', '1', '1', '1', '1', '1', '9', '6', '2', '9', '2', '1', '3', '2', '2', '5', '8', '9', '1', '6', '3', '1', '2', '2', '2', '6', '1', '8', '10', '8', '3', '3', '13', '2', '9', '9', '10', '6', '12', '8', '2', '9', '2', '3', '8', '8', '1', '2', '2', '3', '2', '10', '1', '8', '9', '1', '8', '9', '8', '8', '2', '10', '2', '1', '1', '1', '9', '9', '12', '1', '1', '4', '8', '3', '2', '2', '4', '4', '2', '2', '1', '1', '1', '9', '2', '2', '4', '1', '1', '8', '8', '8', '9', '2', '13', '9', '9', '4', '1', '9', '7', '10', '10', '8', '1', '9', '1', '1', '1', '8', '12', '4', '1', '2', '1', '1', '1', '7', '8', '3', '3', '1', '2', '2', '7', '2', '2', '10', '1', '8', '4', '0', '1', '9', '4', '1', '1', '2', '10', '10', '7', '9', '3', '12', '11', '2', '8', '10', '8', '9', '4', '3', '4', '10', '1', '1', '9', '3', '10', '1', '5', '3', '3', '2', '2', '1', '1', '6', '1', '7', '1', '1', '12', '1', '1', '1', '10', '1', '8', '8', '1', '3', '8', '3', '1', '2', '8', '3', '4', '2', '8', '8', '6', '1', '1', '4', '1', '2', '9', '5', '1', '7', '8', '9', '2', '4', '9', '8', '5', '9', '9', '9', '1', '2', '1', '3', '8', '9', '2', '9', '8', '9', '1', '8', '13', '9', '3', '8', '1', '4', '1', '3', '1', '6', '4', '10', '2', '2', '7', '8', '8', '12', '8', '9', '9', '9', '10', '3', '12', '12', '2', '9', '10', '9', '7', '2', '3', '4', '10', '10', '2', '7', '7', '1', '7', '3', '9', '1', '9', '8', '2', '9', '1', '8', '1', '10', '8', '8', '2', '7', '8', '1', '8', '9', '8', '1', '4', '1', '8', '8', '8', '1', '1', '10', '3', '2', '2', '6', '8', '9', '8', '2', '1', '1', '3', '8', '8', '1', '4', '4', '3', '9', '10', '4', '2', '4', '2', '9', '2', '2', '9', '1', '1', '10', '1', '1', '2', '1', '1', '1', '9', '1', '8', '8', '8', '2', '3', '8', '1', '6', '7', '3', '1', '9', '1', '2', '1', '6', '1', '2', '10', '1', '4', '2', '3', '2', '9', '9', '4', '9', '9', '1', '2', '8', '4', '7', '8', '7', '13', '9', '1', '2', '1', '9', '3', '3', '6', '8', '9', '10', '9', '2', '2', '3', '12', '1', '1', '1', '13', '2', '1', '7', '10', '8', '9', '1', '2', '9', '1', '1', '1', '2', '1', '12', '5', '6', '9', '10', '3', '1', '10', '1', '3', '2', '8', '9', '2', '10', '8', '8', '9', '2', '8', '9', '2', '2', '2', '8', '8', '2', '3', '1', '4', '8', '1', '9', '9', '2', '8', '10', '9', '1', '2', '12', '8', '8', '0', '2', '1', '1', '9', '10', '1', '6', '2', '2', '10', '2', '2', '4', '2', '2', '12', '1', '6', '7', '8', '9', '1', '3', '5', '7', '3', '1', '3', '1', '1', '3', '2', '6', '3', '2', '1', '8', '3', '5', '2', '1', '1', '8', '1', '1', '0', '2', '3', '5', '3', '1', '1', '9', '8', '8', '8', '8', '4', '7', '2', '9', '12', '5', '9', '3', '12', '3', '9', '1', '9', '1', '7', '3', '9', '3', '10', '1', '2', '1', '4', '9', '8', '2', '9', '11', '2', '8', '9', '4', '1', '1', '1', '1', '1', '1', '1', '9', '1', '9', '1', '1', '1', '12', '3', '10', '3', '6', '1', '7', '10', '8', '3', '7', '1', '1', '2', '6', '1', '3', '9', '2', '8', '1', '8', '8', '1', '8', '10', '12', '1', '10', '12', '10', '1', '8', '8', '1', '4', '7', '9', '2', '1', '9', '3', '8', '12', '10', '6', '1', '6', '8', '3', '1', '3', '1', '10', '10', '9', '8', '1', '2', '9', '8', '8', '1', '9', '8', '8', '3', '1', '3', '9', '12', '1', '8', '1', '5', '8', '11', '5', '5', '1', '2', '1', '3', '1', '1', '1', '9', '8', '8', '1', '9', '10', '8', '4', '1', '1', '8', '10', '8', '9', '1', '10', '1', '7', '2', '1', '1', '9', '6', '2', '9', '8', '1', '1', '1', '9', '8', '8', '1', '9', '9', '1', '1', '2', '7', '9', '2', '9', '9', '8', '7', '6', '8', '7', '8', '2', '9', '2', '1', '8', '8', '8', '2', '6', '3', '1', '8', '12', '12', '8', '7', '1', '2', '2', '7', '1', '9', '8', '1', '4', '1', '1', '8', '5', '8', '1', '8', '8', '8', '8', '1', '2', '11', '1', '1', '1', '2', '7', '1', '9', '2', '2', '2', '9', '8', '9', '2', '9', '1', '8', '1', '7', '11', '13', '1', '9', '8', '2', '2', '1', '2', '3', '10', '3', '10', '4', '3', '9', '1', '8', '1', '1', '13', '1', '1', '3', '8', '1', '2', '2', '2', '9', '4', '1', '8', '1', '9', '3', '6', '8', '5', '2', '8', '2', '8', '1', '9', '2', '2', '8', '1', '1', '4', '8', '9', '2', '2', '7', '1', '8', '12', '2', '12', '8', '2', '9', '9', '3', '8', '1', '1', '9', '9', '9', '2', '8', '1', '9', '1', '8', '9', '8', '2', '2', '4', '1', '9', '10', '10', '8', '8', '1', '1', '2', '1', '4', '1', '2', '1', '1', '3', '12', '8', '8', '8', '8', '10', '2', '8', '12', '1', '9', '9', '8', '9', '1', '2', '1', '8', '8', '8', '2', '8', '8', '3', '10', '8', '9', '1', '1', '1', '3', '4', '8', '10', '6', '3', '1', '3', '2', '8', '9', '3', '5', '4', '1', '5', '2', '9', '9', '9', '1', '1', '1', '9', '9', '6', '10', '9', '8', '12', '2', '9', '1', '2', '1', '1', '1', '8', '1', '12', '1', '12', '1', '9', '1', '8', '1', '2', '1', '2', '8', '8', '10', '1', '9', '1', '2', '2', '8', '8', '3', '12', '1', '11', '2', '10', '12', '11', '2', '1', '10', '1', '1', '9', '3', '10', '2', '7', '1', '8', '1', '9', '9', '2', '1', '3', '12', '5', '9', '9', '8', '1', '8', '8', '8', '1', '1', '3', '9', '8', '9', '8', '3', '10', '3', '7', '8', '9', '2', '3', '1', '9', '10', '9', '2', '8', '2', '6', '4', '1', '1', '3', '8', '2', '3', '9', '1', '2', '1', '8', '1', '2', '2', '2', '2', '9', '9', '4', '8', '9', '1', '1', '2', '9', '1', '5', '2', '9', '8', '10', '1', '2', '10', '8', '5', '6', '4', '8', '1', '1', '1', '8', '9', '2', '8', '8', '8', '12', '2', '2', '1', '8', '1', '2', '1', '9', '8', '8', '9', '1', '1', '8', '7', '1', '8', '0', '2', '3', '8', '1', '1', '8', '1', '1', '12', '3', '2', '10', '8', '10', '6', '1', '8', '8', '10', '8', '8', '1', '4', '8', '1', '2', '9', '8', '1', '1', '3', '3', '5', '1', '1', '2', '1', '2', '1', '9', '8', '3', '9', '8', '2', '10', '9', '1', '8', '1', '10', '9', '10', '1', '2', '3', '8', '1', '3', '1', '9', '9', '8', '1', '9', '8', '8', '9', '1', '9', '2', '8', '8', '2', '13', '8', '10', '8', '9', '3', '2', '1', '3', '1', '12', '10', '10', '1', '11', '1', '2', '4', '2', '10', '1', '8', '11', '1', '1', '1', '2', '2', '8', '2', '9', '8', '9', '9', '12', '8', '1', '9', '2', '4', '1', '8', '3', '9', '1', '6', '5', '8', '9', '3', '3', '2', '1', '1', '1', '8', '11', '1', '8', '7', '1', '11', '9', '1', '7', '9', '9', '8', '8', '8', '12', '8', '1', '2', '9', '8', '8', '2', '10', '9', '8', '8', '4', '10', '9', '8', '2', '10', '10', '2', '1', '2', '1', '2', '1', '4', '1', '7', '1', '1', '3', '12', '9', '8', '5', '9', '10', '1', '9', '9', '2', '7', '4', '1', '1', '8', '12', '8', '2', '2', '9', '4', '1', '1', '5', '9', '1', '8', '12', '8', '1', '1', '4', '9', '10', '2', '3', '1', '3', '3', '3', '8', '1', '2', '1', '2', '1', '2', '9', '1', '2', '8', '9', '4', '8', '8', '10', '2', '12', '5', '5', '1', '8', '1', '7', '8', '1', '1', '8', '9', '12', '4', '9', '3', '1', '2', '5', '9', '2', '1', '2', '2', '1', '2', '8', '1', '8', '8', '2', '2', '1', '8', '4', '1', '2', '10', '9', '10', '8', '1', '10', '4', '8', '8', '10', '2', '8', '1', '1', '3', '10', '1', '4', '2', '8', '3', '3', '2', '9', '8', '5', '5', '9', '3', '10', '8', '1', '2', '1', '2', '9', '1', '1', '2', '9', '9', '8', '8', '11', '9', '4', '1', '8', '2', '7', '1', '2', '8', '2', '9', '9', '8', '2', '9', '9', '8', '8', '1', '2', '8', '1', '1', '2', '9', '8', '1', '4', '9', '1', '8', '12', '1', '12', '2', '8', '2', '9', '1', '12', '1', '2', '9', '1', '1', '9', '3', '2', '9', '9', '2', '11', '8', '7', '9', '9', '9', '8', '8', '1', '2', '8', '1', '1', '4', '1', '2', '10', '1', '9', '2', '1', '1', '1', '9', '1', '13', '4', '3', '2', '1', '1', '2', '2', '7', '8', '7', '1', '1', '2', '8', '8', '3', '4', '8', '2', '10', '13', '8', '8', '9', '2', '2', '1', '1', '10', '4', '2', '7', '9', '8', '2', '1', '9', '8', '1', '1', '9', '8', '9', '8', '2', '8', '2', '2', '9', '2', '1', '9', '1', '2', '4', '8', '2', '1', '1', '8', '1', '4', '1', '9', '3', '9', '2', '7', '1', '9', '6', '1', '10', '2', '2', '2', '2', '2', '10', '10', '10', '9', '9', '9', '3', '12', '10', '1', '1', '10', '1', '1', '2', '2', '1', '2', '3', '12', '8', '8', '1', '2', '9', '9', '1', '1', '10', '8', '9', '8', '3', '1', '1', '3', '1', '10', '1', '1', '1', '4', '1', '1', '1', '7', '10', '2', '1', '8', '1', '9', '3', '1', '10', '8', '8', '1', '5', '9', '1', '2', '1', '8', '8', '1', '3', '1', '10', '2', '1', '1', '2', '8', '2', '5', '3', '5', '2', '1', '1', '9', '1', '7', '9', '9', '2', '8', '1', '12', '4', '1', '4', '9', '9', '4', '8', '1', '10', '10', '8', '8', '8', '9', '2', '3', '5', '1', '8', '1', '1', '1', '7', '10', '1', '4', '12', '3', '2', '2', '9', '1', '1', '1', '2', '8', '4', '1', '7', '3', '12', '8', '6', '2', '2', '7', '9', '11', '1', '10', '9', '1', '8', '12', '2', '11', '1', '2', '3', '1', '1', '1', '8', '9', '2', '2', '2', '2', '3', '3', '8', '10', '4', '1', '2', '9', '8', '2', '2', '9', '1', '8', '11', '4', '2', '8', '7', '1', '1', '7', '3', '9', '9', '4', '5', '8', '10', '2', '9', '9', '9', '8', '1', '9', '2', '2', '2', '1', '2', '9', '4', '3', '12', '4', '3', '2', '3', '8', '3', '10', '1', '10', '1', '1', '8', '10', '6', '8', '9', '9', '1', '2', '9', '2', '9', '3', '8', '1', '1', '12', '1', '5', '1', '10', '8', '10', '2', '7', '1', '1', '1', '1', '3', '2', '3', '5', '9', '1', '2', '1', '9', '8', '10', '9', '8', '1', '8', '1', '8', '2', '8', '1', '2', '8', '1', '2', '5', '8', '12', '2', '1', '1', '1', '1', '1', '8', '8', '2', '1', '4', '2', '8', '8', '8', '2', '9', '9', '11', '1', '2', '9', '3', '1', '1', '8', '9', '2', '4', '10', '1', '9', '8', '4', '9', '8', '10', '8', '8', '9', '2', '8', '9', '3', '3', '3', '4', '3', '2', '2', '10', '8', '2', '1', '5', '1', '9', '1', '1', '2', '1', '2', '3', '1', '1', '8', '8', '1', '10', '8', '8', '1', '6', '4', '8', '2', '5', '8', '1', '3', '8', '2', '8', '9', '1', '1', '8', '12', '10', '1', '5', '8', '2', '8', '10', '9', '9', '1', '6', '9', '1', '10', '8', '2', '9', '10', '1', '2', '1', '1', '8', '8', '3', '9', '8', '1', '10', '2', '8', '1', '10', '1', '5', '1', '1', '1', '5', '1', '1', '1', '4', '1', '1', '1', '8', '2', '8', '1', '8', '3', '8', '9', '1', '8', '1', '8', '1', '1', '2', '2', '1', '12', '9', '9', '1', '8', '2', '1', '2', '8', '1', '9', '1', '1', '2', '1', '2', '2', '2', '1', '9', '9', '8', '8', '1', '1', '9', '10', '8', '4', '3', '3', '4', '9', '1', '3', '3', '10', '2', '2', '1', '8', '4', '1', '9', '4', '1', '1', '9', '8', '10', '4', '9', '9', '8', '2', '1', '3', '4', '9', '3', '8', '9', '1', '1', '2', '1', '7', '8', '2', '9', '1', '9', '2', '6', '9', '10', '8', '1', '1', '5', '9', '5', '1', '9', '10', '9', '3', '3', '8', '8', '2', '9', '2', '8', '2', '1', '8', '5', '8', '3', '10', '1', '1', '2', '1', '8', '1', '4', '3', '2', '9', '4', '9', '8', '8', '8', '1', '3', '9', '1', '9', '9', '1', '2', '1', '9', '8', '9', '8', '1', '1', '1', '4', '2', '2', '9', '9', '1', '1', '1', '3', '6', '1', '2', '3', '1', '4', '2', '9', '1', '9', '9', '1', '8', '2', '1', '9', '1', '8', '1', '8', '8', '2', '1', '9', '9', '6', '8', '8', '9', '8', '10', '8', '9', '5', '1', '1', '1', '1', '8', '1', '8', '12', '8', '1', '12', '9', '2', '8', '1', '1', '2', '1', '8', '6', '9', '9', '8', '1', '8', '9', '7', '7', '8', '9', '9', '1', '8', '3', '8', '1', '3', '3', '8', '9', '8', '2', '8', '1', '2', '9', '8', '1', '1', '1', '1', '9', '1', '9', '8', '12', '3', '8', '8', '1', '10', '10', '10', '2', '2', '1', '10', '2', '2', '10', '8', '12', '8', '1', '1', '1', '2', '1', '1', '3', '8', '1', '2', '2', '2', '1', '1', '6', '2', '8', '8', '8', '1', '1', '1', '9', '9', '2', '8', '10', '2', '8', '2', '1', '2', '9', '9', '2', '1', '1', '8', '8', '1', '3', '1', '8', '9', '1', '10', '9', '1', '1', '1', '1', '7', '2', '2', '1', '2', '8', '8', '1', '2', '1', '3', '2', '1', '8', '3', '9', '2', '10', '2', '10', '1', '2', '1', '1', '13', '9', '10', '8', '2', '2', '1', '1', '8', '2', '2', '1', '9', '1', '1', '8', '9', '9', '2', '9', '2', '2', '1', '1', '1', '8', '9', '4', '9', '9', '1', '8', '2', '1', '1', '8', '2', '2', '2', '8', '1', '1', '8', '2', '10', '2', '9', '9', '7', '1', '1', '2', '2', '8', '3', '1', '2', '1', '1', '8', '1', '1', '1', '9', '1', '1', '8', '8', '1', '9', '6', '1', '9', '1', '8', '1', '8', '8', '8', '1', '3', '8', '8', '2', '6', '3', '2', '1', '10', '1', '8', '2', '1', '6', '1', '1', '1', '8', '1', '10', '1', '1', '8', '9', '2', '6', '9', '2', '9', '4', '1', '8', '1', '3', '7', '8', '8', '5', '8', '7', '1', '1', '1', '13', '8', '3', '1', '1', '1', '1', '2', '8', '2', '12', '8', '1', '2', '5', '1', '8', '2', '10', '8', '2', '10', '1', '5', '2', '1', '1', '5', '2', '2', '7', '8', '2', '9', '9', '1', '10', '1', '8', '2', '5', '8', '1', '5', '1', '10', '1', '1', '9', '8', '6', '1', '8', '9', '8', '1', '8', '2', '2', '8', '3', '9', '1', '9', '3', '8', '4', '4', '3', '3', '1', '1', '1', '8', '9', '1', '1', '8', '3', '1', '2', '8', '2', '1', '1', '10', '1', '2', '2', '1', '1', '8', '2', '8', '2', '10', '2', '2', '10', '8', '8', '8', '13', '2', '2', '9', '2', '5', '9', '2', '1', '8', '8', '8', '1', '12', '12', '2', '9', '2', '1', '8', '2', '1', '8', '1', '9', '9', '2', '7', '1', '1', '7', '1', '4', '3', '1', '10', '3', '10', '9', '4', '6', '7', '5', '2', '1', '8', '8', '1', '4', '9', '2', '1', '8', '1', '1', '1', '1', '1', '8', '9', '8', '8', '8', '8', '9', '8', '10', '1', '8', '2', '7', '8', '9', '1', '2', '5', '9', '10', '8', '9', '12', '1', '6', '1', '2', '10', '9', '4', '1', '8', '10', '10', '1', '1', '1', '1', '1', '2', '3', '8', '1', '2', '10', '2', '1', '2', '4', '2', '2', '9', '1', '4', '2', '1', '1', '12', '12', '9', '9', '7', '1', '8', '4', '9', '9', '8', '1', '7', '3', '2', '1', '1', '4', '8', '1', '5', '8', '2', '8', '1', '14', '9', '12', '1', '10', '2', '1', '1', '8', '8', '6', '6', '3', '1', '2', '8', '1', '3', '2', '1', '8', '8', '1', '10', '8', '2', '8', '2', '5', '9', '1', '8', '8', '1', '1', '8', '8', '13', '1', '3', '8', '4', '1', '9', '7', '8', '8', '8', '3', '8', '1', '4', '2', '8', '8', '8', '7', '1', '1', '2', '8', '8', '9', '8', '8', '2', '3', '9', '9', '2', '1', '2', '2', '1', '8', '2', '6', '2', '1', '1', '9', '10', '3', '8', '2', '3', '2', '8', '8', '1', '8', '8', '8', '9', '1', '4', '1', '9', '2', '1', '8', '0', '1', '8', '1', '1', '8', '9', '9', '1', '3', '1', '2', '8', '8', '7', '1', '1', '2', '9', '2', '10', '2', '8', '2', '2', '2', '7', '1', '9', '8', '1', '3', '9', '2', '1', '10', '8', '1', '4', '1', '8', '5', '9', '8', '8', '1', '2', '2', '1', '8', '8', '6', '8', '1', '1', '8', '3', '2', '2', '1', '8', '1', '7', '6', '8', '1', '1', '1', '9', '1', '1', '1', '2', '1', '11', '10', '10', '11', '9', '']\n",
            "['8', '1', '8', '2', '8', '8', '8', '9', '7', '8', '1', '2', '1', '1', '8', '4', '8', '8', '12', '3', '3', '7', '3', '12', '1', '8', '8', '8', '8', '8', '8', '8', '1', '9', '5', '9', '9', '9', '11', '8', '8', '8', '4', '8', '8', '8', '8', '1', '3', '9', '3', '7', '1', '2', '9', '9', '7', '8', '8', '1', '10', '7', '8', '8', '9', '8', '7', '9', '9', '12', '7', '2', '8', '1', '11', '11', '1', '7', '7', '12', '1', '9', '8', '10', '12', '7', '8', '2', '8', '9', '9', '1', '8', '9', '1', '7', '12', '10', '10', '10', '8', '3', '7', '9', '8', '9', '1', '8', '8', '2', '7', '2', '9', '9', '11', '8', '8', '12', '12', '7', '8', '12', '4', '9', '3', '1', '12', '1', '1', '8', '8', '3', '8', '8', '8', '8', '9', '1', '8', '8', '10', '1', '8', '2', '8', '8', '7', '3', '8', '2', '4', '4', '9', '8', '10', '12', '12', '1', '1', '9', '1', '1', '1', '8', '2', '2', '8', '1', '1', '2', '2', '1', '2', '8', '1', '9', '9', '8', '8', '4', '2', '9', '9', '8', '3', '4', '3', '1', '8', '8', '2', '1', '9', '7', '8', '8', '1', '12', '3', '8', '2', '4', '2', '9', '12', '1', '4', '1', '8', '8', '8', '2', '2', '8', '9', '8', '8', '8', '10', '9', '8', '7', '9', '1', '1', '9', '4', '2', '4', '2', '2', '1', '7', '8', '11', '11', '3', '9', '2', '4', '8', '9', '1', '8', '1', '1', '4', '9', '1', '1', '8', '8', '2', '1', '8', '4', '2', '8', '9', '8', '8', '2', '8', '8', '8', '7', '1', '1', '1', '2', '1', '1', '8', '7', '8', '8', '12', '2', '12', '12', '8', '10', '12', '8', '3', '3', '12', '10', '1', '8', '12', '1', '8', '8', '2', '8', '4', '7', '8', '7', '10', '8', '10', '9', '8', '12', '12', '1', '8', '8', '3', '8', '8', '8', '8', '8', '1', '9', '8', '11', '1', '1', '1', '9', '8', '1', '9', '2', '3', '11', '8', '9', '9', '9', '2', '1', '8', '8', '9', '7', '1', '4', '9', '4', '8', '8', '4', '8', '12', '9', '4', '8', '2', '10', '10', '10', '8', '9', '9', '8', '8', '12', '7', '1', '8', '8', '8', '4', '1', '1', '1', '1', '1', '1', '8', '1', '9', '8', '9', '9', '4', '8', '12', '9', '8', '8', '2', '8', '8', '8', '6', '9', '8', '3', '7', '8', '8', '4', '12', '8', '8', '9', '12', '12', '9', '8', '2', '9', '2', '3', '1', '12', '8', '10', '9', '9', '9', '10', '10', '3', '8', '12', '1', '4', '2', '1', '10', '8', '2', '8', '4', '8', '9', '1', '9', '9', '10', '10', '1', '4', '9', '2', '4', '9', '1', '1', '3', '10', '3', '3', '8', '7', '3', '8', '9', '9', '12', '4', '8', '12', '2', '2', '4', '1', '9', '9', '4', '1', '4', '2', '8', '12', '2', '3', '10', '10', '9', '8', '9', '9', '1', '12', '8', '8', '8', '12', '4', '1', '8', '8', '1', '9', '8', '8', '2', '1', '8', '9', '8', '3', '3', '3', '1', '8', '8', '9', '1', '10', '9', '9', '9', '9', '5', '9', '9', '8', '8', '8', '8', '8', '8', '8', '8', '11', '12', '8', '8', '1', '8', '9', '11', '2', '2', '2', '2', '3', '1', '2', '2', '8', '2', '4', '9', '1', '2', '9', '8', '2', '8', '9', '9', '3', '10', '9', '9', '2', '8', '9', '8', '12', '12', '1', '3', '8', '8', '8', '2', '7', '7', '7', '7', '3', '9', '1', '9', '8', '9', '9', '1', '1', '1', '2', '9', '9', '9', '11', '1', '8', '8', '9', '1', '9', '8', '8', '8', '1', '1', '8', '7', '1', '1', '8', '8', '9', '4', '4', '8', '2', '2', '8', '8', '8', '8', '8', '8', '11', '8', '2', '9', '4', '9', '3', '9', '9', '1', '3', '9', '3', '1', '12', '8', '9', '12', '1', '8', '4', '2', '1', '4', '8', '3', '3', '8', '2', '8', '9', '7', '8', '8', '8', '5', '8', '3', '9', '8', '8', '13', '12', '1', '1', '2', '8', '4', '1', '9', '9', '12', '8', '9', '12', '9', '1', '9', '9', '9', '9', '3', '2', '9', '9', '4', '8', '12', '2', '4', '9', '3', '1', '9', '7', '8', '9', '9', '8', '4', '8', '8', '7', '9', '10', '3', '8', '8', '8', '1', '1', '1', '1', '8', '8', '4', '1', '10', '1', '5', '7', '7', '1', '8', '9', '3', '7', '2', '7', '7', '2', '4', '8', '12', '7', '4', '2', '9', '9', '12', '6', '10', '8', '2', '4', '12', '9', '9', '3', '8', '8', '1', '2', '10', '9', '9', '8', '4', '12', '2', '1', '8', '8', '8', '12', '10', '10', '9', '3', '8', '8', '9', '2', '8', '10', '1', '1', '1', '1', '2', '1', '1', '1', '1', '9', '8', '12', '9', '4', '8', '8', '9', '1', '9', '3', '9', '8', '8', '1', '7', '7', '10', '1', '8', '8', '1', '9', '8', '10', '3', '1', '7', '1', '8', '8', '12', '8', '8', '1', '8', '7', '1', '7', '7', '8', '2', '1', '8', '8', '2', '10', '8', '8', '8', '8', '8', '10', '1', '8', '8', '12', '8', '3', '3', '2', '2', '2', '10', '8', '8', '8', '2', '9', '1', '8', '9', '3', '2', '8', '10', '8', '6', '1', '1', '8', '4', '1', '9', '10', '8', '1', '7', '1', '2', '8', '1', '1', '1', '12', '1', '9', '12', '8', '12', '12', '12', '8', '8', '12', '4', '8', '8', '8', '8', '9', '9', '1', '3', '3', '3', '3', '1', '12', '12', '9', '10', '8', '8', '1', '9', '2', '2', '13', '9', '8', '9', '2', '1', '9', '1', '8', '8', '8', '4', '8', '1', '1', '1', '12', '12', '7', '2', '2', '2', '8', '3', '8', '9', '2', '10', '7', '8', '9', '2', '1', '2', '12', '12', '8', '8', '9', '2', '2', '9', '11', '1', '8', '1', '10', '9', '2', '1', '4', '7', '7', '7', '7', '7', '12', '8', '8', '8', '1', '1', '10', '1', '12', '1', '8', '2', '1', '1', '12', '8', '7', '9', '12', '8', '9', '3', '9', '8', '8', '8', '8', '3', '11', '2', '2', '9', '8', '8', '10', '8', '2', '7', '3', '1', '4', '7', '8', '8', '1', '8', '3', '7', '12', '8', '10', '9', '9', '8', '8', '2', '8', '9', '9', '1', '2', '8', '8', '9', '8', '3', '8', '1', '8', '10', '9', '8', '9', '9', '12', '4', '4', '8', '9', '9', '8', '2', '10', '1', '2', '8', '9', '1', '9', '9', '9', '7', '12', '12', '8', '1', '1', '1', '1', '8', '3', '1', '1', '8', '1', '8', '8', '7', '8', '8', '8', '8', '3', '2', '2', '10', '10', '10', '7', '8', '1', '2', '12', '7', '9', '8', '7', '8', '12', '2', '8', '9', '2', '6', '6', '7', '9', '8', '1', '8', '9', '8', '1', '12', '1', '2', '8', '7', '7', '7', '8', '2', '2', '8', '1', '2', '2', '9', '9', '1', '8', '8', '4', '3', '3', '1', '6', '3', '3', '12', '3', '8', '9', '1', '4', '3', '1', '8', '3', '9', '2', '8', '2', '8', '8', '8', '1', '1', '1', '9', '9', '8', '1', '9', '8', '1', '1', '3', '10', '8', '1', '1', '3', '9', '1', '4', '4', '1', '8', '9', '9', '2', '0', '0', '1', '8', '3', '1', '8', '8', '9', '8', '8', '1', '1', '8', '9', '8', '8', '8', '7', '9', '8', '8', '8', '10', '9', '8', '1', '2', '6', '1', '9', '9', '8', '12', '12', '12', '8', '8', '2', '8', '1', '2', '2', '2', '1', '9', '8', '2', '12', '2', '8', '12', '8', '9', '8', '8', '9', '7', '1', '1', '1', '1', '1', '8', '8', '1', '8', '8', '1', '1', '3', '2', '8', '8', '9', '10', '10', '2', '2', '1', '9', '2', '9', '9', '4', '12', '12', '12', '10', '7', '3', '3', '4', '2', '2', '9', '2', '8', '4', '2', '4', '1', '10', '9', '7', '8', '7', '1', '1', '3', '3', '1', '1', '3', '3', '3', '1', '1', '1', '1', '8', '2', '3', '1', '1', '2', '8', '8', '12', '8', '8', '8', '8', '11', '9', '1', '8', '9', '2', '8', '8', '8', '3', '9', '1', '9', '2', '7', '2', '8', '2', '8', '10', '8', '1', '10', '1', '1', '9', '9', '8', '8', '1', '8', '8', '8', '12', '8', '8', '8', '1', '8', '8', '8', '1', '9', '1', '1', '8', '1', '8', '9', '8', '2', '12', '9', '9', '0', '1', '8', '8', '1', '8', '12', '8', '8', '10', '8', '8', '8', '7', '8', '1', '8', '7', '3', '10', '1', '8', '9', '1', '8', '8', '8', '10', '1', '10', '3', '9', '1', '8', '9', '2', '8', '3', '3', '9', '9', '7', '9', '1', '1', '9', '2', '1', '1', '1', '7', '1', '1', '8', '8', '1', '1', '8', '1', '8', '3', '12', '9', '3', '3', '8', '8', '8', '8', '3', '1', '3', '3', '1', '11', '0', '8', '8', '7', '8', '12', '1', '8', '9', '8', '9', '8', '8', '3', '8', '8', '1', '1', '1', '9', '2', '2', '2', '8', '7', '12', '8', '8', '9', '10', '10', '7', '8', '1', '9', '8', '7', '3', '1', '3', '8', '2', '2', '3', '9', '8', '4', '4', '8', '9', '2', '1', '1', '7', '8', '9', '9', '7', '8', '7', '7', '8', '2', '2', '8', '4', '9', '7', '10', '0', '9', '8', '3', '7', '8', '1', '1', '8', '9', '9', '2', '2', '10', '1', '9', '10', '10', '10', '8', '3', '2', '12', '9', '9', '10', '12', '9', '12', '12', '9', '1', '2', '4', '12', '12', '7', '8', '9', '7', '7', '7', '3', '9', '8', '9', '1', '12', '8', '9', '4', '1', '3', '12', '12', '12', '12', '8', '8', '2', '1', '1', '2', '1', '1', '1', '12', '12', '8', '12', '2', '2', '12', '3', '3', '12', '8', '2', '8', '8', '12', '2', '1', '10', '3', '2', '8', '7', '1', '8', '1', '3', '7', '8', '9', '8', '3', '1', '1', '7', '8', '8', '9', '8', '2', '9', '2', '2', '9', '8', '1', '8', '8', '1', '3', '3', '1', '1', '10', '1', '2', '8', '1', '1', '1', '1', '9', '1', '4', '1', '7', '7', '7', '7', '2', '2', '8', '8', '12', '1', '9', '1', '7', '3', '3', '1', '8', '8', '10', '8', '9', '2', '9', '1', '3', '8', '8', '3', '12', '2', '8', '12', '2', '9', '1', '3', '3', '3', '3', '2', '8', '7', '9', '8', '3', '3', '1', '7', '8', '3', '1', '1', '12', '8', '9', '1', '2', '3', '8', '1', '1', '3', '3', '9', '1', '1', '1', '12', '1', '7', '3', '3', '1', '8', '8', '8', '1', '2', '4', '8', '1', '10', '2', '5', '3', '3', '12', '10', '9', '9', '12', '9', '0', '2', '8', '8', '9', '9', '9', '8', '1', '3', '1', '1', '4', '8', '1', '10', '8', '7', '2', '8', '2', '8', '4', '7', '8', '1', '9', '1', '9', '8', '2', '8', '2', '7', '9', '2', '2', '9', '1', '8', '12', '1', '8', '1', '4', '1', '9', '9', '1', '10', '12', '4', '8', '1', '7', '3', '9', '2', '12', '7', '8', '8', '2', '1', '12', '9', '8', '1', '2', '2', '8', '10', '2', '1', '7', '7', '7', '12', '3', '3', '8', '3', '8', '8', '3', '9', '8', '9', '1', '1', '8', '7', '9', '3', '3', '8', '1', '0', '9', '9', '9', '1', '8', '9', '9', '10', '1', '8', '7', '8', '8', '8', '9', '9', '8', '9', '10', '4', '9', '3', '7', '12', '1', '9', '9', '8', '9', '1', '9', '3', '1', '8', '4', '12', '10', '9', '8', '7', '10', '8', '12', '12', '3', '10', '8', '12', '2', '1', '2', '3', '9', '8', '8', '7', '1', '1', '2', '2', '1', '2', '2', '7', '1', '3', '9', '9', '3', '8', '8', '8', '8', '8', '7', '8', '8', '10', '8', '1', '8', '2', '8', '2', '2', '2', '2', '2', '1', '7', '12', '10', '1', '2', '8', '1', '4', '7', '8', '1', '12', '8', '7', '9', '2', '2', '2', '8', '1', '8', '1', '1', '0', '2', '2', '6', '1', '8', '2', '1', '8', '1', '1', '2', '2', '3', '2', '2', '10', '10', '9', '1', '7', '7', '8', '8', '1', '12', '10', '12', '3', '8', '8', '8', '3', '8', '3', '10', '2', '2', '2', '1', '2', '2', '1', '0', '1', '8', '9', '1', '1', '1', '2', '2', '2', '2', '2', '1', '1', '1', '1', '12', '9', '9', '1', '10', '11', '12', '9', '1', '1', '8', '8', '1', '2', '12', '8', '3', '7', '2', '10', '9', '7', '8', '1', '2', '2', '7', '9', '12', '2', '2', '1', '8', '9', '2', '3', '2', '2', '2', '9', '9', '8', '9', '2', '3', '8', '8', '9', '7', '3', '10', '8', '8', '9', '1', '1', '2', '8', '1', '10', '8', '8', '9', '7', '1', '7', '12', '8', '1', '7', '2', '1', '3', '9', '2', '8', '8', '1', '2', '3', '9', '3', '8', '9', '7', '7', '1', '10', '2', '8', '8', '8', '1', '8', '2', '8', '8', '1', '2', '8', '10', '2', '1', '0', '8', '3', '8', '10', '10', '12', '3', '8', '9', '3', '10', '8', '8', '8', '1', '8', '8', '2', '2', '2', '2', '2', '2', '1', '1', '10', '1', '10', '8', '2', '1', '2', '2', '8', '3', '3', '2', '2', '1', '8', '1', '3', '1', '9', '2', '2', '1', '3', '2', '2', '3', '2', '2', '3', '2', '1', '1', '2', '1', '2', '7', '8', '2', '1', '12', '12', '3', '1', '9', '9', '7', '8', '7', '2', '7', '9', '2', '2', '2', '8', '9', '7', '9', '2', '8', '9', '9', '2', '9', '1', '1', '9', '8', '3', '3', '8', '2', '7', '8', '11', '2', '8', '9', '1', '8', '3', '1', '2', '1', '2', '2', '2', '2', '8', '3', '9', '2', '10', '9', '9', '2', '7', '7', '7', '1', '3', '8', '8', '1', '1', '8', '7', '1', '8', '8', '9', '3', '3', '3', '2', '4', '8', '12', '8', '1', '12', '9', '2', '8', '2', '12', '2', '1', '3', '8', '12', '12', '11', '8', '10', '9', '2', '9', '4', '10', '1', '12', '12', '9', '3', '8', '9', '12', '9', '9', '2', '2', '2', '3', '5', '1', '1', '1', '7', '7', '8', '2', '2', '8', '2', '1', '7', '3', '2', '9', '8', '9', '1', '1', '9', '8', '9', '3', '3', '2', '9', '4', '9', '8', '8', '1', '8', '10', '2', '0', '3', '8', '8', '8', '9', '12', '9', '8', '2', '2', '8', '10', '9', '8', '2', '2', '4', '1', '9', '9', '8', '8', '10', '3', '1', '1', '2', '8', '7', '8', '2', '1', '1', '8', '12', '2', '9', '4', '8', '3', '3', '3', '8', '2', '12', '12', '8', '9', '12', '8', '2', '12', '7', '9', '2', '2', '1', '3', '1', '3', '11', '9', '8', '1', '10', '2', '12', '3', '8', '4', '1', '2', '8', '8', '3', '9', '3', '8', '7', '1', '1', '3', '1', '8', '2', '9', '8', '12', '1', '1', '1', '9', '9', '1', '8', '8', '8', '2', '2', '12', '3', '1', '1', '1', '9', '9', '3', '2', '1', '1', '9', '9', '1', '9', '9', '1', '9', '3', '9', '1', '8', '1', '9', '8', '3', '8', '8', '9', '9', '8', '2', '8', '1', '2', '2', '10', '1', '2', '1', '1', '9', '1', '9', '3', '7', '7', '2', '10', '3', '3', '1', '1', '9', '1', '2', '1', '3', '2', '1', '1', '1', '1', '1', '1', '1', '1', '1', '2', '8', '8', '7', '1', '1', '8', '8', '2', '1', '12', '1', '8', '1', '9', '8', '8', '2', '1', '9', '9', '2', '2', '8', '1', '7', '7', '9', '8', '1', '6', '8', '8', '2', '2', '8', '3', '8', '3', '2', '3', '8', '1', '2', '2', '2', '8', '9', '9', '9', '12', '1', '9', '2', '1', '9', '2', '9', '9', '10', '8', '1', '1', '9', '2', '1', '7', '1', '3', '7', '1', '1', '1', '3', '8', '8', '8', '9', '9', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '1', '0', '1', '1', '1', '2', '2', '3', '1', '1', '2', '3', '3', '3', '7', '3', '1', '9', '3', '9', '1', '4', '1', '1', '9', '10', '9', '8', '2', '3', '10', '9', '3', '10', '12', '8', '9', '8', '2', '1', '9', '8', '1', '7', '8', '1', '9', '1', '2', '10', '7', '7', '8', '1', '3', '3', '9', '3', '3', '1', '1', '1', '9', '1', '3', '8', '8', '8', '1', '8', '1', '9', '7', '8', '8', '2', '1', '3', '8', '2', '1', '6', '1', '8', '8', '2', '8', '2', '2', '9', '8', '1', '1', '6', '8', '6', '10', '2', '1', '1', '8', '9', '3', '3', '3', '1', '3', '9', '6', '8', '1', '8', '3', '1', '2', '2', '12', '7', '1', '1', '1', '1', '12', '1', '8', '8', '3', '2', '1', '9', '2', '3', '3', '2', '2', '7', '2', '2', '2', '3', '1', '9', '8', '7', '1', '1', '3', '1', '9', '4', '3', '9', '2', '1', '1', '9', '8', '8', '7', '2', '1', '3', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '2', '10', '1', '1', '8', '2', '8', '1', '8', '1', '3', '1', '2', '1', '1', '3', '1', '2', '1', '1', '1', '1', '1', '1', '1', '2', '1', '8', '1', '9', '8', '8', '1', '10', '8', '8', '1', '3', '9', '8', '9', '8', '2', '1', '3', '4', '1', '12', '1', '9', '8', '2', '7', '2', '1', '2', '9', '2', '1', '3', '8', '1', '2', '3', '8', '2', '10', '10', '10', '9', '9', '9', '3', '8', '3', '1', '1', '1', '1', '1', '1', '2', '1', '9', '2', '7', '9', '8', '7', '9', '1', '1', '1', '1', '8', '2', '2', '3', '3', '2', '12', '8', '3', '1', '1', '12', '9', '8', '2', '2', '9', '1', '2', '2', '9', '1', '1', '2', '4', '8', '1', '8', '10', '2', '9', '9', '9', '1', '2', '1', '1', '2', '2', '2', '12', '1', '2', '8', '3', '9', '9', '3', '2', '9', '9', '7', '2', '8', '8', '9', '2', '9', '1', '1', '1', '1', '1', '2', '8', '2', '1', '2', '2', '8', '3', '1', '1', '9', '1', '3', '10', '7', '9', '8', '8', '2', '2', '9', '2', '2', '2', '2', '2', '2', '9', '1', '3', '6', '4', '2', '2', '9', '1', '9', '8', '1', '1', '1', '7', '3', '2', '9', '9', '1', '9', '7', '2', '8', '9', '9', '12', '10', '8', '10', '2', '3', '1', '2', '2', '12', '12', '3', '12', '1', '2', '1', '2', '1', '2', '8', '2', '3', '12', '12', '8', '2', '2', '9', '2', '9', '2', '3', '3', '1', '1', '1', '12', '3', '7', '1', '3', '1', '2', '9', '2', '7', '2', '1', '8', '3', '7', '8', '1', '1', '3', '8', '3', '2', '9', '3', '1', '1', '1', '1', '11', '1', '2', '6', '1', '2', '3', '8', '9', '9', '2', '9', '2', '1', '1', '3', '1', '12', '9', '1', '1', '9', '1', '9', '9', '8', '9', '8', '1', '2', '7', '1', '0', '9', '8', '2', '3', '2', '4', '1', '1', '1', '8', '1', '12', '1', '2', '3', '3', '9', '9', '9', '9', '9', '7', '9', '3', '9', '1', '7', '3', '3', '3', '3', '7', '8', '2', '2', '2', '8', '2', '3', '9', '9', '3', '1', '8', '9', '12', '8', '8', '3', '2', '1', '6', '1', '9', '1', '1', '1', '1', '1', '1', '2', '2', '1', '2', '2', '2', '2', '9', '2', '5', '3', '2', '2', '8', '2', '1', '8', '3', '3', '8', '3', '1', '3', '9', '2', '8', '9', '4', '4', '9', '8', '8', '7', '7', '3', '1', '8', '8', '3', '2', '2', '10', '3', '3', '2', '2', '2', '2', '12', '3', '1', '2', '7', '7', '12', '2', '1', '2', '1', '1', '2', '1', '3', '3', '3', '1', '3', '9', '4', '8', '8', '4', '2', '4', '2', '8', '9', '1', '1', '2', '2', '2', '8', '7', '2', '8', '2', '9', '1', '8', '1', '1', '2', '1', '1', '1', '2', '9', '9', '8', '1', '8', '1', '4', '8', '1', '2', '7', '9', '2', '1', '8', '9', '8', '9', '1', '1', '2', '3', '2', '12', '7', '1', '2', '4', '4', '4', '2', '2', '2', '9', '8', '8', '1', '12', '3', '3', '2', '2', '3', '12', '1', '5', '1', '1', '3', '2', '8', '8', '2', '2', '2', '3', '9', '8', '8', '9', '8', '8', '11', '8', '2', '9', '2', '3', '1', '7', '1', '8', '2', '8', '1', '1', '1', '8', '1', '1', '1', '7', '8', '8', '2', '3', '2', '1', '8', '8', '1', '8', '4', '8', '9', '8', '9', '8', '1', '2', '3', '2', '1', '2', '2', '3', '1', '9', '4', '8', '1', '1', '9', '1', '1', '8', '2', '2', '2', '1', '7', '3', '3', '1', '3', '3', '12', '3', '1', '1', '3', '1', '1', '4', '8', '4', '4', '3', '3', '3', '4', '8', '1', '9', '9', '9', '4', '1', '7', '4', '8', '4', '9', '8', '1', '1', '3', '8', '9', '1', '9', '7', '1', '1', '9', '1', '8', '8', '8', '1', '1', '8', '2', '2', '2', '9', '4', '4', '1', '2', '1', '2', '9', '1', '1', '8', '5', '5', '5', '9', '12', '4', '1', '2', '1', '8', '8', '1', '9', '9', '2', '12', '1', '1', '7', '8', '8', '4', '1', '2', '8', '11', '2', '3', '2', '2', '1', '11', '2', '2', '2', '2', '9', '9', '2', '2', '9', '3', '1', '9', '1', '1', '1', '7', '8', '1', '8', '9', '1', '2', '9', '8', '1', '2', '9', '8', '12', '2', '2', '10', '8', '2', '2', '8', '8', '1', '2', '6', '1', '1', '7', '7', '2', '3', '2', '1', '1', '8', '1', '10', '8', '12', '6', '4', '1', '4', '1', '2', '9', '8', '9', '4', '8', '8', '10', '8', '8', '8', '8', '8', '2', '2', '8', '4', '9', '3', '3', '3', '3', '3', '9', '2', '1', '1', '2', '3', '2', '1', '2', '3', '3', '3', '2', '2', '3', '3', '2', '1', '2', '3', '3', '3', '3', '12', '9', '3', '2', '2', '3', '1', '5', '1', '1', '3', '2', '2', '1', '2', '9', '3', '9', '8', '1', '8', '7', '7', '9', '1', '1', '7', '9', '10', '1', '7', '1', '4', '3', '8', '9', '4', '9', '9', '2', '9', '2', '8', '4', '2', '2', '9', '5', '2', '1', '10', '9', '3', '1', '1', '1', '2', '1', '2', '9', '1', '7', '9', '9', '8', '2', '2', '8', '9', '1', '9', '8', '2', '3', '2', '2', '1', '2', '2', '2', '1', '1', '8', '1', '9', '4', '12', '8', '2', '7', '2', '2', '1', '9', '3', '1', '8', '12', '1', '1', '2', '1', '10', '2', '6', '12', '12', '4', '9', '1', '9', '10', '1', '2', '8', '2', '1', '8', '6', '12', '9', '9', '2', '1', '7', '8', '9', '5', '8', '1', '8', '8', '3', '1', '8', '2', '8', '2', '2', '1', '2', '9', '2', '8', '1', '12', '8', '2', '7', '3', '3', '12', '2', '3', '3', '9', '9', '3', '3', '3', '3', '3', '2', '1', '4', '1', '4', '8', '8', '2', '1', '2', '9', '12', '1', '10', '12', '1', '2', '9', '4', '8', '10', '3', '2', '8', '7', '4', '8', '9', '2', '7', '1', '10', '2', '2', '2', '8', '4', '2', '2', '1', '1', '8', '7', '9', '2', '1', '9', '8', '9', '12', '4', '8', '2', '8', '7', '7', '1', '9', '2', '1', '1', '9', '1', '8', '2', '3', '9', '9', '1', '3', '2', '9', '2', '2', '8', '1', '1', '2', '1', '2', '1', '9', '8', '2', '2', '4', '8', '8', '9', '5', '5', '9', '8', '9', '6', '2', '1', '12', '2', '3', '8', '8', '9', '2', '2', '1', '3', '2', '10', '9', '10', '7', '2', '8', '4', '2', '8', '9', '8', '1', '3', '8', '8', '1', '8', '3', '8', '8', '1', '10', '3', '1', '5', '8', '8', '9', '2', '2', '9', '2', '1', '9', '1', '4', '1', '12', '8', '8', '2', '1', '1', '1', '1', '1', '9', '2', '9', '8', '9', '5', '9', '1', '8', '2', '4', '1', '1', '1', '9', '12', '4', '1', '1', '9', '8', '4', '1', '8', '9', '8', '8', '9', '9', '9', '9', '9', '9', '1', '3', '4', '2', '2', '8', '2', '9', '2', '3', '8', '3', '1', '7', '2', '1', '1', '4', '2', '9', '2', '1', '3', '2', '9', '2', '9', '3', '8', '1', '9', '1', '1', '2', '8', '8', '8', '2', '3', '9', '9', '8', '2', '4', '1', '5', '1', '1', '9', '9', '2', '1', '1', '1', '9', '9', '1', '3', '9', '2', '2', '9', '9', '8', '3', '9', '9', '8', '2', '2', '8', '9', '2', '2', '8', '9', '10', '10', '2', '8', '7', '9', '9', '4', '2', '4', '11', '2', '1', '7', '8', '8', '8', '9', '4', '9', '8', '8', '2', '1', '1', '9', '8', '3', '3', '8', '8', '7', '9', '1', '3', '4', '8', '10', '2', '4', '4', '8', '2', '8', '2', '8', '9', '2', '1', '2', '2', '3', '1', '9', '4', '5', '9', '9', '1', '1', '1', '1', '1', '1', '7', '1', '1', '1', '8', '1', '1', '1', '1', '1', '9', '1', '2', '1', '12', '9', '3', '9', '9', '4', '1', '9', '1', '1', '2', '11', '9', '3', '2', '2', '2', '4', '2', '3', '2', '7', '7', '8', '1', '10', '9', '5', '7', '1', '10', '8', '1', '2', '7', '1', '12', '5', '8', '4', '2', '9', '9', '8', '2', '9', '8', '8', '10', '2', '3', '2', '7', '2', '8', '10', '3', '2', '9', '9', '9', '1', '1', '1', '1', '8', '2', '10', '9', '8', '1', '2', '2', '2', '1', '2', '1', '3', '9', '12', '2', '2', '4', '8', '1', '3', '9', '4', '9', '1', '9', '9', '2', '1', '1', '9', '7', '10', '3', '10', '2', '9', '2', '9', '2', '8', '2', '1', '2', '3', '1', '1', '1', '8', '5', '8', '3', '1', '1', '4', '2', '2', '1', '3', '8', '2', '1', '1', '1', '2', '9', '4', '1', '8', '1', '2', '8', '2', '9', '2', '2', '5', '5', '2', '5', '2', '1', '8', '8', '1', '3', '12', '2', '4', '3', '2', '2', '2', '6', '2', '3', '3', '1', '8', '2', '2', '1', '1', '8', '8', '10', '1', '9', '2', '9', '12', '0', '2', '1', '3', '2', '2', '1', '2', '1', '9', '9', '2', '9', '8', '8', '1', '7', '8', '1', '2', '6', '9', '8', '10', '1', '12', '2', '2', '1', '8', '2', '1', '12', '7', '4', '2', '10', '2', '2', '1', '2', '1', '2', '1', '1', '2', '1', '9', '8', '10', '2', '10', '9', '9', '12', '5', '3', '2', '8', '2', '8', '3', '3', '1', '4', '9', '10', '2', '4', '8', '1', '2', '10', '12', '12', '3', '1', '1', '2', '9', '6', '6', '1', '9', '1', '8', '1', '8', '8', '8', '8', '2', '2', '2', '8', '8', '2', '1', '1', '1', '1', '1', '8', '8', '5', '8', '8', '1', '8', '9', '9', '1', '7', '12', '9', '9', '7', '2', '2', '7', '8', '9', '8', '2', '9', '6', '3', '4', '8', '4', '1', '2', '9', '8', '8', '8', '2', '1', '1', '8', '8', '3', '1', '9', '9', '7', '1', '2', '2', '2', '9', '4', '1', '2', '12', '1', '8', '2', '1', '9', '1', '8', '9', '9', '2', '5', '4', '2', '1', '2', '8', '12', '8', '10', '3', '8', '8', '8', '2', '2', '8', '2', '8', '2', '4', '8', '10', '2', '7', '2', '1', '8', '8', '9', '5', '8', '1', '12', '7', '10', '2', '2', '2', '1', '4', '8', '12', '1', '8', '2', '2', '9', '1', '8', '3', '1', '1', '5', '10', '2', '1', '2', '4', '8', '9', '3', '2', '8', '7', '4', '4', '9', '9', '2', '8', '1', '1', '2', '4', '7', '1', '9', '1', '1', '1', '8', '4', '1', '3', '1', '2', '2', '9', '1', '8', '8', '2', '2', '9', '9', '1', '8', '8', '1', '9', '8', '8', '2', '2', '2', '2', '1', '1', '1', '7', '4', '1', '1', '4', '2', '3', '3', '2', '3', '9', '2', '8', '2', '1', '5', '1', '2', '2', '1', '3', '5', '2', '9', '1', '1', '9', '8', '1', '4', '8', '1', '8', '2', '4', '8', '4', '7', '9', '8', '1', '2', '2', '4', '4', '11', '3', '11', '1', '3', '8', '1', '9', '6', '8', '9', '3', '9', '8', '9', '2', '3', '3', '7', '1', '7', '2', '9', '9', '1', '8', '8', '1', '5', '5', '8', '8', '1', '2', '1', '3', '9', '9', '5', '9', '8', '1', '4', '3', '8', '1', '1', '2', '1', '8', '9', '8', '2', '2', '2', '1', '2', '4', '10', '8', '1', '9', '1', '8', '1', '1', '9', '9', '8', '11', '1', '8', '2', '1', '2', '8', '2', '8', '8', '2', '9', '6', '10', '1', '8', '6', '4', '5', '11', '2', '2', '8', '1', '8', '1', '8', '1', '4', '8', '1', '1', '1', '8', '3', '7', '3', '3', '7', '1', '1', '4', '8', '1', '6', '4', '2', '2', '1', '1', '1', '1', '6', '2', '2', '8', '1', '8', '5', '5', '2', '1', '1', '2', '3', '7', '1', '1', '6', '2', '9', '3', '9', '8', '9', '2', '1', '4', '2', '9', '13', '9', '2', '8', '12', '4', '1', '1', '9', '5', '1', '1', '1', '1', '8', '2', '1', '1', '2', '9', '0', '12', '1', '1', '8', '9', '8', '3', '2', '2', '12', '8', '2', '2', '2', '1', '10', '1', '9', '8', '9', '5', '2', '2', '9', '2', '8', '4', '8', '3', '7', '9', '2', '2', '7', '2', '2', '8', '1', '1', '9', '10', '9', '8', '2', '9', '0', '7', '1', '1', '1', '9', '8', '2', '8', '12', '8', '3', '2', '8', '8', '7', '7', '8', '12', '2', '1', '3', '9', '2', '2', '8', '2', '8', '8', '12', '8', '8', '1', '8', '9', '2', '4', '4', '7', '2', '1', '1', '9', '3', '7', '1', '3', '9', '8', '2', '2', '3', '1', '3', '10', '2', '3', '7', '1', '8', '1', '1', '10', '3', '10', '8', '4', '1', '2', '4', '3', '10', '9', '3', '9', '5', '8', '7', '2', '9', '3', '3', '2', '11', '8', '1', '1', '7', '2', '7', '2', '9', '4', '8', '1', '12', '8', '7', '9', '1', '0', '2', '6', '4', '9', '3', '2', '4', '12', '9', '8', '5', '9', '2', '8', '4', '6', '8', '9', '3', '1', '8', '7', '3', '1', '1', '1', '8', '9', '1', '8', '7', '9', '8', '2', '1', '2', '2', '3', '2', '2', '1', '1', '4', '9', '7', '3', '2', '8', '8', '9', '1', '9', '7', '4', '2', '2', '8', '5', '9', '1', '5', '7', '1', '4', '6', '9', '10', '1', '8', '2', '9', '1', '9', '2', '9', '8', '2', '9', '9', '11', '2', '12', '2', '2', '9', '4', '8', '1', '9', '9', '2', '8', '9', '1', '2', '9', '3', '8', '8', '1', '7', '8', '8', '2', '3', '2', '2', '2', '2', '9', '8', '10', '2', '2', '1', '9', '11', '1', '8', '8', '2', '4', '2', '1', '2', '8', '8', '9', '2', '8', '9', '2', '3', '1', '2', '1', '3', '10', '8', '1', '2', '4', '1', '9', '8', '9', '3', '3', '9', '8', '2', '11', '3', '7', '1', '8', '7', '1', '8', '8', '8', '1', '4', '8', '8', '1', '9', '3', '1', '1', '2', '8', '8', '8', '2', '1', '8', '10', '2', '12', '8', '8', '9', '1', '8', '1', '3', '2', '2', '10', '7', '2', '2', '1', '1', '8', '2', '9', '1', '2', '9', '3', '3', '10', '1', '4', '7', '10', '12', '2', '4', '10', '8', '6', '2', '9', '1', '8', '3', '1', '2', '8', '8', '2', '12', '10', '2', '10', '9', '1', '2', '5', '8', '12', '8', '8', '7', '8', '8', '1', '8', '2', '2', '9', '1', '8', '10', '5', '5', '5', '8', '1', '9', '1', '13', '1', '2', '2', '1', '2', '4', '9', '2', '1', '9', '13', '11', '1', '9', '9', '3', '10', '8', '7', '8', '8', '4', '2', '1', '9', '4', '3', '1', '1', '10', '9', '2', '8', '6', '2', '1', '1', '3', '2', '8', '1', '1', '1', '1', '2', '4', '1', '1', '10', '1', '8', '1', '1', '9', '1', '7', '1', '1', '9', '9', '2', '12', '9', '1', '8', '1', '12', '1', '8', '3', '8', '1', '8', '1', '2', '10', '1', '1', '2', '2', '9', '3', '9', '12', '1', '9', '1', '2', '8', '7', '2', '1', '1', '3', '2', '8', '4', '4', '5', '12', '7', '8', '9', '6', '1', '8', '9', '2', '1', '9', '1', '4', '1', '8', '2', '1', '1', '8', '7', '4', '9', '1', '8', '4', '2', '7', '5', '1', '3', '9', '2', '8', '2', '1', '2', '9', '1', '1', '2', '3', '4', '3', '1', '2', '2', '10', '2', '2', '8', '1', '1', '2', '4', '9', '2', '11', '9', '8', '9', '2', '1', '10', '4', '1', '1', '10', '7', '2', '10', '8', '1', '10', '8', '8', '8', '8', '8', '8', '2', '2', '2', '8', '3', '4', '1', '1', '1', '2', '1', '8', '8', '2', '8', '3', '8', '3', '1', '2', '3', '1', '1', '10', '1', '4', '4', '3', '3', '1', '9', '1', '1', '2', '1', '1', '6', '1', '1', '1', '9', '1', '4', '1', '1', '1', '1', '1', '1', '2', '12', '8', '8', '1', '9', '12', '10', '8', '2', '12', '1', '1', '2', '1', '7', '10', '7', '1', '2', '1', '10', '8', '2', '2', '8', '2', '2', '1', '9', '8', '8', '8', '8', '3', '1', '4', '2', '8', '3', '10', '10', '1', '9', '1', '9', '1', '9', '1', '8', '3', '1', '1', '8', '8', '8', '6', '3', '4', '1', '1', '1', '9', '9', '5', '2', '10', '2', '6', '2', '2', '7', '1', '8', '2', '11', '1', '1', '1', '2', '1', '9', '4', '4', '2', '8', '4', '6', '8', '8', '10', '10', '2', '1', '2', '9', '1', '8', '8', '3', '4', '8', '8', '2', '8', '8', '1', '2', '1', '2', '9', '4', '1', '3', '3', '3', '8', '8', '8', '2', '6', '4', '3', '3', '12', '3', '4', '9', '6', '8', '7', '7', '8', '6', '4', '1', '10', '6', '3', '3', '8', '1', '1', '8', '1', '11', '8', '1', '1', '2', '3', '11', '7', '2', '1', '1', '1', '9', '1', '2', '1', '1', '9', '8', '1', '1', '8', '0', '4', '1', '1', '2', '1', '1', '2', '2', '9', '1', '10', '1', '3', '8', '9', '3', '3', '1', '10', '1', '9', '1', '1', '7', '4', '1', '8', '10', '1', '2', '10', '1', '1', '2', '3', '3', '9', '1', '8', '10', '1', '7', '9', '1', '9', '6', '3', '9', '2', '8', '4', '12', '12', '3', '1', '1', '9', '1', '9', '2', '1', '1', '1', '1', '1', '9', '2', '1', '2', '10', '9', '8', '8', '10', '8', '2', '3', '2', '1', '12', '8', '12', '2', '9', '1', '3', '2', '5', '2', '2', '8', '2', '2', '4', '10', '9', '10', '9', '4', '2', '4', '12', '8', '2', '1', '8', '3', '9', '2', '9', '4', '1', '1', '1', '1', '1', '9', '6', '2', '9', '2', '1', '3', '2', '2', '5', '8', '9', '1', '6', '3', '1', '2', '2', '2', '6', '1', '8', '10', '8', '3', '3', '13', '2', '9', '9', '10', '6', '12', '8', '2', '9', '2', '3', '8', '8', '1', '2', '2', '3', '2', '10', '1', '8', '9', '1', '8', '9', '8', '8', '2', '10', '2', '1', '1', '1', '9', '9', '12', '1', '1', '4', '8', '3', '2', '2', '4', '4', '2', '2', '1', '1', '1', '9', '2', '2', '4', '1', '1', '8', '8', '8', '9', '2', '13', '9', '9', '4', '1', '9', '7', '10', '10', '8', '1', '9', '1', '1', '1', '8', '12', '4', '1', '2', '1', '1', '1', '7', '8', '3', '3', '1', '2', '2', '7', '2', '2', '10', '1', '8', '4', '0', '1', '9', '4', '1', '1', '2', '10', '10', '7', '9', '3', '12', '11', '2', '8', '10', '8', '9', '4', '3', '4', '10', '1', '1', '9', '3', '10', '1', '5', '3', '3', '2', '2', '1', '1', '6', '1', '7', '1', '1', '12', '1', '1', '1', '10', '1', '8', '8', '1', '3', '8', '3', '1', '2', '8', '3', '4', '2', '8', '8', '6', '1', '1', '4', '1', '2', '9', '5', '1', '7', '8', '9', '2', '4', '9', '8', '5', '9', '9', '9', '1', '2', '1', '3', '8', '9', '2', '9', '8', '9', '1', '8', '13', '9', '3', '8', '1', '4', '1', '3', '1', '6', '4', '10', '2', '2', '7', '8', '8', '12', '8', '9', '9', '9', '10', '3', '12', '12', '2', '9', '10', '9', '7', '2', '3', '4', '10', '10', '2', '7', '7', '1', '7', '3', '9', '1', '9', '8', '2', '9', '1', '8', '1', '10', '8', '8', '2', '7', '8', '1', '8', '9', '8', '1', '4', '1', '8', '8', '8', '1', '1', '10', '3', '2', '2', '6', '8', '9', '8', '2', '1', '1', '3', '8', '8', '1', '4', '4', '3', '9', '10', '4', '2', '4', '2', '9', '2', '2', '9', '1', '1', '10', '1', '1', '2', '1', '1', '1', '9', '1', '8', '8', '8', '2', '3', '8', '1', '6', '7', '3', '1', '9', '1', '2', '1', '6', '1', '2', '10', '1', '4', '2', '3', '2', '9', '9', '4', '9', '9', '1', '2', '8', '4', '7', '8', '7', '13', '9', '1', '2', '1', '9', '3', '3', '6', '8', '9', '10', '9', '2', '2', '3', '12', '1', '1', '1', '13', '2', '1', '7', '10', '8', '9', '1', '2', '9', '1', '1', '1', '2', '1', '12', '5', '6', '9', '10', '3', '1', '10', '1', '3', '2', '8', '9', '2', '10', '8', '8', '9', '2', '8', '9', '2', '2', '2', '8', '8', '2', '3', '1', '4', '8', '1', '9', '9', '2', '8', '10', '9', '1', '2', '12', '8', '8', '0', '2', '1', '1', '9', '10', '1', '6', '2', '2', '10', '2', '2', '4', '2', '2', '12', '1', '6', '7', '8', '9', '1', '3', '5', '7', '3', '1', '3', '1', '1', '3', '2', '6', '3', '2', '1', '8', '3', '5', '2', '1', '1', '8', '1', '1', '0', '2', '3', '5', '3', '1', '1', '9', '8', '8', '8', '8', '4', '7', '2', '9', '12', '5', '9', '3', '12', '3', '9', '1', '9', '1', '7', '3', '9', '3', '10', '1', '2', '1', '4', '9', '8', '2', '9', '11', '2', '8', '9', '4', '1', '1', '1', '1', '1', '1', '1', '9', '1', '9', '1', '1', '1', '12', '3', '10', '3', '6', '1', '7', '10', '8', '3', '7', '1', '1', '2', '6', '1', '3', '9', '2', '8', '1', '8', '8', '1', '8', '10', '12', '1', '10', '12', '10', '1', '8', '8', '1', '4', '7', '9', '2', '1', '9', '3', '8', '12', '10', '6', '1', '6', '8', '3', '1', '3', '1', '10', '10', '9', '8', '1', '2', '9', '8', '8', '1', '9', '8', '8', '3', '1', '3', '9', '12', '1', '8', '1', '5', '8', '11', '5', '5', '1', '2', '1', '3', '1', '1', '1', '9', '8', '8', '1', '9', '10', '8', '4', '1', '1', '8', '10', '8', '9', '1', '10', '1', '7', '2', '1', '1', '9', '6', '2', '9', '8', '1', '1', '1', '9', '8', '8', '1', '9', '9', '1', '1', '2', '7', '9', '2', '9', '9', '8', '7', '6', '8', '7', '8', '2', '9', '2', '1', '8', '8', '8', '2', '6', '3', '1', '8', '12', '12', '8', '7', '1', '2', '2', '7', '1', '9', '8', '1', '4', '1', '1', '8', '5', '8', '1', '8', '8', '8', '8', '1', '2', '11', '1', '1', '1', '2', '7', '1', '9', '2', '2', '2', '9', '8', '9', '2', '9', '1', '8', '1', '7', '11', '13', '1', '9', '8', '2', '2', '1', '2', '3', '10', '3', '10', '4', '3', '9', '1', '8', '1', '1', '13', '1', '1', '3', '8', '1', '2', '2', '2', '9', '4', '1', '8', '1', '9', '3', '6', '8', '5', '2', '8', '2', '8', '1', '9', '2', '2', '8', '1', '1', '4', '8', '9', '2', '2', '7', '1', '8', '12', '2', '12', '8', '2', '9', '9', '3', '8', '1', '1', '9', '9', '9', '2', '8', '1', '9', '1', '8', '9', '8', '2', '2', '4', '1', '9', '10', '10', '8', '8', '1', '1', '2', '1', '4', '1', '2', '1', '1', '3', '12', '8', '8', '8', '8', '10', '2', '8', '12', '1', '9', '9', '8', '9', '1', '2', '1', '8', '8', '8', '2', '8', '8', '3', '10', '8', '9', '1', '1', '1', '3', '4', '8', '10', '6', '3', '1', '3', '2', '8', '9', '3', '5', '4', '1', '5', '2', '9', '9', '9', '1', '1', '1', '9', '9', '6', '10', '9', '8', '12', '2', '9', '1', '2', '1', '1', '1', '8', '1', '12', '1', '12', '1', '9', '1', '8', '1', '2', '1', '2', '8', '8', '10', '1', '9', '1', '2', '2', '8', '8', '3', '12', '1', '11', '2', '10', '12', '11', '2', '1', '10', '1', '1', '9', '3', '10', '2', '7', '1', '8', '1', '9', '9', '2', '1', '3', '12', '5', '9', '9', '8', '1', '8', '8', '8', '1', '1', '3', '9', '8', '9', '8', '3', '10', '3', '7', '8', '9', '2', '3', '1', '9', '10', '9', '2', '8', '2', '6', '4', '1', '1', '3', '8', '2', '3', '9', '1', '2', '1', '8', '1', '2', '2', '2', '2', '9', '9', '4', '8', '9', '1', '1', '2', '9', '1', '5', '2', '9', '8', '10', '1', '2', '10', '8', '5', '6', '4', '8', '1', '1', '1', '8', '9', '2', '8', '8', '8', '12', '2', '2', '1', '8', '1', '2', '1', '9', '8', '8', '9', '1', '1', '8', '7', '1', '8', '0', '2', '3', '8', '1', '1', '8', '1', '1', '12', '3', '2', '10', '8', '10', '6', '1', '8', '8', '10', '8', '8', '1', '4', '8', '1', '2', '9', '8', '1', '1', '3', '3', '5', '1', '1', '2', '1', '2', '1', '9', '8', '3', '9', '8', '2', '10', '9', '1', '8', '1', '10', '9', '10', '1', '2', '3', '8', '1', '3', '1', '9', '9', '8', '1', '9', '8', '8', '9', '1', '9', '2', '8', '8', '2', '13', '8', '10', '8', '9', '3', '2', '1', '3', '1', '12', '10', '10', '1', '11', '1', '2', '4', '2', '10', '1', '8', '11', '1', '1', '1', '2', '2', '8', '2', '9', '8', '9', '9', '12', '8', '1', '9', '2', '4', '1', '8', '3', '9', '1', '6', '5', '8', '9', '3', '3', '2', '1', '1', '1', '8', '11', '1', '8', '7', '1', '11', '9', '1', '7', '9', '9', '8', '8', '8', '12', '8', '1', '2', '9', '8', '8', '2', '10', '9', '8', '8', '4', '10', '9', '8', '2', '10', '10', '2', '1', '2', '1', '2', '1', '4', '1', '7', '1', '1', '3', '12', '9', '8', '5', '9', '10', '1', '9', '9', '2', '7', '4', '1', '1', '8', '12', '8', '2', '2', '9', '4', '1', '1', '5', '9', '1', '8', '12', '8', '1', '1', '4', '9', '10', '2', '3', '1', '3', '3', '3', '8', '1', '2', '1', '2', '1', '2', '9', '1', '2', '8', '9', '4', '8', '8', '10', '2', '12', '5', '5', '1', '8', '1', '7', '8', '1', '1', '8', '9', '12', '4', '9', '3', '1', '2', '5', '9', '2', '1', '2', '2', '1', '2', '8', '1', '8', '8', '2', '2', '1', '8', '4', '1', '2', '10', '9', '10', '8', '1', '10', '4', '8', '8', '10', '2', '8', '1', '1', '3', '10', '1', '4', '2', '8', '3', '3', '2', '9', '8', '5', '5', '9', '3', '10', '8', '1', '2', '1', '2', '9', '1', '1', '2', '9', '9', '8', '8', '11', '9', '4', '1', '8', '2', '7', '1', '2', '8', '2', '9', '9', '8', '2', '9', '9', '8', '8', '1', '2', '8', '1', '1', '2', '9', '8', '1', '4', '9', '1', '8', '12', '1', '12', '2', '8', '2', '9', '1', '12', '1', '2', '9', '1', '1', '9', '3', '2', '9', '9', '2', '11', '8', '7', '9', '9', '9', '8', '8', '1', '2', '8', '1', '1', '4', '1', '2', '10', '1', '9', '2', '1', '1', '1', '9', '1', '13', '4', '3', '2', '1', '1', '2', '2', '7', '8', '7', '1', '1', '2', '8', '8', '3', '4', '8', '2', '10', '13', '8', '8', '9', '2', '2', '1', '1', '10', '4', '2', '7', '9', '8', '2', '1', '9', '8', '1', '1', '9', '8', '9', '8', '2', '8', '2', '2', '9', '2', '1', '9', '1', '2', '4', '8', '2', '1', '1', '8', '1', '4', '1', '9', '3', '9', '2', '7', '1', '9', '6', '1', '10', '2', '2', '2', '2', '2', '10', '10', '10', '9', '9', '9', '3', '12', '10', '1', '1', '10', '1', '1', '2', '2', '1', '2', '3', '12', '8', '8', '1', '2', '9', '9', '1', '1', '10', '8', '9', '8', '3', '1', '1', '3', '1', '10', '1', '1', '1', '4', '1', '1', '1', '7', '10', '2', '1', '8', '1', '9', '3', '1', '10', '8', '8', '1', '5', '9', '1', '2', '1', '8', '8', '1', '3', '1', '10', '2', '1', '1', '2', '8', '2', '5', '3', '5', '2', '1', '1', '9', '1', '7', '9', '9', '2', '8', '1', '12', '4', '1', '4', '9', '9', '4', '8', '1', '10', '10', '8', '8', '8', '9', '2', '3', '5', '1', '8', '1', '1', '1', '7', '10', '1', '4', '12', '3', '2', '2', '9', '1', '1', '1', '2', '8', '4', '1', '7', '3', '12', '8', '6', '2', '2', '7', '9', '11', '1', '10', '9', '1', '8', '12', '2', '11', '1', '2', '3', '1', '1', '1', '8', '9', '2', '2', '2', '2', '3', '3', '8', '10', '4', '1', '2', '9', '8', '2', '2', '9', '1', '8', '11', '4', '2', '8', '7', '1', '1', '7', '3', '9', '9', '4', '5', '8', '10', '2', '9', '9', '9', '8', '1', '9', '2', '2', '2', '1', '2', '9', '4', '3', '12', '4', '3', '2', '3', '8', '3', '10', '1', '10', '1', '1', '8', '10', '6', '8', '9', '9', '1', '2', '9', '2', '9', '3', '8', '1', '1', '12', '1', '5', '1', '10', '8', '10', '2', '7', '1', '1', '1', '1', '3', '2', '3', '5', '9', '1', '2', '1', '9', '8', '10', '9', '8', '1', '8', '1', '8', '2', '8', '1', '2', '8', '1', '2', '5', '8', '12', '2', '1', '1', '1', '1', '1', '8', '8', '2', '1', '4', '2', '8', '8', '8', '2', '9', '9', '11', '1', '2', '9', '3', '1', '1', '8', '9', '2', '4', '10', '1', '9', '8', '4', '9', '8', '10', '8', '8', '9', '2', '8', '9', '3', '3', '3', '4', '3', '2', '2', '10', '8', '2', '1', '5', '1', '9', '1', '1', '2', '1', '2', '3', '1', '1', '8', '8', '1', '10', '8', '8', '1', '6', '4', '8', '2', '5', '8', '1', '3', '8', '2', '8', '9', '1', '1', '8', '12', '10', '1', '5', '8', '2', '8', '10', '9', '9', '1', '6', '9', '1', '10', '8', '2', '9', '10', '1', '2', '1', '1', '8', '8', '3', '9', '8', '1', '10', '2', '8', '1', '10', '1', '5', '1', '1', '1', '5', '1', '1', '1', '4', '1', '1', '1', '8', '2', '8', '1', '8', '3', '8', '9', '1', '8', '1', '8', '1', '1', '2', '2', '1', '12', '9', '9', '1', '8', '2', '1', '2', '8', '1', '9', '1', '1', '2', '1', '2', '2', '2', '1', '9', '9', '8', '8', '1', '1', '9', '10', '8', '4', '3', '3', '4', '9', '1', '3', '3', '10', '2', '2', '1', '8', '4', '1', '9', '4', '1', '1', '9', '8', '10', '4', '9', '9', '8', '2', '1', '3', '4', '9', '3', '8', '9', '1', '1', '2', '1', '7', '8', '2', '9', '1', '9', '2', '6', '9', '10', '8', '1', '1', '5', '9', '5', '1', '9', '10', '9', '3', '3', '8', '8', '2', '9', '2', '8', '2', '1', '8', '5', '8', '3', '10', '1', '1', '2', '1', '8', '1', '4', '3', '2', '9', '4', '9', '8', '8', '8', '1', '3', '9', '1', '9', '9', '1', '2', '1', '9', '8', '9', '8', '1', '1', '1', '4', '2', '2', '9', '9', '1', '1', '1', '3', '6', '1', '2', '3', '1', '4', '2', '9', '1', '9', '9', '1', '8', '2', '1', '9', '1', '8', '1', '8', '8', '2', '1', '9', '9', '6', '8', '8', '9', '8', '10', '8', '9', '5', '1', '1', '1', '1', '8', '1', '8', '12', '8', '1', '12', '9', '2', '8', '1', '1', '2', '1', '8', '6', '9', '9', '8', '1', '8', '9', '7', '7', '8', '9', '9', '1', '8', '3', '8', '1', '3', '3', '8', '9', '8', '2', '8', '1', '2', '9', '8', '1', '1', '1', '1', '9', '1', '9', '8', '12', '3', '8', '8', '1', '10', '10', '10', '2', '2', '1', '10', '2', '2', '10', '8', '12', '8', '1', '1', '1', '2', '1', '1', '3', '8', '1', '2', '2', '2', '1', '1', '6', '2', '8', '8', '8', '1', '1', '1', '9', '9', '2', '8', '10', '2', '8', '2', '1', '2', '9', '9', '2', '1', '1', '8', '8', '1', '3', '1', '8', '9', '1', '10', '9', '1', '1', '1', '1', '7', '2', '2', '1', '2', '8', '8', '1', '2', '1', '3', '2', '1', '8', '3', '9', '2', '10', '2', '10', '1', '2', '1', '1', '13', '9', '10', '8', '2', '2', '1', '1', '8', '2', '2', '1', '9', '1', '1', '8', '9', '9', '2', '9', '2', '2', '1', '1', '1', '8', '9', '4', '9', '9', '1', '8', '2', '1', '1', '8', '2', '2', '2', '8', '1', '1', '8', '2', '10', '2', '9', '9', '7', '1', '1', '2', '2', '8', '3', '1', '2', '1', '1', '8', '1', '1', '1', '9', '1', '1', '8', '8', '1', '9', '6', '1', '9', '1', '8', '1', '8', '8', '8', '1', '3', '8', '8', '2', '6', '3', '2', '1', '10', '1', '8', '2', '1', '6', '1', '1', '1', '8', '1', '10', '1', '1', '8', '9', '2', '6', '9', '2', '9', '4', '1', '8', '1', '3', '7', '8', '8', '5', '8', '7', '1', '1', '1', '13', '8', '3', '1', '1', '1', '1', '2', '8', '2', '12', '8', '1', '2', '5', '1', '8', '2', '10', '8', '2', '10', '1', '5', '2', '1', '1', '5', '2', '2', '7', '8', '2', '9', '9', '1', '10', '1', '8', '2', '5', '8', '1', '5', '1', '10', '1', '1', '9', '8', '6', '1', '8', '9', '8', '1', '8', '2', '2', '8', '3', '9', '1', '9', '3', '8', '4', '4', '3', '3', '1', '1', '1', '8', '9', '1', '1', '8', '3', '1', '2', '8', '2', '1', '1', '10', '1', '2', '2', '1', '1', '8', '2', '8', '2', '10', '2', '2', '10', '8', '8', '8', '13', '2', '2', '9', '2', '5', '9', '2', '1', '8', '8', '8', '1', '12', '12', '2', '9', '2', '1', '8', '2', '1', '8', '1', '9', '9', '2', '7', '1', '1', '7', '1', '4', '3', '1', '10', '3', '10', '9', '4', '6', '7', '5', '2', '1', '8', '8', '1', '4', '9', '2', '1', '8', '1', '1', '1', '1', '1', '8', '9', '8', '8', '8', '8', '9', '8', '10', '1', '8', '2', '7', '8', '9', '1', '2', '5', '9', '10', '8', '9', '12', '1', '6', '1', '2', '10', '9', '4', '1', '8', '10', '10', '1', '1', '1', '1', '1', '2', '3', '8', '1', '2', '10', '2', '1', '2', '4', '2', '2', '9', '1', '4', '2', '1', '1', '12', '12', '9', '9', '7', '1', '8', '4', '9', '9', '8', '1', '7', '3', '2', '1', '1', '4', '8', '1', '5', '8', '2', '8', '1', '14', '9', '12', '1', '10', '2', '1', '1', '8', '8', '6', '6', '3', '1', '2', '8', '1', '3', '2', '1', '8', '8', '1', '10', '8', '2', '8', '2', '5', '9', '1', '8', '8', '1', '1', '8', '8', '13', '1', '3', '8', '4', '1', '9', '7', '8', '8', '8', '3', '8', '1', '4', '2', '8', '8', '8', '7', '1', '1', '2', '8', '8', '9', '8', '8', '2', '3', '9', '9', '2', '1', '2', '2', '1', '8', '2', '6', '2', '1', '1', '9', '10', '3', '8', '2', '3', '2', '8', '8', '1', '8', '8', '8', '9', '1', '4', '1', '9', '2', '1', '8', '0', '1', '8', '1', '1', '8', '9', '9', '1', '3', '1', '2', '8', '8', '7', '1', '1', '2', '9', '2', '10', '2', '8', '2', '2', '2', '7', '1', '9', '8', '1', '3', '9', '2', '1', '10', '8', '1', '4', '1', '8', '5', '9', '8', '8', '1', '2', '2', '1', '8', '8', '6', '8', '1', '1', '8', '3', '2', '2', '1', '8', '1', '7', '6', '8', '1', '1', '1', '9', '1', '1', '1', '2', '1', '11', '10', '10', '11', '9']\n",
            "     filenum                                               text  label\n",
            "0          1  each other. But the section length and therefo...      8\n",
            "1          2  v. United States, L.R.A.,N.S. see Athanasaw v....      1\n",
            "2          3  has made no order which changes the appellant'...      8\n",
            "3          4  to withhold from judicial scrutiny has now bee...      2\n",
            "4          6  instrumentality whose sole purpose is to expor...      8\n",
            "...      ...                                                ...    ...\n",
            "6717    8411  substantially similar to those to which we giv...      1\n",
            "6718    8412  is \"strong evidence\" to the contrary. Ylst v. ...      1\n",
            "6719    8413  \"for the purpose of disturbing the public peac...      2\n",
            "6720    8414  that the sentencing phase of his trial violate...      1\n",
            "6721    8417  detailed exceptions. The controversy is now be...     10\n",
            "\n",
            "[6722 rows x 3 columns]\n"
          ]
        }
      ],
      "source": [
        "fh = open('sc_model_2.txt')\n",
        "filenums=[]\n",
        "records=[]\n",
        "for record in fh:\n",
        "    record=record.split(\"---\")\n",
        "    filenum=record[0]\n",
        "    modelnum=record[1]\n",
        "    text=record[2]\n",
        "    filenums.append(filenum)\n",
        "    records.append(text)\n",
        "    \n",
        "print(len(filenums))\n",
        "print(len(records))\n",
        "\n",
        "summarized_data = pd.DataFrame(list(zip(filenums, records)),columns =['filenum','text'])\n",
        "len_list = [len(ele.split()) for ele in records]\n",
        "res = 0 if len(len_list) == 0 else (float(sum(len_list)) / len(len_list))\n",
        "print(\"Average Length %s\" % res)\n",
        "\n",
        "temp_file = open(\"labels_sc.txt\", \"r\")\n",
        "#temp_file = open(\"labels_sc_279.txt\", \"r\")\n",
        "\n",
        "data = temp_file.read()\n",
        "\n",
        "label_list = data.split(\"\\n\")\n",
        "print(label_list)\n",
        "label_list = label_list[0:-1]\n",
        "print(label_list)\n",
        "label_list = [int(i) for i in label_list]\n",
        "temp_file.close()\n",
        "\n",
        "\n",
        "new_label_list=[]\n",
        "for num in filenums:\n",
        "  new_label_list.append(label_list[int(num)-1])\n",
        "summarized_data['label'] = new_label_list\n",
        "\n",
        "print(summarized_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bK-mvfm1v8-i",
        "outputId": "30b493f0-987a-4e4f-85f0-550334094f09"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_1[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_roberta_model[0][0]']       \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 15)           7695        ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 8  1  8 ...  2  1 10]\n",
            "[   1    2    3 ... 8413 8414 8417]\n",
            "[6622   35 6713 4989 8233 7444 5061 3942 8344  454 5464  317 4515 3001\n",
            " 6991 2875 1125 3885 6647 4857 4504  214 8340 2969 3847 4609 5092 1346\n",
            " 5993 7358 2752 7651 7301 8188 7073 4782 6069 2346 6445 6483 1663 4952\n",
            " 5072 1967 4071 7590  860 6418 5030 7297  626 5463 6029 6302 3819 7256\n",
            " 5667 1235 3993 1439 1323 4263 3835 2624 5801 4293 8129  320 5866 3247\n",
            " 5777   19 6059 4837 3843 1666  858 1418 5409 5274 6205 3167 2356 5997\n",
            " 6685  223 3444 3310 7721 5928 2360 7197 4252 8343  909 4354 4752 1421\n",
            "  649  829 1916 3488 2496 5765 6612 4004 4052 1403 6697 5268 1010 6293\n",
            " 6380 4602 6057 1851 4577 4236 7978 5083 6552 1058  361 7706 4186 4452\n",
            " 4396  749 8351 3834  812 5108  463 1743 6600 8264 8064 7121 8279 5556\n",
            " 5119 4673 7947 1882 3350 7617 7152 5199 4282 1092 8003 5025 5051  933\n",
            " 3969 1927 3615 1221 8062 1489 3296 7442 5459 2031 7351  234 2313 8015\n",
            " 7416 7921 6574 6257 8396  941 4520 3300 5640  240 7810 7425 1321 6580\n",
            "  928 1170 5404 8273 6742 7005   94 7459    9 7861 7975 1018 5029 2140\n",
            " 1415 7061 7767 4990 8124 5927 6051 3269 6916 5959 2846 6818 7052 7775\n",
            " 7582 8116 4240 4767 5248  430 1812 2175 6055 8331 3916  998 1564 7326\n",
            " 1559  292 3065 6764 6854 4915 5552 3583 1917 5902 4229 2144 7565 3712\n",
            " 7224 4502 1406 1651 6256 3827 6525 6318 2686 3616 5369 8235 1704 1011\n",
            " 3351 8089 8021  102  224 5671 8294 7707 2985 6624 5146 1428 7381 3709\n",
            " 6824 6308  701 2281 6837 5436 1741 3520 5995 2649 4886  966 5476 2406\n",
            " 1199 8387  513 5449 1028 2226 6166 4387  350 6123  678 5236 5041  734\n",
            " 3412 6085  587  489 5760 3376 4982 5115 4655 5575 3839 6743 7173 5724\n",
            " 7689 7787  520 8302 3150 5713 2247 8128 4607 6660 3505 4610 2866 4161\n",
            " 6905 7096  768 1062 4411 6087 2096 4903 7546 6630 2928 4640 7125 1792\n",
            " 2063 1700 2305 7928 8095  737 5849  922 2359 6015 4489 4361 6081 6346\n",
            " 5014 5167 1023 8009 3335  268 4777 5221 4323 6168  759  711 5702 6129\n",
            " 7181 1860 7192 1347 5467 3378 6579   67 2693 6134 4841 4270 4611 2943\n",
            " 6746 7696 3466 3950 4693 6125 8370 6142 6131  995 5548 2112 1721 5067\n",
            " 4015 7092 3156   25 1749 5554 4878 8248  458 6836 1006 5310 2698 6960\n",
            " 2339 1650 6227 4104 5130 7089 6993 2400 1289 4305 5585 7933 6284 7057\n",
            " 3807 5790 8347 2390 5280 2841 7220 1384 7693 8090 4171 5260  133 7451\n",
            " 3957 6793 1719 7379 4743 4746 5435 6216 4858 4417 8355 4492 2648 5104\n",
            " 5278 6539  743 7308  158 7543 2168 5913 7111 3414 5788 7592   39 7157\n",
            " 5647 4555 3956 1045 3617 4630 4384 6596   96 4539 6749 7202 2413 4749\n",
            " 5122 5052 1598 5929  252 5565  336 6008 4564 5580 7237 1466 1568 5958\n",
            " 7657 4327 5091 6918 6487 4082 2989 7448 4715 3382  467 3784 4624  764\n",
            " 4419 1117 7954 7122 6392  505 3174 1534  323 4164 3659 7526  498 5944\n",
            " 4572 2307 2929 7480 4412  731 4563 7692 1869 2472 6966 4238 6549 1594\n",
            " 2276 7269 2545 2216  605 6587 6315 5297 5282 4816 1619 5504 2536 6399\n",
            " 3917 6342 5754 5721 7385 7909 6126 7382 8382 4383 5502 7130  485 5862\n",
            " 4936 7674 3575 5372 3893 7109 4605 5211 5462 8065   50 4080 3334 8167\n",
            " 8378 5595 1873 8207 7076 4106 3719 4627  395  231 2405 5419 6973 3480\n",
            " 3246 5414 5931 8208 4825 6203 3019 5822   63 5854 6543 5787 4764 6701\n",
            " 6562 3822 4861 3051 1315 4805 8318 1089 8169 8218 7549  707 1284 1076\n",
            " 2465 3815 4636 3188  697   80 1455  381 7025 7127 3992 4026 2777 4961\n",
            " 7531  744 6640 1047 6684 4297 7939 7384  446 3287 6675 7752 5279 8196\n",
            " 3020 1203 4192 1552 4253 2887 2556 5729 1898 3208 5973 7033 1431 5915\n",
            " 8312 5799 7105  511 6404 2852 1561 4150 4835 1178]\n"
          ]
        }
      ],
      "source": [
        "gc.collect()\n",
        "tf.keras.backend.clear_session()\n",
        "dbert_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "dbert_model = TFRobertaModel.from_pretrained('roberta-base')\n",
        "max_len=512\n",
        "sentences=summarized_data['text']\n",
        "labels=summarized_data['label']\n",
        "filenumbers=summarized_data['filenum']\n",
        "len(sentences),len(labels),len(filenumbers)\n",
        "\n",
        "model_2=create_model()\n",
        "input_ids=[]\n",
        "attention_masks=[]\n",
        "\n",
        "for sent in sentences:\n",
        "  dbert_inps=dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n",
        "  input_ids.append(dbert_inps['input_ids'])\n",
        "  attention_masks.append(dbert_inps['attention_mask'])\n",
        "input_ids=np.asarray(input_ids)\n",
        "\n",
        "attention_masks=np.array(attention_masks)\n",
        "labels=np.array(labels)\n",
        "filenumbers=[int(i) for i in filenumbers]\n",
        "filenumbers=np.array(filenumbers)\n",
        "print(labels)\n",
        "print(filenumbers)\n",
        "\n",
        "#train_inp,val_inp,train_label,val_label,train_mask,val_mask,train_fnum,val_fnum=train_test_split(input_ids,labels,attention_masks,filenumbers,test_size=0.1,random_state=42)\n",
        "\n",
        "new_train_inp_2=[]\n",
        "new_val_inp_2=[]\n",
        "new_train_label_2=[]\n",
        "new_val_label_2=[]\n",
        "new_train_mask_2=[]\n",
        "new_train_fnum_2=[]\n",
        "new_val_fnum_2=[]\n",
        "new_val_mask_2=[]\n",
        "for i in range(len(train_fnum)):\n",
        "  if train_fnum[i] in filenumbers:\n",
        "    new_train_inp_2.append(train_inp[i])\n",
        "    #new_val_inp.append(val_inp[i])\n",
        "    new_train_label_2.append(train_label[i])\n",
        "    #new_val_label.append(val_label[i])\n",
        "    new_train_mask_2.append(train_mask[i])\n",
        "    #new_val_mask.append(val_mask[i])\n",
        "    new_train_fnum_2.append(train_fnum[i])\n",
        "\n",
        "for i in range(len(val_fnum)):\n",
        "  if val_fnum[i] in filenumbers:\n",
        "    #new_train_inp.append(train_inp[i])\n",
        "    new_val_inp_2.append(val_inp[i])\n",
        "    #new_train_label.append(train_label[i])\n",
        "    new_val_label_2.append(val_label[i])\n",
        "    #new_train_mask.append(train_mask[i])\n",
        "    new_val_mask_2.append(val_mask[i])\n",
        "    new_val_fnum_2.append(val_fnum[i])\n",
        "\n",
        "# print(new_train_label)\n",
        "# print(len(new_train_label))\n",
        "# print(new_val_label)\n",
        "# print(len(new_val_label))\n",
        "new_train_inp_2=np.array(new_train_inp_2)\n",
        "new_val_inp_2=np.array(new_val_inp_2)\n",
        "new_train_label_2=np.array(new_train_label_2)\n",
        "new_val_label_2=np.array(new_val_label_2)\n",
        "new_train_mask_2=np.array(new_train_mask_2)\n",
        "new_train_fnum_2=np.array(new_train_fnum_2)\n",
        "new_val_fnum_2=np.array(new_val_fnum_2)\n",
        "new_val_mask_2=np.array(new_val_mask_2)\n",
        "\n",
        "print(new_val_fnum_2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "RFTWfoELNpYx",
        "outputId": "83e3b9a0-1455-4802-b2eb-3ba940b3929a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_1[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_roberta_model[0][0]']       \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 15)           7695        ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 8  1  8 ...  2  1 10]\n",
            "[   1    2    3 ... 8413 8414 8417]\n",
            "[6622   35 6713 4989 8233 7444 5061 3942 8344  454 5464  317 4515 3001\n",
            " 6991 2875 1125 3885 6647 4857 4504  214 8340 2969 3847 4609 5092 1346\n",
            " 5993 7358 2752 7651 7301 8188 7073 4782 6069 2346 6445 6483 1663 4952\n",
            " 5072 1967 4071 7590  860 6418 5030 7297  626 5463 6029 6302 3819 7256\n",
            " 5667 1235 3993 1439 1323 4263 3835 2624 5801 4293 8129  320 5866 3247\n",
            " 5777   19 6059 4837 3843 1666  858 1418 5409 5274 6205 3167 2356 5997\n",
            " 6685  223 3444 3310 7721 5928 2360 7197 4252 8343  909 4354 4752 1421\n",
            "  649  829 1916 3488 2496 5765 6612 4004 4052 1403 6697 5268 1010 6293\n",
            " 6380 4602 6057 1851 4577 4236 7978 5083 6552 1058  361 7706 4186 4452\n",
            " 4396  749 8351 3834  812 5108  463 1743 6600 8264 8064 7121 8279 5556\n",
            " 5119 4673 7947 1882 3350 7617 7152 5199 4282 1092 8003 5025 5051  933\n",
            " 3969 1927 3615 1221 8062 1489 3296 7442 5459 2031 7351  234 2313 8015\n",
            " 7416 7921 6574 6257 8396  941 4520 3300 5640  240 7810 7425 1321 6580\n",
            "  928 1170 5404 8273 6742 7005   94 7459    9 7861 7975 1018 5029 2140\n",
            " 1415 7061 7767 4990 8124 5927 6051 3269 6916 5959 2846 6818 7052 7775\n",
            " 7582 8116 4240 4767 5248  430 1812 2175 6055 8331 3916  998 1564 7326\n",
            " 1559  292 3065 6764 6854 4915 5552 3583 1917 5902 4229 2144 7565 3712\n",
            " 7224 4502 1406 1651 6256 3827 6525 6318 2686 3616 5369 8235 1704 1011\n",
            " 3351 8089 8021  102  224 5671 8294 7707 2985 6624 5146 1428 7381 3709\n",
            " 6824 6308  701 2281 6837 5436 1741 3520 5995 2649 4886  966 5476 2406\n",
            " 1199 8387  513 5449 1028 2226 6166 4387  350 6123  678 5236 5041  734\n",
            " 3412 6085  587  489 5760 3376 4982 5115 4655 5575 3839 6743 7173 5724\n",
            " 7689 7787  520 8302 3150 5713 2247 8128 4607 6660 3505 4610 2866 4161\n",
            " 6905 7096  768 1062 4411 6087 2096 4903 7546 6630 2928 4640 7125 1792\n",
            " 2063 1700 2305 7928 8095  737 5849  922 2359 6015 4489 4361 6081 6346\n",
            " 5014 5167 1023 8009 3335  268 4777 5221 4323 6168  759  711 5702 6129\n",
            " 7181 1860 7192 1347 5467 3378 6579   67 2693 6134 4841 4270 4611 2943\n",
            " 6746 7696 3466 3950 4693 6125 8370 6142 6131  995 5548 2112 1721 5067\n",
            " 4015 7092 3156   25 1749 5554 4878 8248  458 6836 1006 5310 2698 6960\n",
            " 2339 1650 6227 4104 5130 7089 6993 2400 1289 4305 5585 7933 6284 7057\n",
            " 3807 5790 8347 2390 5280 2841 7220 1384 7693 8090 4171 5260  133 7451\n",
            " 3957 6793 1719 7379 4743 4746 5435 6216 4858 4417 8355 4492 2648 5104\n",
            " 5278 6539  743 7308  158 7543 2168 5913 7111 3414 5788 7592   39 7157\n",
            " 5647 4555 3956 1045 3617 4630 4384 6596   96 4539 6749 7202 2413 4749\n",
            " 5122 5052 1598 5929  252 5565  336 6008 4564 5580 7237 1466 1568 5958\n",
            " 7657 4327 5091 6918 6487 4082 2989 7448 4715 3382  467 3784 4624  764\n",
            " 4419 1117 7954 7122 6392  505 3174 1534  323 4164 3659 7526  498 5944\n",
            " 4572 2307 2929 7480 4412  731 4563 7692 1869 2472 6966 4238 6549 1594\n",
            " 2276 7269 2545 2216  605 6587 6315 5297 5282 4816 1619 5504 2536 6399\n",
            " 3917 6342 5754 5721 7385 7909 6126 7382 8382 4383 5502 7130  485 5862\n",
            " 4936 7674 3575 5372 3893 7109 4605 5211 5462 8065   50 4080 3334 8167\n",
            " 8378 5595 1873 8207 7076 4106 3719 4627  395  231 2405 5419 6973 3480\n",
            " 3246 5414 5931 8208 4825 6203 3019 5822   63 5854 6543 5787 4764 6701\n",
            " 6562 3822 4861 3051 1315 4805 8318 1089 8169 8218 7549  707 1284 1076\n",
            " 2465 3815 4636 3188  697   80 1455  381 7025 7127 3992 4026 2777 4961\n",
            " 7531  744 6640 1047 6684 4297 7939 7384  446 3287 6675 7752 5279 8196\n",
            " 3020 1203 4192 1552 4253 2887 2556 5729 1898 3208 5973 7033 1431 5915\n",
            " 8312 5799 7105  511 6404 2852 1561 4150 4835 1178]\n",
            "Sun Jun  5 06:18:33 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0    39W / 300W |  15237MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "757/757 [==============================] - 277s 346ms/step - loss: 7.5566 - accuracy: 0.5593 - val_loss: 6.8514 - val_accuracy: 0.7126\n",
            "Epoch 2/5\n",
            "757/757 [==============================] - 262s 346ms/step - loss: 6.5529 - accuracy: 0.7636 - val_loss: 6.3611 - val_accuracy: 0.7500\n",
            "Epoch 3/5\n",
            "757/757 [==============================] - 261s 345ms/step - loss: 6.0330 - accuracy: 0.8208 - val_loss: 6.0442 - val_accuracy: 0.7605\n",
            "Epoch 4/5\n",
            "757/757 [==============================] - 258s 340ms/step - loss: 5.6220 - accuracy: 0.8555 - val_loss: 5.7666 - val_accuracy: 0.7605\n",
            "Epoch 5/5\n",
            "757/757 [==============================] - 258s 341ms/step - loss: 5.1962 - accuracy: 0.8943 - val_loss: 5.4981 - val_accuracy: 0.7545\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_3[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_roberta_model[1][0]']       \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 15)           7695        ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.7363420427553444\n",
            "Weighted F1: 0.7349716695723489\n",
            "Micro F1: 0.7363420427553444\n",
            "Weighted Precision: 0.7433190830221269\n",
            "Micro Precision: 0.7363420427553444\n",
            "Weighted Recall: 0.7363420427553444\n",
            "Micro Recall: 0.7363420427553444\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_1[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_roberta_model[0][0]']       \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 15)           7695        ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 8  1  8 ...  2  1 10]\n",
            "[   1    2    3 ... 8413 8414 8417]\n",
            "[6622   35 6713 4989 8233 7444 5061 3942 8344  454 5464  317 4515 3001\n",
            " 6991 2875 1125 3885 6647 4857 4504  214 8340 2969 3847 4609 5092 1346\n",
            " 5993 7358 2752 7651 7301 8188 7073 4782 6069 2346 6445 6483 1663 4952\n",
            " 5072 1967 4071 7590  860 6418 5030 7297  626 5463 6029 6302 3819 7256\n",
            " 5667 1235 3993 1439 1323 4263 3835 2624 5801 4293 8129  320 5866 3247\n",
            " 5777   19 6059 4837 3843 1666  858 1418 5409 5274 6205 3167 2356 5997\n",
            " 6685  223 3444 3310 7721 5928 2360 7197 4252 8343  909 4354 4752 1421\n",
            "  649  829 1916 3488 2496 5765 6612 4004 4052 1403 6697 5268 1010 6293\n",
            " 6380 4602 6057 1851 4577 4236 7978 5083 6552 1058  361 7706 4186 4452\n",
            " 4396  749 8351 3834  812 5108  463 1743 6600 8264 8064 7121 8279 5556\n",
            " 5119 4673 7947 1882 3350 7617 7152 5199 4282 1092 8003 5025 5051  933\n",
            " 3969 1927 3615 1221 8062 1489 3296 7442 5459 2031 7351  234 2313 8015\n",
            " 7416 7921 6574 6257 8396  941 4520 3300 5640  240 7810 7425 1321 6580\n",
            "  928 1170 5404 8273 6742 7005   94 7459    9 7861 7975 1018 5029 2140\n",
            " 1415 7061 7767 4990 8124 5927 6051 3269 6916 5959 2846 6818 7052 7775\n",
            " 7582 8116 4240 4767 5248  430 1812 2175 6055 8331 3916  998 1564 7326\n",
            " 1559  292 3065 6764 6854 4915 5552 3583 1917 5902 4229 2144 7565 3712\n",
            " 7224 4502 1406 1651 6256 3827 6525 6318 2686 3616 5369 8235 1704 1011\n",
            " 3351 8089 8021  102  224 5671 8294 7707 2985 6624 5146 1428 7381 3709\n",
            " 6824 6308  701 2281 6837 5436 1741 3520 5995 2649 4886  966 5476 2406\n",
            " 1199 8387  513 5449 1028 2226 6166 4387  350 6123  678 5236 5041  734\n",
            " 3412 6085  587  489 5760 3376 4982 5115 4655 5575 3839 6743 7173 5724\n",
            " 7689 7787  520 8302 3150 5713 2247 8128 4607 6660 3505 4610 2866 4161\n",
            " 6905 7096  768 1062 4411 6087 2096 4903 7546 6630 2928 4640 7125 1792\n",
            " 2063 1700 2305 7928 8095  737 5849  922 2359 6015 4489 4361 6081 6346\n",
            " 5014 5167 1023 8009 3335  268 4777 5221 4323 6168  759  711 5702 6129\n",
            " 7181 1860 7192 1347 5467 3378 6579   67 2693 6134 4841 4270 4611 2943\n",
            " 6746 7696 3466 3950 4693 6125 8370 6142 6131  995 5548 2112 1721 5067\n",
            " 4015 7092 3156   25 1749 5554 4878 8248  458 6836 1006 5310 2698 6960\n",
            " 2339 1650 6227 4104 5130 7089 6993 2400 1289 4305 5585 7933 6284 7057\n",
            " 3807 5790 8347 2390 5280 2841 7220 1384 7693 8090 4171 5260  133 7451\n",
            " 3957 6793 1719 7379 4743 4746 5435 6216 4858 4417 8355 4492 2648 5104\n",
            " 5278 6539  743 7308  158 7543 2168 5913 7111 3414 5788 7592   39 7157\n",
            " 5647 4555 3956 1045 3617 4630 4384 6596   96 4539 6749 7202 2413 4749\n",
            " 5122 5052 1598 5929  252 5565  336 6008 4564 5580 7237 1466 1568 5958\n",
            " 7657 4327 5091 6918 6487 4082 2989 7448 4715 3382  467 3784 4624  764\n",
            " 4419 1117 7954 7122 6392  505 3174 1534  323 4164 3659 7526  498 5944\n",
            " 4572 2307 2929 7480 4412  731 4563 7692 1869 2472 6966 4238 6549 1594\n",
            " 2276 7269 2545 2216  605 6587 6315 5297 5282 4816 1619 5504 2536 6399\n",
            " 3917 6342 5754 5721 7385 7909 6126 7382 8382 4383 5502 7130  485 5862\n",
            " 4936 7674 3575 5372 3893 7109 4605 5211 5462 8065   50 4080 3334 8167\n",
            " 8378 5595 1873 8207 7076 4106 3719 4627  395  231 2405 5419 6973 3480\n",
            " 3246 5414 5931 8208 4825 6203 3019 5822   63 5854 6543 5787 4764 6701\n",
            " 6562 3822 4861 3051 1315 4805 8318 1089 8169 8218 7549  707 1284 1076\n",
            " 2465 3815 4636 3188  697   80 1455  381 7025 7127 3992 4026 2777 4961\n",
            " 7531  744 6640 1047 6684 4297 7939 7384  446 3287 6675 7752 5279 8196\n",
            " 3020 1203 4192 1552 4253 2887 2556 5729 1898 3208 5973 7033 1431 5915\n",
            " 8312 5799 7105  511 6404 2852 1561 4150 4835 1178]\n",
            "Sun Jun  5 06:42:16 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P0    39W / 300W |  15237MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "757/757 [==============================] - 275s 345ms/step - loss: 7.5250 - accuracy: 0.5786 - val_loss: 6.8516 - val_accuracy: 0.7201\n",
            "Epoch 2/5\n",
            "757/757 [==============================] - 262s 346ms/step - loss: 6.5647 - accuracy: 0.7623 - val_loss: 6.4307 - val_accuracy: 0.7380\n",
            "Epoch 3/5\n",
            "757/757 [==============================] - 262s 346ms/step - loss: 6.0538 - accuracy: 0.8181 - val_loss: 6.1304 - val_accuracy: 0.7635\n",
            "Epoch 4/5\n",
            "757/757 [==============================] - 258s 340ms/step - loss: 5.6341 - accuracy: 0.8608 - val_loss: 5.8096 - val_accuracy: 0.7560\n",
            "Epoch 5/5\n",
            "757/757 [==============================] - 258s 340ms/step - loss: 5.2183 - accuracy: 0.8923 - val_loss: 5.5902 - val_accuracy: 0.7590\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_3[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_roberta_model[1][0]']       \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 15)           7695        ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.7375296912114014\n",
            "Weighted F1: 0.7303537351403838\n",
            "Micro F1: 0.7375296912114016\n",
            "Weighted Precision: 0.7349774550980597\n",
            "Micro Precision: 0.7375296912114014\n",
            "Weighted Recall: 0.7375296912114014\n",
            "Micro Recall: 0.7375296912114014\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_1[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_roberta_model[0][0]']       \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 15)           7695        ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 8  1  8 ...  2  1 10]\n",
            "[   1    2    3 ... 8413 8414 8417]\n",
            "[6622   35 6713 4989 8233 7444 5061 3942 8344  454 5464  317 4515 3001\n",
            " 6991 2875 1125 3885 6647 4857 4504  214 8340 2969 3847 4609 5092 1346\n",
            " 5993 7358 2752 7651 7301 8188 7073 4782 6069 2346 6445 6483 1663 4952\n",
            " 5072 1967 4071 7590  860 6418 5030 7297  626 5463 6029 6302 3819 7256\n",
            " 5667 1235 3993 1439 1323 4263 3835 2624 5801 4293 8129  320 5866 3247\n",
            " 5777   19 6059 4837 3843 1666  858 1418 5409 5274 6205 3167 2356 5997\n",
            " 6685  223 3444 3310 7721 5928 2360 7197 4252 8343  909 4354 4752 1421\n",
            "  649  829 1916 3488 2496 5765 6612 4004 4052 1403 6697 5268 1010 6293\n",
            " 6380 4602 6057 1851 4577 4236 7978 5083 6552 1058  361 7706 4186 4452\n",
            " 4396  749 8351 3834  812 5108  463 1743 6600 8264 8064 7121 8279 5556\n",
            " 5119 4673 7947 1882 3350 7617 7152 5199 4282 1092 8003 5025 5051  933\n",
            " 3969 1927 3615 1221 8062 1489 3296 7442 5459 2031 7351  234 2313 8015\n",
            " 7416 7921 6574 6257 8396  941 4520 3300 5640  240 7810 7425 1321 6580\n",
            "  928 1170 5404 8273 6742 7005   94 7459    9 7861 7975 1018 5029 2140\n",
            " 1415 7061 7767 4990 8124 5927 6051 3269 6916 5959 2846 6818 7052 7775\n",
            " 7582 8116 4240 4767 5248  430 1812 2175 6055 8331 3916  998 1564 7326\n",
            " 1559  292 3065 6764 6854 4915 5552 3583 1917 5902 4229 2144 7565 3712\n",
            " 7224 4502 1406 1651 6256 3827 6525 6318 2686 3616 5369 8235 1704 1011\n",
            " 3351 8089 8021  102  224 5671 8294 7707 2985 6624 5146 1428 7381 3709\n",
            " 6824 6308  701 2281 6837 5436 1741 3520 5995 2649 4886  966 5476 2406\n",
            " 1199 8387  513 5449 1028 2226 6166 4387  350 6123  678 5236 5041  734\n",
            " 3412 6085  587  489 5760 3376 4982 5115 4655 5575 3839 6743 7173 5724\n",
            " 7689 7787  520 8302 3150 5713 2247 8128 4607 6660 3505 4610 2866 4161\n",
            " 6905 7096  768 1062 4411 6087 2096 4903 7546 6630 2928 4640 7125 1792\n",
            " 2063 1700 2305 7928 8095  737 5849  922 2359 6015 4489 4361 6081 6346\n",
            " 5014 5167 1023 8009 3335  268 4777 5221 4323 6168  759  711 5702 6129\n",
            " 7181 1860 7192 1347 5467 3378 6579   67 2693 6134 4841 4270 4611 2943\n",
            " 6746 7696 3466 3950 4693 6125 8370 6142 6131  995 5548 2112 1721 5067\n",
            " 4015 7092 3156   25 1749 5554 4878 8248  458 6836 1006 5310 2698 6960\n",
            " 2339 1650 6227 4104 5130 7089 6993 2400 1289 4305 5585 7933 6284 7057\n",
            " 3807 5790 8347 2390 5280 2841 7220 1384 7693 8090 4171 5260  133 7451\n",
            " 3957 6793 1719 7379 4743 4746 5435 6216 4858 4417 8355 4492 2648 5104\n",
            " 5278 6539  743 7308  158 7543 2168 5913 7111 3414 5788 7592   39 7157\n",
            " 5647 4555 3956 1045 3617 4630 4384 6596   96 4539 6749 7202 2413 4749\n",
            " 5122 5052 1598 5929  252 5565  336 6008 4564 5580 7237 1466 1568 5958\n",
            " 7657 4327 5091 6918 6487 4082 2989 7448 4715 3382  467 3784 4624  764\n",
            " 4419 1117 7954 7122 6392  505 3174 1534  323 4164 3659 7526  498 5944\n",
            " 4572 2307 2929 7480 4412  731 4563 7692 1869 2472 6966 4238 6549 1594\n",
            " 2276 7269 2545 2216  605 6587 6315 5297 5282 4816 1619 5504 2536 6399\n",
            " 3917 6342 5754 5721 7385 7909 6126 7382 8382 4383 5502 7130  485 5862\n",
            " 4936 7674 3575 5372 3893 7109 4605 5211 5462 8065   50 4080 3334 8167\n",
            " 8378 5595 1873 8207 7076 4106 3719 4627  395  231 2405 5419 6973 3480\n",
            " 3246 5414 5931 8208 4825 6203 3019 5822   63 5854 6543 5787 4764 6701\n",
            " 6562 3822 4861 3051 1315 4805 8318 1089 8169 8218 7549  707 1284 1076\n",
            " 2465 3815 4636 3188  697   80 1455  381 7025 7127 3992 4026 2777 4961\n",
            " 7531  744 6640 1047 6684 4297 7939 7384  446 3287 6675 7752 5279 8196\n",
            " 3020 1203 4192 1552 4253 2887 2556 5729 1898 3208 5973 7033 1431 5915\n",
            " 8312 5799 7105  511 6404 2852 1561 4150 4835 1178]\n",
            "Sun Jun  5 07:06:01 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P0    39W / 300W |  15237MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "757/757 [==============================] - 277s 346ms/step - loss: 7.4924 - accuracy: 0.5920 - val_loss: 6.8207 - val_accuracy: 0.7216\n",
            "Epoch 2/5\n",
            "757/757 [==============================] - 263s 347ms/step - loss: 6.5777 - accuracy: 0.7618 - val_loss: 6.3906 - val_accuracy: 0.7485\n",
            "Epoch 3/5\n",
            "757/757 [==============================] - 390s 515ms/step - loss: 6.0500 - accuracy: 0.8208 - val_loss: 6.0713 - val_accuracy: 0.7560\n",
            "Epoch 4/5\n",
            "757/757 [==============================] - 263s 347ms/step - loss: 5.6157 - accuracy: 0.8541 - val_loss: 5.8006 - val_accuracy: 0.7695\n",
            "Epoch 5/5\n",
            "757/757 [==============================] - 258s 341ms/step - loss: 5.1920 - accuracy: 0.8903 - val_loss: 5.5015 - val_accuracy: 0.7560\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_3[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_roberta_model[1][0]']       \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 15)           7695        ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.7422802850356295\n",
            "Weighted F1: 0.7400846176326862\n",
            "Micro F1: 0.7422802850356294\n",
            "Weighted Precision: 0.7432489327469318\n",
            "Micro Precision: 0.7422802850356295\n",
            "Weighted Recall: 0.7422802850356295\n",
            "Micro Recall: 0.7422802850356295\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_1[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_roberta_model[0][0]']       \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 15)           7695        ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 8  1  8 ...  2  1 10]\n",
            "[   1    2    3 ... 8413 8414 8417]\n",
            "[6622   35 6713 4989 8233 7444 5061 3942 8344  454 5464  317 4515 3001\n",
            " 6991 2875 1125 3885 6647 4857 4504  214 8340 2969 3847 4609 5092 1346\n",
            " 5993 7358 2752 7651 7301 8188 7073 4782 6069 2346 6445 6483 1663 4952\n",
            " 5072 1967 4071 7590  860 6418 5030 7297  626 5463 6029 6302 3819 7256\n",
            " 5667 1235 3993 1439 1323 4263 3835 2624 5801 4293 8129  320 5866 3247\n",
            " 5777   19 6059 4837 3843 1666  858 1418 5409 5274 6205 3167 2356 5997\n",
            " 6685  223 3444 3310 7721 5928 2360 7197 4252 8343  909 4354 4752 1421\n",
            "  649  829 1916 3488 2496 5765 6612 4004 4052 1403 6697 5268 1010 6293\n",
            " 6380 4602 6057 1851 4577 4236 7978 5083 6552 1058  361 7706 4186 4452\n",
            " 4396  749 8351 3834  812 5108  463 1743 6600 8264 8064 7121 8279 5556\n",
            " 5119 4673 7947 1882 3350 7617 7152 5199 4282 1092 8003 5025 5051  933\n",
            " 3969 1927 3615 1221 8062 1489 3296 7442 5459 2031 7351  234 2313 8015\n",
            " 7416 7921 6574 6257 8396  941 4520 3300 5640  240 7810 7425 1321 6580\n",
            "  928 1170 5404 8273 6742 7005   94 7459    9 7861 7975 1018 5029 2140\n",
            " 1415 7061 7767 4990 8124 5927 6051 3269 6916 5959 2846 6818 7052 7775\n",
            " 7582 8116 4240 4767 5248  430 1812 2175 6055 8331 3916  998 1564 7326\n",
            " 1559  292 3065 6764 6854 4915 5552 3583 1917 5902 4229 2144 7565 3712\n",
            " 7224 4502 1406 1651 6256 3827 6525 6318 2686 3616 5369 8235 1704 1011\n",
            " 3351 8089 8021  102  224 5671 8294 7707 2985 6624 5146 1428 7381 3709\n",
            " 6824 6308  701 2281 6837 5436 1741 3520 5995 2649 4886  966 5476 2406\n",
            " 1199 8387  513 5449 1028 2226 6166 4387  350 6123  678 5236 5041  734\n",
            " 3412 6085  587  489 5760 3376 4982 5115 4655 5575 3839 6743 7173 5724\n",
            " 7689 7787  520 8302 3150 5713 2247 8128 4607 6660 3505 4610 2866 4161\n",
            " 6905 7096  768 1062 4411 6087 2096 4903 7546 6630 2928 4640 7125 1792\n",
            " 2063 1700 2305 7928 8095  737 5849  922 2359 6015 4489 4361 6081 6346\n",
            " 5014 5167 1023 8009 3335  268 4777 5221 4323 6168  759  711 5702 6129\n",
            " 7181 1860 7192 1347 5467 3378 6579   67 2693 6134 4841 4270 4611 2943\n",
            " 6746 7696 3466 3950 4693 6125 8370 6142 6131  995 5548 2112 1721 5067\n",
            " 4015 7092 3156   25 1749 5554 4878 8248  458 6836 1006 5310 2698 6960\n",
            " 2339 1650 6227 4104 5130 7089 6993 2400 1289 4305 5585 7933 6284 7057\n",
            " 3807 5790 8347 2390 5280 2841 7220 1384 7693 8090 4171 5260  133 7451\n",
            " 3957 6793 1719 7379 4743 4746 5435 6216 4858 4417 8355 4492 2648 5104\n",
            " 5278 6539  743 7308  158 7543 2168 5913 7111 3414 5788 7592   39 7157\n",
            " 5647 4555 3956 1045 3617 4630 4384 6596   96 4539 6749 7202 2413 4749\n",
            " 5122 5052 1598 5929  252 5565  336 6008 4564 5580 7237 1466 1568 5958\n",
            " 7657 4327 5091 6918 6487 4082 2989 7448 4715 3382  467 3784 4624  764\n",
            " 4419 1117 7954 7122 6392  505 3174 1534  323 4164 3659 7526  498 5944\n",
            " 4572 2307 2929 7480 4412  731 4563 7692 1869 2472 6966 4238 6549 1594\n",
            " 2276 7269 2545 2216  605 6587 6315 5297 5282 4816 1619 5504 2536 6399\n",
            " 3917 6342 5754 5721 7385 7909 6126 7382 8382 4383 5502 7130  485 5862\n",
            " 4936 7674 3575 5372 3893 7109 4605 5211 5462 8065   50 4080 3334 8167\n",
            " 8378 5595 1873 8207 7076 4106 3719 4627  395  231 2405 5419 6973 3480\n",
            " 3246 5414 5931 8208 4825 6203 3019 5822   63 5854 6543 5787 4764 6701\n",
            " 6562 3822 4861 3051 1315 4805 8318 1089 8169 8218 7549  707 1284 1076\n",
            " 2465 3815 4636 3188  697   80 1455  381 7025 7127 3992 4026 2777 4961\n",
            " 7531  744 6640 1047 6684 4297 7939 7384  446 3287 6675 7752 5279 8196\n",
            " 3020 1203 4192 1552 4253 2887 2556 5729 1898 3208 5973 7033 1431 5915\n",
            " 8312 5799 7105  511 6404 2852 1561 4150 4835 1178]\n",
            "Sun Jun  5 07:32:01 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P0    40W / 300W |  15237MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "757/757 [==============================] - 276s 345ms/step - loss: 7.4866 - accuracy: 0.5672 - val_loss: 6.7504 - val_accuracy: 0.7171\n",
            "Epoch 2/5\n",
            "757/757 [==============================] - 262s 346ms/step - loss: 6.4858 - accuracy: 0.7526 - val_loss: 6.3073 - val_accuracy: 0.7485\n",
            "Epoch 3/5\n",
            "757/757 [==============================] - 258s 340ms/step - loss: 5.9612 - accuracy: 0.8115 - val_loss: 6.1025 - val_accuracy: 0.7275\n",
            "Epoch 4/5\n",
            "757/757 [==============================] - 262s 346ms/step - loss: 5.5151 - accuracy: 0.8609 - val_loss: 5.6771 - val_accuracy: 0.7635\n",
            "Epoch 5/5\n",
            "757/757 [==============================] - 258s 340ms/step - loss: 5.1020 - accuracy: 0.8954 - val_loss: 5.4761 - val_accuracy: 0.7545\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_3[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_roberta_model[1][0]']       \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 15)           7695        ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.7434679334916865\n",
            "Weighted F1: 0.7456816960752629\n",
            "Micro F1: 0.7434679334916865\n",
            "Weighted Precision: 0.7505806665903246\n",
            "Micro Precision: 0.7434679334916865\n",
            "Weighted Recall: 0.7434679334916865\n",
            "Micro Recall: 0.7434679334916865\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_1[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_roberta_model[0][0]']       \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 15)           7695        ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 8  1  8 ...  2  1 10]\n",
            "[   1    2    3 ... 8413 8414 8417]\n",
            "[6622   35 6713 4989 8233 7444 5061 3942 8344  454 5464  317 4515 3001\n",
            " 6991 2875 1125 3885 6647 4857 4504  214 8340 2969 3847 4609 5092 1346\n",
            " 5993 7358 2752 7651 7301 8188 7073 4782 6069 2346 6445 6483 1663 4952\n",
            " 5072 1967 4071 7590  860 6418 5030 7297  626 5463 6029 6302 3819 7256\n",
            " 5667 1235 3993 1439 1323 4263 3835 2624 5801 4293 8129  320 5866 3247\n",
            " 5777   19 6059 4837 3843 1666  858 1418 5409 5274 6205 3167 2356 5997\n",
            " 6685  223 3444 3310 7721 5928 2360 7197 4252 8343  909 4354 4752 1421\n",
            "  649  829 1916 3488 2496 5765 6612 4004 4052 1403 6697 5268 1010 6293\n",
            " 6380 4602 6057 1851 4577 4236 7978 5083 6552 1058  361 7706 4186 4452\n",
            " 4396  749 8351 3834  812 5108  463 1743 6600 8264 8064 7121 8279 5556\n",
            " 5119 4673 7947 1882 3350 7617 7152 5199 4282 1092 8003 5025 5051  933\n",
            " 3969 1927 3615 1221 8062 1489 3296 7442 5459 2031 7351  234 2313 8015\n",
            " 7416 7921 6574 6257 8396  941 4520 3300 5640  240 7810 7425 1321 6580\n",
            "  928 1170 5404 8273 6742 7005   94 7459    9 7861 7975 1018 5029 2140\n",
            " 1415 7061 7767 4990 8124 5927 6051 3269 6916 5959 2846 6818 7052 7775\n",
            " 7582 8116 4240 4767 5248  430 1812 2175 6055 8331 3916  998 1564 7326\n",
            " 1559  292 3065 6764 6854 4915 5552 3583 1917 5902 4229 2144 7565 3712\n",
            " 7224 4502 1406 1651 6256 3827 6525 6318 2686 3616 5369 8235 1704 1011\n",
            " 3351 8089 8021  102  224 5671 8294 7707 2985 6624 5146 1428 7381 3709\n",
            " 6824 6308  701 2281 6837 5436 1741 3520 5995 2649 4886  966 5476 2406\n",
            " 1199 8387  513 5449 1028 2226 6166 4387  350 6123  678 5236 5041  734\n",
            " 3412 6085  587  489 5760 3376 4982 5115 4655 5575 3839 6743 7173 5724\n",
            " 7689 7787  520 8302 3150 5713 2247 8128 4607 6660 3505 4610 2866 4161\n",
            " 6905 7096  768 1062 4411 6087 2096 4903 7546 6630 2928 4640 7125 1792\n",
            " 2063 1700 2305 7928 8095  737 5849  922 2359 6015 4489 4361 6081 6346\n",
            " 5014 5167 1023 8009 3335  268 4777 5221 4323 6168  759  711 5702 6129\n",
            " 7181 1860 7192 1347 5467 3378 6579   67 2693 6134 4841 4270 4611 2943\n",
            " 6746 7696 3466 3950 4693 6125 8370 6142 6131  995 5548 2112 1721 5067\n",
            " 4015 7092 3156   25 1749 5554 4878 8248  458 6836 1006 5310 2698 6960\n",
            " 2339 1650 6227 4104 5130 7089 6993 2400 1289 4305 5585 7933 6284 7057\n",
            " 3807 5790 8347 2390 5280 2841 7220 1384 7693 8090 4171 5260  133 7451\n",
            " 3957 6793 1719 7379 4743 4746 5435 6216 4858 4417 8355 4492 2648 5104\n",
            " 5278 6539  743 7308  158 7543 2168 5913 7111 3414 5788 7592   39 7157\n",
            " 5647 4555 3956 1045 3617 4630 4384 6596   96 4539 6749 7202 2413 4749\n",
            " 5122 5052 1598 5929  252 5565  336 6008 4564 5580 7237 1466 1568 5958\n",
            " 7657 4327 5091 6918 6487 4082 2989 7448 4715 3382  467 3784 4624  764\n",
            " 4419 1117 7954 7122 6392  505 3174 1534  323 4164 3659 7526  498 5944\n",
            " 4572 2307 2929 7480 4412  731 4563 7692 1869 2472 6966 4238 6549 1594\n",
            " 2276 7269 2545 2216  605 6587 6315 5297 5282 4816 1619 5504 2536 6399\n",
            " 3917 6342 5754 5721 7385 7909 6126 7382 8382 4383 5502 7130  485 5862\n",
            " 4936 7674 3575 5372 3893 7109 4605 5211 5462 8065   50 4080 3334 8167\n",
            " 8378 5595 1873 8207 7076 4106 3719 4627  395  231 2405 5419 6973 3480\n",
            " 3246 5414 5931 8208 4825 6203 3019 5822   63 5854 6543 5787 4764 6701\n",
            " 6562 3822 4861 3051 1315 4805 8318 1089 8169 8218 7549  707 1284 1076\n",
            " 2465 3815 4636 3188  697   80 1455  381 7025 7127 3992 4026 2777 4961\n",
            " 7531  744 6640 1047 6684 4297 7939 7384  446 3287 6675 7752 5279 8196\n",
            " 3020 1203 4192 1552 4253 2887 2556 5729 1898 3208 5973 7033 1431 5915\n",
            " 8312 5799 7105  511 6404 2852 1561 4150 4835 1178]\n",
            "Sun Jun  5 07:55:45 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P0    40W / 300W |  15237MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "757/757 [==============================] - 277s 346ms/step - loss: 7.4848 - accuracy: 0.5806 - val_loss: 6.7738 - val_accuracy: 0.7380\n",
            "Epoch 2/5\n",
            "757/757 [==============================] - 258s 341ms/step - loss: 6.5155 - accuracy: 0.7664 - val_loss: 6.3296 - val_accuracy: 0.7380\n",
            "Epoch 3/5\n",
            "757/757 [==============================] - 263s 347ms/step - loss: 6.0271 - accuracy: 0.8143 - val_loss: 6.0870 - val_accuracy: 0.7425\n",
            "Epoch 4/5\n",
            "757/757 [==============================] - 262s 346ms/step - loss: 5.5903 - accuracy: 0.8586 - val_loss: 5.7461 - val_accuracy: 0.7530\n",
            "Epoch 5/5\n",
            "757/757 [==============================] - 262s 346ms/step - loss: 5.1795 - accuracy: 0.8926 - val_loss: 5.4557 - val_accuracy: 0.7650\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_3[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_roberta_model[1][0]']       \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 15)           7695        ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.7363420427553444\n",
            "Weighted F1: 0.7338911798886172\n",
            "Micro F1: 0.7363420427553444\n",
            "Weighted Precision: 0.7381738741275699\n",
            "Micro Precision: 0.7363420427553444\n",
            "Weighted Recall: 0.7363420427553444\n",
            "Micro Recall: 0.7363420427553444\n",
            "Average Accuracy: 0.7391923990498812\n",
            "Average Weighted F1: 0.7369965796618598\n",
            "Average Micro F1: 0.7391923990498812\n",
            "Average Weighted Precision: 0.7420600023170026\n",
            "Average Micro Precision: 0.7391923990498812\n",
            "Average Weighted Recall: 0.7391923990498812\n",
            "Average Micro Recall: 0.7391923990498812\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "total_accuracy=0\n",
        "total_weighted_f1=0\n",
        "total_micro_f1=0\n",
        "total_weighted_precision=0\n",
        "total_micro_precision=0\n",
        "total_weighted_recall=0\n",
        "total_micro_recall=0\n",
        "\n",
        "for f in range(5):\n",
        "  gc.collect()\n",
        "  tf.keras.backend.clear_session()\n",
        "  dbert_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "  dbert_model = TFRobertaModel.from_pretrained('roberta-base')\n",
        "  max_len=512\n",
        "  sentences=summarized_data['text']\n",
        "  labels=summarized_data['label']\n",
        "  filenumbers=summarized_data['filenum']\n",
        "  len(sentences),len(labels),len(filenumbers)\n",
        "  \n",
        "  model_2=create_model()\n",
        "  input_ids=[]\n",
        "  attention_masks=[]\n",
        "\n",
        "  for sent in sentences:\n",
        "    dbert_inps=dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n",
        "    input_ids.append(dbert_inps['input_ids'])\n",
        "    attention_masks.append(dbert_inps['attention_mask'])\n",
        "  input_ids=np.asarray(input_ids)\n",
        "\n",
        "  attention_masks=np.array(attention_masks)\n",
        "  labels=np.array(labels)\n",
        "  filenumbers=[int(i) for i in filenumbers]\n",
        "  filenumbers=np.array(filenumbers)\n",
        "  print(labels)\n",
        "  print(filenumbers)\n",
        "\n",
        "  #train_inp,val_inp,train_label,val_label,train_mask,val_mask,train_fnum,val_fnum=train_test_split(input_ids,labels,attention_masks,filenumbers,test_size=0.1,random_state=42)\n",
        "\n",
        "  new_train_inp_2=[]\n",
        "  new_val_inp_2=[]\n",
        "  new_train_label_2=[]\n",
        "  new_val_label_2=[]\n",
        "  new_train_mask_2=[]\n",
        "  new_train_fnum_2=[]\n",
        "  new_val_fnum_2=[]\n",
        "  new_val_mask_2=[]\n",
        "  for i in range(len(train_fnum)):\n",
        "    if train_fnum[i] in filenumbers:\n",
        "      new_train_inp_2.append(train_inp[i])\n",
        "      #new_val_inp.append(val_inp[i])\n",
        "      new_train_label_2.append(train_label[i])\n",
        "      #new_val_label.append(val_label[i])\n",
        "      new_train_mask_2.append(train_mask[i])\n",
        "      #new_val_mask.append(val_mask[i])\n",
        "      new_train_fnum_2.append(train_fnum[i])\n",
        "\n",
        "  for i in range(len(val_fnum)):\n",
        "    if val_fnum[i] in filenumbers:\n",
        "      #new_train_inp.append(train_inp[i])\n",
        "      new_val_inp_2.append(val_inp[i])\n",
        "      #new_train_label.append(train_label[i])\n",
        "      new_val_label_2.append(val_label[i])\n",
        "      #new_train_mask.append(train_mask[i])\n",
        "      new_val_mask_2.append(val_mask[i])\n",
        "      new_val_fnum_2.append(val_fnum[i])\n",
        "\n",
        "  # print(new_train_label)\n",
        "  # print(len(new_train_label))\n",
        "  # print(new_val_label)\n",
        "  # print(len(new_val_label))\n",
        "  new_train_inp_2=np.array(new_train_inp_2)\n",
        "  new_val_inp_2=np.array(new_val_inp_2)\n",
        "  new_train_label_2=np.array(new_train_label_2)\n",
        "  new_val_label_2=np.array(new_val_label_2)\n",
        "  new_train_mask_2=np.array(new_train_mask_2)\n",
        "  new_train_fnum_2=np.array(new_train_fnum_2)\n",
        "  new_val_fnum_2=np.array(new_val_fnum_2)\n",
        "  new_val_mask_2=np.array(new_val_mask_2)\n",
        "  \n",
        "  print(new_val_fnum_2)\n",
        "  \n",
        "  log_dir='dbert_model'\n",
        "\n",
        "  model_save_path='./drive/MyDrive/Ensemble/roberta-ensemble-512-model_2-'+str(f)+'-15labels.h5'\n",
        "\n",
        "  loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "  accuracy = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
        "  callbacks= [tf.keras.callbacks.ModelCheckpoint(filepath=model_save_path,monitor='val_accuracy',mode='max',save_best_only=True,save_weights_only=True),keras.callbacks.TensorBoard(log_dir=log_dir)]\n",
        "  model_2.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])\n",
        "  gpu_info = !nvidia-smi\n",
        "  gpu_info = '\\n'.join(gpu_info)\n",
        "  if gpu_info.find('failed') >= 0:\n",
        "    print('Not connected to a GPU')\n",
        "  else:\n",
        "    print(gpu_info)\n",
        "  \n",
        "  history=model_2.fit([new_train_inp_2,new_train_mask_2],new_train_label_2,batch_size=8,epochs=5,validation_data=([new_val_inp_2,new_val_mask_2],new_val_label_2),callbacks=callbacks)\n",
        "\n",
        "  pred_labels=[]\n",
        "\n",
        "  model_saved_2= create_model()\n",
        "  model_saved_2.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])\n",
        "  model_saved_2.load_weights('./drive/MyDrive/Ensemble/roberta-ensemble-512-model_2-'+str(f)+'-15labels.h5')\n",
        "\n",
        "  for i in range(0,len(val_inp)):\n",
        "    pred=model_saved_2.predict([val_inp[i].reshape(1,512),val_mask[i].reshape(1,512)])\n",
        "    pred_label = pred.argmax(axis=1)\n",
        "    pred_labels.append(pred_label)\n",
        "  accuracy=accuracy_score(val_label, pred_labels)\n",
        "  print(\"Accuracy: \"+str(accuracy))\n",
        "  total_accuracy=total_accuracy+accuracy\n",
        "  \n",
        "  weighted_f1=f1_score(val_label,pred_labels, average='weighted')\n",
        "  print(\"Weighted F1: \"+ str(weighted_f1))\n",
        "  total_weighted_f1=total_weighted_f1+weighted_f1\n",
        "  micro_f1=f1_score(val_label,pred_labels, average='micro')\n",
        "  print(\"Micro F1: \"+ str(micro_f1))\n",
        "  total_micro_f1=total_micro_f1+micro_f1\n",
        "\n",
        "  weighted_precision=precision_score(val_label, pred_labels, average='weighted')\n",
        "  print(\"Weighted Precision: \" + str(weighted_precision))\n",
        "  total_weighted_precision=total_weighted_precision+weighted_precision\n",
        "  micro_precision=precision_score(val_label, pred_labels, average='micro')\n",
        "  print(\"Micro Precision: \" + str(micro_precision))\n",
        "  total_micro_precision=total_micro_precision+micro_precision\n",
        "\n",
        "  weighted_recall=recall_score(val_label, pred_labels, average='weighted')\n",
        "  print(\"Weighted Recall: \" + str(weighted_recall))\n",
        "  total_weighted_recall=total_weighted_recall+weighted_recall\n",
        "  micro_recall=recall_score(val_label, pred_labels, average='micro')\n",
        "  print(\"Micro Recall: \" + str(micro_recall))\n",
        "  total_micro_recall=total_micro_recall+micro_recall\n",
        "\n",
        "\n",
        "print(\"Average Accuracy: \"+str(total_accuracy/5))\n",
        "print(\"Average Weighted F1: \"+str(total_weighted_f1/5))\n",
        "print(\"Average Micro F1: \"+str(total_micro_f1/5))\n",
        "print(\"Average Weighted Precision: \"+str(total_weighted_precision/5))\n",
        "print(\"Average Micro Precision: \"+str(total_micro_precision/5))\n",
        "print(\"Average Weighted Recall: \"+str(total_weighted_recall/5))\n",
        "print(\"Average Micro Recall: \"+str(total_micro_recall/5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5uUYcxMONKM",
        "outputId": "7c6d7cdb-625f-41cc-a36b-7ab85be357bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6005\n",
            "6005\n",
            "Average Length 511.7708576186511\n",
            "['8', '1', '8', '2', '8', '8', '8', '9', '7', '8', '1', '2', '1', '1', '8', '4', '8', '8', '12', '3', '3', '7', '3', '12', '1', '8', '8', '8', '8', '8', '8', '8', '1', '9', '5', '9', '9', '9', '11', '8', '8', '8', '4', '8', '8', '8', '8', '1', '3', '9', '3', '7', '1', '2', '9', '9', '7', '8', '8', '1', '10', '7', '8', '8', '9', '8', '7', '9', '9', '12', '7', '2', '8', '1', '11', '11', '1', '7', '7', '12', '1', '9', '8', '10', '12', '7', '8', '2', '8', '9', '9', '1', '8', '9', '1', '7', '12', '10', '10', '10', '8', '3', '7', '9', '8', '9', '1', '8', '8', '2', '7', '2', '9', '9', '11', '8', '8', '12', '12', '7', '8', '12', '4', '9', '3', '1', '12', '1', '1', '8', '8', '3', '8', '8', '8', '8', '9', '1', '8', '8', '10', '1', '8', '2', '8', '8', '7', '3', '8', '2', '4', '4', '9', '8', '10', '12', '12', '1', '1', '9', '1', '1', '1', '8', '2', '2', '8', '1', '1', '2', '2', '1', '2', '8', '1', '9', '9', '8', '8', '4', '2', '9', '9', '8', '3', '4', '3', '1', '8', '8', '2', '1', '9', '7', '8', '8', '1', '12', '3', '8', '2', '4', '2', '9', '12', '1', '4', '1', '8', '8', '8', '2', '2', '8', '9', '8', '8', '8', '10', '9', '8', '7', '9', '1', '1', '9', '4', '2', '4', '2', '2', '1', '7', '8', '11', '11', '3', '9', '2', '4', '8', '9', '1', '8', '1', '1', '4', '9', '1', '1', '8', '8', '2', '1', '8', '4', '2', '8', '9', '8', '8', '2', '8', '8', '8', '7', '1', '1', '1', '2', '1', '1', '8', '7', '8', '8', '12', '2', '12', '12', '8', '10', '12', '8', '3', '3', '12', '10', '1', '8', '12', '1', '8', '8', '2', '8', '4', '7', '8', '7', '10', '8', '10', '9', '8', '12', '12', '1', '8', '8', '3', '8', '8', '8', '8', '8', '1', '9', '8', '11', '1', '1', '1', '9', '8', '1', '9', '2', '3', '11', '8', '9', '9', '9', '2', '1', '8', '8', '9', '7', '1', '4', '9', '4', '8', '8', '4', '8', '12', '9', '4', '8', '2', '10', '10', '10', '8', '9', '9', '8', '8', '12', '7', '1', '8', '8', '8', '4', '1', '1', '1', '1', '1', '1', '8', '1', '9', '8', '9', '9', '4', '8', '12', '9', '8', '8', '2', '8', '8', '8', '6', '9', '8', '3', '7', '8', '8', '4', '12', '8', '8', '9', '12', '12', '9', '8', '2', '9', '2', '3', '1', '12', '8', '10', '9', '9', '9', '10', '10', '3', '8', '12', '1', '4', '2', '1', '10', '8', '2', '8', '4', '8', '9', '1', '9', '9', '10', '10', '1', '4', '9', '2', '4', '9', '1', '1', '3', '10', '3', '3', '8', '7', '3', '8', '9', '9', '12', '4', '8', '12', '2', '2', '4', '1', '9', '9', '4', '1', '4', '2', '8', '12', '2', '3', '10', '10', '9', '8', '9', '9', '1', '12', '8', '8', '8', '12', '4', '1', '8', '8', '1', '9', '8', '8', '2', '1', '8', '9', '8', '3', '3', '3', '1', '8', '8', '9', '1', '10', '9', '9', '9', '9', '5', '9', '9', '8', '8', '8', '8', '8', '8', '8', '8', '11', '12', '8', '8', '1', '8', '9', '11', '2', '2', '2', '2', '3', '1', '2', '2', '8', '2', '4', '9', '1', '2', '9', '8', '2', '8', '9', '9', '3', '10', '9', '9', '2', '8', '9', '8', '12', '12', '1', '3', '8', '8', '8', '2', '7', '7', '7', '7', '3', '9', '1', '9', '8', '9', '9', '1', '1', '1', '2', '9', '9', '9', '11', '1', '8', '8', '9', '1', '9', '8', '8', '8', '1', '1', '8', '7', '1', '1', '8', '8', '9', '4', '4', '8', '2', '2', '8', '8', '8', '8', '8', '8', '11', '8', '2', '9', '4', '9', '3', '9', '9', '1', '3', '9', '3', '1', '12', '8', '9', '12', '1', '8', '4', '2', '1', '4', '8', '3', '3', '8', '2', '8', '9', '7', '8', '8', '8', '5', '8', '3', '9', '8', '8', '13', '12', '1', '1', '2', '8', '4', '1', '9', '9', '12', '8', '9', '12', '9', '1', '9', '9', '9', '9', '3', '2', '9', '9', '4', '8', '12', '2', '4', '9', '3', '1', '9', '7', '8', '9', '9', '8', '4', '8', '8', '7', '9', '10', '3', '8', '8', '8', '1', '1', '1', '1', '8', '8', '4', '1', '10', '1', '5', '7', '7', '1', '8', '9', '3', '7', '2', '7', '7', '2', '4', '8', '12', '7', '4', '2', '9', '9', '12', '6', '10', '8', '2', '4', '12', '9', '9', '3', '8', '8', '1', '2', '10', '9', '9', '8', '4', '12', '2', '1', '8', '8', '8', '12', '10', '10', '9', '3', '8', '8', '9', '2', '8', '10', '1', '1', '1', '1', '2', '1', '1', '1', '1', '9', '8', '12', '9', '4', '8', '8', '9', '1', '9', '3', '9', '8', '8', '1', '7', '7', '10', '1', '8', '8', '1', '9', '8', '10', '3', '1', '7', '1', '8', '8', '12', '8', '8', '1', '8', '7', '1', '7', '7', '8', '2', '1', '8', '8', '2', '10', '8', '8', '8', '8', '8', '10', '1', '8', '8', '12', '8', '3', '3', '2', '2', '2', '10', '8', '8', '8', '2', '9', '1', '8', '9', '3', '2', '8', '10', '8', '6', '1', '1', '8', '4', '1', '9', '10', '8', '1', '7', '1', '2', '8', '1', '1', '1', '12', '1', '9', '12', '8', '12', '12', '12', '8', '8', '12', '4', '8', '8', '8', '8', '9', '9', '1', '3', '3', '3', '3', '1', '12', '12', '9', '10', '8', '8', '1', '9', '2', '2', '13', '9', '8', '9', '2', '1', '9', '1', '8', '8', '8', '4', '8', '1', '1', '1', '12', '12', '7', '2', '2', '2', '8', '3', '8', '9', '2', '10', '7', '8', '9', '2', '1', '2', '12', '12', '8', '8', '9', '2', '2', '9', '11', '1', '8', '1', '10', '9', '2', '1', '4', '7', '7', '7', '7', '7', '12', '8', '8', '8', '1', '1', '10', '1', '12', '1', '8', '2', '1', '1', '12', '8', '7', '9', '12', '8', '9', '3', '9', '8', '8', '8', '8', '3', '11', '2', '2', '9', '8', '8', '10', '8', '2', '7', '3', '1', '4', '7', '8', '8', '1', '8', '3', '7', '12', '8', '10', '9', '9', '8', '8', '2', '8', '9', '9', '1', '2', '8', '8', '9', '8', '3', '8', '1', '8', '10', '9', '8', '9', '9', '12', '4', '4', '8', '9', '9', '8', '2', '10', '1', '2', '8', '9', '1', '9', '9', '9', '7', '12', '12', '8', '1', '1', '1', '1', '8', '3', '1', '1', '8', '1', '8', '8', '7', '8', '8', '8', '8', '3', '2', '2', '10', '10', '10', '7', '8', '1', '2', '12', '7', '9', '8', '7', '8', '12', '2', '8', '9', '2', '6', '6', '7', '9', '8', '1', '8', '9', '8', '1', '12', '1', '2', '8', '7', '7', '7', '8', '2', '2', '8', '1', '2', '2', '9', '9', '1', '8', '8', '4', '3', '3', '1', '6', '3', '3', '12', '3', '8', '9', '1', '4', '3', '1', '8', '3', '9', '2', '8', '2', '8', '8', '8', '1', '1', '1', '9', '9', '8', '1', '9', '8', '1', '1', '3', '10', '8', '1', '1', '3', '9', '1', '4', '4', '1', '8', '9', '9', '2', '0', '0', '1', '8', '3', '1', '8', '8', '9', '8', '8', '1', '1', '8', '9', '8', '8', '8', '7', '9', '8', '8', '8', '10', '9', '8', '1', '2', '6', '1', '9', '9', '8', '12', '12', '12', '8', '8', '2', '8', '1', '2', '2', '2', '1', '9', '8', '2', '12', '2', '8', '12', '8', '9', '8', '8', '9', '7', '1', '1', '1', '1', '1', '8', '8', '1', '8', '8', '1', '1', '3', '2', '8', '8', '9', '10', '10', '2', '2', '1', '9', '2', '9', '9', '4', '12', '12', '12', '10', '7', '3', '3', '4', '2', '2', '9', '2', '8', '4', '2', '4', '1', '10', '9', '7', '8', '7', '1', '1', '3', '3', '1', '1', '3', '3', '3', '1', '1', '1', '1', '8', '2', '3', '1', '1', '2', '8', '8', '12', '8', '8', '8', '8', '11', '9', '1', '8', '9', '2', '8', '8', '8', '3', '9', '1', '9', '2', '7', '2', '8', '2', '8', '10', '8', '1', '10', '1', '1', '9', '9', '8', '8', '1', '8', '8', '8', '12', '8', '8', '8', '1', '8', '8', '8', '1', '9', '1', '1', '8', '1', '8', '9', '8', '2', '12', '9', '9', '0', '1', '8', '8', '1', '8', '12', '8', '8', '10', '8', '8', '8', '7', '8', '1', '8', '7', '3', '10', '1', '8', '9', '1', '8', '8', '8', '10', '1', '10', '3', '9', '1', '8', '9', '2', '8', '3', '3', '9', '9', '7', '9', '1', '1', '9', '2', '1', '1', '1', '7', '1', '1', '8', '8', '1', '1', '8', '1', '8', '3', '12', '9', '3', '3', '8', '8', '8', '8', '3', '1', '3', '3', '1', '11', '0', '8', '8', '7', '8', '12', '1', '8', '9', '8', '9', '8', '8', '3', '8', '8', '1', '1', '1', '9', '2', '2', '2', '8', '7', '12', '8', '8', '9', '10', '10', '7', '8', '1', '9', '8', '7', '3', '1', '3', '8', '2', '2', '3', '9', '8', '4', '4', '8', '9', '2', '1', '1', '7', '8', '9', '9', '7', '8', '7', '7', '8', '2', '2', '8', '4', '9', '7', '10', '0', '9', '8', '3', '7', '8', '1', '1', '8', '9', '9', '2', '2', '10', '1', '9', '10', '10', '10', '8', '3', '2', '12', '9', '9', '10', '12', '9', '12', '12', '9', '1', '2', '4', '12', '12', '7', '8', '9', '7', '7', '7', '3', '9', '8', '9', '1', '12', '8', '9', '4', '1', '3', '12', '12', '12', '12', '8', '8', '2', '1', '1', '2', '1', '1', '1', '12', '12', '8', '12', '2', '2', '12', '3', '3', '12', '8', '2', '8', '8', '12', '2', '1', '10', '3', '2', '8', '7', '1', '8', '1', '3', '7', '8', '9', '8', '3', '1', '1', '7', '8', '8', '9', '8', '2', '9', '2', '2', '9', '8', '1', '8', '8', '1', '3', '3', '1', '1', '10', '1', '2', '8', '1', '1', '1', '1', '9', '1', '4', '1', '7', '7', '7', '7', '2', '2', '8', '8', '12', '1', '9', '1', '7', '3', '3', '1', '8', '8', '10', '8', '9', '2', '9', '1', '3', '8', '8', '3', '12', '2', '8', '12', '2', '9', '1', '3', '3', '3', '3', '2', '8', '7', '9', '8', '3', '3', '1', '7', '8', '3', '1', '1', '12', '8', '9', '1', '2', '3', '8', '1', '1', '3', '3', '9', '1', '1', '1', '12', '1', '7', '3', '3', '1', '8', '8', '8', '1', '2', '4', '8', '1', '10', '2', '5', '3', '3', '12', '10', '9', '9', '12', '9', '0', '2', '8', '8', '9', '9', '9', '8', '1', '3', '1', '1', '4', '8', '1', '10', '8', '7', '2', '8', '2', '8', '4', '7', '8', '1', '9', '1', '9', '8', '2', '8', '2', '7', '9', '2', '2', '9', '1', '8', '12', '1', '8', '1', '4', '1', '9', '9', '1', '10', '12', '4', '8', '1', '7', '3', '9', '2', '12', '7', '8', '8', '2', '1', '12', '9', '8', '1', '2', '2', '8', '10', '2', '1', '7', '7', '7', '12', '3', '3', '8', '3', '8', '8', '3', '9', '8', '9', '1', '1', '8', '7', '9', '3', '3', '8', '1', '0', '9', '9', '9', '1', '8', '9', '9', '10', '1', '8', '7', '8', '8', '8', '9', '9', '8', '9', '10', '4', '9', '3', '7', '12', '1', '9', '9', '8', '9', '1', '9', '3', '1', '8', '4', '12', '10', '9', '8', '7', '10', '8', '12', '12', '3', '10', '8', '12', '2', '1', '2', '3', '9', '8', '8', '7', '1', '1', '2', '2', '1', '2', '2', '7', '1', '3', '9', '9', '3', '8', '8', '8', '8', '8', '7', '8', '8', '10', '8', '1', '8', '2', '8', '2', '2', '2', '2', '2', '1', '7', '12', '10', '1', '2', '8', '1', '4', '7', '8', '1', '12', '8', '7', '9', '2', '2', '2', '8', '1', '8', '1', '1', '0', '2', '2', '6', '1', '8', '2', '1', '8', '1', '1', '2', '2', '3', '2', '2', '10', '10', '9', '1', '7', '7', '8', '8', '1', '12', '10', '12', '3', '8', '8', '8', '3', '8', '3', '10', '2', '2', '2', '1', '2', '2', '1', '0', '1', '8', '9', '1', '1', '1', '2', '2', '2', '2', '2', '1', '1', '1', '1', '12', '9', '9', '1', '10', '11', '12', '9', '1', '1', '8', '8', '1', '2', '12', '8', '3', '7', '2', '10', '9', '7', '8', '1', '2', '2', '7', '9', '12', '2', '2', '1', '8', '9', '2', '3', '2', '2', '2', '9', '9', '8', '9', '2', '3', '8', '8', '9', '7', '3', '10', '8', '8', '9', '1', '1', '2', '8', '1', '10', '8', '8', '9', '7', '1', '7', '12', '8', '1', '7', '2', '1', '3', '9', '2', '8', '8', '1', '2', '3', '9', '3', '8', '9', '7', '7', '1', '10', '2', '8', '8', '8', '1', '8', '2', '8', '8', '1', '2', '8', '10', '2', '1', '0', '8', '3', '8', '10', '10', '12', '3', '8', '9', '3', '10', '8', '8', '8', '1', '8', '8', '2', '2', '2', '2', '2', '2', '1', '1', '10', '1', '10', '8', '2', '1', '2', '2', '8', '3', '3', '2', '2', '1', '8', '1', '3', '1', '9', '2', '2', '1', '3', '2', '2', '3', '2', '2', '3', '2', '1', '1', '2', '1', '2', '7', '8', '2', '1', '12', '12', '3', '1', '9', '9', '7', '8', '7', '2', '7', '9', '2', '2', '2', '8', '9', '7', '9', '2', '8', '9', '9', '2', '9', '1', '1', '9', '8', '3', '3', '8', '2', '7', '8', '11', '2', '8', '9', '1', '8', '3', '1', '2', '1', '2', '2', '2', '2', '8', '3', '9', '2', '10', '9', '9', '2', '7', '7', '7', '1', '3', '8', '8', '1', '1', '8', '7', '1', '8', '8', '9', '3', '3', '3', '2', '4', '8', '12', '8', '1', '12', '9', '2', '8', '2', '12', '2', '1', '3', '8', '12', '12', '11', '8', '10', '9', '2', '9', '4', '10', '1', '12', '12', '9', '3', '8', '9', '12', '9', '9', '2', '2', '2', '3', '5', '1', '1', '1', '7', '7', '8', '2', '2', '8', '2', '1', '7', '3', '2', '9', '8', '9', '1', '1', '9', '8', '9', '3', '3', '2', '9', '4', '9', '8', '8', '1', '8', '10', '2', '0', '3', '8', '8', '8', '9', '12', '9', '8', '2', '2', '8', '10', '9', '8', '2', '2', '4', '1', '9', '9', '8', '8', '10', '3', '1', '1', '2', '8', '7', '8', '2', '1', '1', '8', '12', '2', '9', '4', '8', '3', '3', '3', '8', '2', '12', '12', '8', '9', '12', '8', '2', '12', '7', '9', '2', '2', '1', '3', '1', '3', '11', '9', '8', '1', '10', '2', '12', '3', '8', '4', '1', '2', '8', '8', '3', '9', '3', '8', '7', '1', '1', '3', '1', '8', '2', '9', '8', '12', '1', '1', '1', '9', '9', '1', '8', '8', '8', '2', '2', '12', '3', '1', '1', '1', '9', '9', '3', '2', '1', '1', '9', '9', '1', '9', '9', '1', '9', '3', '9', '1', '8', '1', '9', '8', '3', '8', '8', '9', '9', '8', '2', '8', '1', '2', '2', '10', '1', '2', '1', '1', '9', '1', '9', '3', '7', '7', '2', '10', '3', '3', '1', '1', '9', '1', '2', '1', '3', '2', '1', '1', '1', '1', '1', '1', '1', '1', '1', '2', '8', '8', '7', '1', '1', '8', '8', '2', '1', '12', '1', '8', '1', '9', '8', '8', '2', '1', '9', '9', '2', '2', '8', '1', '7', '7', '9', '8', '1', '6', '8', '8', '2', '2', '8', '3', '8', '3', '2', '3', '8', '1', '2', '2', '2', '8', '9', '9', '9', '12', '1', '9', '2', '1', '9', '2', '9', '9', '10', '8', '1', '1', '9', '2', '1', '7', '1', '3', '7', '1', '1', '1', '3', '8', '8', '8', '9', '9', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '1', '0', '1', '1', '1', '2', '2', '3', '1', '1', '2', '3', '3', '3', '7', '3', '1', '9', '3', '9', '1', '4', '1', '1', '9', '10', '9', '8', '2', '3', '10', '9', '3', '10', '12', '8', '9', '8', '2', '1', '9', '8', '1', '7', '8', '1', '9', '1', '2', '10', '7', '7', '8', '1', '3', '3', '9', '3', '3', '1', '1', '1', '9', '1', '3', '8', '8', '8', '1', '8', '1', '9', '7', '8', '8', '2', '1', '3', '8', '2', '1', '6', '1', '8', '8', '2', '8', '2', '2', '9', '8', '1', '1', '6', '8', '6', '10', '2', '1', '1', '8', '9', '3', '3', '3', '1', '3', '9', '6', '8', '1', '8', '3', '1', '2', '2', '12', '7', '1', '1', '1', '1', '12', '1', '8', '8', '3', '2', '1', '9', '2', '3', '3', '2', '2', '7', '2', '2', '2', '3', '1', '9', '8', '7', '1', '1', '3', '1', '9', '4', '3', '9', '2', '1', '1', '9', '8', '8', '7', '2', '1', '3', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '2', '10', '1', '1', '8', '2', '8', '1', '8', '1', '3', '1', '2', '1', '1', '3', '1', '2', '1', '1', '1', '1', '1', '1', '1', '2', '1', '8', '1', '9', '8', '8', '1', '10', '8', '8', '1', '3', '9', '8', '9', '8', '2', '1', '3', '4', '1', '12', '1', '9', '8', '2', '7', '2', '1', '2', '9', '2', '1', '3', '8', '1', '2', '3', '8', '2', '10', '10', '10', '9', '9', '9', '3', '8', '3', '1', '1', '1', '1', '1', '1', '2', '1', '9', '2', '7', '9', '8', '7', '9', '1', '1', '1', '1', '8', '2', '2', '3', '3', '2', '12', '8', '3', '1', '1', '12', '9', '8', '2', '2', '9', '1', '2', '2', '9', '1', '1', '2', '4', '8', '1', '8', '10', '2', '9', '9', '9', '1', '2', '1', '1', '2', '2', '2', '12', '1', '2', '8', '3', '9', '9', '3', '2', '9', '9', '7', '2', '8', '8', '9', '2', '9', '1', '1', '1', '1', '1', '2', '8', '2', '1', '2', '2', '8', '3', '1', '1', '9', '1', '3', '10', '7', '9', '8', '8', '2', '2', '9', '2', '2', '2', '2', '2', '2', '9', '1', '3', '6', '4', '2', '2', '9', '1', '9', '8', '1', '1', '1', '7', '3', '2', '9', '9', '1', '9', '7', '2', '8', '9', '9', '12', '10', '8', '10', '2', '3', '1', '2', '2', '12', '12', '3', '12', '1', '2', '1', '2', '1', '2', '8', '2', '3', '12', '12', '8', '2', '2', '9', '2', '9', '2', '3', '3', '1', '1', '1', '12', '3', '7', '1', '3', '1', '2', '9', '2', '7', '2', '1', '8', '3', '7', '8', '1', '1', '3', '8', '3', '2', '9', '3', '1', '1', '1', '1', '11', '1', '2', '6', '1', '2', '3', '8', '9', '9', '2', '9', '2', '1', '1', '3', '1', '12', '9', '1', '1', '9', '1', '9', '9', '8', '9', '8', '1', '2', '7', '1', '0', '9', '8', '2', '3', '2', '4', '1', '1', '1', '8', '1', '12', '1', '2', '3', '3', '9', '9', '9', '9', '9', '7', '9', '3', '9', '1', '7', '3', '3', '3', '3', '7', '8', '2', '2', '2', '8', '2', '3', '9', '9', '3', '1', '8', '9', '12', '8', '8', '3', '2', '1', '6', '1', '9', '1', '1', '1', '1', '1', '1', '2', '2', '1', '2', '2', '2', '2', '9', '2', '5', '3', '2', '2', '8', '2', '1', '8', '3', '3', '8', '3', '1', '3', '9', '2', '8', '9', '4', '4', '9', '8', '8', '7', '7', '3', '1', '8', '8', '3', '2', '2', '10', '3', '3', '2', '2', '2', '2', '12', '3', '1', '2', '7', '7', '12', '2', '1', '2', '1', '1', '2', '1', '3', '3', '3', '1', '3', '9', '4', '8', '8', '4', '2', '4', '2', '8', '9', '1', '1', '2', '2', '2', '8', '7', '2', '8', '2', '9', '1', '8', '1', '1', '2', '1', '1', '1', '2', '9', '9', '8', '1', '8', '1', '4', '8', '1', '2', '7', '9', '2', '1', '8', '9', '8', '9', '1', '1', '2', '3', '2', '12', '7', '1', '2', '4', '4', '4', '2', '2', '2', '9', '8', '8', '1', '12', '3', '3', '2', '2', '3', '12', '1', '5', '1', '1', '3', '2', '8', '8', '2', '2', '2', '3', '9', '8', '8', '9', '8', '8', '11', '8', '2', '9', '2', '3', '1', '7', '1', '8', '2', '8', '1', '1', '1', '8', '1', '1', '1', '7', '8', '8', '2', '3', '2', '1', '8', '8', '1', '8', '4', '8', '9', '8', '9', '8', '1', '2', '3', '2', '1', '2', '2', '3', '1', '9', '4', '8', '1', '1', '9', '1', '1', '8', '2', '2', '2', '1', '7', '3', '3', '1', '3', '3', '12', '3', '1', '1', '3', '1', '1', '4', '8', '4', '4', '3', '3', '3', '4', '8', '1', '9', '9', '9', '4', '1', '7', '4', '8', '4', '9', '8', '1', '1', '3', '8', '9', '1', '9', '7', '1', '1', '9', '1', '8', '8', '8', '1', '1', '8', '2', '2', '2', '9', '4', '4', '1', '2', '1', '2', '9', '1', '1', '8', '5', '5', '5', '9', '12', '4', '1', '2', '1', '8', '8', '1', '9', '9', '2', '12', '1', '1', '7', '8', '8', '4', '1', '2', '8', '11', '2', '3', '2', '2', '1', '11', '2', '2', '2', '2', '9', '9', '2', '2', '9', '3', '1', '9', '1', '1', '1', '7', '8', '1', '8', '9', '1', '2', '9', '8', '1', '2', '9', '8', '12', '2', '2', '10', '8', '2', '2', '8', '8', '1', '2', '6', '1', '1', '7', '7', '2', '3', '2', '1', '1', '8', '1', '10', '8', '12', '6', '4', '1', '4', '1', '2', '9', '8', '9', '4', '8', '8', '10', '8', '8', '8', '8', '8', '2', '2', '8', '4', '9', '3', '3', '3', '3', '3', '9', '2', '1', '1', '2', '3', '2', '1', '2', '3', '3', '3', '2', '2', '3', '3', '2', '1', '2', '3', '3', '3', '3', '12', '9', '3', '2', '2', '3', '1', '5', '1', '1', '3', '2', '2', '1', '2', '9', '3', '9', '8', '1', '8', '7', '7', '9', '1', '1', '7', '9', '10', '1', '7', '1', '4', '3', '8', '9', '4', '9', '9', '2', '9', '2', '8', '4', '2', '2', '9', '5', '2', '1', '10', '9', '3', '1', '1', '1', '2', '1', '2', '9', '1', '7', '9', '9', '8', '2', '2', '8', '9', '1', '9', '8', '2', '3', '2', '2', '1', '2', '2', '2', '1', '1', '8', '1', '9', '4', '12', '8', '2', '7', '2', '2', '1', '9', '3', '1', '8', '12', '1', '1', '2', '1', '10', '2', '6', '12', '12', '4', '9', '1', '9', '10', '1', '2', '8', '2', '1', '8', '6', '12', '9', '9', '2', '1', '7', '8', '9', '5', '8', '1', '8', '8', '3', '1', '8', '2', '8', '2', '2', '1', '2', '9', '2', '8', '1', '12', '8', '2', '7', '3', '3', '12', '2', '3', '3', '9', '9', '3', '3', '3', '3', '3', '2', '1', '4', '1', '4', '8', '8', '2', '1', '2', '9', '12', '1', '10', '12', '1', '2', '9', '4', '8', '10', '3', '2', '8', '7', '4', '8', '9', '2', '7', '1', '10', '2', '2', '2', '8', '4', '2', '2', '1', '1', '8', '7', '9', '2', '1', '9', '8', '9', '12', '4', '8', '2', '8', '7', '7', '1', '9', '2', '1', '1', '9', '1', '8', '2', '3', '9', '9', '1', '3', '2', '9', '2', '2', '8', '1', '1', '2', '1', '2', '1', '9', '8', '2', '2', '4', '8', '8', '9', '5', '5', '9', '8', '9', '6', '2', '1', '12', '2', '3', '8', '8', '9', '2', '2', '1', '3', '2', '10', '9', '10', '7', '2', '8', '4', '2', '8', '9', '8', '1', '3', '8', '8', '1', '8', '3', '8', '8', '1', '10', '3', '1', '5', '8', '8', '9', '2', '2', '9', '2', '1', '9', '1', '4', '1', '12', '8', '8', '2', '1', '1', '1', '1', '1', '9', '2', '9', '8', '9', '5', '9', '1', '8', '2', '4', '1', '1', '1', '9', '12', '4', '1', '1', '9', '8', '4', '1', '8', '9', '8', '8', '9', '9', '9', '9', '9', '9', '1', '3', '4', '2', '2', '8', '2', '9', '2', '3', '8', '3', '1', '7', '2', '1', '1', '4', '2', '9', '2', '1', '3', '2', '9', '2', '9', '3', '8', '1', '9', '1', '1', '2', '8', '8', '8', '2', '3', '9', '9', '8', '2', '4', '1', '5', '1', '1', '9', '9', '2', '1', '1', '1', '9', '9', '1', '3', '9', '2', '2', '9', '9', '8', '3', '9', '9', '8', '2', '2', '8', '9', '2', '2', '8', '9', '10', '10', '2', '8', '7', '9', '9', '4', '2', '4', '11', '2', '1', '7', '8', '8', '8', '9', '4', '9', '8', '8', '2', '1', '1', '9', '8', '3', '3', '8', '8', '7', '9', '1', '3', '4', '8', '10', '2', '4', '4', '8', '2', '8', '2', '8', '9', '2', '1', '2', '2', '3', '1', '9', '4', '5', '9', '9', '1', '1', '1', '1', '1', '1', '7', '1', '1', '1', '8', '1', '1', '1', '1', '1', '9', '1', '2', '1', '12', '9', '3', '9', '9', '4', '1', '9', '1', '1', '2', '11', '9', '3', '2', '2', '2', '4', '2', '3', '2', '7', '7', '8', '1', '10', '9', '5', '7', '1', '10', '8', '1', '2', '7', '1', '12', '5', '8', '4', '2', '9', '9', '8', '2', '9', '8', '8', '10', '2', '3', '2', '7', '2', '8', '10', '3', '2', '9', '9', '9', '1', '1', '1', '1', '8', '2', '10', '9', '8', '1', '2', '2', '2', '1', '2', '1', '3', '9', '12', '2', '2', '4', '8', '1', '3', '9', '4', '9', '1', '9', '9', '2', '1', '1', '9', '7', '10', '3', '10', '2', '9', '2', '9', '2', '8', '2', '1', '2', '3', '1', '1', '1', '8', '5', '8', '3', '1', '1', '4', '2', '2', '1', '3', '8', '2', '1', '1', '1', '2', '9', '4', '1', '8', '1', '2', '8', '2', '9', '2', '2', '5', '5', '2', '5', '2', '1', '8', '8', '1', '3', '12', '2', '4', '3', '2', '2', '2', '6', '2', '3', '3', '1', '8', '2', '2', '1', '1', '8', '8', '10', '1', '9', '2', '9', '12', '0', '2', '1', '3', '2', '2', '1', '2', '1', '9', '9', '2', '9', '8', '8', '1', '7', '8', '1', '2', '6', '9', '8', '10', '1', '12', '2', '2', '1', '8', '2', '1', '12', '7', '4', '2', '10', '2', '2', '1', '2', '1', '2', '1', '1', '2', '1', '9', '8', '10', '2', '10', '9', '9', '12', '5', '3', '2', '8', '2', '8', '3', '3', '1', '4', '9', '10', '2', '4', '8', '1', '2', '10', '12', '12', '3', '1', '1', '2', '9', '6', '6', '1', '9', '1', '8', '1', '8', '8', '8', '8', '2', '2', '2', '8', '8', '2', '1', '1', '1', '1', '1', '8', '8', '5', '8', '8', '1', '8', '9', '9', '1', '7', '12', '9', '9', '7', '2', '2', '7', '8', '9', '8', '2', '9', '6', '3', '4', '8', '4', '1', '2', '9', '8', '8', '8', '2', '1', '1', '8', '8', '3', '1', '9', '9', '7', '1', '2', '2', '2', '9', '4', '1', '2', '12', '1', '8', '2', '1', '9', '1', '8', '9', '9', '2', '5', '4', '2', '1', '2', '8', '12', '8', '10', '3', '8', '8', '8', '2', '2', '8', '2', '8', '2', '4', '8', '10', '2', '7', '2', '1', '8', '8', '9', '5', '8', '1', '12', '7', '10', '2', '2', '2', '1', '4', '8', '12', '1', '8', '2', '2', '9', '1', '8', '3', '1', '1', '5', '10', '2', '1', '2', '4', '8', '9', '3', '2', '8', '7', '4', '4', '9', '9', '2', '8', '1', '1', '2', '4', '7', '1', '9', '1', '1', '1', '8', '4', '1', '3', '1', '2', '2', '9', '1', '8', '8', '2', '2', '9', '9', '1', '8', '8', '1', '9', '8', '8', '2', '2', '2', '2', '1', '1', '1', '7', '4', '1', '1', '4', '2', '3', '3', '2', '3', '9', '2', '8', '2', '1', '5', '1', '2', '2', '1', '3', '5', '2', '9', '1', '1', '9', '8', '1', '4', '8', '1', '8', '2', '4', '8', '4', '7', '9', '8', '1', '2', '2', '4', '4', '11', '3', '11', '1', '3', '8', '1', '9', '6', '8', '9', '3', '9', '8', '9', '2', '3', '3', '7', '1', '7', '2', '9', '9', '1', '8', '8', '1', '5', '5', '8', '8', '1', '2', '1', '3', '9', '9', '5', '9', '8', '1', '4', '3', '8', '1', '1', '2', '1', '8', '9', '8', '2', '2', '2', '1', '2', '4', '10', '8', '1', '9', '1', '8', '1', '1', '9', '9', '8', '11', '1', '8', '2', '1', '2', '8', '2', '8', '8', '2', '9', '6', '10', '1', '8', '6', '4', '5', '11', '2', '2', '8', '1', '8', '1', '8', '1', '4', '8', '1', '1', '1', '8', '3', '7', '3', '3', '7', '1', '1', '4', '8', '1', '6', '4', '2', '2', '1', '1', '1', '1', '6', '2', '2', '8', '1', '8', '5', '5', '2', '1', '1', '2', '3', '7', '1', '1', '6', '2', '9', '3', '9', '8', '9', '2', '1', '4', '2', '9', '13', '9', '2', '8', '12', '4', '1', '1', '9', '5', '1', '1', '1', '1', '8', '2', '1', '1', '2', '9', '0', '12', '1', '1', '8', '9', '8', '3', '2', '2', '12', '8', '2', '2', '2', '1', '10', '1', '9', '8', '9', '5', '2', '2', '9', '2', '8', '4', '8', '3', '7', '9', '2', '2', '7', '2', '2', '8', '1', '1', '9', '10', '9', '8', '2', '9', '0', '7', '1', '1', '1', '9', '8', '2', '8', '12', '8', '3', '2', '8', '8', '7', '7', '8', '12', '2', '1', '3', '9', '2', '2', '8', '2', '8', '8', '12', '8', '8', '1', '8', '9', '2', '4', '4', '7', '2', '1', '1', '9', '3', '7', '1', '3', '9', '8', '2', '2', '3', '1', '3', '10', '2', '3', '7', '1', '8', '1', '1', '10', '3', '10', '8', '4', '1', '2', '4', '3', '10', '9', '3', '9', '5', '8', '7', '2', '9', '3', '3', '2', '11', '8', '1', '1', '7', '2', '7', '2', '9', '4', '8', '1', '12', '8', '7', '9', '1', '0', '2', '6', '4', '9', '3', '2', '4', '12', '9', '8', '5', '9', '2', '8', '4', '6', '8', '9', '3', '1', '8', '7', '3', '1', '1', '1', '8', '9', '1', '8', '7', '9', '8', '2', '1', '2', '2', '3', '2', '2', '1', '1', '4', '9', '7', '3', '2', '8', '8', '9', '1', '9', '7', '4', '2', '2', '8', '5', '9', '1', '5', '7', '1', '4', '6', '9', '10', '1', '8', '2', '9', '1', '9', '2', '9', '8', '2', '9', '9', '11', '2', '12', '2', '2', '9', '4', '8', '1', '9', '9', '2', '8', '9', '1', '2', '9', '3', '8', '8', '1', '7', '8', '8', '2', '3', '2', '2', '2', '2', '9', '8', '10', '2', '2', '1', '9', '11', '1', '8', '8', '2', '4', '2', '1', '2', '8', '8', '9', '2', '8', '9', '2', '3', '1', '2', '1', '3', '10', '8', '1', '2', '4', '1', '9', '8', '9', '3', '3', '9', '8', '2', '11', '3', '7', '1', '8', '7', '1', '8', '8', '8', '1', '4', '8', '8', '1', '9', '3', '1', '1', '2', '8', '8', '8', '2', '1', '8', '10', '2', '12', '8', '8', '9', '1', '8', '1', '3', '2', '2', '10', '7', '2', '2', '1', '1', '8', '2', '9', '1', '2', '9', '3', '3', '10', '1', '4', '7', '10', '12', '2', '4', '10', '8', '6', '2', '9', '1', '8', '3', '1', '2', '8', '8', '2', '12', '10', '2', '10', '9', '1', '2', '5', '8', '12', '8', '8', '7', '8', '8', '1', '8', '2', '2', '9', '1', '8', '10', '5', '5', '5', '8', '1', '9', '1', '13', '1', '2', '2', '1', '2', '4', '9', '2', '1', '9', '13', '11', '1', '9', '9', '3', '10', '8', '7', '8', '8', '4', '2', '1', '9', '4', '3', '1', '1', '10', '9', '2', '8', '6', '2', '1', '1', '3', '2', '8', '1', '1', '1', '1', '2', '4', '1', '1', '10', '1', '8', '1', '1', '9', '1', '7', '1', '1', '9', '9', '2', '12', '9', '1', '8', '1', '12', '1', '8', '3', '8', '1', '8', '1', '2', '10', '1', '1', '2', '2', '9', '3', '9', '12', '1', '9', '1', '2', '8', '7', '2', '1', '1', '3', '2', '8', '4', '4', '5', '12', '7', '8', '9', '6', '1', '8', '9', '2', '1', '9', '1', '4', '1', '8', '2', '1', '1', '8', '7', '4', '9', '1', '8', '4', '2', '7', '5', '1', '3', '9', '2', '8', '2', '1', '2', '9', '1', '1', '2', '3', '4', '3', '1', '2', '2', '10', '2', '2', '8', '1', '1', '2', '4', '9', '2', '11', '9', '8', '9', '2', '1', '10', '4', '1', '1', '10', '7', '2', '10', '8', '1', '10', '8', '8', '8', '8', '8', '8', '2', '2', '2', '8', '3', '4', '1', '1', '1', '2', '1', '8', '8', '2', '8', '3', '8', '3', '1', '2', '3', '1', '1', '10', '1', '4', '4', '3', '3', '1', '9', '1', '1', '2', '1', '1', '6', '1', '1', '1', '9', '1', '4', '1', '1', '1', '1', '1', '1', '2', '12', '8', '8', '1', '9', '12', '10', '8', '2', '12', '1', '1', '2', '1', '7', '10', '7', '1', '2', '1', '10', '8', '2', '2', '8', '2', '2', '1', '9', '8', '8', '8', '8', '3', '1', '4', '2', '8', '3', '10', '10', '1', '9', '1', '9', '1', '9', '1', '8', '3', '1', '1', '8', '8', '8', '6', '3', '4', '1', '1', '1', '9', '9', '5', '2', '10', '2', '6', '2', '2', '7', '1', '8', '2', '11', '1', '1', '1', '2', '1', '9', '4', '4', '2', '8', '4', '6', '8', '8', '10', '10', '2', '1', '2', '9', '1', '8', '8', '3', '4', '8', '8', '2', '8', '8', '1', '2', '1', '2', '9', '4', '1', '3', '3', '3', '8', '8', '8', '2', '6', '4', '3', '3', '12', '3', '4', '9', '6', '8', '7', '7', '8', '6', '4', '1', '10', '6', '3', '3', '8', '1', '1', '8', '1', '11', '8', '1', '1', '2', '3', '11', '7', '2', '1', '1', '1', '9', '1', '2', '1', '1', '9', '8', '1', '1', '8', '0', '4', '1', '1', '2', '1', '1', '2', '2', '9', '1', '10', '1', '3', '8', '9', '3', '3', '1', '10', '1', '9', '1', '1', '7', '4', '1', '8', '10', '1', '2', '10', '1', '1', '2', '3', '3', '9', '1', '8', '10', '1', '7', '9', '1', '9', '6', '3', '9', '2', '8', '4', '12', '12', '3', '1', '1', '9', '1', '9', '2', '1', '1', '1', '1', '1', '9', '2', '1', '2', '10', '9', '8', '8', '10', '8', '2', '3', '2', '1', '12', '8', '12', '2', '9', '1', '3', '2', '5', '2', '2', '8', '2', '2', '4', '10', '9', '10', '9', '4', '2', '4', '12', '8', '2', '1', '8', '3', '9', '2', '9', '4', '1', '1', '1', '1', '1', '9', '6', '2', '9', '2', '1', '3', '2', '2', '5', '8', '9', '1', '6', '3', '1', '2', '2', '2', '6', '1', '8', '10', '8', '3', '3', '13', '2', '9', '9', '10', '6', '12', '8', '2', '9', '2', '3', '8', '8', '1', '2', '2', '3', '2', '10', '1', '8', '9', '1', '8', '9', '8', '8', '2', '10', '2', '1', '1', '1', '9', '9', '12', '1', '1', '4', '8', '3', '2', '2', '4', '4', '2', '2', '1', '1', '1', '9', '2', '2', '4', '1', '1', '8', '8', '8', '9', '2', '13', '9', '9', '4', '1', '9', '7', '10', '10', '8', '1', '9', '1', '1', '1', '8', '12', '4', '1', '2', '1', '1', '1', '7', '8', '3', '3', '1', '2', '2', '7', '2', '2', '10', '1', '8', '4', '0', '1', '9', '4', '1', '1', '2', '10', '10', '7', '9', '3', '12', '11', '2', '8', '10', '8', '9', '4', '3', '4', '10', '1', '1', '9', '3', '10', '1', '5', '3', '3', '2', '2', '1', '1', '6', '1', '7', '1', '1', '12', '1', '1', '1', '10', '1', '8', '8', '1', '3', '8', '3', '1', '2', '8', '3', '4', '2', '8', '8', '6', '1', '1', '4', '1', '2', '9', '5', '1', '7', '8', '9', '2', '4', '9', '8', '5', '9', '9', '9', '1', '2', '1', '3', '8', '9', '2', '9', '8', '9', '1', '8', '13', '9', '3', '8', '1', '4', '1', '3', '1', '6', '4', '10', '2', '2', '7', '8', '8', '12', '8', '9', '9', '9', '10', '3', '12', '12', '2', '9', '10', '9', '7', '2', '3', '4', '10', '10', '2', '7', '7', '1', '7', '3', '9', '1', '9', '8', '2', '9', '1', '8', '1', '10', '8', '8', '2', '7', '8', '1', '8', '9', '8', '1', '4', '1', '8', '8', '8', '1', '1', '10', '3', '2', '2', '6', '8', '9', '8', '2', '1', '1', '3', '8', '8', '1', '4', '4', '3', '9', '10', '4', '2', '4', '2', '9', '2', '2', '9', '1', '1', '10', '1', '1', '2', '1', '1', '1', '9', '1', '8', '8', '8', '2', '3', '8', '1', '6', '7', '3', '1', '9', '1', '2', '1', '6', '1', '2', '10', '1', '4', '2', '3', '2', '9', '9', '4', '9', '9', '1', '2', '8', '4', '7', '8', '7', '13', '9', '1', '2', '1', '9', '3', '3', '6', '8', '9', '10', '9', '2', '2', '3', '12', '1', '1', '1', '13', '2', '1', '7', '10', '8', '9', '1', '2', '9', '1', '1', '1', '2', '1', '12', '5', '6', '9', '10', '3', '1', '10', '1', '3', '2', '8', '9', '2', '10', '8', '8', '9', '2', '8', '9', '2', '2', '2', '8', '8', '2', '3', '1', '4', '8', '1', '9', '9', '2', '8', '10', '9', '1', '2', '12', '8', '8', '0', '2', '1', '1', '9', '10', '1', '6', '2', '2', '10', '2', '2', '4', '2', '2', '12', '1', '6', '7', '8', '9', '1', '3', '5', '7', '3', '1', '3', '1', '1', '3', '2', '6', '3', '2', '1', '8', '3', '5', '2', '1', '1', '8', '1', '1', '0', '2', '3', '5', '3', '1', '1', '9', '8', '8', '8', '8', '4', '7', '2', '9', '12', '5', '9', '3', '12', '3', '9', '1', '9', '1', '7', '3', '9', '3', '10', '1', '2', '1', '4', '9', '8', '2', '9', '11', '2', '8', '9', '4', '1', '1', '1', '1', '1', '1', '1', '9', '1', '9', '1', '1', '1', '12', '3', '10', '3', '6', '1', '7', '10', '8', '3', '7', '1', '1', '2', '6', '1', '3', '9', '2', '8', '1', '8', '8', '1', '8', '10', '12', '1', '10', '12', '10', '1', '8', '8', '1', '4', '7', '9', '2', '1', '9', '3', '8', '12', '10', '6', '1', '6', '8', '3', '1', '3', '1', '10', '10', '9', '8', '1', '2', '9', '8', '8', '1', '9', '8', '8', '3', '1', '3', '9', '12', '1', '8', '1', '5', '8', '11', '5', '5', '1', '2', '1', '3', '1', '1', '1', '9', '8', '8', '1', '9', '10', '8', '4', '1', '1', '8', '10', '8', '9', '1', '10', '1', '7', '2', '1', '1', '9', '6', '2', '9', '8', '1', '1', '1', '9', '8', '8', '1', '9', '9', '1', '1', '2', '7', '9', '2', '9', '9', '8', '7', '6', '8', '7', '8', '2', '9', '2', '1', '8', '8', '8', '2', '6', '3', '1', '8', '12', '12', '8', '7', '1', '2', '2', '7', '1', '9', '8', '1', '4', '1', '1', '8', '5', '8', '1', '8', '8', '8', '8', '1', '2', '11', '1', '1', '1', '2', '7', '1', '9', '2', '2', '2', '9', '8', '9', '2', '9', '1', '8', '1', '7', '11', '13', '1', '9', '8', '2', '2', '1', '2', '3', '10', '3', '10', '4', '3', '9', '1', '8', '1', '1', '13', '1', '1', '3', '8', '1', '2', '2', '2', '9', '4', '1', '8', '1', '9', '3', '6', '8', '5', '2', '8', '2', '8', '1', '9', '2', '2', '8', '1', '1', '4', '8', '9', '2', '2', '7', '1', '8', '12', '2', '12', '8', '2', '9', '9', '3', '8', '1', '1', '9', '9', '9', '2', '8', '1', '9', '1', '8', '9', '8', '2', '2', '4', '1', '9', '10', '10', '8', '8', '1', '1', '2', '1', '4', '1', '2', '1', '1', '3', '12', '8', '8', '8', '8', '10', '2', '8', '12', '1', '9', '9', '8', '9', '1', '2', '1', '8', '8', '8', '2', '8', '8', '3', '10', '8', '9', '1', '1', '1', '3', '4', '8', '10', '6', '3', '1', '3', '2', '8', '9', '3', '5', '4', '1', '5', '2', '9', '9', '9', '1', '1', '1', '9', '9', '6', '10', '9', '8', '12', '2', '9', '1', '2', '1', '1', '1', '8', '1', '12', '1', '12', '1', '9', '1', '8', '1', '2', '1', '2', '8', '8', '10', '1', '9', '1', '2', '2', '8', '8', '3', '12', '1', '11', '2', '10', '12', '11', '2', '1', '10', '1', '1', '9', '3', '10', '2', '7', '1', '8', '1', '9', '9', '2', '1', '3', '12', '5', '9', '9', '8', '1', '8', '8', '8', '1', '1', '3', '9', '8', '9', '8', '3', '10', '3', '7', '8', '9', '2', '3', '1', '9', '10', '9', '2', '8', '2', '6', '4', '1', '1', '3', '8', '2', '3', '9', '1', '2', '1', '8', '1', '2', '2', '2', '2', '9', '9', '4', '8', '9', '1', '1', '2', '9', '1', '5', '2', '9', '8', '10', '1', '2', '10', '8', '5', '6', '4', '8', '1', '1', '1', '8', '9', '2', '8', '8', '8', '12', '2', '2', '1', '8', '1', '2', '1', '9', '8', '8', '9', '1', '1', '8', '7', '1', '8', '0', '2', '3', '8', '1', '1', '8', '1', '1', '12', '3', '2', '10', '8', '10', '6', '1', '8', '8', '10', '8', '8', '1', '4', '8', '1', '2', '9', '8', '1', '1', '3', '3', '5', '1', '1', '2', '1', '2', '1', '9', '8', '3', '9', '8', '2', '10', '9', '1', '8', '1', '10', '9', '10', '1', '2', '3', '8', '1', '3', '1', '9', '9', '8', '1', '9', '8', '8', '9', '1', '9', '2', '8', '8', '2', '13', '8', '10', '8', '9', '3', '2', '1', '3', '1', '12', '10', '10', '1', '11', '1', '2', '4', '2', '10', '1', '8', '11', '1', '1', '1', '2', '2', '8', '2', '9', '8', '9', '9', '12', '8', '1', '9', '2', '4', '1', '8', '3', '9', '1', '6', '5', '8', '9', '3', '3', '2', '1', '1', '1', '8', '11', '1', '8', '7', '1', '11', '9', '1', '7', '9', '9', '8', '8', '8', '12', '8', '1', '2', '9', '8', '8', '2', '10', '9', '8', '8', '4', '10', '9', '8', '2', '10', '10', '2', '1', '2', '1', '2', '1', '4', '1', '7', '1', '1', '3', '12', '9', '8', '5', '9', '10', '1', '9', '9', '2', '7', '4', '1', '1', '8', '12', '8', '2', '2', '9', '4', '1', '1', '5', '9', '1', '8', '12', '8', '1', '1', '4', '9', '10', '2', '3', '1', '3', '3', '3', '8', '1', '2', '1', '2', '1', '2', '9', '1', '2', '8', '9', '4', '8', '8', '10', '2', '12', '5', '5', '1', '8', '1', '7', '8', '1', '1', '8', '9', '12', '4', '9', '3', '1', '2', '5', '9', '2', '1', '2', '2', '1', '2', '8', '1', '8', '8', '2', '2', '1', '8', '4', '1', '2', '10', '9', '10', '8', '1', '10', '4', '8', '8', '10', '2', '8', '1', '1', '3', '10', '1', '4', '2', '8', '3', '3', '2', '9', '8', '5', '5', '9', '3', '10', '8', '1', '2', '1', '2', '9', '1', '1', '2', '9', '9', '8', '8', '11', '9', '4', '1', '8', '2', '7', '1', '2', '8', '2', '9', '9', '8', '2', '9', '9', '8', '8', '1', '2', '8', '1', '1', '2', '9', '8', '1', '4', '9', '1', '8', '12', '1', '12', '2', '8', '2', '9', '1', '12', '1', '2', '9', '1', '1', '9', '3', '2', '9', '9', '2', '11', '8', '7', '9', '9', '9', '8', '8', '1', '2', '8', '1', '1', '4', '1', '2', '10', '1', '9', '2', '1', '1', '1', '9', '1', '13', '4', '3', '2', '1', '1', '2', '2', '7', '8', '7', '1', '1', '2', '8', '8', '3', '4', '8', '2', '10', '13', '8', '8', '9', '2', '2', '1', '1', '10', '4', '2', '7', '9', '8', '2', '1', '9', '8', '1', '1', '9', '8', '9', '8', '2', '8', '2', '2', '9', '2', '1', '9', '1', '2', '4', '8', '2', '1', '1', '8', '1', '4', '1', '9', '3', '9', '2', '7', '1', '9', '6', '1', '10', '2', '2', '2', '2', '2', '10', '10', '10', '9', '9', '9', '3', '12', '10', '1', '1', '10', '1', '1', '2', '2', '1', '2', '3', '12', '8', '8', '1', '2', '9', '9', '1', '1', '10', '8', '9', '8', '3', '1', '1', '3', '1', '10', '1', '1', '1', '4', '1', '1', '1', '7', '10', '2', '1', '8', '1', '9', '3', '1', '10', '8', '8', '1', '5', '9', '1', '2', '1', '8', '8', '1', '3', '1', '10', '2', '1', '1', '2', '8', '2', '5', '3', '5', '2', '1', '1', '9', '1', '7', '9', '9', '2', '8', '1', '12', '4', '1', '4', '9', '9', '4', '8', '1', '10', '10', '8', '8', '8', '9', '2', '3', '5', '1', '8', '1', '1', '1', '7', '10', '1', '4', '12', '3', '2', '2', '9', '1', '1', '1', '2', '8', '4', '1', '7', '3', '12', '8', '6', '2', '2', '7', '9', '11', '1', '10', '9', '1', '8', '12', '2', '11', '1', '2', '3', '1', '1', '1', '8', '9', '2', '2', '2', '2', '3', '3', '8', '10', '4', '1', '2', '9', '8', '2', '2', '9', '1', '8', '11', '4', '2', '8', '7', '1', '1', '7', '3', '9', '9', '4', '5', '8', '10', '2', '9', '9', '9', '8', '1', '9', '2', '2', '2', '1', '2', '9', '4', '3', '12', '4', '3', '2', '3', '8', '3', '10', '1', '10', '1', '1', '8', '10', '6', '8', '9', '9', '1', '2', '9', '2', '9', '3', '8', '1', '1', '12', '1', '5', '1', '10', '8', '10', '2', '7', '1', '1', '1', '1', '3', '2', '3', '5', '9', '1', '2', '1', '9', '8', '10', '9', '8', '1', '8', '1', '8', '2', '8', '1', '2', '8', '1', '2', '5', '8', '12', '2', '1', '1', '1', '1', '1', '8', '8', '2', '1', '4', '2', '8', '8', '8', '2', '9', '9', '11', '1', '2', '9', '3', '1', '1', '8', '9', '2', '4', '10', '1', '9', '8', '4', '9', '8', '10', '8', '8', '9', '2', '8', '9', '3', '3', '3', '4', '3', '2', '2', '10', '8', '2', '1', '5', '1', '9', '1', '1', '2', '1', '2', '3', '1', '1', '8', '8', '1', '10', '8', '8', '1', '6', '4', '8', '2', '5', '8', '1', '3', '8', '2', '8', '9', '1', '1', '8', '12', '10', '1', '5', '8', '2', '8', '10', '9', '9', '1', '6', '9', '1', '10', '8', '2', '9', '10', '1', '2', '1', '1', '8', '8', '3', '9', '8', '1', '10', '2', '8', '1', '10', '1', '5', '1', '1', '1', '5', '1', '1', '1', '4', '1', '1', '1', '8', '2', '8', '1', '8', '3', '8', '9', '1', '8', '1', '8', '1', '1', '2', '2', '1', '12', '9', '9', '1', '8', '2', '1', '2', '8', '1', '9', '1', '1', '2', '1', '2', '2', '2', '1', '9', '9', '8', '8', '1', '1', '9', '10', '8', '4', '3', '3', '4', '9', '1', '3', '3', '10', '2', '2', '1', '8', '4', '1', '9', '4', '1', '1', '9', '8', '10', '4', '9', '9', '8', '2', '1', '3', '4', '9', '3', '8', '9', '1', '1', '2', '1', '7', '8', '2', '9', '1', '9', '2', '6', '9', '10', '8', '1', '1', '5', '9', '5', '1', '9', '10', '9', '3', '3', '8', '8', '2', '9', '2', '8', '2', '1', '8', '5', '8', '3', '10', '1', '1', '2', '1', '8', '1', '4', '3', '2', '9', '4', '9', '8', '8', '8', '1', '3', '9', '1', '9', '9', '1', '2', '1', '9', '8', '9', '8', '1', '1', '1', '4', '2', '2', '9', '9', '1', '1', '1', '3', '6', '1', '2', '3', '1', '4', '2', '9', '1', '9', '9', '1', '8', '2', '1', '9', '1', '8', '1', '8', '8', '2', '1', '9', '9', '6', '8', '8', '9', '8', '10', '8', '9', '5', '1', '1', '1', '1', '8', '1', '8', '12', '8', '1', '12', '9', '2', '8', '1', '1', '2', '1', '8', '6', '9', '9', '8', '1', '8', '9', '7', '7', '8', '9', '9', '1', '8', '3', '8', '1', '3', '3', '8', '9', '8', '2', '8', '1', '2', '9', '8', '1', '1', '1', '1', '9', '1', '9', '8', '12', '3', '8', '8', '1', '10', '10', '10', '2', '2', '1', '10', '2', '2', '10', '8', '12', '8', '1', '1', '1', '2', '1', '1', '3', '8', '1', '2', '2', '2', '1', '1', '6', '2', '8', '8', '8', '1', '1', '1', '9', '9', '2', '8', '10', '2', '8', '2', '1', '2', '9', '9', '2', '1', '1', '8', '8', '1', '3', '1', '8', '9', '1', '10', '9', '1', '1', '1', '1', '7', '2', '2', '1', '2', '8', '8', '1', '2', '1', '3', '2', '1', '8', '3', '9', '2', '10', '2', '10', '1', '2', '1', '1', '13', '9', '10', '8', '2', '2', '1', '1', '8', '2', '2', '1', '9', '1', '1', '8', '9', '9', '2', '9', '2', '2', '1', '1', '1', '8', '9', '4', '9', '9', '1', '8', '2', '1', '1', '8', '2', '2', '2', '8', '1', '1', '8', '2', '10', '2', '9', '9', '7', '1', '1', '2', '2', '8', '3', '1', '2', '1', '1', '8', '1', '1', '1', '9', '1', '1', '8', '8', '1', '9', '6', '1', '9', '1', '8', '1', '8', '8', '8', '1', '3', '8', '8', '2', '6', '3', '2', '1', '10', '1', '8', '2', '1', '6', '1', '1', '1', '8', '1', '10', '1', '1', '8', '9', '2', '6', '9', '2', '9', '4', '1', '8', '1', '3', '7', '8', '8', '5', '8', '7', '1', '1', '1', '13', '8', '3', '1', '1', '1', '1', '2', '8', '2', '12', '8', '1', '2', '5', '1', '8', '2', '10', '8', '2', '10', '1', '5', '2', '1', '1', '5', '2', '2', '7', '8', '2', '9', '9', '1', '10', '1', '8', '2', '5', '8', '1', '5', '1', '10', '1', '1', '9', '8', '6', '1', '8', '9', '8', '1', '8', '2', '2', '8', '3', '9', '1', '9', '3', '8', '4', '4', '3', '3', '1', '1', '1', '8', '9', '1', '1', '8', '3', '1', '2', '8', '2', '1', '1', '10', '1', '2', '2', '1', '1', '8', '2', '8', '2', '10', '2', '2', '10', '8', '8', '8', '13', '2', '2', '9', '2', '5', '9', '2', '1', '8', '8', '8', '1', '12', '12', '2', '9', '2', '1', '8', '2', '1', '8', '1', '9', '9', '2', '7', '1', '1', '7', '1', '4', '3', '1', '10', '3', '10', '9', '4', '6', '7', '5', '2', '1', '8', '8', '1', '4', '9', '2', '1', '8', '1', '1', '1', '1', '1', '8', '9', '8', '8', '8', '8', '9', '8', '10', '1', '8', '2', '7', '8', '9', '1', '2', '5', '9', '10', '8', '9', '12', '1', '6', '1', '2', '10', '9', '4', '1', '8', '10', '10', '1', '1', '1', '1', '1', '2', '3', '8', '1', '2', '10', '2', '1', '2', '4', '2', '2', '9', '1', '4', '2', '1', '1', '12', '12', '9', '9', '7', '1', '8', '4', '9', '9', '8', '1', '7', '3', '2', '1', '1', '4', '8', '1', '5', '8', '2', '8', '1', '14', '9', '12', '1', '10', '2', '1', '1', '8', '8', '6', '6', '3', '1', '2', '8', '1', '3', '2', '1', '8', '8', '1', '10', '8', '2', '8', '2', '5', '9', '1', '8', '8', '1', '1', '8', '8', '13', '1', '3', '8', '4', '1', '9', '7', '8', '8', '8', '3', '8', '1', '4', '2', '8', '8', '8', '7', '1', '1', '2', '8', '8', '9', '8', '8', '2', '3', '9', '9', '2', '1', '2', '2', '1', '8', '2', '6', '2', '1', '1', '9', '10', '3', '8', '2', '3', '2', '8', '8', '1', '8', '8', '8', '9', '1', '4', '1', '9', '2', '1', '8', '0', '1', '8', '1', '1', '8', '9', '9', '1', '3', '1', '2', '8', '8', '7', '1', '1', '2', '9', '2', '10', '2', '8', '2', '2', '2', '7', '1', '9', '8', '1', '3', '9', '2', '1', '10', '8', '1', '4', '1', '8', '5', '9', '8', '8', '1', '2', '2', '1', '8', '8', '6', '8', '1', '1', '8', '3', '2', '2', '1', '8', '1', '7', '6', '8', '1', '1', '1', '9', '1', '1', '1', '2', '1', '11', '10', '10', '11', '9', '']\n",
            "['8', '1', '8', '2', '8', '8', '8', '9', '7', '8', '1', '2', '1', '1', '8', '4', '8', '8', '12', '3', '3', '7', '3', '12', '1', '8', '8', '8', '8', '8', '8', '8', '1', '9', '5', '9', '9', '9', '11', '8', '8', '8', '4', '8', '8', '8', '8', '1', '3', '9', '3', '7', '1', '2', '9', '9', '7', '8', '8', '1', '10', '7', '8', '8', '9', '8', '7', '9', '9', '12', '7', '2', '8', '1', '11', '11', '1', '7', '7', '12', '1', '9', '8', '10', '12', '7', '8', '2', '8', '9', '9', '1', '8', '9', '1', '7', '12', '10', '10', '10', '8', '3', '7', '9', '8', '9', '1', '8', '8', '2', '7', '2', '9', '9', '11', '8', '8', '12', '12', '7', '8', '12', '4', '9', '3', '1', '12', '1', '1', '8', '8', '3', '8', '8', '8', '8', '9', '1', '8', '8', '10', '1', '8', '2', '8', '8', '7', '3', '8', '2', '4', '4', '9', '8', '10', '12', '12', '1', '1', '9', '1', '1', '1', '8', '2', '2', '8', '1', '1', '2', '2', '1', '2', '8', '1', '9', '9', '8', '8', '4', '2', '9', '9', '8', '3', '4', '3', '1', '8', '8', '2', '1', '9', '7', '8', '8', '1', '12', '3', '8', '2', '4', '2', '9', '12', '1', '4', '1', '8', '8', '8', '2', '2', '8', '9', '8', '8', '8', '10', '9', '8', '7', '9', '1', '1', '9', '4', '2', '4', '2', '2', '1', '7', '8', '11', '11', '3', '9', '2', '4', '8', '9', '1', '8', '1', '1', '4', '9', '1', '1', '8', '8', '2', '1', '8', '4', '2', '8', '9', '8', '8', '2', '8', '8', '8', '7', '1', '1', '1', '2', '1', '1', '8', '7', '8', '8', '12', '2', '12', '12', '8', '10', '12', '8', '3', '3', '12', '10', '1', '8', '12', '1', '8', '8', '2', '8', '4', '7', '8', '7', '10', '8', '10', '9', '8', '12', '12', '1', '8', '8', '3', '8', '8', '8', '8', '8', '1', '9', '8', '11', '1', '1', '1', '9', '8', '1', '9', '2', '3', '11', '8', '9', '9', '9', '2', '1', '8', '8', '9', '7', '1', '4', '9', '4', '8', '8', '4', '8', '12', '9', '4', '8', '2', '10', '10', '10', '8', '9', '9', '8', '8', '12', '7', '1', '8', '8', '8', '4', '1', '1', '1', '1', '1', '1', '8', '1', '9', '8', '9', '9', '4', '8', '12', '9', '8', '8', '2', '8', '8', '8', '6', '9', '8', '3', '7', '8', '8', '4', '12', '8', '8', '9', '12', '12', '9', '8', '2', '9', '2', '3', '1', '12', '8', '10', '9', '9', '9', '10', '10', '3', '8', '12', '1', '4', '2', '1', '10', '8', '2', '8', '4', '8', '9', '1', '9', '9', '10', '10', '1', '4', '9', '2', '4', '9', '1', '1', '3', '10', '3', '3', '8', '7', '3', '8', '9', '9', '12', '4', '8', '12', '2', '2', '4', '1', '9', '9', '4', '1', '4', '2', '8', '12', '2', '3', '10', '10', '9', '8', '9', '9', '1', '12', '8', '8', '8', '12', '4', '1', '8', '8', '1', '9', '8', '8', '2', '1', '8', '9', '8', '3', '3', '3', '1', '8', '8', '9', '1', '10', '9', '9', '9', '9', '5', '9', '9', '8', '8', '8', '8', '8', '8', '8', '8', '11', '12', '8', '8', '1', '8', '9', '11', '2', '2', '2', '2', '3', '1', '2', '2', '8', '2', '4', '9', '1', '2', '9', '8', '2', '8', '9', '9', '3', '10', '9', '9', '2', '8', '9', '8', '12', '12', '1', '3', '8', '8', '8', '2', '7', '7', '7', '7', '3', '9', '1', '9', '8', '9', '9', '1', '1', '1', '2', '9', '9', '9', '11', '1', '8', '8', '9', '1', '9', '8', '8', '8', '1', '1', '8', '7', '1', '1', '8', '8', '9', '4', '4', '8', '2', '2', '8', '8', '8', '8', '8', '8', '11', '8', '2', '9', '4', '9', '3', '9', '9', '1', '3', '9', '3', '1', '12', '8', '9', '12', '1', '8', '4', '2', '1', '4', '8', '3', '3', '8', '2', '8', '9', '7', '8', '8', '8', '5', '8', '3', '9', '8', '8', '13', '12', '1', '1', '2', '8', '4', '1', '9', '9', '12', '8', '9', '12', '9', '1', '9', '9', '9', '9', '3', '2', '9', '9', '4', '8', '12', '2', '4', '9', '3', '1', '9', '7', '8', '9', '9', '8', '4', '8', '8', '7', '9', '10', '3', '8', '8', '8', '1', '1', '1', '1', '8', '8', '4', '1', '10', '1', '5', '7', '7', '1', '8', '9', '3', '7', '2', '7', '7', '2', '4', '8', '12', '7', '4', '2', '9', '9', '12', '6', '10', '8', '2', '4', '12', '9', '9', '3', '8', '8', '1', '2', '10', '9', '9', '8', '4', '12', '2', '1', '8', '8', '8', '12', '10', '10', '9', '3', '8', '8', '9', '2', '8', '10', '1', '1', '1', '1', '2', '1', '1', '1', '1', '9', '8', '12', '9', '4', '8', '8', '9', '1', '9', '3', '9', '8', '8', '1', '7', '7', '10', '1', '8', '8', '1', '9', '8', '10', '3', '1', '7', '1', '8', '8', '12', '8', '8', '1', '8', '7', '1', '7', '7', '8', '2', '1', '8', '8', '2', '10', '8', '8', '8', '8', '8', '10', '1', '8', '8', '12', '8', '3', '3', '2', '2', '2', '10', '8', '8', '8', '2', '9', '1', '8', '9', '3', '2', '8', '10', '8', '6', '1', '1', '8', '4', '1', '9', '10', '8', '1', '7', '1', '2', '8', '1', '1', '1', '12', '1', '9', '12', '8', '12', '12', '12', '8', '8', '12', '4', '8', '8', '8', '8', '9', '9', '1', '3', '3', '3', '3', '1', '12', '12', '9', '10', '8', '8', '1', '9', '2', '2', '13', '9', '8', '9', '2', '1', '9', '1', '8', '8', '8', '4', '8', '1', '1', '1', '12', '12', '7', '2', '2', '2', '8', '3', '8', '9', '2', '10', '7', '8', '9', '2', '1', '2', '12', '12', '8', '8', '9', '2', '2', '9', '11', '1', '8', '1', '10', '9', '2', '1', '4', '7', '7', '7', '7', '7', '12', '8', '8', '8', '1', '1', '10', '1', '12', '1', '8', '2', '1', '1', '12', '8', '7', '9', '12', '8', '9', '3', '9', '8', '8', '8', '8', '3', '11', '2', '2', '9', '8', '8', '10', '8', '2', '7', '3', '1', '4', '7', '8', '8', '1', '8', '3', '7', '12', '8', '10', '9', '9', '8', '8', '2', '8', '9', '9', '1', '2', '8', '8', '9', '8', '3', '8', '1', '8', '10', '9', '8', '9', '9', '12', '4', '4', '8', '9', '9', '8', '2', '10', '1', '2', '8', '9', '1', '9', '9', '9', '7', '12', '12', '8', '1', '1', '1', '1', '8', '3', '1', '1', '8', '1', '8', '8', '7', '8', '8', '8', '8', '3', '2', '2', '10', '10', '10', '7', '8', '1', '2', '12', '7', '9', '8', '7', '8', '12', '2', '8', '9', '2', '6', '6', '7', '9', '8', '1', '8', '9', '8', '1', '12', '1', '2', '8', '7', '7', '7', '8', '2', '2', '8', '1', '2', '2', '9', '9', '1', '8', '8', '4', '3', '3', '1', '6', '3', '3', '12', '3', '8', '9', '1', '4', '3', '1', '8', '3', '9', '2', '8', '2', '8', '8', '8', '1', '1', '1', '9', '9', '8', '1', '9', '8', '1', '1', '3', '10', '8', '1', '1', '3', '9', '1', '4', '4', '1', '8', '9', '9', '2', '0', '0', '1', '8', '3', '1', '8', '8', '9', '8', '8', '1', '1', '8', '9', '8', '8', '8', '7', '9', '8', '8', '8', '10', '9', '8', '1', '2', '6', '1', '9', '9', '8', '12', '12', '12', '8', '8', '2', '8', '1', '2', '2', '2', '1', '9', '8', '2', '12', '2', '8', '12', '8', '9', '8', '8', '9', '7', '1', '1', '1', '1', '1', '8', '8', '1', '8', '8', '1', '1', '3', '2', '8', '8', '9', '10', '10', '2', '2', '1', '9', '2', '9', '9', '4', '12', '12', '12', '10', '7', '3', '3', '4', '2', '2', '9', '2', '8', '4', '2', '4', '1', '10', '9', '7', '8', '7', '1', '1', '3', '3', '1', '1', '3', '3', '3', '1', '1', '1', '1', '8', '2', '3', '1', '1', '2', '8', '8', '12', '8', '8', '8', '8', '11', '9', '1', '8', '9', '2', '8', '8', '8', '3', '9', '1', '9', '2', '7', '2', '8', '2', '8', '10', '8', '1', '10', '1', '1', '9', '9', '8', '8', '1', '8', '8', '8', '12', '8', '8', '8', '1', '8', '8', '8', '1', '9', '1', '1', '8', '1', '8', '9', '8', '2', '12', '9', '9', '0', '1', '8', '8', '1', '8', '12', '8', '8', '10', '8', '8', '8', '7', '8', '1', '8', '7', '3', '10', '1', '8', '9', '1', '8', '8', '8', '10', '1', '10', '3', '9', '1', '8', '9', '2', '8', '3', '3', '9', '9', '7', '9', '1', '1', '9', '2', '1', '1', '1', '7', '1', '1', '8', '8', '1', '1', '8', '1', '8', '3', '12', '9', '3', '3', '8', '8', '8', '8', '3', '1', '3', '3', '1', '11', '0', '8', '8', '7', '8', '12', '1', '8', '9', '8', '9', '8', '8', '3', '8', '8', '1', '1', '1', '9', '2', '2', '2', '8', '7', '12', '8', '8', '9', '10', '10', '7', '8', '1', '9', '8', '7', '3', '1', '3', '8', '2', '2', '3', '9', '8', '4', '4', '8', '9', '2', '1', '1', '7', '8', '9', '9', '7', '8', '7', '7', '8', '2', '2', '8', '4', '9', '7', '10', '0', '9', '8', '3', '7', '8', '1', '1', '8', '9', '9', '2', '2', '10', '1', '9', '10', '10', '10', '8', '3', '2', '12', '9', '9', '10', '12', '9', '12', '12', '9', '1', '2', '4', '12', '12', '7', '8', '9', '7', '7', '7', '3', '9', '8', '9', '1', '12', '8', '9', '4', '1', '3', '12', '12', '12', '12', '8', '8', '2', '1', '1', '2', '1', '1', '1', '12', '12', '8', '12', '2', '2', '12', '3', '3', '12', '8', '2', '8', '8', '12', '2', '1', '10', '3', '2', '8', '7', '1', '8', '1', '3', '7', '8', '9', '8', '3', '1', '1', '7', '8', '8', '9', '8', '2', '9', '2', '2', '9', '8', '1', '8', '8', '1', '3', '3', '1', '1', '10', '1', '2', '8', '1', '1', '1', '1', '9', '1', '4', '1', '7', '7', '7', '7', '2', '2', '8', '8', '12', '1', '9', '1', '7', '3', '3', '1', '8', '8', '10', '8', '9', '2', '9', '1', '3', '8', '8', '3', '12', '2', '8', '12', '2', '9', '1', '3', '3', '3', '3', '2', '8', '7', '9', '8', '3', '3', '1', '7', '8', '3', '1', '1', '12', '8', '9', '1', '2', '3', '8', '1', '1', '3', '3', '9', '1', '1', '1', '12', '1', '7', '3', '3', '1', '8', '8', '8', '1', '2', '4', '8', '1', '10', '2', '5', '3', '3', '12', '10', '9', '9', '12', '9', '0', '2', '8', '8', '9', '9', '9', '8', '1', '3', '1', '1', '4', '8', '1', '10', '8', '7', '2', '8', '2', '8', '4', '7', '8', '1', '9', '1', '9', '8', '2', '8', '2', '7', '9', '2', '2', '9', '1', '8', '12', '1', '8', '1', '4', '1', '9', '9', '1', '10', '12', '4', '8', '1', '7', '3', '9', '2', '12', '7', '8', '8', '2', '1', '12', '9', '8', '1', '2', '2', '8', '10', '2', '1', '7', '7', '7', '12', '3', '3', '8', '3', '8', '8', '3', '9', '8', '9', '1', '1', '8', '7', '9', '3', '3', '8', '1', '0', '9', '9', '9', '1', '8', '9', '9', '10', '1', '8', '7', '8', '8', '8', '9', '9', '8', '9', '10', '4', '9', '3', '7', '12', '1', '9', '9', '8', '9', '1', '9', '3', '1', '8', '4', '12', '10', '9', '8', '7', '10', '8', '12', '12', '3', '10', '8', '12', '2', '1', '2', '3', '9', '8', '8', '7', '1', '1', '2', '2', '1', '2', '2', '7', '1', '3', '9', '9', '3', '8', '8', '8', '8', '8', '7', '8', '8', '10', '8', '1', '8', '2', '8', '2', '2', '2', '2', '2', '1', '7', '12', '10', '1', '2', '8', '1', '4', '7', '8', '1', '12', '8', '7', '9', '2', '2', '2', '8', '1', '8', '1', '1', '0', '2', '2', '6', '1', '8', '2', '1', '8', '1', '1', '2', '2', '3', '2', '2', '10', '10', '9', '1', '7', '7', '8', '8', '1', '12', '10', '12', '3', '8', '8', '8', '3', '8', '3', '10', '2', '2', '2', '1', '2', '2', '1', '0', '1', '8', '9', '1', '1', '1', '2', '2', '2', '2', '2', '1', '1', '1', '1', '12', '9', '9', '1', '10', '11', '12', '9', '1', '1', '8', '8', '1', '2', '12', '8', '3', '7', '2', '10', '9', '7', '8', '1', '2', '2', '7', '9', '12', '2', '2', '1', '8', '9', '2', '3', '2', '2', '2', '9', '9', '8', '9', '2', '3', '8', '8', '9', '7', '3', '10', '8', '8', '9', '1', '1', '2', '8', '1', '10', '8', '8', '9', '7', '1', '7', '12', '8', '1', '7', '2', '1', '3', '9', '2', '8', '8', '1', '2', '3', '9', '3', '8', '9', '7', '7', '1', '10', '2', '8', '8', '8', '1', '8', '2', '8', '8', '1', '2', '8', '10', '2', '1', '0', '8', '3', '8', '10', '10', '12', '3', '8', '9', '3', '10', '8', '8', '8', '1', '8', '8', '2', '2', '2', '2', '2', '2', '1', '1', '10', '1', '10', '8', '2', '1', '2', '2', '8', '3', '3', '2', '2', '1', '8', '1', '3', '1', '9', '2', '2', '1', '3', '2', '2', '3', '2', '2', '3', '2', '1', '1', '2', '1', '2', '7', '8', '2', '1', '12', '12', '3', '1', '9', '9', '7', '8', '7', '2', '7', '9', '2', '2', '2', '8', '9', '7', '9', '2', '8', '9', '9', '2', '9', '1', '1', '9', '8', '3', '3', '8', '2', '7', '8', '11', '2', '8', '9', '1', '8', '3', '1', '2', '1', '2', '2', '2', '2', '8', '3', '9', '2', '10', '9', '9', '2', '7', '7', '7', '1', '3', '8', '8', '1', '1', '8', '7', '1', '8', '8', '9', '3', '3', '3', '2', '4', '8', '12', '8', '1', '12', '9', '2', '8', '2', '12', '2', '1', '3', '8', '12', '12', '11', '8', '10', '9', '2', '9', '4', '10', '1', '12', '12', '9', '3', '8', '9', '12', '9', '9', '2', '2', '2', '3', '5', '1', '1', '1', '7', '7', '8', '2', '2', '8', '2', '1', '7', '3', '2', '9', '8', '9', '1', '1', '9', '8', '9', '3', '3', '2', '9', '4', '9', '8', '8', '1', '8', '10', '2', '0', '3', '8', '8', '8', '9', '12', '9', '8', '2', '2', '8', '10', '9', '8', '2', '2', '4', '1', '9', '9', '8', '8', '10', '3', '1', '1', '2', '8', '7', '8', '2', '1', '1', '8', '12', '2', '9', '4', '8', '3', '3', '3', '8', '2', '12', '12', '8', '9', '12', '8', '2', '12', '7', '9', '2', '2', '1', '3', '1', '3', '11', '9', '8', '1', '10', '2', '12', '3', '8', '4', '1', '2', '8', '8', '3', '9', '3', '8', '7', '1', '1', '3', '1', '8', '2', '9', '8', '12', '1', '1', '1', '9', '9', '1', '8', '8', '8', '2', '2', '12', '3', '1', '1', '1', '9', '9', '3', '2', '1', '1', '9', '9', '1', '9', '9', '1', '9', '3', '9', '1', '8', '1', '9', '8', '3', '8', '8', '9', '9', '8', '2', '8', '1', '2', '2', '10', '1', '2', '1', '1', '9', '1', '9', '3', '7', '7', '2', '10', '3', '3', '1', '1', '9', '1', '2', '1', '3', '2', '1', '1', '1', '1', '1', '1', '1', '1', '1', '2', '8', '8', '7', '1', '1', '8', '8', '2', '1', '12', '1', '8', '1', '9', '8', '8', '2', '1', '9', '9', '2', '2', '8', '1', '7', '7', '9', '8', '1', '6', '8', '8', '2', '2', '8', '3', '8', '3', '2', '3', '8', '1', '2', '2', '2', '8', '9', '9', '9', '12', '1', '9', '2', '1', '9', '2', '9', '9', '10', '8', '1', '1', '9', '2', '1', '7', '1', '3', '7', '1', '1', '1', '3', '8', '8', '8', '9', '9', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '1', '0', '1', '1', '1', '2', '2', '3', '1', '1', '2', '3', '3', '3', '7', '3', '1', '9', '3', '9', '1', '4', '1', '1', '9', '10', '9', '8', '2', '3', '10', '9', '3', '10', '12', '8', '9', '8', '2', '1', '9', '8', '1', '7', '8', '1', '9', '1', '2', '10', '7', '7', '8', '1', '3', '3', '9', '3', '3', '1', '1', '1', '9', '1', '3', '8', '8', '8', '1', '8', '1', '9', '7', '8', '8', '2', '1', '3', '8', '2', '1', '6', '1', '8', '8', '2', '8', '2', '2', '9', '8', '1', '1', '6', '8', '6', '10', '2', '1', '1', '8', '9', '3', '3', '3', '1', '3', '9', '6', '8', '1', '8', '3', '1', '2', '2', '12', '7', '1', '1', '1', '1', '12', '1', '8', '8', '3', '2', '1', '9', '2', '3', '3', '2', '2', '7', '2', '2', '2', '3', '1', '9', '8', '7', '1', '1', '3', '1', '9', '4', '3', '9', '2', '1', '1', '9', '8', '8', '7', '2', '1', '3', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '2', '10', '1', '1', '8', '2', '8', '1', '8', '1', '3', '1', '2', '1', '1', '3', '1', '2', '1', '1', '1', '1', '1', '1', '1', '2', '1', '8', '1', '9', '8', '8', '1', '10', '8', '8', '1', '3', '9', '8', '9', '8', '2', '1', '3', '4', '1', '12', '1', '9', '8', '2', '7', '2', '1', '2', '9', '2', '1', '3', '8', '1', '2', '3', '8', '2', '10', '10', '10', '9', '9', '9', '3', '8', '3', '1', '1', '1', '1', '1', '1', '2', '1', '9', '2', '7', '9', '8', '7', '9', '1', '1', '1', '1', '8', '2', '2', '3', '3', '2', '12', '8', '3', '1', '1', '12', '9', '8', '2', '2', '9', '1', '2', '2', '9', '1', '1', '2', '4', '8', '1', '8', '10', '2', '9', '9', '9', '1', '2', '1', '1', '2', '2', '2', '12', '1', '2', '8', '3', '9', '9', '3', '2', '9', '9', '7', '2', '8', '8', '9', '2', '9', '1', '1', '1', '1', '1', '2', '8', '2', '1', '2', '2', '8', '3', '1', '1', '9', '1', '3', '10', '7', '9', '8', '8', '2', '2', '9', '2', '2', '2', '2', '2', '2', '9', '1', '3', '6', '4', '2', '2', '9', '1', '9', '8', '1', '1', '1', '7', '3', '2', '9', '9', '1', '9', '7', '2', '8', '9', '9', '12', '10', '8', '10', '2', '3', '1', '2', '2', '12', '12', '3', '12', '1', '2', '1', '2', '1', '2', '8', '2', '3', '12', '12', '8', '2', '2', '9', '2', '9', '2', '3', '3', '1', '1', '1', '12', '3', '7', '1', '3', '1', '2', '9', '2', '7', '2', '1', '8', '3', '7', '8', '1', '1', '3', '8', '3', '2', '9', '3', '1', '1', '1', '1', '11', '1', '2', '6', '1', '2', '3', '8', '9', '9', '2', '9', '2', '1', '1', '3', '1', '12', '9', '1', '1', '9', '1', '9', '9', '8', '9', '8', '1', '2', '7', '1', '0', '9', '8', '2', '3', '2', '4', '1', '1', '1', '8', '1', '12', '1', '2', '3', '3', '9', '9', '9', '9', '9', '7', '9', '3', '9', '1', '7', '3', '3', '3', '3', '7', '8', '2', '2', '2', '8', '2', '3', '9', '9', '3', '1', '8', '9', '12', '8', '8', '3', '2', '1', '6', '1', '9', '1', '1', '1', '1', '1', '1', '2', '2', '1', '2', '2', '2', '2', '9', '2', '5', '3', '2', '2', '8', '2', '1', '8', '3', '3', '8', '3', '1', '3', '9', '2', '8', '9', '4', '4', '9', '8', '8', '7', '7', '3', '1', '8', '8', '3', '2', '2', '10', '3', '3', '2', '2', '2', '2', '12', '3', '1', '2', '7', '7', '12', '2', '1', '2', '1', '1', '2', '1', '3', '3', '3', '1', '3', '9', '4', '8', '8', '4', '2', '4', '2', '8', '9', '1', '1', '2', '2', '2', '8', '7', '2', '8', '2', '9', '1', '8', '1', '1', '2', '1', '1', '1', '2', '9', '9', '8', '1', '8', '1', '4', '8', '1', '2', '7', '9', '2', '1', '8', '9', '8', '9', '1', '1', '2', '3', '2', '12', '7', '1', '2', '4', '4', '4', '2', '2', '2', '9', '8', '8', '1', '12', '3', '3', '2', '2', '3', '12', '1', '5', '1', '1', '3', '2', '8', '8', '2', '2', '2', '3', '9', '8', '8', '9', '8', '8', '11', '8', '2', '9', '2', '3', '1', '7', '1', '8', '2', '8', '1', '1', '1', '8', '1', '1', '1', '7', '8', '8', '2', '3', '2', '1', '8', '8', '1', '8', '4', '8', '9', '8', '9', '8', '1', '2', '3', '2', '1', '2', '2', '3', '1', '9', '4', '8', '1', '1', '9', '1', '1', '8', '2', '2', '2', '1', '7', '3', '3', '1', '3', '3', '12', '3', '1', '1', '3', '1', '1', '4', '8', '4', '4', '3', '3', '3', '4', '8', '1', '9', '9', '9', '4', '1', '7', '4', '8', '4', '9', '8', '1', '1', '3', '8', '9', '1', '9', '7', '1', '1', '9', '1', '8', '8', '8', '1', '1', '8', '2', '2', '2', '9', '4', '4', '1', '2', '1', '2', '9', '1', '1', '8', '5', '5', '5', '9', '12', '4', '1', '2', '1', '8', '8', '1', '9', '9', '2', '12', '1', '1', '7', '8', '8', '4', '1', '2', '8', '11', '2', '3', '2', '2', '1', '11', '2', '2', '2', '2', '9', '9', '2', '2', '9', '3', '1', '9', '1', '1', '1', '7', '8', '1', '8', '9', '1', '2', '9', '8', '1', '2', '9', '8', '12', '2', '2', '10', '8', '2', '2', '8', '8', '1', '2', '6', '1', '1', '7', '7', '2', '3', '2', '1', '1', '8', '1', '10', '8', '12', '6', '4', '1', '4', '1', '2', '9', '8', '9', '4', '8', '8', '10', '8', '8', '8', '8', '8', '2', '2', '8', '4', '9', '3', '3', '3', '3', '3', '9', '2', '1', '1', '2', '3', '2', '1', '2', '3', '3', '3', '2', '2', '3', '3', '2', '1', '2', '3', '3', '3', '3', '12', '9', '3', '2', '2', '3', '1', '5', '1', '1', '3', '2', '2', '1', '2', '9', '3', '9', '8', '1', '8', '7', '7', '9', '1', '1', '7', '9', '10', '1', '7', '1', '4', '3', '8', '9', '4', '9', '9', '2', '9', '2', '8', '4', '2', '2', '9', '5', '2', '1', '10', '9', '3', '1', '1', '1', '2', '1', '2', '9', '1', '7', '9', '9', '8', '2', '2', '8', '9', '1', '9', '8', '2', '3', '2', '2', '1', '2', '2', '2', '1', '1', '8', '1', '9', '4', '12', '8', '2', '7', '2', '2', '1', '9', '3', '1', '8', '12', '1', '1', '2', '1', '10', '2', '6', '12', '12', '4', '9', '1', '9', '10', '1', '2', '8', '2', '1', '8', '6', '12', '9', '9', '2', '1', '7', '8', '9', '5', '8', '1', '8', '8', '3', '1', '8', '2', '8', '2', '2', '1', '2', '9', '2', '8', '1', '12', '8', '2', '7', '3', '3', '12', '2', '3', '3', '9', '9', '3', '3', '3', '3', '3', '2', '1', '4', '1', '4', '8', '8', '2', '1', '2', '9', '12', '1', '10', '12', '1', '2', '9', '4', '8', '10', '3', '2', '8', '7', '4', '8', '9', '2', '7', '1', '10', '2', '2', '2', '8', '4', '2', '2', '1', '1', '8', '7', '9', '2', '1', '9', '8', '9', '12', '4', '8', '2', '8', '7', '7', '1', '9', '2', '1', '1', '9', '1', '8', '2', '3', '9', '9', '1', '3', '2', '9', '2', '2', '8', '1', '1', '2', '1', '2', '1', '9', '8', '2', '2', '4', '8', '8', '9', '5', '5', '9', '8', '9', '6', '2', '1', '12', '2', '3', '8', '8', '9', '2', '2', '1', '3', '2', '10', '9', '10', '7', '2', '8', '4', '2', '8', '9', '8', '1', '3', '8', '8', '1', '8', '3', '8', '8', '1', '10', '3', '1', '5', '8', '8', '9', '2', '2', '9', '2', '1', '9', '1', '4', '1', '12', '8', '8', '2', '1', '1', '1', '1', '1', '9', '2', '9', '8', '9', '5', '9', '1', '8', '2', '4', '1', '1', '1', '9', '12', '4', '1', '1', '9', '8', '4', '1', '8', '9', '8', '8', '9', '9', '9', '9', '9', '9', '1', '3', '4', '2', '2', '8', '2', '9', '2', '3', '8', '3', '1', '7', '2', '1', '1', '4', '2', '9', '2', '1', '3', '2', '9', '2', '9', '3', '8', '1', '9', '1', '1', '2', '8', '8', '8', '2', '3', '9', '9', '8', '2', '4', '1', '5', '1', '1', '9', '9', '2', '1', '1', '1', '9', '9', '1', '3', '9', '2', '2', '9', '9', '8', '3', '9', '9', '8', '2', '2', '8', '9', '2', '2', '8', '9', '10', '10', '2', '8', '7', '9', '9', '4', '2', '4', '11', '2', '1', '7', '8', '8', '8', '9', '4', '9', '8', '8', '2', '1', '1', '9', '8', '3', '3', '8', '8', '7', '9', '1', '3', '4', '8', '10', '2', '4', '4', '8', '2', '8', '2', '8', '9', '2', '1', '2', '2', '3', '1', '9', '4', '5', '9', '9', '1', '1', '1', '1', '1', '1', '7', '1', '1', '1', '8', '1', '1', '1', '1', '1', '9', '1', '2', '1', '12', '9', '3', '9', '9', '4', '1', '9', '1', '1', '2', '11', '9', '3', '2', '2', '2', '4', '2', '3', '2', '7', '7', '8', '1', '10', '9', '5', '7', '1', '10', '8', '1', '2', '7', '1', '12', '5', '8', '4', '2', '9', '9', '8', '2', '9', '8', '8', '10', '2', '3', '2', '7', '2', '8', '10', '3', '2', '9', '9', '9', '1', '1', '1', '1', '8', '2', '10', '9', '8', '1', '2', '2', '2', '1', '2', '1', '3', '9', '12', '2', '2', '4', '8', '1', '3', '9', '4', '9', '1', '9', '9', '2', '1', '1', '9', '7', '10', '3', '10', '2', '9', '2', '9', '2', '8', '2', '1', '2', '3', '1', '1', '1', '8', '5', '8', '3', '1', '1', '4', '2', '2', '1', '3', '8', '2', '1', '1', '1', '2', '9', '4', '1', '8', '1', '2', '8', '2', '9', '2', '2', '5', '5', '2', '5', '2', '1', '8', '8', '1', '3', '12', '2', '4', '3', '2', '2', '2', '6', '2', '3', '3', '1', '8', '2', '2', '1', '1', '8', '8', '10', '1', '9', '2', '9', '12', '0', '2', '1', '3', '2', '2', '1', '2', '1', '9', '9', '2', '9', '8', '8', '1', '7', '8', '1', '2', '6', '9', '8', '10', '1', '12', '2', '2', '1', '8', '2', '1', '12', '7', '4', '2', '10', '2', '2', '1', '2', '1', '2', '1', '1', '2', '1', '9', '8', '10', '2', '10', '9', '9', '12', '5', '3', '2', '8', '2', '8', '3', '3', '1', '4', '9', '10', '2', '4', '8', '1', '2', '10', '12', '12', '3', '1', '1', '2', '9', '6', '6', '1', '9', '1', '8', '1', '8', '8', '8', '8', '2', '2', '2', '8', '8', '2', '1', '1', '1', '1', '1', '8', '8', '5', '8', '8', '1', '8', '9', '9', '1', '7', '12', '9', '9', '7', '2', '2', '7', '8', '9', '8', '2', '9', '6', '3', '4', '8', '4', '1', '2', '9', '8', '8', '8', '2', '1', '1', '8', '8', '3', '1', '9', '9', '7', '1', '2', '2', '2', '9', '4', '1', '2', '12', '1', '8', '2', '1', '9', '1', '8', '9', '9', '2', '5', '4', '2', '1', '2', '8', '12', '8', '10', '3', '8', '8', '8', '2', '2', '8', '2', '8', '2', '4', '8', '10', '2', '7', '2', '1', '8', '8', '9', '5', '8', '1', '12', '7', '10', '2', '2', '2', '1', '4', '8', '12', '1', '8', '2', '2', '9', '1', '8', '3', '1', '1', '5', '10', '2', '1', '2', '4', '8', '9', '3', '2', '8', '7', '4', '4', '9', '9', '2', '8', '1', '1', '2', '4', '7', '1', '9', '1', '1', '1', '8', '4', '1', '3', '1', '2', '2', '9', '1', '8', '8', '2', '2', '9', '9', '1', '8', '8', '1', '9', '8', '8', '2', '2', '2', '2', '1', '1', '1', '7', '4', '1', '1', '4', '2', '3', '3', '2', '3', '9', '2', '8', '2', '1', '5', '1', '2', '2', '1', '3', '5', '2', '9', '1', '1', '9', '8', '1', '4', '8', '1', '8', '2', '4', '8', '4', '7', '9', '8', '1', '2', '2', '4', '4', '11', '3', '11', '1', '3', '8', '1', '9', '6', '8', '9', '3', '9', '8', '9', '2', '3', '3', '7', '1', '7', '2', '9', '9', '1', '8', '8', '1', '5', '5', '8', '8', '1', '2', '1', '3', '9', '9', '5', '9', '8', '1', '4', '3', '8', '1', '1', '2', '1', '8', '9', '8', '2', '2', '2', '1', '2', '4', '10', '8', '1', '9', '1', '8', '1', '1', '9', '9', '8', '11', '1', '8', '2', '1', '2', '8', '2', '8', '8', '2', '9', '6', '10', '1', '8', '6', '4', '5', '11', '2', '2', '8', '1', '8', '1', '8', '1', '4', '8', '1', '1', '1', '8', '3', '7', '3', '3', '7', '1', '1', '4', '8', '1', '6', '4', '2', '2', '1', '1', '1', '1', '6', '2', '2', '8', '1', '8', '5', '5', '2', '1', '1', '2', '3', '7', '1', '1', '6', '2', '9', '3', '9', '8', '9', '2', '1', '4', '2', '9', '13', '9', '2', '8', '12', '4', '1', '1', '9', '5', '1', '1', '1', '1', '8', '2', '1', '1', '2', '9', '0', '12', '1', '1', '8', '9', '8', '3', '2', '2', '12', '8', '2', '2', '2', '1', '10', '1', '9', '8', '9', '5', '2', '2', '9', '2', '8', '4', '8', '3', '7', '9', '2', '2', '7', '2', '2', '8', '1', '1', '9', '10', '9', '8', '2', '9', '0', '7', '1', '1', '1', '9', '8', '2', '8', '12', '8', '3', '2', '8', '8', '7', '7', '8', '12', '2', '1', '3', '9', '2', '2', '8', '2', '8', '8', '12', '8', '8', '1', '8', '9', '2', '4', '4', '7', '2', '1', '1', '9', '3', '7', '1', '3', '9', '8', '2', '2', '3', '1', '3', '10', '2', '3', '7', '1', '8', '1', '1', '10', '3', '10', '8', '4', '1', '2', '4', '3', '10', '9', '3', '9', '5', '8', '7', '2', '9', '3', '3', '2', '11', '8', '1', '1', '7', '2', '7', '2', '9', '4', '8', '1', '12', '8', '7', '9', '1', '0', '2', '6', '4', '9', '3', '2', '4', '12', '9', '8', '5', '9', '2', '8', '4', '6', '8', '9', '3', '1', '8', '7', '3', '1', '1', '1', '8', '9', '1', '8', '7', '9', '8', '2', '1', '2', '2', '3', '2', '2', '1', '1', '4', '9', '7', '3', '2', '8', '8', '9', '1', '9', '7', '4', '2', '2', '8', '5', '9', '1', '5', '7', '1', '4', '6', '9', '10', '1', '8', '2', '9', '1', '9', '2', '9', '8', '2', '9', '9', '11', '2', '12', '2', '2', '9', '4', '8', '1', '9', '9', '2', '8', '9', '1', '2', '9', '3', '8', '8', '1', '7', '8', '8', '2', '3', '2', '2', '2', '2', '9', '8', '10', '2', '2', '1', '9', '11', '1', '8', '8', '2', '4', '2', '1', '2', '8', '8', '9', '2', '8', '9', '2', '3', '1', '2', '1', '3', '10', '8', '1', '2', '4', '1', '9', '8', '9', '3', '3', '9', '8', '2', '11', '3', '7', '1', '8', '7', '1', '8', '8', '8', '1', '4', '8', '8', '1', '9', '3', '1', '1', '2', '8', '8', '8', '2', '1', '8', '10', '2', '12', '8', '8', '9', '1', '8', '1', '3', '2', '2', '10', '7', '2', '2', '1', '1', '8', '2', '9', '1', '2', '9', '3', '3', '10', '1', '4', '7', '10', '12', '2', '4', '10', '8', '6', '2', '9', '1', '8', '3', '1', '2', '8', '8', '2', '12', '10', '2', '10', '9', '1', '2', '5', '8', '12', '8', '8', '7', '8', '8', '1', '8', '2', '2', '9', '1', '8', '10', '5', '5', '5', '8', '1', '9', '1', '13', '1', '2', '2', '1', '2', '4', '9', '2', '1', '9', '13', '11', '1', '9', '9', '3', '10', '8', '7', '8', '8', '4', '2', '1', '9', '4', '3', '1', '1', '10', '9', '2', '8', '6', '2', '1', '1', '3', '2', '8', '1', '1', '1', '1', '2', '4', '1', '1', '10', '1', '8', '1', '1', '9', '1', '7', '1', '1', '9', '9', '2', '12', '9', '1', '8', '1', '12', '1', '8', '3', '8', '1', '8', '1', '2', '10', '1', '1', '2', '2', '9', '3', '9', '12', '1', '9', '1', '2', '8', '7', '2', '1', '1', '3', '2', '8', '4', '4', '5', '12', '7', '8', '9', '6', '1', '8', '9', '2', '1', '9', '1', '4', '1', '8', '2', '1', '1', '8', '7', '4', '9', '1', '8', '4', '2', '7', '5', '1', '3', '9', '2', '8', '2', '1', '2', '9', '1', '1', '2', '3', '4', '3', '1', '2', '2', '10', '2', '2', '8', '1', '1', '2', '4', '9', '2', '11', '9', '8', '9', '2', '1', '10', '4', '1', '1', '10', '7', '2', '10', '8', '1', '10', '8', '8', '8', '8', '8', '8', '2', '2', '2', '8', '3', '4', '1', '1', '1', '2', '1', '8', '8', '2', '8', '3', '8', '3', '1', '2', '3', '1', '1', '10', '1', '4', '4', '3', '3', '1', '9', '1', '1', '2', '1', '1', '6', '1', '1', '1', '9', '1', '4', '1', '1', '1', '1', '1', '1', '2', '12', '8', '8', '1', '9', '12', '10', '8', '2', '12', '1', '1', '2', '1', '7', '10', '7', '1', '2', '1', '10', '8', '2', '2', '8', '2', '2', '1', '9', '8', '8', '8', '8', '3', '1', '4', '2', '8', '3', '10', '10', '1', '9', '1', '9', '1', '9', '1', '8', '3', '1', '1', '8', '8', '8', '6', '3', '4', '1', '1', '1', '9', '9', '5', '2', '10', '2', '6', '2', '2', '7', '1', '8', '2', '11', '1', '1', '1', '2', '1', '9', '4', '4', '2', '8', '4', '6', '8', '8', '10', '10', '2', '1', '2', '9', '1', '8', '8', '3', '4', '8', '8', '2', '8', '8', '1', '2', '1', '2', '9', '4', '1', '3', '3', '3', '8', '8', '8', '2', '6', '4', '3', '3', '12', '3', '4', '9', '6', '8', '7', '7', '8', '6', '4', '1', '10', '6', '3', '3', '8', '1', '1', '8', '1', '11', '8', '1', '1', '2', '3', '11', '7', '2', '1', '1', '1', '9', '1', '2', '1', '1', '9', '8', '1', '1', '8', '0', '4', '1', '1', '2', '1', '1', '2', '2', '9', '1', '10', '1', '3', '8', '9', '3', '3', '1', '10', '1', '9', '1', '1', '7', '4', '1', '8', '10', '1', '2', '10', '1', '1', '2', '3', '3', '9', '1', '8', '10', '1', '7', '9', '1', '9', '6', '3', '9', '2', '8', '4', '12', '12', '3', '1', '1', '9', '1', '9', '2', '1', '1', '1', '1', '1', '9', '2', '1', '2', '10', '9', '8', '8', '10', '8', '2', '3', '2', '1', '12', '8', '12', '2', '9', '1', '3', '2', '5', '2', '2', '8', '2', '2', '4', '10', '9', '10', '9', '4', '2', '4', '12', '8', '2', '1', '8', '3', '9', '2', '9', '4', '1', '1', '1', '1', '1', '9', '6', '2', '9', '2', '1', '3', '2', '2', '5', '8', '9', '1', '6', '3', '1', '2', '2', '2', '6', '1', '8', '10', '8', '3', '3', '13', '2', '9', '9', '10', '6', '12', '8', '2', '9', '2', '3', '8', '8', '1', '2', '2', '3', '2', '10', '1', '8', '9', '1', '8', '9', '8', '8', '2', '10', '2', '1', '1', '1', '9', '9', '12', '1', '1', '4', '8', '3', '2', '2', '4', '4', '2', '2', '1', '1', '1', '9', '2', '2', '4', '1', '1', '8', '8', '8', '9', '2', '13', '9', '9', '4', '1', '9', '7', '10', '10', '8', '1', '9', '1', '1', '1', '8', '12', '4', '1', '2', '1', '1', '1', '7', '8', '3', '3', '1', '2', '2', '7', '2', '2', '10', '1', '8', '4', '0', '1', '9', '4', '1', '1', '2', '10', '10', '7', '9', '3', '12', '11', '2', '8', '10', '8', '9', '4', '3', '4', '10', '1', '1', '9', '3', '10', '1', '5', '3', '3', '2', '2', '1', '1', '6', '1', '7', '1', '1', '12', '1', '1', '1', '10', '1', '8', '8', '1', '3', '8', '3', '1', '2', '8', '3', '4', '2', '8', '8', '6', '1', '1', '4', '1', '2', '9', '5', '1', '7', '8', '9', '2', '4', '9', '8', '5', '9', '9', '9', '1', '2', '1', '3', '8', '9', '2', '9', '8', '9', '1', '8', '13', '9', '3', '8', '1', '4', '1', '3', '1', '6', '4', '10', '2', '2', '7', '8', '8', '12', '8', '9', '9', '9', '10', '3', '12', '12', '2', '9', '10', '9', '7', '2', '3', '4', '10', '10', '2', '7', '7', '1', '7', '3', '9', '1', '9', '8', '2', '9', '1', '8', '1', '10', '8', '8', '2', '7', '8', '1', '8', '9', '8', '1', '4', '1', '8', '8', '8', '1', '1', '10', '3', '2', '2', '6', '8', '9', '8', '2', '1', '1', '3', '8', '8', '1', '4', '4', '3', '9', '10', '4', '2', '4', '2', '9', '2', '2', '9', '1', '1', '10', '1', '1', '2', '1', '1', '1', '9', '1', '8', '8', '8', '2', '3', '8', '1', '6', '7', '3', '1', '9', '1', '2', '1', '6', '1', '2', '10', '1', '4', '2', '3', '2', '9', '9', '4', '9', '9', '1', '2', '8', '4', '7', '8', '7', '13', '9', '1', '2', '1', '9', '3', '3', '6', '8', '9', '10', '9', '2', '2', '3', '12', '1', '1', '1', '13', '2', '1', '7', '10', '8', '9', '1', '2', '9', '1', '1', '1', '2', '1', '12', '5', '6', '9', '10', '3', '1', '10', '1', '3', '2', '8', '9', '2', '10', '8', '8', '9', '2', '8', '9', '2', '2', '2', '8', '8', '2', '3', '1', '4', '8', '1', '9', '9', '2', '8', '10', '9', '1', '2', '12', '8', '8', '0', '2', '1', '1', '9', '10', '1', '6', '2', '2', '10', '2', '2', '4', '2', '2', '12', '1', '6', '7', '8', '9', '1', '3', '5', '7', '3', '1', '3', '1', '1', '3', '2', '6', '3', '2', '1', '8', '3', '5', '2', '1', '1', '8', '1', '1', '0', '2', '3', '5', '3', '1', '1', '9', '8', '8', '8', '8', '4', '7', '2', '9', '12', '5', '9', '3', '12', '3', '9', '1', '9', '1', '7', '3', '9', '3', '10', '1', '2', '1', '4', '9', '8', '2', '9', '11', '2', '8', '9', '4', '1', '1', '1', '1', '1', '1', '1', '9', '1', '9', '1', '1', '1', '12', '3', '10', '3', '6', '1', '7', '10', '8', '3', '7', '1', '1', '2', '6', '1', '3', '9', '2', '8', '1', '8', '8', '1', '8', '10', '12', '1', '10', '12', '10', '1', '8', '8', '1', '4', '7', '9', '2', '1', '9', '3', '8', '12', '10', '6', '1', '6', '8', '3', '1', '3', '1', '10', '10', '9', '8', '1', '2', '9', '8', '8', '1', '9', '8', '8', '3', '1', '3', '9', '12', '1', '8', '1', '5', '8', '11', '5', '5', '1', '2', '1', '3', '1', '1', '1', '9', '8', '8', '1', '9', '10', '8', '4', '1', '1', '8', '10', '8', '9', '1', '10', '1', '7', '2', '1', '1', '9', '6', '2', '9', '8', '1', '1', '1', '9', '8', '8', '1', '9', '9', '1', '1', '2', '7', '9', '2', '9', '9', '8', '7', '6', '8', '7', '8', '2', '9', '2', '1', '8', '8', '8', '2', '6', '3', '1', '8', '12', '12', '8', '7', '1', '2', '2', '7', '1', '9', '8', '1', '4', '1', '1', '8', '5', '8', '1', '8', '8', '8', '8', '1', '2', '11', '1', '1', '1', '2', '7', '1', '9', '2', '2', '2', '9', '8', '9', '2', '9', '1', '8', '1', '7', '11', '13', '1', '9', '8', '2', '2', '1', '2', '3', '10', '3', '10', '4', '3', '9', '1', '8', '1', '1', '13', '1', '1', '3', '8', '1', '2', '2', '2', '9', '4', '1', '8', '1', '9', '3', '6', '8', '5', '2', '8', '2', '8', '1', '9', '2', '2', '8', '1', '1', '4', '8', '9', '2', '2', '7', '1', '8', '12', '2', '12', '8', '2', '9', '9', '3', '8', '1', '1', '9', '9', '9', '2', '8', '1', '9', '1', '8', '9', '8', '2', '2', '4', '1', '9', '10', '10', '8', '8', '1', '1', '2', '1', '4', '1', '2', '1', '1', '3', '12', '8', '8', '8', '8', '10', '2', '8', '12', '1', '9', '9', '8', '9', '1', '2', '1', '8', '8', '8', '2', '8', '8', '3', '10', '8', '9', '1', '1', '1', '3', '4', '8', '10', '6', '3', '1', '3', '2', '8', '9', '3', '5', '4', '1', '5', '2', '9', '9', '9', '1', '1', '1', '9', '9', '6', '10', '9', '8', '12', '2', '9', '1', '2', '1', '1', '1', '8', '1', '12', '1', '12', '1', '9', '1', '8', '1', '2', '1', '2', '8', '8', '10', '1', '9', '1', '2', '2', '8', '8', '3', '12', '1', '11', '2', '10', '12', '11', '2', '1', '10', '1', '1', '9', '3', '10', '2', '7', '1', '8', '1', '9', '9', '2', '1', '3', '12', '5', '9', '9', '8', '1', '8', '8', '8', '1', '1', '3', '9', '8', '9', '8', '3', '10', '3', '7', '8', '9', '2', '3', '1', '9', '10', '9', '2', '8', '2', '6', '4', '1', '1', '3', '8', '2', '3', '9', '1', '2', '1', '8', '1', '2', '2', '2', '2', '9', '9', '4', '8', '9', '1', '1', '2', '9', '1', '5', '2', '9', '8', '10', '1', '2', '10', '8', '5', '6', '4', '8', '1', '1', '1', '8', '9', '2', '8', '8', '8', '12', '2', '2', '1', '8', '1', '2', '1', '9', '8', '8', '9', '1', '1', '8', '7', '1', '8', '0', '2', '3', '8', '1', '1', '8', '1', '1', '12', '3', '2', '10', '8', '10', '6', '1', '8', '8', '10', '8', '8', '1', '4', '8', '1', '2', '9', '8', '1', '1', '3', '3', '5', '1', '1', '2', '1', '2', '1', '9', '8', '3', '9', '8', '2', '10', '9', '1', '8', '1', '10', '9', '10', '1', '2', '3', '8', '1', '3', '1', '9', '9', '8', '1', '9', '8', '8', '9', '1', '9', '2', '8', '8', '2', '13', '8', '10', '8', '9', '3', '2', '1', '3', '1', '12', '10', '10', '1', '11', '1', '2', '4', '2', '10', '1', '8', '11', '1', '1', '1', '2', '2', '8', '2', '9', '8', '9', '9', '12', '8', '1', '9', '2', '4', '1', '8', '3', '9', '1', '6', '5', '8', '9', '3', '3', '2', '1', '1', '1', '8', '11', '1', '8', '7', '1', '11', '9', '1', '7', '9', '9', '8', '8', '8', '12', '8', '1', '2', '9', '8', '8', '2', '10', '9', '8', '8', '4', '10', '9', '8', '2', '10', '10', '2', '1', '2', '1', '2', '1', '4', '1', '7', '1', '1', '3', '12', '9', '8', '5', '9', '10', '1', '9', '9', '2', '7', '4', '1', '1', '8', '12', '8', '2', '2', '9', '4', '1', '1', '5', '9', '1', '8', '12', '8', '1', '1', '4', '9', '10', '2', '3', '1', '3', '3', '3', '8', '1', '2', '1', '2', '1', '2', '9', '1', '2', '8', '9', '4', '8', '8', '10', '2', '12', '5', '5', '1', '8', '1', '7', '8', '1', '1', '8', '9', '12', '4', '9', '3', '1', '2', '5', '9', '2', '1', '2', '2', '1', '2', '8', '1', '8', '8', '2', '2', '1', '8', '4', '1', '2', '10', '9', '10', '8', '1', '10', '4', '8', '8', '10', '2', '8', '1', '1', '3', '10', '1', '4', '2', '8', '3', '3', '2', '9', '8', '5', '5', '9', '3', '10', '8', '1', '2', '1', '2', '9', '1', '1', '2', '9', '9', '8', '8', '11', '9', '4', '1', '8', '2', '7', '1', '2', '8', '2', '9', '9', '8', '2', '9', '9', '8', '8', '1', '2', '8', '1', '1', '2', '9', '8', '1', '4', '9', '1', '8', '12', '1', '12', '2', '8', '2', '9', '1', '12', '1', '2', '9', '1', '1', '9', '3', '2', '9', '9', '2', '11', '8', '7', '9', '9', '9', '8', '8', '1', '2', '8', '1', '1', '4', '1', '2', '10', '1', '9', '2', '1', '1', '1', '9', '1', '13', '4', '3', '2', '1', '1', '2', '2', '7', '8', '7', '1', '1', '2', '8', '8', '3', '4', '8', '2', '10', '13', '8', '8', '9', '2', '2', '1', '1', '10', '4', '2', '7', '9', '8', '2', '1', '9', '8', '1', '1', '9', '8', '9', '8', '2', '8', '2', '2', '9', '2', '1', '9', '1', '2', '4', '8', '2', '1', '1', '8', '1', '4', '1', '9', '3', '9', '2', '7', '1', '9', '6', '1', '10', '2', '2', '2', '2', '2', '10', '10', '10', '9', '9', '9', '3', '12', '10', '1', '1', '10', '1', '1', '2', '2', '1', '2', '3', '12', '8', '8', '1', '2', '9', '9', '1', '1', '10', '8', '9', '8', '3', '1', '1', '3', '1', '10', '1', '1', '1', '4', '1', '1', '1', '7', '10', '2', '1', '8', '1', '9', '3', '1', '10', '8', '8', '1', '5', '9', '1', '2', '1', '8', '8', '1', '3', '1', '10', '2', '1', '1', '2', '8', '2', '5', '3', '5', '2', '1', '1', '9', '1', '7', '9', '9', '2', '8', '1', '12', '4', '1', '4', '9', '9', '4', '8', '1', '10', '10', '8', '8', '8', '9', '2', '3', '5', '1', '8', '1', '1', '1', '7', '10', '1', '4', '12', '3', '2', '2', '9', '1', '1', '1', '2', '8', '4', '1', '7', '3', '12', '8', '6', '2', '2', '7', '9', '11', '1', '10', '9', '1', '8', '12', '2', '11', '1', '2', '3', '1', '1', '1', '8', '9', '2', '2', '2', '2', '3', '3', '8', '10', '4', '1', '2', '9', '8', '2', '2', '9', '1', '8', '11', '4', '2', '8', '7', '1', '1', '7', '3', '9', '9', '4', '5', '8', '10', '2', '9', '9', '9', '8', '1', '9', '2', '2', '2', '1', '2', '9', '4', '3', '12', '4', '3', '2', '3', '8', '3', '10', '1', '10', '1', '1', '8', '10', '6', '8', '9', '9', '1', '2', '9', '2', '9', '3', '8', '1', '1', '12', '1', '5', '1', '10', '8', '10', '2', '7', '1', '1', '1', '1', '3', '2', '3', '5', '9', '1', '2', '1', '9', '8', '10', '9', '8', '1', '8', '1', '8', '2', '8', '1', '2', '8', '1', '2', '5', '8', '12', '2', '1', '1', '1', '1', '1', '8', '8', '2', '1', '4', '2', '8', '8', '8', '2', '9', '9', '11', '1', '2', '9', '3', '1', '1', '8', '9', '2', '4', '10', '1', '9', '8', '4', '9', '8', '10', '8', '8', '9', '2', '8', '9', '3', '3', '3', '4', '3', '2', '2', '10', '8', '2', '1', '5', '1', '9', '1', '1', '2', '1', '2', '3', '1', '1', '8', '8', '1', '10', '8', '8', '1', '6', '4', '8', '2', '5', '8', '1', '3', '8', '2', '8', '9', '1', '1', '8', '12', '10', '1', '5', '8', '2', '8', '10', '9', '9', '1', '6', '9', '1', '10', '8', '2', '9', '10', '1', '2', '1', '1', '8', '8', '3', '9', '8', '1', '10', '2', '8', '1', '10', '1', '5', '1', '1', '1', '5', '1', '1', '1', '4', '1', '1', '1', '8', '2', '8', '1', '8', '3', '8', '9', '1', '8', '1', '8', '1', '1', '2', '2', '1', '12', '9', '9', '1', '8', '2', '1', '2', '8', '1', '9', '1', '1', '2', '1', '2', '2', '2', '1', '9', '9', '8', '8', '1', '1', '9', '10', '8', '4', '3', '3', '4', '9', '1', '3', '3', '10', '2', '2', '1', '8', '4', '1', '9', '4', '1', '1', '9', '8', '10', '4', '9', '9', '8', '2', '1', '3', '4', '9', '3', '8', '9', '1', '1', '2', '1', '7', '8', '2', '9', '1', '9', '2', '6', '9', '10', '8', '1', '1', '5', '9', '5', '1', '9', '10', '9', '3', '3', '8', '8', '2', '9', '2', '8', '2', '1', '8', '5', '8', '3', '10', '1', '1', '2', '1', '8', '1', '4', '3', '2', '9', '4', '9', '8', '8', '8', '1', '3', '9', '1', '9', '9', '1', '2', '1', '9', '8', '9', '8', '1', '1', '1', '4', '2', '2', '9', '9', '1', '1', '1', '3', '6', '1', '2', '3', '1', '4', '2', '9', '1', '9', '9', '1', '8', '2', '1', '9', '1', '8', '1', '8', '8', '2', '1', '9', '9', '6', '8', '8', '9', '8', '10', '8', '9', '5', '1', '1', '1', '1', '8', '1', '8', '12', '8', '1', '12', '9', '2', '8', '1', '1', '2', '1', '8', '6', '9', '9', '8', '1', '8', '9', '7', '7', '8', '9', '9', '1', '8', '3', '8', '1', '3', '3', '8', '9', '8', '2', '8', '1', '2', '9', '8', '1', '1', '1', '1', '9', '1', '9', '8', '12', '3', '8', '8', '1', '10', '10', '10', '2', '2', '1', '10', '2', '2', '10', '8', '12', '8', '1', '1', '1', '2', '1', '1', '3', '8', '1', '2', '2', '2', '1', '1', '6', '2', '8', '8', '8', '1', '1', '1', '9', '9', '2', '8', '10', '2', '8', '2', '1', '2', '9', '9', '2', '1', '1', '8', '8', '1', '3', '1', '8', '9', '1', '10', '9', '1', '1', '1', '1', '7', '2', '2', '1', '2', '8', '8', '1', '2', '1', '3', '2', '1', '8', '3', '9', '2', '10', '2', '10', '1', '2', '1', '1', '13', '9', '10', '8', '2', '2', '1', '1', '8', '2', '2', '1', '9', '1', '1', '8', '9', '9', '2', '9', '2', '2', '1', '1', '1', '8', '9', '4', '9', '9', '1', '8', '2', '1', '1', '8', '2', '2', '2', '8', '1', '1', '8', '2', '10', '2', '9', '9', '7', '1', '1', '2', '2', '8', '3', '1', '2', '1', '1', '8', '1', '1', '1', '9', '1', '1', '8', '8', '1', '9', '6', '1', '9', '1', '8', '1', '8', '8', '8', '1', '3', '8', '8', '2', '6', '3', '2', '1', '10', '1', '8', '2', '1', '6', '1', '1', '1', '8', '1', '10', '1', '1', '8', '9', '2', '6', '9', '2', '9', '4', '1', '8', '1', '3', '7', '8', '8', '5', '8', '7', '1', '1', '1', '13', '8', '3', '1', '1', '1', '1', '2', '8', '2', '12', '8', '1', '2', '5', '1', '8', '2', '10', '8', '2', '10', '1', '5', '2', '1', '1', '5', '2', '2', '7', '8', '2', '9', '9', '1', '10', '1', '8', '2', '5', '8', '1', '5', '1', '10', '1', '1', '9', '8', '6', '1', '8', '9', '8', '1', '8', '2', '2', '8', '3', '9', '1', '9', '3', '8', '4', '4', '3', '3', '1', '1', '1', '8', '9', '1', '1', '8', '3', '1', '2', '8', '2', '1', '1', '10', '1', '2', '2', '1', '1', '8', '2', '8', '2', '10', '2', '2', '10', '8', '8', '8', '13', '2', '2', '9', '2', '5', '9', '2', '1', '8', '8', '8', '1', '12', '12', '2', '9', '2', '1', '8', '2', '1', '8', '1', '9', '9', '2', '7', '1', '1', '7', '1', '4', '3', '1', '10', '3', '10', '9', '4', '6', '7', '5', '2', '1', '8', '8', '1', '4', '9', '2', '1', '8', '1', '1', '1', '1', '1', '8', '9', '8', '8', '8', '8', '9', '8', '10', '1', '8', '2', '7', '8', '9', '1', '2', '5', '9', '10', '8', '9', '12', '1', '6', '1', '2', '10', '9', '4', '1', '8', '10', '10', '1', '1', '1', '1', '1', '2', '3', '8', '1', '2', '10', '2', '1', '2', '4', '2', '2', '9', '1', '4', '2', '1', '1', '12', '12', '9', '9', '7', '1', '8', '4', '9', '9', '8', '1', '7', '3', '2', '1', '1', '4', '8', '1', '5', '8', '2', '8', '1', '14', '9', '12', '1', '10', '2', '1', '1', '8', '8', '6', '6', '3', '1', '2', '8', '1', '3', '2', '1', '8', '8', '1', '10', '8', '2', '8', '2', '5', '9', '1', '8', '8', '1', '1', '8', '8', '13', '1', '3', '8', '4', '1', '9', '7', '8', '8', '8', '3', '8', '1', '4', '2', '8', '8', '8', '7', '1', '1', '2', '8', '8', '9', '8', '8', '2', '3', '9', '9', '2', '1', '2', '2', '1', '8', '2', '6', '2', '1', '1', '9', '10', '3', '8', '2', '3', '2', '8', '8', '1', '8', '8', '8', '9', '1', '4', '1', '9', '2', '1', '8', '0', '1', '8', '1', '1', '8', '9', '9', '1', '3', '1', '2', '8', '8', '7', '1', '1', '2', '9', '2', '10', '2', '8', '2', '2', '2', '7', '1', '9', '8', '1', '3', '9', '2', '1', '10', '8', '1', '4', '1', '8', '5', '9', '8', '8', '1', '2', '2', '1', '8', '8', '6', '8', '1', '1', '8', '3', '2', '2', '1', '8', '1', '7', '6', '8', '1', '1', '1', '9', '1', '1', '1', '2', '1', '11', '10', '10', '11', '9']\n",
            "     filenum                                               text  label\n",
            "0          1  novelty.' General Electric Co. v. Wabash Appli...      8\n",
            "1          2  Court properly to shift to Congress the respon...      1\n",
            "2          3  would sell' at the carrier's price. In the Val...      8\n",
            "3          4  not to coerce the surrender of lands without c...      2\n",
            "4          6  for the privilege of doing it, which place an ...      8\n",
            "...      ...                                                ...    ...\n",
            "6000    8411  too here, \"[w]e see no reason to reject Califo...      1\n",
            "6001    8412  the courts of the State.\" U. S. C. If the stat...      1\n",
            "6002    8413  abandoned. A mere recital in briefs of the exi...      2\n",
            "6003    8414  sentence less than death.\" ' \" U. S., at (quot...      1\n",
            "6004    8417  \"As we pointed out in United States v. Califor...     10\n",
            "\n",
            "[6005 rows x 3 columns]\n"
          ]
        }
      ],
      "source": [
        "fh = open('sc_model_3.txt')\n",
        "filenums=[]\n",
        "records=[]\n",
        "for record in fh:\n",
        "    record=record.split(\"---\")\n",
        "    filenum=record[0]\n",
        "    modelnum=record[1]\n",
        "    text=record[2]\n",
        "    filenums.append(filenum)\n",
        "    records.append(text)\n",
        "    \n",
        "print(len(filenums))\n",
        "print(len(records))\n",
        "\n",
        "summarized_data = pd.DataFrame(list(zip(filenums, records)),columns =['filenum','text'])\n",
        "len_list = [len(ele.split()) for ele in records]\n",
        "res = 0 if len(len_list) == 0 else (float(sum(len_list)) / len(len_list))\n",
        "print(\"Average Length %s\" % res)\n",
        "\n",
        "temp_file = open(\"labels_sc.txt\", \"r\")\n",
        "#temp_file = open(\"labels_sc_279.txt\", \"r\")\n",
        "\n",
        "data = temp_file.read()\n",
        "\n",
        "label_list = data.split(\"\\n\")\n",
        "print(label_list)\n",
        "label_list = label_list[0:-1]\n",
        "print(label_list)\n",
        "label_list = [int(i) for i in label_list]\n",
        "temp_file.close()\n",
        "\n",
        "\n",
        "new_label_list=[]\n",
        "for num in filenums:\n",
        "  new_label_list.append(label_list[int(num)-1])\n",
        "summarized_data['label'] = new_label_list\n",
        "\n",
        "print(summarized_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBg7k7sMwIiY",
        "outputId": "f5fc8dea-0985-4f35-e51d-489fdfc8348c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_1[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_roberta_model[0][0]']       \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 15)           7695        ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 8  1  8 ...  2  1 10]\n",
            "[   1    2    3 ... 8413 8414 8417]\n",
            "[6622   35 6713 4989 8233 7444 5061 3942 8344 5464  317 4515 3001 6991\n",
            " 2875 1125 3885 6647 4857 4504  214 8340 2969 3847 4609 1346 5993 7358\n",
            " 2752 7651 8188 7073 4782 6069 2346 6483 1663 4952 5072 1967 4071 7590\n",
            "  860 6418 5030 7297  626 5463 6029 6302 7256 5667 1235 3993 1323 4263\n",
            " 3835 2624 5801 4293 8129 5866 5777 6059 4837 3843 1418 5409 5274 6205\n",
            " 2356 5997 6685 3444 3310 7721 5928 2360 7197 4252 8343  909 4354 4752\n",
            "  649 1916 3488 2496 5765 6612 4004 4052 1403 6697 5268 6293 6380 4602\n",
            " 6057 1851 4577 4236 7978 5083 6552  361 7706 4186 4452 4396 8351 3834\n",
            "  812 5108  463 6600 8264 8064 7121 8279 5556 5119 7947 3350 7152 5199\n",
            " 4282 1092 5025 5051  933 1927 3615 1221 8062 1489 7442 5459 2031 7351\n",
            "  234 8015 7416 7921 6574 6257 8396  941 4520 3300 5640  240 7810 7425\n",
            " 1321 6580  928 1170 5404 8273 6742   94    9 7861 7975 1018 2140 1415\n",
            " 7061 7767 4990 8124 5927 6051 3269 6916 5959 2846 6818 7052 7775 7582\n",
            " 8116 4240 4767 5248 1812 2175 6055 8331  998 1564 7326 1559  292 3065\n",
            " 6764 6854 4915 5552 3583 5902 4229 2144 7565 3712 7224 4502 1406 1651\n",
            " 6256 3827 6525 6318 2686 3616 5369 8235 1704 1011 3351 8089 8021  102\n",
            "  224 5671 8294 7707 2985 6624 5146 7381 3709 6824 6308 2281 6837 5436\n",
            " 1741 3520 2649 4886  966 5476 1199 8387 5449 1028 2226 6166 4387  350\n",
            " 6123 5236 5041  734 3412 6085  587  489 5760 4982 5115 5575 3839 6743\n",
            " 7173 5724 7689 7787 8302 3150 2247 8128 6660 3505 4610 2866 4161 6905\n",
            " 7096  768 1062 4411 6087 2096 4903 7546 6630 2928 4640 7125 1792 2063\n",
            " 1700 7928 8095  737 5849  922 2359 6015 4489 4361 6081 6346 5014 5167\n",
            " 1023 8009 3335  268 4777 5221 4323 6168  711 5702 6129 7181 1860 7192\n",
            " 1347 5467 6579   67 2693 6134 4841 4270 4611 2943 6746 7696 3466 3950\n",
            " 6125 8370 6142 6131  995 5548 1721 5067 4015 7092 3156   25 1749 5554\n",
            " 4878 8248 6836 5310 2698 6960 2339 1650 6227 4104 5130 7089 6993 2400\n",
            " 1289 4305 5585 7933 6284 7057 3807 5790 8347 5280 2841 7220 1384 7693\n",
            " 8090 4171 5260  133 7451 3957 1719 7379 4743 4746 5435 6216 4858 4417\n",
            " 8355 4492 2648 5104 5278  743 7308  158 7543 2168 5913 7111 3414 7592\n",
            "   39 7157 5647 4555 3956 3617 4630 4384 6596   96 4539 6749 7202 2413\n",
            " 4749 5122 5052 5929  252 5565  336 6008 4564 5580 7237 1466 1568 5958\n",
            " 7657 4327 5091 6918 6487 4082 2989 7448 4715 3382  467 3784 4624 4419\n",
            " 1117 7954 7122 6392 3174 1534 4164 3659 7526  498 5944 4572 2307 2929\n",
            " 7480 4412  731 4563 7692 1869 2472 6966 4238 6549 1594 2276 7269 2545\n",
            " 2216  605 6587 6315 5297 5282 1619 5504 6399 3917 6342 5754 5721 7385\n",
            " 7909 6126 7382 8382 4383 5502 7130  485 4936 7674 5372 3893 7109 4605\n",
            " 5211 5462 8065   50 4080 3334 8167 8378 5595 1873 8207 7076 4106 3719\n",
            " 4627  395  231 2405 6973 3480 3246 5414 5931 8208 4825 6203 3019 5822\n",
            "   63 5854 6543 5787 4764 6701 6562 3822 4861 3051 1315 8318 1089 8169\n",
            " 8218 7549  707 1284 1076 2465 3815 4636   80 7025 7127 4026 2777 4961\n",
            " 7531  744 6640 6684 4297 7939 7384  446 3287 6675 7752 5279 8196 3020\n",
            " 1203 4192 4253 2887 2556 5729 1898 3208 5973 7033 1431 5915 8312 5799\n",
            " 7105  511 6404 2852 1561 4150 1178]\n"
          ]
        }
      ],
      "source": [
        "gc.collect()\n",
        "tf.keras.backend.clear_session()\n",
        "dbert_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "dbert_model = TFRobertaModel.from_pretrained('roberta-base')\n",
        "max_len=512\n",
        "sentences=summarized_data['text']\n",
        "labels=summarized_data['label']\n",
        "filenumbers=summarized_data['filenum']\n",
        "len(sentences),len(labels),len(filenumbers)\n",
        "\n",
        "model_3=create_model()\n",
        "input_ids=[]\n",
        "attention_masks=[]\n",
        "\n",
        "for sent in sentences:\n",
        "  dbert_inps=dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n",
        "  input_ids.append(dbert_inps['input_ids'])\n",
        "  attention_masks.append(dbert_inps['attention_mask'])\n",
        "input_ids=np.asarray(input_ids)\n",
        "\n",
        "attention_masks=np.array(attention_masks)\n",
        "labels=np.array(labels)\n",
        "filenumbers=[int(i) for i in filenumbers]\n",
        "filenumbers=np.array(filenumbers)\n",
        "print(labels)\n",
        "print(filenumbers)\n",
        "\n",
        "#train_inp,val_inp,train_label,val_label,train_mask,val_mask,train_fnum,val_fnum=train_test_split(input_ids,labels,attention_masks,filenumbers,test_size=0.1,random_state=42)\n",
        "\n",
        "new_train_inp_3=[]\n",
        "new_val_inp_3=[]\n",
        "new_train_label_3=[]\n",
        "new_val_label_3=[]\n",
        "new_train_mask_3=[]\n",
        "new_train_fnum_3=[]\n",
        "new_val_fnum_3=[]\n",
        "new_val_mask_3=[]\n",
        "for i in range(len(train_fnum)):\n",
        "  if train_fnum[i] in filenumbers:\n",
        "    new_train_inp_3.append(train_inp[i])\n",
        "    #new_val_inp.append(val_inp[i])\n",
        "    new_train_label_3.append(train_label[i])\n",
        "    #new_val_label.append(val_label[i])\n",
        "    new_train_mask_3.append(train_mask[i])\n",
        "    #new_val_mask.append(val_mask[i])\n",
        "    new_train_fnum_3.append(train_fnum[i])\n",
        "\n",
        "for i in range(len(val_fnum)):\n",
        "  if val_fnum[i] in filenumbers:\n",
        "    #new_train_inp.append(train_inp[i])\n",
        "    new_val_inp_3.append(val_inp[i])\n",
        "    #new_train_label.append(train_label[i])\n",
        "    new_val_label_3.append(val_label[i])\n",
        "    #new_train_mask.append(train_mask[i])\n",
        "    new_val_mask_3.append(val_mask[i])\n",
        "    new_val_fnum_3.append(val_fnum[i])\n",
        "\n",
        "# print(new_train_label)\n",
        "# print(len(new_train_label))\n",
        "# print(new_val_label)\n",
        "# print(len(new_val_label))\n",
        "new_train_inp_3=np.array(new_train_inp_3)\n",
        "new_val_inp_3=np.array(new_val_inp_3)\n",
        "new_train_label_3=np.array(new_train_label_3)\n",
        "new_val_label_3=np.array(new_val_label_3)\n",
        "new_train_mask_3=np.array(new_train_mask_3)\n",
        "new_train_fnum_3=np.array(new_train_fnum_3)\n",
        "new_val_fnum_3=np.array(new_val_fnum_3)\n",
        "new_val_mask_3=np.array(new_val_mask_3)\n",
        "\n",
        "print(new_val_fnum_3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYIS7zLEOS8u",
        "outputId": "2d67123b-e08f-41f2-9a34-e8c050fe3226"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_1[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_roberta_model[0][0]']       \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 15)           7695        ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 8  1  8 ...  2  1 10]\n",
            "[   1    2    3 ... 8413 8414 8417]\n",
            "[6622   35 6713 4989 8233 7444 5061 3942 8344 5464  317 4515 3001 6991\n",
            " 2875 1125 3885 6647 4857 4504  214 8340 2969 3847 4609 1346 5993 7358\n",
            " 2752 7651 8188 7073 4782 6069 2346 6483 1663 4952 5072 1967 4071 7590\n",
            "  860 6418 5030 7297  626 5463 6029 6302 7256 5667 1235 3993 1323 4263\n",
            " 3835 2624 5801 4293 8129 5866 5777 6059 4837 3843 1418 5409 5274 6205\n",
            " 2356 5997 6685 3444 3310 7721 5928 2360 7197 4252 8343  909 4354 4752\n",
            "  649 1916 3488 2496 5765 6612 4004 4052 1403 6697 5268 6293 6380 4602\n",
            " 6057 1851 4577 4236 7978 5083 6552  361 7706 4186 4452 4396 8351 3834\n",
            "  812 5108  463 6600 8264 8064 7121 8279 5556 5119 7947 3350 7152 5199\n",
            " 4282 1092 5025 5051  933 1927 3615 1221 8062 1489 7442 5459 2031 7351\n",
            "  234 8015 7416 7921 6574 6257 8396  941 4520 3300 5640  240 7810 7425\n",
            " 1321 6580  928 1170 5404 8273 6742   94    9 7861 7975 1018 2140 1415\n",
            " 7061 7767 4990 8124 5927 6051 3269 6916 5959 2846 6818 7052 7775 7582\n",
            " 8116 4240 4767 5248 1812 2175 6055 8331  998 1564 7326 1559  292 3065\n",
            " 6764 6854 4915 5552 3583 5902 4229 2144 7565 3712 7224 4502 1406 1651\n",
            " 6256 3827 6525 6318 2686 3616 5369 8235 1704 1011 3351 8089 8021  102\n",
            "  224 5671 8294 7707 2985 6624 5146 7381 3709 6824 6308 2281 6837 5436\n",
            " 1741 3520 2649 4886  966 5476 1199 8387 5449 1028 2226 6166 4387  350\n",
            " 6123 5236 5041  734 3412 6085  587  489 5760 4982 5115 5575 3839 6743\n",
            " 7173 5724 7689 7787 8302 3150 2247 8128 6660 3505 4610 2866 4161 6905\n",
            " 7096  768 1062 4411 6087 2096 4903 7546 6630 2928 4640 7125 1792 2063\n",
            " 1700 7928 8095  737 5849  922 2359 6015 4489 4361 6081 6346 5014 5167\n",
            " 1023 8009 3335  268 4777 5221 4323 6168  711 5702 6129 7181 1860 7192\n",
            " 1347 5467 6579   67 2693 6134 4841 4270 4611 2943 6746 7696 3466 3950\n",
            " 6125 8370 6142 6131  995 5548 1721 5067 4015 7092 3156   25 1749 5554\n",
            " 4878 8248 6836 5310 2698 6960 2339 1650 6227 4104 5130 7089 6993 2400\n",
            " 1289 4305 5585 7933 6284 7057 3807 5790 8347 5280 2841 7220 1384 7693\n",
            " 8090 4171 5260  133 7451 3957 1719 7379 4743 4746 5435 6216 4858 4417\n",
            " 8355 4492 2648 5104 5278  743 7308  158 7543 2168 5913 7111 3414 7592\n",
            "   39 7157 5647 4555 3956 3617 4630 4384 6596   96 4539 6749 7202 2413\n",
            " 4749 5122 5052 5929  252 5565  336 6008 4564 5580 7237 1466 1568 5958\n",
            " 7657 4327 5091 6918 6487 4082 2989 7448 4715 3382  467 3784 4624 4419\n",
            " 1117 7954 7122 6392 3174 1534 4164 3659 7526  498 5944 4572 2307 2929\n",
            " 7480 4412  731 4563 7692 1869 2472 6966 4238 6549 1594 2276 7269 2545\n",
            " 2216  605 6587 6315 5297 5282 1619 5504 6399 3917 6342 5754 5721 7385\n",
            " 7909 6126 7382 8382 4383 5502 7130  485 4936 7674 5372 3893 7109 4605\n",
            " 5211 5462 8065   50 4080 3334 8167 8378 5595 1873 8207 7076 4106 3719\n",
            " 4627  395  231 2405 6973 3480 3246 5414 5931 8208 4825 6203 3019 5822\n",
            "   63 5854 6543 5787 4764 6701 6562 3822 4861 3051 1315 8318 1089 8169\n",
            " 8218 7549  707 1284 1076 2465 3815 4636   80 7025 7127 4026 2777 4961\n",
            " 7531  744 6640 6684 4297 7939 7384  446 3287 6675 7752 5279 8196 3020\n",
            " 1203 4192 4253 2887 2556 5729 1898 3208 5973 7033 1431 5915 8312 5799\n",
            " 7105  511 6404 2852 1561 4150 1178]\n",
            "Sun Jun  5 14:27:44 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    37W / 300W |   4723MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "677/677 [==============================] - 249s 344ms/step - loss: 7.6005 - accuracy: 0.5715 - val_loss: 6.8641 - val_accuracy: 0.7143\n",
            "Epoch 2/5\n",
            "677/677 [==============================] - 234s 345ms/step - loss: 6.6640 - accuracy: 0.7551 - val_loss: 6.5209 - val_accuracy: 0.7361\n",
            "Epoch 3/5\n",
            "677/677 [==============================] - 230s 339ms/step - loss: 6.1948 - accuracy: 0.8161 - val_loss: 6.3001 - val_accuracy: 0.7294\n",
            "Epoch 4/5\n",
            "677/677 [==============================] - 230s 339ms/step - loss: 5.7785 - accuracy: 0.8608 - val_loss: 6.0205 - val_accuracy: 0.7345\n",
            "Epoch 5/5\n",
            "677/677 [==============================] - 234s 345ms/step - loss: 5.3928 - accuracy: 0.8935 - val_loss: 5.8144 - val_accuracy: 0.7546\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_3[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_roberta_model[1][0]']       \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 15)           7695        ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.7327790973871734\n",
            "Weighted F1: 0.7339462214262884\n",
            "Micro F1: 0.7327790973871734\n",
            "Weighted Precision: 0.7409136010937606\n",
            "Micro Precision: 0.7327790973871734\n",
            "Weighted Recall: 0.7327790973871734\n",
            "Micro Recall: 0.7327790973871734\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_1[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_roberta_model[0][0]']       \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 15)           7695        ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 8  1  8 ...  2  1 10]\n",
            "[   1    2    3 ... 8413 8414 8417]\n",
            "[6622   35 6713 4989 8233 7444 5061 3942 8344 5464  317 4515 3001 6991\n",
            " 2875 1125 3885 6647 4857 4504  214 8340 2969 3847 4609 1346 5993 7358\n",
            " 2752 7651 8188 7073 4782 6069 2346 6483 1663 4952 5072 1967 4071 7590\n",
            "  860 6418 5030 7297  626 5463 6029 6302 7256 5667 1235 3993 1323 4263\n",
            " 3835 2624 5801 4293 8129 5866 5777 6059 4837 3843 1418 5409 5274 6205\n",
            " 2356 5997 6685 3444 3310 7721 5928 2360 7197 4252 8343  909 4354 4752\n",
            "  649 1916 3488 2496 5765 6612 4004 4052 1403 6697 5268 6293 6380 4602\n",
            " 6057 1851 4577 4236 7978 5083 6552  361 7706 4186 4452 4396 8351 3834\n",
            "  812 5108  463 6600 8264 8064 7121 8279 5556 5119 7947 3350 7152 5199\n",
            " 4282 1092 5025 5051  933 1927 3615 1221 8062 1489 7442 5459 2031 7351\n",
            "  234 8015 7416 7921 6574 6257 8396  941 4520 3300 5640  240 7810 7425\n",
            " 1321 6580  928 1170 5404 8273 6742   94    9 7861 7975 1018 2140 1415\n",
            " 7061 7767 4990 8124 5927 6051 3269 6916 5959 2846 6818 7052 7775 7582\n",
            " 8116 4240 4767 5248 1812 2175 6055 8331  998 1564 7326 1559  292 3065\n",
            " 6764 6854 4915 5552 3583 5902 4229 2144 7565 3712 7224 4502 1406 1651\n",
            " 6256 3827 6525 6318 2686 3616 5369 8235 1704 1011 3351 8089 8021  102\n",
            "  224 5671 8294 7707 2985 6624 5146 7381 3709 6824 6308 2281 6837 5436\n",
            " 1741 3520 2649 4886  966 5476 1199 8387 5449 1028 2226 6166 4387  350\n",
            " 6123 5236 5041  734 3412 6085  587  489 5760 4982 5115 5575 3839 6743\n",
            " 7173 5724 7689 7787 8302 3150 2247 8128 6660 3505 4610 2866 4161 6905\n",
            " 7096  768 1062 4411 6087 2096 4903 7546 6630 2928 4640 7125 1792 2063\n",
            " 1700 7928 8095  737 5849  922 2359 6015 4489 4361 6081 6346 5014 5167\n",
            " 1023 8009 3335  268 4777 5221 4323 6168  711 5702 6129 7181 1860 7192\n",
            " 1347 5467 6579   67 2693 6134 4841 4270 4611 2943 6746 7696 3466 3950\n",
            " 6125 8370 6142 6131  995 5548 1721 5067 4015 7092 3156   25 1749 5554\n",
            " 4878 8248 6836 5310 2698 6960 2339 1650 6227 4104 5130 7089 6993 2400\n",
            " 1289 4305 5585 7933 6284 7057 3807 5790 8347 5280 2841 7220 1384 7693\n",
            " 8090 4171 5260  133 7451 3957 1719 7379 4743 4746 5435 6216 4858 4417\n",
            " 8355 4492 2648 5104 5278  743 7308  158 7543 2168 5913 7111 3414 7592\n",
            "   39 7157 5647 4555 3956 3617 4630 4384 6596   96 4539 6749 7202 2413\n",
            " 4749 5122 5052 5929  252 5565  336 6008 4564 5580 7237 1466 1568 5958\n",
            " 7657 4327 5091 6918 6487 4082 2989 7448 4715 3382  467 3784 4624 4419\n",
            " 1117 7954 7122 6392 3174 1534 4164 3659 7526  498 5944 4572 2307 2929\n",
            " 7480 4412  731 4563 7692 1869 2472 6966 4238 6549 1594 2276 7269 2545\n",
            " 2216  605 6587 6315 5297 5282 1619 5504 6399 3917 6342 5754 5721 7385\n",
            " 7909 6126 7382 8382 4383 5502 7130  485 4936 7674 5372 3893 7109 4605\n",
            " 5211 5462 8065   50 4080 3334 8167 8378 5595 1873 8207 7076 4106 3719\n",
            " 4627  395  231 2405 6973 3480 3246 5414 5931 8208 4825 6203 3019 5822\n",
            "   63 5854 6543 5787 4764 6701 6562 3822 4861 3051 1315 8318 1089 8169\n",
            " 8218 7549  707 1284 1076 2465 3815 4636   80 7025 7127 4026 2777 4961\n",
            " 7531  744 6640 6684 4297 7939 7384  446 3287 6675 7752 5279 8196 3020\n",
            " 1203 4192 4253 2887 2556 5729 1898 3208 5973 7033 1431 5915 8312 5799\n",
            " 7105  511 6404 2852 1561 4150 1178]\n",
            "Sun Jun  5 14:49:03 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P0    37W / 300W |  15237MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "677/677 [==============================] - 250s 346ms/step - loss: 7.5620 - accuracy: 0.5536 - val_loss: 6.7626 - val_accuracy: 0.7092\n",
            "Epoch 2/5\n",
            "677/677 [==============================] - 234s 346ms/step - loss: 6.5325 - accuracy: 0.7645 - val_loss: 6.3788 - val_accuracy: 0.7630\n",
            "Epoch 3/5\n",
            "677/677 [==============================] - 230s 340ms/step - loss: 6.0487 - accuracy: 0.8163 - val_loss: 6.1747 - val_accuracy: 0.7513\n",
            "Epoch 4/5\n",
            "677/677 [==============================] - 230s 340ms/step - loss: 5.6322 - accuracy: 0.8608 - val_loss: 5.7813 - val_accuracy: 0.7529\n",
            "Epoch 5/5\n",
            "677/677 [==============================] - 230s 340ms/step - loss: 5.2537 - accuracy: 0.8969 - val_loss: 5.5815 - val_accuracy: 0.7630\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_3[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_roberta_model[1][0]']       \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 15)           7695        ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.7399049881235155\n",
            "Weighted F1: 0.7419119995312675\n",
            "Micro F1: 0.7399049881235154\n",
            "Weighted Precision: 0.7556223477523407\n",
            "Micro Precision: 0.7399049881235155\n",
            "Weighted Recall: 0.7399049881235155\n",
            "Micro Recall: 0.7399049881235155\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_1[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_roberta_model[0][0]']       \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 15)           7695        ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 8  1  8 ...  2  1 10]\n",
            "[   1    2    3 ... 8413 8414 8417]\n",
            "[6622   35 6713 4989 8233 7444 5061 3942 8344 5464  317 4515 3001 6991\n",
            " 2875 1125 3885 6647 4857 4504  214 8340 2969 3847 4609 1346 5993 7358\n",
            " 2752 7651 8188 7073 4782 6069 2346 6483 1663 4952 5072 1967 4071 7590\n",
            "  860 6418 5030 7297  626 5463 6029 6302 7256 5667 1235 3993 1323 4263\n",
            " 3835 2624 5801 4293 8129 5866 5777 6059 4837 3843 1418 5409 5274 6205\n",
            " 2356 5997 6685 3444 3310 7721 5928 2360 7197 4252 8343  909 4354 4752\n",
            "  649 1916 3488 2496 5765 6612 4004 4052 1403 6697 5268 6293 6380 4602\n",
            " 6057 1851 4577 4236 7978 5083 6552  361 7706 4186 4452 4396 8351 3834\n",
            "  812 5108  463 6600 8264 8064 7121 8279 5556 5119 7947 3350 7152 5199\n",
            " 4282 1092 5025 5051  933 1927 3615 1221 8062 1489 7442 5459 2031 7351\n",
            "  234 8015 7416 7921 6574 6257 8396  941 4520 3300 5640  240 7810 7425\n",
            " 1321 6580  928 1170 5404 8273 6742   94    9 7861 7975 1018 2140 1415\n",
            " 7061 7767 4990 8124 5927 6051 3269 6916 5959 2846 6818 7052 7775 7582\n",
            " 8116 4240 4767 5248 1812 2175 6055 8331  998 1564 7326 1559  292 3065\n",
            " 6764 6854 4915 5552 3583 5902 4229 2144 7565 3712 7224 4502 1406 1651\n",
            " 6256 3827 6525 6318 2686 3616 5369 8235 1704 1011 3351 8089 8021  102\n",
            "  224 5671 8294 7707 2985 6624 5146 7381 3709 6824 6308 2281 6837 5436\n",
            " 1741 3520 2649 4886  966 5476 1199 8387 5449 1028 2226 6166 4387  350\n",
            " 6123 5236 5041  734 3412 6085  587  489 5760 4982 5115 5575 3839 6743\n",
            " 7173 5724 7689 7787 8302 3150 2247 8128 6660 3505 4610 2866 4161 6905\n",
            " 7096  768 1062 4411 6087 2096 4903 7546 6630 2928 4640 7125 1792 2063\n",
            " 1700 7928 8095  737 5849  922 2359 6015 4489 4361 6081 6346 5014 5167\n",
            " 1023 8009 3335  268 4777 5221 4323 6168  711 5702 6129 7181 1860 7192\n",
            " 1347 5467 6579   67 2693 6134 4841 4270 4611 2943 6746 7696 3466 3950\n",
            " 6125 8370 6142 6131  995 5548 1721 5067 4015 7092 3156   25 1749 5554\n",
            " 4878 8248 6836 5310 2698 6960 2339 1650 6227 4104 5130 7089 6993 2400\n",
            " 1289 4305 5585 7933 6284 7057 3807 5790 8347 5280 2841 7220 1384 7693\n",
            " 8090 4171 5260  133 7451 3957 1719 7379 4743 4746 5435 6216 4858 4417\n",
            " 8355 4492 2648 5104 5278  743 7308  158 7543 2168 5913 7111 3414 7592\n",
            "   39 7157 5647 4555 3956 3617 4630 4384 6596   96 4539 6749 7202 2413\n",
            " 4749 5122 5052 5929  252 5565  336 6008 4564 5580 7237 1466 1568 5958\n",
            " 7657 4327 5091 6918 6487 4082 2989 7448 4715 3382  467 3784 4624 4419\n",
            " 1117 7954 7122 6392 3174 1534 4164 3659 7526  498 5944 4572 2307 2929\n",
            " 7480 4412  731 4563 7692 1869 2472 6966 4238 6549 1594 2276 7269 2545\n",
            " 2216  605 6587 6315 5297 5282 1619 5504 6399 3917 6342 5754 5721 7385\n",
            " 7909 6126 7382 8382 4383 5502 7130  485 4936 7674 5372 3893 7109 4605\n",
            " 5211 5462 8065   50 4080 3334 8167 8378 5595 1873 8207 7076 4106 3719\n",
            " 4627  395  231 2405 6973 3480 3246 5414 5931 8208 4825 6203 3019 5822\n",
            "   63 5854 6543 5787 4764 6701 6562 3822 4861 3051 1315 8318 1089 8169\n",
            " 8218 7549  707 1284 1076 2465 3815 4636   80 7025 7127 4026 2777 4961\n",
            " 7531  744 6640 6684 4297 7939 7384  446 3287 6675 7752 5279 8196 3020\n",
            " 1203 4192 4253 2887 2556 5729 1898 3208 5973 7033 1431 5915 8312 5799\n",
            " 7105  511 6404 2852 1561 4150 1178]\n",
            "Sun Jun  5 15:10:25 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P0    37W / 300W |  15237MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "677/677 [==============================] - 250s 346ms/step - loss: 7.5966 - accuracy: 0.5368 - val_loss: 6.7387 - val_accuracy: 0.7176\n",
            "Epoch 2/5\n",
            "677/677 [==============================] - 235s 347ms/step - loss: 6.5161 - accuracy: 0.7497 - val_loss: 6.2978 - val_accuracy: 0.7462\n",
            "Epoch 3/5\n",
            "677/677 [==============================] - 230s 340ms/step - loss: 6.0045 - accuracy: 0.8098 - val_loss: 6.0583 - val_accuracy: 0.7445\n",
            "Epoch 4/5\n",
            "677/677 [==============================] - 234s 345ms/step - loss: 5.5634 - accuracy: 0.8606 - val_loss: 5.6875 - val_accuracy: 0.7731\n",
            "Epoch 5/5\n",
            "677/677 [==============================] - 230s 340ms/step - loss: 5.1634 - accuracy: 0.8982 - val_loss: 5.4874 - val_accuracy: 0.7496\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_3[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_roberta_model[1][0]']       \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 15)           7695        ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.7494061757719715\n",
            "Weighted F1: 0.7491129463816246\n",
            "Micro F1: 0.7494061757719715\n",
            "Weighted Precision: 0.7517007256228272\n",
            "Micro Precision: 0.7494061757719715\n",
            "Weighted Recall: 0.7494061757719715\n",
            "Micro Recall: 0.7494061757719715\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_1[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_roberta_model[0][0]']       \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 15)           7695        ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 8  1  8 ...  2  1 10]\n",
            "[   1    2    3 ... 8413 8414 8417]\n",
            "[6622   35 6713 4989 8233 7444 5061 3942 8344 5464  317 4515 3001 6991\n",
            " 2875 1125 3885 6647 4857 4504  214 8340 2969 3847 4609 1346 5993 7358\n",
            " 2752 7651 8188 7073 4782 6069 2346 6483 1663 4952 5072 1967 4071 7590\n",
            "  860 6418 5030 7297  626 5463 6029 6302 7256 5667 1235 3993 1323 4263\n",
            " 3835 2624 5801 4293 8129 5866 5777 6059 4837 3843 1418 5409 5274 6205\n",
            " 2356 5997 6685 3444 3310 7721 5928 2360 7197 4252 8343  909 4354 4752\n",
            "  649 1916 3488 2496 5765 6612 4004 4052 1403 6697 5268 6293 6380 4602\n",
            " 6057 1851 4577 4236 7978 5083 6552  361 7706 4186 4452 4396 8351 3834\n",
            "  812 5108  463 6600 8264 8064 7121 8279 5556 5119 7947 3350 7152 5199\n",
            " 4282 1092 5025 5051  933 1927 3615 1221 8062 1489 7442 5459 2031 7351\n",
            "  234 8015 7416 7921 6574 6257 8396  941 4520 3300 5640  240 7810 7425\n",
            " 1321 6580  928 1170 5404 8273 6742   94    9 7861 7975 1018 2140 1415\n",
            " 7061 7767 4990 8124 5927 6051 3269 6916 5959 2846 6818 7052 7775 7582\n",
            " 8116 4240 4767 5248 1812 2175 6055 8331  998 1564 7326 1559  292 3065\n",
            " 6764 6854 4915 5552 3583 5902 4229 2144 7565 3712 7224 4502 1406 1651\n",
            " 6256 3827 6525 6318 2686 3616 5369 8235 1704 1011 3351 8089 8021  102\n",
            "  224 5671 8294 7707 2985 6624 5146 7381 3709 6824 6308 2281 6837 5436\n",
            " 1741 3520 2649 4886  966 5476 1199 8387 5449 1028 2226 6166 4387  350\n",
            " 6123 5236 5041  734 3412 6085  587  489 5760 4982 5115 5575 3839 6743\n",
            " 7173 5724 7689 7787 8302 3150 2247 8128 6660 3505 4610 2866 4161 6905\n",
            " 7096  768 1062 4411 6087 2096 4903 7546 6630 2928 4640 7125 1792 2063\n",
            " 1700 7928 8095  737 5849  922 2359 6015 4489 4361 6081 6346 5014 5167\n",
            " 1023 8009 3335  268 4777 5221 4323 6168  711 5702 6129 7181 1860 7192\n",
            " 1347 5467 6579   67 2693 6134 4841 4270 4611 2943 6746 7696 3466 3950\n",
            " 6125 8370 6142 6131  995 5548 1721 5067 4015 7092 3156   25 1749 5554\n",
            " 4878 8248 6836 5310 2698 6960 2339 1650 6227 4104 5130 7089 6993 2400\n",
            " 1289 4305 5585 7933 6284 7057 3807 5790 8347 5280 2841 7220 1384 7693\n",
            " 8090 4171 5260  133 7451 3957 1719 7379 4743 4746 5435 6216 4858 4417\n",
            " 8355 4492 2648 5104 5278  743 7308  158 7543 2168 5913 7111 3414 7592\n",
            "   39 7157 5647 4555 3956 3617 4630 4384 6596   96 4539 6749 7202 2413\n",
            " 4749 5122 5052 5929  252 5565  336 6008 4564 5580 7237 1466 1568 5958\n",
            " 7657 4327 5091 6918 6487 4082 2989 7448 4715 3382  467 3784 4624 4419\n",
            " 1117 7954 7122 6392 3174 1534 4164 3659 7526  498 5944 4572 2307 2929\n",
            " 7480 4412  731 4563 7692 1869 2472 6966 4238 6549 1594 2276 7269 2545\n",
            " 2216  605 6587 6315 5297 5282 1619 5504 6399 3917 6342 5754 5721 7385\n",
            " 7909 6126 7382 8382 4383 5502 7130  485 4936 7674 5372 3893 7109 4605\n",
            " 5211 5462 8065   50 4080 3334 8167 8378 5595 1873 8207 7076 4106 3719\n",
            " 4627  395  231 2405 6973 3480 3246 5414 5931 8208 4825 6203 3019 5822\n",
            "   63 5854 6543 5787 4764 6701 6562 3822 4861 3051 1315 8318 1089 8169\n",
            " 8218 7549  707 1284 1076 2465 3815 4636   80 7025 7127 4026 2777 4961\n",
            " 7531  744 6640 6684 4297 7939 7384  446 3287 6675 7752 5279 8196 3020\n",
            " 1203 4192 4253 2887 2556 5729 1898 3208 5973 7033 1431 5915 8312 5799\n",
            " 7105  511 6404 2852 1561 4150 1178]\n",
            "Sun Jun  5 15:31:53 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P0    37W / 300W |  15237MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "677/677 [==============================] - 250s 346ms/step - loss: 7.5759 - accuracy: 0.5697 - val_loss: 6.8550 - val_accuracy: 0.7143\n",
            "Epoch 2/5\n",
            "677/677 [==============================] - 235s 347ms/step - loss: 6.6175 - accuracy: 0.7640 - val_loss: 6.4804 - val_accuracy: 0.7529\n",
            "Epoch 3/5\n",
            "677/677 [==============================] - 230s 340ms/step - loss: 6.1301 - accuracy: 0.8205 - val_loss: 6.2434 - val_accuracy: 0.7479\n",
            "Epoch 4/5\n",
            "677/677 [==============================] - 234s 345ms/step - loss: 5.7088 - accuracy: 0.8604 - val_loss: 5.8664 - val_accuracy: 0.7647\n",
            "Epoch 5/5\n",
            "677/677 [==============================] - 230s 340ms/step - loss: 5.3070 - accuracy: 0.8967 - val_loss: 5.7556 - val_accuracy: 0.7462\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_3[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_roberta_model[1][0]']       \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 15)           7695        ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.7422802850356295\n",
            "Weighted F1: 0.7447095740911618\n",
            "Micro F1: 0.7422802850356294\n",
            "Weighted Precision: 0.7510233027998795\n",
            "Micro Precision: 0.7422802850356295\n",
            "Weighted Recall: 0.7422802850356295\n",
            "Micro Recall: 0.7422802850356295\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_1[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_roberta_model[0][0]']       \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 15)           7695        ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 8  1  8 ...  2  1 10]\n",
            "[   1    2    3 ... 8413 8414 8417]\n",
            "[6622   35 6713 4989 8233 7444 5061 3942 8344 5464  317 4515 3001 6991\n",
            " 2875 1125 3885 6647 4857 4504  214 8340 2969 3847 4609 1346 5993 7358\n",
            " 2752 7651 8188 7073 4782 6069 2346 6483 1663 4952 5072 1967 4071 7590\n",
            "  860 6418 5030 7297  626 5463 6029 6302 7256 5667 1235 3993 1323 4263\n",
            " 3835 2624 5801 4293 8129 5866 5777 6059 4837 3843 1418 5409 5274 6205\n",
            " 2356 5997 6685 3444 3310 7721 5928 2360 7197 4252 8343  909 4354 4752\n",
            "  649 1916 3488 2496 5765 6612 4004 4052 1403 6697 5268 6293 6380 4602\n",
            " 6057 1851 4577 4236 7978 5083 6552  361 7706 4186 4452 4396 8351 3834\n",
            "  812 5108  463 6600 8264 8064 7121 8279 5556 5119 7947 3350 7152 5199\n",
            " 4282 1092 5025 5051  933 1927 3615 1221 8062 1489 7442 5459 2031 7351\n",
            "  234 8015 7416 7921 6574 6257 8396  941 4520 3300 5640  240 7810 7425\n",
            " 1321 6580  928 1170 5404 8273 6742   94    9 7861 7975 1018 2140 1415\n",
            " 7061 7767 4990 8124 5927 6051 3269 6916 5959 2846 6818 7052 7775 7582\n",
            " 8116 4240 4767 5248 1812 2175 6055 8331  998 1564 7326 1559  292 3065\n",
            " 6764 6854 4915 5552 3583 5902 4229 2144 7565 3712 7224 4502 1406 1651\n",
            " 6256 3827 6525 6318 2686 3616 5369 8235 1704 1011 3351 8089 8021  102\n",
            "  224 5671 8294 7707 2985 6624 5146 7381 3709 6824 6308 2281 6837 5436\n",
            " 1741 3520 2649 4886  966 5476 1199 8387 5449 1028 2226 6166 4387  350\n",
            " 6123 5236 5041  734 3412 6085  587  489 5760 4982 5115 5575 3839 6743\n",
            " 7173 5724 7689 7787 8302 3150 2247 8128 6660 3505 4610 2866 4161 6905\n",
            " 7096  768 1062 4411 6087 2096 4903 7546 6630 2928 4640 7125 1792 2063\n",
            " 1700 7928 8095  737 5849  922 2359 6015 4489 4361 6081 6346 5014 5167\n",
            " 1023 8009 3335  268 4777 5221 4323 6168  711 5702 6129 7181 1860 7192\n",
            " 1347 5467 6579   67 2693 6134 4841 4270 4611 2943 6746 7696 3466 3950\n",
            " 6125 8370 6142 6131  995 5548 1721 5067 4015 7092 3156   25 1749 5554\n",
            " 4878 8248 6836 5310 2698 6960 2339 1650 6227 4104 5130 7089 6993 2400\n",
            " 1289 4305 5585 7933 6284 7057 3807 5790 8347 5280 2841 7220 1384 7693\n",
            " 8090 4171 5260  133 7451 3957 1719 7379 4743 4746 5435 6216 4858 4417\n",
            " 8355 4492 2648 5104 5278  743 7308  158 7543 2168 5913 7111 3414 7592\n",
            "   39 7157 5647 4555 3956 3617 4630 4384 6596   96 4539 6749 7202 2413\n",
            " 4749 5122 5052 5929  252 5565  336 6008 4564 5580 7237 1466 1568 5958\n",
            " 7657 4327 5091 6918 6487 4082 2989 7448 4715 3382  467 3784 4624 4419\n",
            " 1117 7954 7122 6392 3174 1534 4164 3659 7526  498 5944 4572 2307 2929\n",
            " 7480 4412  731 4563 7692 1869 2472 6966 4238 6549 1594 2276 7269 2545\n",
            " 2216  605 6587 6315 5297 5282 1619 5504 6399 3917 6342 5754 5721 7385\n",
            " 7909 6126 7382 8382 4383 5502 7130  485 4936 7674 5372 3893 7109 4605\n",
            " 5211 5462 8065   50 4080 3334 8167 8378 5595 1873 8207 7076 4106 3719\n",
            " 4627  395  231 2405 6973 3480 3246 5414 5931 8208 4825 6203 3019 5822\n",
            "   63 5854 6543 5787 4764 6701 6562 3822 4861 3051 1315 8318 1089 8169\n",
            " 8218 7549  707 1284 1076 2465 3815 4636   80 7025 7127 4026 2777 4961\n",
            " 7531  744 6640 6684 4297 7939 7384  446 3287 6675 7752 5279 8196 3020\n",
            " 1203 4192 4253 2887 2556 5729 1898 3208 5973 7033 1431 5915 8312 5799\n",
            " 7105  511 6404 2852 1561 4150 1178]\n",
            "Sun Jun  5 15:53:19 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P0    37W / 300W |  15237MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "677/677 [==============================] - 250s 346ms/step - loss: 7.5845 - accuracy: 0.5616 - val_loss: 6.8767 - val_accuracy: 0.7025\n",
            "Epoch 2/5\n",
            "677/677 [==============================] - 234s 346ms/step - loss: 6.6460 - accuracy: 0.7499 - val_loss: 6.4518 - val_accuracy: 0.7395\n",
            "Epoch 3/5\n",
            "677/677 [==============================] - 234s 346ms/step - loss: 6.1656 - accuracy: 0.8063 - val_loss: 6.1422 - val_accuracy: 0.7546\n",
            "Epoch 4/5\n",
            "677/677 [==============================] - 230s 340ms/step - loss: 5.7467 - accuracy: 0.8470 - val_loss: 5.9189 - val_accuracy: 0.7529\n",
            "Epoch 5/5\n",
            "677/677 [==============================] - 234s 346ms/step - loss: 5.3624 - accuracy: 0.8815 - val_loss: 5.6254 - val_accuracy: 0.7815\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_3[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_roberta_model[1][0]']       \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 15)           7695        ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.7600950118764845\n",
            "Weighted F1: 0.7571395688381617\n",
            "Micro F1: 0.7600950118764844\n",
            "Weighted Precision: 0.7623605664623896\n",
            "Micro Precision: 0.7600950118764845\n",
            "Weighted Recall: 0.7600950118764845\n",
            "Micro Recall: 0.7600950118764845\n",
            "Average Accuracy: 0.7448931116389549\n",
            "Average Weighted F1: 0.7453640620537009\n",
            "Average Micro F1: 0.7448931116389549\n",
            "Average Weighted Precision: 0.7523241087462396\n",
            "Average Micro Precision: 0.7448931116389549\n",
            "Average Weighted Recall: 0.7448931116389549\n",
            "Average Micro Recall: 0.7448931116389549\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "total_accuracy=0\n",
        "total_weighted_f1=0\n",
        "total_micro_f1=0\n",
        "total_weighted_precision=0\n",
        "total_micro_precision=0\n",
        "total_weighted_recall=0\n",
        "total_micro_recall=0\n",
        "\n",
        "for f in range(5):\n",
        "  gc.collect()\n",
        "  tf.keras.backend.clear_session()\n",
        "  dbert_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "  dbert_model = TFRobertaModel.from_pretrained('roberta-base')\n",
        "  max_len=512\n",
        "  sentences=summarized_data['text']\n",
        "  labels=summarized_data['label']\n",
        "  filenumbers=summarized_data['filenum']\n",
        "  len(sentences),len(labels),len(filenumbers)\n",
        "  \n",
        "  model_3=create_model()\n",
        "  input_ids=[]\n",
        "  attention_masks=[]\n",
        "\n",
        "  for sent in sentences:\n",
        "    dbert_inps=dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n",
        "    input_ids.append(dbert_inps['input_ids'])\n",
        "    attention_masks.append(dbert_inps['attention_mask'])\n",
        "  input_ids=np.asarray(input_ids)\n",
        "\n",
        "  attention_masks=np.array(attention_masks)\n",
        "  labels=np.array(labels)\n",
        "  filenumbers=[int(i) for i in filenumbers]\n",
        "  filenumbers=np.array(filenumbers)\n",
        "  print(labels)\n",
        "  print(filenumbers)\n",
        "\n",
        "  #train_inp,val_inp,train_label,val_label,train_mask,val_mask,train_fnum,val_fnum=train_test_split(input_ids,labels,attention_masks,filenumbers,test_size=0.1,random_state=42)\n",
        "\n",
        "  new_train_inp_3=[]\n",
        "  new_val_inp_3=[]\n",
        "  new_train_label_3=[]\n",
        "  new_val_label_3=[]\n",
        "  new_train_mask_3=[]\n",
        "  new_train_fnum_3=[]\n",
        "  new_val_fnum_3=[]\n",
        "  new_val_mask_3=[]\n",
        "  for i in range(len(train_fnum)):\n",
        "    if train_fnum[i] in filenumbers:\n",
        "      new_train_inp_3.append(train_inp[i])\n",
        "      #new_val_inp.append(val_inp[i])\n",
        "      new_train_label_3.append(train_label[i])\n",
        "      #new_val_label.append(val_label[i])\n",
        "      new_train_mask_3.append(train_mask[i])\n",
        "      #new_val_mask.append(val_mask[i])\n",
        "      new_train_fnum_3.append(train_fnum[i])\n",
        "\n",
        "  for i in range(len(val_fnum)):\n",
        "    if val_fnum[i] in filenumbers:\n",
        "      #new_train_inp.append(train_inp[i])\n",
        "      new_val_inp_3.append(val_inp[i])\n",
        "      #new_train_label.append(train_label[i])\n",
        "      new_val_label_3.append(val_label[i])\n",
        "      #new_train_mask.append(train_mask[i])\n",
        "      new_val_mask_3.append(val_mask[i])\n",
        "      new_val_fnum_3.append(val_fnum[i])\n",
        "\n",
        "  # print(new_train_label)\n",
        "  # print(len(new_train_label))\n",
        "  # print(new_val_label)\n",
        "  # print(len(new_val_label))\n",
        "  new_train_inp_3=np.array(new_train_inp_3)\n",
        "  new_val_inp_3=np.array(new_val_inp_3)\n",
        "  new_train_label_3=np.array(new_train_label_3)\n",
        "  new_val_label_3=np.array(new_val_label_3)\n",
        "  new_train_mask_3=np.array(new_train_mask_3)\n",
        "  new_train_fnum_3=np.array(new_train_fnum_3)\n",
        "  new_val_fnum_3=np.array(new_val_fnum_3)\n",
        "  new_val_mask_3=np.array(new_val_mask_3)\n",
        "  \n",
        "  print(new_val_fnum_3)\n",
        "  \n",
        "  log_dir='dbert_model'\n",
        "\n",
        "  model_save_path='./drive/MyDrive/Ensemble/roberta-ensemble-512-model_3-'+str(f)+'-15labels.h5'\n",
        "\n",
        "  loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "  accuracy = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
        "  callbacks= [tf.keras.callbacks.ModelCheckpoint(filepath=model_save_path,monitor='val_accuracy',mode='max',save_best_only=True,save_weights_only=True),keras.callbacks.TensorBoard(log_dir=log_dir)]\n",
        "  model_3.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])\n",
        "  gpu_info = !nvidia-smi\n",
        "  gpu_info = '\\n'.join(gpu_info)\n",
        "  if gpu_info.find('failed') >= 0:\n",
        "    print('Not connected to a GPU')\n",
        "  else:\n",
        "    print(gpu_info)\n",
        "  \n",
        "  history=model_3.fit([new_train_inp_3,new_train_mask_3],new_train_label_3,batch_size=8,epochs=5,validation_data=([new_val_inp_3,new_val_mask_3],new_val_label_3),callbacks=callbacks)\n",
        "\n",
        "  pred_labels=[]\n",
        "\n",
        "  model_saved_3= create_model()\n",
        "  model_saved_3.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])\n",
        "  model_saved_3.load_weights('./drive/MyDrive/Ensemble/roberta-ensemble-512-model_3-'+str(f)+'-15labels.h5')\n",
        "\n",
        "  for i in range(0,len(val_inp)):\n",
        "    pred=model_saved_3.predict([val_inp[i].reshape(1,512),val_mask[i].reshape(1,512)])\n",
        "    pred_label = pred.argmax(axis=1)\n",
        "    pred_labels.append(pred_label)\n",
        "  accuracy=accuracy_score(val_label, pred_labels)\n",
        "  print(\"Accuracy: \"+str(accuracy))\n",
        "  total_accuracy=total_accuracy+accuracy\n",
        "  \n",
        "  weighted_f1=f1_score(val_label,pred_labels, average='weighted')\n",
        "  print(\"Weighted F1: \"+ str(weighted_f1))\n",
        "  total_weighted_f1=total_weighted_f1+weighted_f1\n",
        "  micro_f1=f1_score(val_label,pred_labels, average='micro')\n",
        "  print(\"Micro F1: \"+ str(micro_f1))\n",
        "  total_micro_f1=total_micro_f1+micro_f1\n",
        "\n",
        "  weighted_precision=precision_score(val_label, pred_labels, average='weighted')\n",
        "  print(\"Weighted Precision: \" + str(weighted_precision))\n",
        "  total_weighted_precision=total_weighted_precision+weighted_precision\n",
        "  micro_precision=precision_score(val_label, pred_labels, average='micro')\n",
        "  print(\"Micro Precision: \" + str(micro_precision))\n",
        "  total_micro_precision=total_micro_precision+micro_precision\n",
        "\n",
        "  weighted_recall=recall_score(val_label, pred_labels, average='weighted')\n",
        "  print(\"Weighted Recall: \" + str(weighted_recall))\n",
        "  total_weighted_recall=total_weighted_recall+weighted_recall\n",
        "  micro_recall=recall_score(val_label, pred_labels, average='micro')\n",
        "  print(\"Micro Recall: \" + str(micro_recall))\n",
        "  total_micro_recall=total_micro_recall+micro_recall\n",
        "\n",
        "\n",
        "print(\"Average Accuracy: \"+str(total_accuracy/5))\n",
        "print(\"Average Weighted F1: \"+str(total_weighted_f1/5))\n",
        "print(\"Average Micro F1: \"+str(total_micro_f1/5))\n",
        "print(\"Average Weighted Precision: \"+str(total_weighted_precision/5))\n",
        "print(\"Average Micro Precision: \"+str(total_micro_precision/5))\n",
        "print(\"Average Weighted Recall: \"+str(total_weighted_recall/5))\n",
        "print(\"Average Micro Recall: \"+str(total_micro_recall/5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2gkm41KOvo3",
        "outputId": "f2358499-bdd7-46b2-eee5-695d2f5e4f3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5275\n",
            "5275\n",
            "Average Length 511.7759241706161\n",
            "['8', '1', '8', '2', '8', '8', '8', '9', '7', '8', '1', '2', '1', '1', '8', '4', '8', '8', '12', '3', '3', '7', '3', '12', '1', '8', '8', '8', '8', '8', '8', '8', '1', '9', '5', '9', '9', '9', '11', '8', '8', '8', '4', '8', '8', '8', '8', '1', '3', '9', '3', '7', '1', '2', '9', '9', '7', '8', '8', '1', '10', '7', '8', '8', '9', '8', '7', '9', '9', '12', '7', '2', '8', '1', '11', '11', '1', '7', '7', '12', '1', '9', '8', '10', '12', '7', '8', '2', '8', '9', '9', '1', '8', '9', '1', '7', '12', '10', '10', '10', '8', '3', '7', '9', '8', '9', '1', '8', '8', '2', '7', '2', '9', '9', '11', '8', '8', '12', '12', '7', '8', '12', '4', '9', '3', '1', '12', '1', '1', '8', '8', '3', '8', '8', '8', '8', '9', '1', '8', '8', '10', '1', '8', '2', '8', '8', '7', '3', '8', '2', '4', '4', '9', '8', '10', '12', '12', '1', '1', '9', '1', '1', '1', '8', '2', '2', '8', '1', '1', '2', '2', '1', '2', '8', '1', '9', '9', '8', '8', '4', '2', '9', '9', '8', '3', '4', '3', '1', '8', '8', '2', '1', '9', '7', '8', '8', '1', '12', '3', '8', '2', '4', '2', '9', '12', '1', '4', '1', '8', '8', '8', '2', '2', '8', '9', '8', '8', '8', '10', '9', '8', '7', '9', '1', '1', '9', '4', '2', '4', '2', '2', '1', '7', '8', '11', '11', '3', '9', '2', '4', '8', '9', '1', '8', '1', '1', '4', '9', '1', '1', '8', '8', '2', '1', '8', '4', '2', '8', '9', '8', '8', '2', '8', '8', '8', '7', '1', '1', '1', '2', '1', '1', '8', '7', '8', '8', '12', '2', '12', '12', '8', '10', '12', '8', '3', '3', '12', '10', '1', '8', '12', '1', '8', '8', '2', '8', '4', '7', '8', '7', '10', '8', '10', '9', '8', '12', '12', '1', '8', '8', '3', '8', '8', '8', '8', '8', '1', '9', '8', '11', '1', '1', '1', '9', '8', '1', '9', '2', '3', '11', '8', '9', '9', '9', '2', '1', '8', '8', '9', '7', '1', '4', '9', '4', '8', '8', '4', '8', '12', '9', '4', '8', '2', '10', '10', '10', '8', '9', '9', '8', '8', '12', '7', '1', '8', '8', '8', '4', '1', '1', '1', '1', '1', '1', '8', '1', '9', '8', '9', '9', '4', '8', '12', '9', '8', '8', '2', '8', '8', '8', '6', '9', '8', '3', '7', '8', '8', '4', '12', '8', '8', '9', '12', '12', '9', '8', '2', '9', '2', '3', '1', '12', '8', '10', '9', '9', '9', '10', '10', '3', '8', '12', '1', '4', '2', '1', '10', '8', '2', '8', '4', '8', '9', '1', '9', '9', '10', '10', '1', '4', '9', '2', '4', '9', '1', '1', '3', '10', '3', '3', '8', '7', '3', '8', '9', '9', '12', '4', '8', '12', '2', '2', '4', '1', '9', '9', '4', '1', '4', '2', '8', '12', '2', '3', '10', '10', '9', '8', '9', '9', '1', '12', '8', '8', '8', '12', '4', '1', '8', '8', '1', '9', '8', '8', '2', '1', '8', '9', '8', '3', '3', '3', '1', '8', '8', '9', '1', '10', '9', '9', '9', '9', '5', '9', '9', '8', '8', '8', '8', '8', '8', '8', '8', '11', '12', '8', '8', '1', '8', '9', '11', '2', '2', '2', '2', '3', '1', '2', '2', '8', '2', '4', '9', '1', '2', '9', '8', '2', '8', '9', '9', '3', '10', '9', '9', '2', '8', '9', '8', '12', '12', '1', '3', '8', '8', '8', '2', '7', '7', '7', '7', '3', '9', '1', '9', '8', '9', '9', '1', '1', '1', '2', '9', '9', '9', '11', '1', '8', '8', '9', '1', '9', '8', '8', '8', '1', '1', '8', '7', '1', '1', '8', '8', '9', '4', '4', '8', '2', '2', '8', '8', '8', '8', '8', '8', '11', '8', '2', '9', '4', '9', '3', '9', '9', '1', '3', '9', '3', '1', '12', '8', '9', '12', '1', '8', '4', '2', '1', '4', '8', '3', '3', '8', '2', '8', '9', '7', '8', '8', '8', '5', '8', '3', '9', '8', '8', '13', '12', '1', '1', '2', '8', '4', '1', '9', '9', '12', '8', '9', '12', '9', '1', '9', '9', '9', '9', '3', '2', '9', '9', '4', '8', '12', '2', '4', '9', '3', '1', '9', '7', '8', '9', '9', '8', '4', '8', '8', '7', '9', '10', '3', '8', '8', '8', '1', '1', '1', '1', '8', '8', '4', '1', '10', '1', '5', '7', '7', '1', '8', '9', '3', '7', '2', '7', '7', '2', '4', '8', '12', '7', '4', '2', '9', '9', '12', '6', '10', '8', '2', '4', '12', '9', '9', '3', '8', '8', '1', '2', '10', '9', '9', '8', '4', '12', '2', '1', '8', '8', '8', '12', '10', '10', '9', '3', '8', '8', '9', '2', '8', '10', '1', '1', '1', '1', '2', '1', '1', '1', '1', '9', '8', '12', '9', '4', '8', '8', '9', '1', '9', '3', '9', '8', '8', '1', '7', '7', '10', '1', '8', '8', '1', '9', '8', '10', '3', '1', '7', '1', '8', '8', '12', '8', '8', '1', '8', '7', '1', '7', '7', '8', '2', '1', '8', '8', '2', '10', '8', '8', '8', '8', '8', '10', '1', '8', '8', '12', '8', '3', '3', '2', '2', '2', '10', '8', '8', '8', '2', '9', '1', '8', '9', '3', '2', '8', '10', '8', '6', '1', '1', '8', '4', '1', '9', '10', '8', '1', '7', '1', '2', '8', '1', '1', '1', '12', '1', '9', '12', '8', '12', '12', '12', '8', '8', '12', '4', '8', '8', '8', '8', '9', '9', '1', '3', '3', '3', '3', '1', '12', '12', '9', '10', '8', '8', '1', '9', '2', '2', '13', '9', '8', '9', '2', '1', '9', '1', '8', '8', '8', '4', '8', '1', '1', '1', '12', '12', '7', '2', '2', '2', '8', '3', '8', '9', '2', '10', '7', '8', '9', '2', '1', '2', '12', '12', '8', '8', '9', '2', '2', '9', '11', '1', '8', '1', '10', '9', '2', '1', '4', '7', '7', '7', '7', '7', '12', '8', '8', '8', '1', '1', '10', '1', '12', '1', '8', '2', '1', '1', '12', '8', '7', '9', '12', '8', '9', '3', '9', '8', '8', '8', '8', '3', '11', '2', '2', '9', '8', '8', '10', '8', '2', '7', '3', '1', '4', '7', '8', '8', '1', '8', '3', '7', '12', '8', '10', '9', '9', '8', '8', '2', '8', '9', '9', '1', '2', '8', '8', '9', '8', '3', '8', '1', '8', '10', '9', '8', '9', '9', '12', '4', '4', '8', '9', '9', '8', '2', '10', '1', '2', '8', '9', '1', '9', '9', '9', '7', '12', '12', '8', '1', '1', '1', '1', '8', '3', '1', '1', '8', '1', '8', '8', '7', '8', '8', '8', '8', '3', '2', '2', '10', '10', '10', '7', '8', '1', '2', '12', '7', '9', '8', '7', '8', '12', '2', '8', '9', '2', '6', '6', '7', '9', '8', '1', '8', '9', '8', '1', '12', '1', '2', '8', '7', '7', '7', '8', '2', '2', '8', '1', '2', '2', '9', '9', '1', '8', '8', '4', '3', '3', '1', '6', '3', '3', '12', '3', '8', '9', '1', '4', '3', '1', '8', '3', '9', '2', '8', '2', '8', '8', '8', '1', '1', '1', '9', '9', '8', '1', '9', '8', '1', '1', '3', '10', '8', '1', '1', '3', '9', '1', '4', '4', '1', '8', '9', '9', '2', '0', '0', '1', '8', '3', '1', '8', '8', '9', '8', '8', '1', '1', '8', '9', '8', '8', '8', '7', '9', '8', '8', '8', '10', '9', '8', '1', '2', '6', '1', '9', '9', '8', '12', '12', '12', '8', '8', '2', '8', '1', '2', '2', '2', '1', '9', '8', '2', '12', '2', '8', '12', '8', '9', '8', '8', '9', '7', '1', '1', '1', '1', '1', '8', '8', '1', '8', '8', '1', '1', '3', '2', '8', '8', '9', '10', '10', '2', '2', '1', '9', '2', '9', '9', '4', '12', '12', '12', '10', '7', '3', '3', '4', '2', '2', '9', '2', '8', '4', '2', '4', '1', '10', '9', '7', '8', '7', '1', '1', '3', '3', '1', '1', '3', '3', '3', '1', '1', '1', '1', '8', '2', '3', '1', '1', '2', '8', '8', '12', '8', '8', '8', '8', '11', '9', '1', '8', '9', '2', '8', '8', '8', '3', '9', '1', '9', '2', '7', '2', '8', '2', '8', '10', '8', '1', '10', '1', '1', '9', '9', '8', '8', '1', '8', '8', '8', '12', '8', '8', '8', '1', '8', '8', '8', '1', '9', '1', '1', '8', '1', '8', '9', '8', '2', '12', '9', '9', '0', '1', '8', '8', '1', '8', '12', '8', '8', '10', '8', '8', '8', '7', '8', '1', '8', '7', '3', '10', '1', '8', '9', '1', '8', '8', '8', '10', '1', '10', '3', '9', '1', '8', '9', '2', '8', '3', '3', '9', '9', '7', '9', '1', '1', '9', '2', '1', '1', '1', '7', '1', '1', '8', '8', '1', '1', '8', '1', '8', '3', '12', '9', '3', '3', '8', '8', '8', '8', '3', '1', '3', '3', '1', '11', '0', '8', '8', '7', '8', '12', '1', '8', '9', '8', '9', '8', '8', '3', '8', '8', '1', '1', '1', '9', '2', '2', '2', '8', '7', '12', '8', '8', '9', '10', '10', '7', '8', '1', '9', '8', '7', '3', '1', '3', '8', '2', '2', '3', '9', '8', '4', '4', '8', '9', '2', '1', '1', '7', '8', '9', '9', '7', '8', '7', '7', '8', '2', '2', '8', '4', '9', '7', '10', '0', '9', '8', '3', '7', '8', '1', '1', '8', '9', '9', '2', '2', '10', '1', '9', '10', '10', '10', '8', '3', '2', '12', '9', '9', '10', '12', '9', '12', '12', '9', '1', '2', '4', '12', '12', '7', '8', '9', '7', '7', '7', '3', '9', '8', '9', '1', '12', '8', '9', '4', '1', '3', '12', '12', '12', '12', '8', '8', '2', '1', '1', '2', '1', '1', '1', '12', '12', '8', '12', '2', '2', '12', '3', '3', '12', '8', '2', '8', '8', '12', '2', '1', '10', '3', '2', '8', '7', '1', '8', '1', '3', '7', '8', '9', '8', '3', '1', '1', '7', '8', '8', '9', '8', '2', '9', '2', '2', '9', '8', '1', '8', '8', '1', '3', '3', '1', '1', '10', '1', '2', '8', '1', '1', '1', '1', '9', '1', '4', '1', '7', '7', '7', '7', '2', '2', '8', '8', '12', '1', '9', '1', '7', '3', '3', '1', '8', '8', '10', '8', '9', '2', '9', '1', '3', '8', '8', '3', '12', '2', '8', '12', '2', '9', '1', '3', '3', '3', '3', '2', '8', '7', '9', '8', '3', '3', '1', '7', '8', '3', '1', '1', '12', '8', '9', '1', '2', '3', '8', '1', '1', '3', '3', '9', '1', '1', '1', '12', '1', '7', '3', '3', '1', '8', '8', '8', '1', '2', '4', '8', '1', '10', '2', '5', '3', '3', '12', '10', '9', '9', '12', '9', '0', '2', '8', '8', '9', '9', '9', '8', '1', '3', '1', '1', '4', '8', '1', '10', '8', '7', '2', '8', '2', '8', '4', '7', '8', '1', '9', '1', '9', '8', '2', '8', '2', '7', '9', '2', '2', '9', '1', '8', '12', '1', '8', '1', '4', '1', '9', '9', '1', '10', '12', '4', '8', '1', '7', '3', '9', '2', '12', '7', '8', '8', '2', '1', '12', '9', '8', '1', '2', '2', '8', '10', '2', '1', '7', '7', '7', '12', '3', '3', '8', '3', '8', '8', '3', '9', '8', '9', '1', '1', '8', '7', '9', '3', '3', '8', '1', '0', '9', '9', '9', '1', '8', '9', '9', '10', '1', '8', '7', '8', '8', '8', '9', '9', '8', '9', '10', '4', '9', '3', '7', '12', '1', '9', '9', '8', '9', '1', '9', '3', '1', '8', '4', '12', '10', '9', '8', '7', '10', '8', '12', '12', '3', '10', '8', '12', '2', '1', '2', '3', '9', '8', '8', '7', '1', '1', '2', '2', '1', '2', '2', '7', '1', '3', '9', '9', '3', '8', '8', '8', '8', '8', '7', '8', '8', '10', '8', '1', '8', '2', '8', '2', '2', '2', '2', '2', '1', '7', '12', '10', '1', '2', '8', '1', '4', '7', '8', '1', '12', '8', '7', '9', '2', '2', '2', '8', '1', '8', '1', '1', '0', '2', '2', '6', '1', '8', '2', '1', '8', '1', '1', '2', '2', '3', '2', '2', '10', '10', '9', '1', '7', '7', '8', '8', '1', '12', '10', '12', '3', '8', '8', '8', '3', '8', '3', '10', '2', '2', '2', '1', '2', '2', '1', '0', '1', '8', '9', '1', '1', '1', '2', '2', '2', '2', '2', '1', '1', '1', '1', '12', '9', '9', '1', '10', '11', '12', '9', '1', '1', '8', '8', '1', '2', '12', '8', '3', '7', '2', '10', '9', '7', '8', '1', '2', '2', '7', '9', '12', '2', '2', '1', '8', '9', '2', '3', '2', '2', '2', '9', '9', '8', '9', '2', '3', '8', '8', '9', '7', '3', '10', '8', '8', '9', '1', '1', '2', '8', '1', '10', '8', '8', '9', '7', '1', '7', '12', '8', '1', '7', '2', '1', '3', '9', '2', '8', '8', '1', '2', '3', '9', '3', '8', '9', '7', '7', '1', '10', '2', '8', '8', '8', '1', '8', '2', '8', '8', '1', '2', '8', '10', '2', '1', '0', '8', '3', '8', '10', '10', '12', '3', '8', '9', '3', '10', '8', '8', '8', '1', '8', '8', '2', '2', '2', '2', '2', '2', '1', '1', '10', '1', '10', '8', '2', '1', '2', '2', '8', '3', '3', '2', '2', '1', '8', '1', '3', '1', '9', '2', '2', '1', '3', '2', '2', '3', '2', '2', '3', '2', '1', '1', '2', '1', '2', '7', '8', '2', '1', '12', '12', '3', '1', '9', '9', '7', '8', '7', '2', '7', '9', '2', '2', '2', '8', '9', '7', '9', '2', '8', '9', '9', '2', '9', '1', '1', '9', '8', '3', '3', '8', '2', '7', '8', '11', '2', '8', '9', '1', '8', '3', '1', '2', '1', '2', '2', '2', '2', '8', '3', '9', '2', '10', '9', '9', '2', '7', '7', '7', '1', '3', '8', '8', '1', '1', '8', '7', '1', '8', '8', '9', '3', '3', '3', '2', '4', '8', '12', '8', '1', '12', '9', '2', '8', '2', '12', '2', '1', '3', '8', '12', '12', '11', '8', '10', '9', '2', '9', '4', '10', '1', '12', '12', '9', '3', '8', '9', '12', '9', '9', '2', '2', '2', '3', '5', '1', '1', '1', '7', '7', '8', '2', '2', '8', '2', '1', '7', '3', '2', '9', '8', '9', '1', '1', '9', '8', '9', '3', '3', '2', '9', '4', '9', '8', '8', '1', '8', '10', '2', '0', '3', '8', '8', '8', '9', '12', '9', '8', '2', '2', '8', '10', '9', '8', '2', '2', '4', '1', '9', '9', '8', '8', '10', '3', '1', '1', '2', '8', '7', '8', '2', '1', '1', '8', '12', '2', '9', '4', '8', '3', '3', '3', '8', '2', '12', '12', '8', '9', '12', '8', '2', '12', '7', '9', '2', '2', '1', '3', '1', '3', '11', '9', '8', '1', '10', '2', '12', '3', '8', '4', '1', '2', '8', '8', '3', '9', '3', '8', '7', '1', '1', '3', '1', '8', '2', '9', '8', '12', '1', '1', '1', '9', '9', '1', '8', '8', '8', '2', '2', '12', '3', '1', '1', '1', '9', '9', '3', '2', '1', '1', '9', '9', '1', '9', '9', '1', '9', '3', '9', '1', '8', '1', '9', '8', '3', '8', '8', '9', '9', '8', '2', '8', '1', '2', '2', '10', '1', '2', '1', '1', '9', '1', '9', '3', '7', '7', '2', '10', '3', '3', '1', '1', '9', '1', '2', '1', '3', '2', '1', '1', '1', '1', '1', '1', '1', '1', '1', '2', '8', '8', '7', '1', '1', '8', '8', '2', '1', '12', '1', '8', '1', '9', '8', '8', '2', '1', '9', '9', '2', '2', '8', '1', '7', '7', '9', '8', '1', '6', '8', '8', '2', '2', '8', '3', '8', '3', '2', '3', '8', '1', '2', '2', '2', '8', '9', '9', '9', '12', '1', '9', '2', '1', '9', '2', '9', '9', '10', '8', '1', '1', '9', '2', '1', '7', '1', '3', '7', '1', '1', '1', '3', '8', '8', '8', '9', '9', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '1', '0', '1', '1', '1', '2', '2', '3', '1', '1', '2', '3', '3', '3', '7', '3', '1', '9', '3', '9', '1', '4', '1', '1', '9', '10', '9', '8', '2', '3', '10', '9', '3', '10', '12', '8', '9', '8', '2', '1', '9', '8', '1', '7', '8', '1', '9', '1', '2', '10', '7', '7', '8', '1', '3', '3', '9', '3', '3', '1', '1', '1', '9', '1', '3', '8', '8', '8', '1', '8', '1', '9', '7', '8', '8', '2', '1', '3', '8', '2', '1', '6', '1', '8', '8', '2', '8', '2', '2', '9', '8', '1', '1', '6', '8', '6', '10', '2', '1', '1', '8', '9', '3', '3', '3', '1', '3', '9', '6', '8', '1', '8', '3', '1', '2', '2', '12', '7', '1', '1', '1', '1', '12', '1', '8', '8', '3', '2', '1', '9', '2', '3', '3', '2', '2', '7', '2', '2', '2', '3', '1', '9', '8', '7', '1', '1', '3', '1', '9', '4', '3', '9', '2', '1', '1', '9', '8', '8', '7', '2', '1', '3', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '2', '10', '1', '1', '8', '2', '8', '1', '8', '1', '3', '1', '2', '1', '1', '3', '1', '2', '1', '1', '1', '1', '1', '1', '1', '2', '1', '8', '1', '9', '8', '8', '1', '10', '8', '8', '1', '3', '9', '8', '9', '8', '2', '1', '3', '4', '1', '12', '1', '9', '8', '2', '7', '2', '1', '2', '9', '2', '1', '3', '8', '1', '2', '3', '8', '2', '10', '10', '10', '9', '9', '9', '3', '8', '3', '1', '1', '1', '1', '1', '1', '2', '1', '9', '2', '7', '9', '8', '7', '9', '1', '1', '1', '1', '8', '2', '2', '3', '3', '2', '12', '8', '3', '1', '1', '12', '9', '8', '2', '2', '9', '1', '2', '2', '9', '1', '1', '2', '4', '8', '1', '8', '10', '2', '9', '9', '9', '1', '2', '1', '1', '2', '2', '2', '12', '1', '2', '8', '3', '9', '9', '3', '2', '9', '9', '7', '2', '8', '8', '9', '2', '9', '1', '1', '1', '1', '1', '2', '8', '2', '1', '2', '2', '8', '3', '1', '1', '9', '1', '3', '10', '7', '9', '8', '8', '2', '2', '9', '2', '2', '2', '2', '2', '2', '9', '1', '3', '6', '4', '2', '2', '9', '1', '9', '8', '1', '1', '1', '7', '3', '2', '9', '9', '1', '9', '7', '2', '8', '9', '9', '12', '10', '8', '10', '2', '3', '1', '2', '2', '12', '12', '3', '12', '1', '2', '1', '2', '1', '2', '8', '2', '3', '12', '12', '8', '2', '2', '9', '2', '9', '2', '3', '3', '1', '1', '1', '12', '3', '7', '1', '3', '1', '2', '9', '2', '7', '2', '1', '8', '3', '7', '8', '1', '1', '3', '8', '3', '2', '9', '3', '1', '1', '1', '1', '11', '1', '2', '6', '1', '2', '3', '8', '9', '9', '2', '9', '2', '1', '1', '3', '1', '12', '9', '1', '1', '9', '1', '9', '9', '8', '9', '8', '1', '2', '7', '1', '0', '9', '8', '2', '3', '2', '4', '1', '1', '1', '8', '1', '12', '1', '2', '3', '3', '9', '9', '9', '9', '9', '7', '9', '3', '9', '1', '7', '3', '3', '3', '3', '7', '8', '2', '2', '2', '8', '2', '3', '9', '9', '3', '1', '8', '9', '12', '8', '8', '3', '2', '1', '6', '1', '9', '1', '1', '1', '1', '1', '1', '2', '2', '1', '2', '2', '2', '2', '9', '2', '5', '3', '2', '2', '8', '2', '1', '8', '3', '3', '8', '3', '1', '3', '9', '2', '8', '9', '4', '4', '9', '8', '8', '7', '7', '3', '1', '8', '8', '3', '2', '2', '10', '3', '3', '2', '2', '2', '2', '12', '3', '1', '2', '7', '7', '12', '2', '1', '2', '1', '1', '2', '1', '3', '3', '3', '1', '3', '9', '4', '8', '8', '4', '2', '4', '2', '8', '9', '1', '1', '2', '2', '2', '8', '7', '2', '8', '2', '9', '1', '8', '1', '1', '2', '1', '1', '1', '2', '9', '9', '8', '1', '8', '1', '4', '8', '1', '2', '7', '9', '2', '1', '8', '9', '8', '9', '1', '1', '2', '3', '2', '12', '7', '1', '2', '4', '4', '4', '2', '2', '2', '9', '8', '8', '1', '12', '3', '3', '2', '2', '3', '12', '1', '5', '1', '1', '3', '2', '8', '8', '2', '2', '2', '3', '9', '8', '8', '9', '8', '8', '11', '8', '2', '9', '2', '3', '1', '7', '1', '8', '2', '8', '1', '1', '1', '8', '1', '1', '1', '7', '8', '8', '2', '3', '2', '1', '8', '8', '1', '8', '4', '8', '9', '8', '9', '8', '1', '2', '3', '2', '1', '2', '2', '3', '1', '9', '4', '8', '1', '1', '9', '1', '1', '8', '2', '2', '2', '1', '7', '3', '3', '1', '3', '3', '12', '3', '1', '1', '3', '1', '1', '4', '8', '4', '4', '3', '3', '3', '4', '8', '1', '9', '9', '9', '4', '1', '7', '4', '8', '4', '9', '8', '1', '1', '3', '8', '9', '1', '9', '7', '1', '1', '9', '1', '8', '8', '8', '1', '1', '8', '2', '2', '2', '9', '4', '4', '1', '2', '1', '2', '9', '1', '1', '8', '5', '5', '5', '9', '12', '4', '1', '2', '1', '8', '8', '1', '9', '9', '2', '12', '1', '1', '7', '8', '8', '4', '1', '2', '8', '11', '2', '3', '2', '2', '1', '11', '2', '2', '2', '2', '9', '9', '2', '2', '9', '3', '1', '9', '1', '1', '1', '7', '8', '1', '8', '9', '1', '2', '9', '8', '1', '2', '9', '8', '12', '2', '2', '10', '8', '2', '2', '8', '8', '1', '2', '6', '1', '1', '7', '7', '2', '3', '2', '1', '1', '8', '1', '10', '8', '12', '6', '4', '1', '4', '1', '2', '9', '8', '9', '4', '8', '8', '10', '8', '8', '8', '8', '8', '2', '2', '8', '4', '9', '3', '3', '3', '3', '3', '9', '2', '1', '1', '2', '3', '2', '1', '2', '3', '3', '3', '2', '2', '3', '3', '2', '1', '2', '3', '3', '3', '3', '12', '9', '3', '2', '2', '3', '1', '5', '1', '1', '3', '2', '2', '1', '2', '9', '3', '9', '8', '1', '8', '7', '7', '9', '1', '1', '7', '9', '10', '1', '7', '1', '4', '3', '8', '9', '4', '9', '9', '2', '9', '2', '8', '4', '2', '2', '9', '5', '2', '1', '10', '9', '3', '1', '1', '1', '2', '1', '2', '9', '1', '7', '9', '9', '8', '2', '2', '8', '9', '1', '9', '8', '2', '3', '2', '2', '1', '2', '2', '2', '1', '1', '8', '1', '9', '4', '12', '8', '2', '7', '2', '2', '1', '9', '3', '1', '8', '12', '1', '1', '2', '1', '10', '2', '6', '12', '12', '4', '9', '1', '9', '10', '1', '2', '8', '2', '1', '8', '6', '12', '9', '9', '2', '1', '7', '8', '9', '5', '8', '1', '8', '8', '3', '1', '8', '2', '8', '2', '2', '1', '2', '9', '2', '8', '1', '12', '8', '2', '7', '3', '3', '12', '2', '3', '3', '9', '9', '3', '3', '3', '3', '3', '2', '1', '4', '1', '4', '8', '8', '2', '1', '2', '9', '12', '1', '10', '12', '1', '2', '9', '4', '8', '10', '3', '2', '8', '7', '4', '8', '9', '2', '7', '1', '10', '2', '2', '2', '8', '4', '2', '2', '1', '1', '8', '7', '9', '2', '1', '9', '8', '9', '12', '4', '8', '2', '8', '7', '7', '1', '9', '2', '1', '1', '9', '1', '8', '2', '3', '9', '9', '1', '3', '2', '9', '2', '2', '8', '1', '1', '2', '1', '2', '1', '9', '8', '2', '2', '4', '8', '8', '9', '5', '5', '9', '8', '9', '6', '2', '1', '12', '2', '3', '8', '8', '9', '2', '2', '1', '3', '2', '10', '9', '10', '7', '2', '8', '4', '2', '8', '9', '8', '1', '3', '8', '8', '1', '8', '3', '8', '8', '1', '10', '3', '1', '5', '8', '8', '9', '2', '2', '9', '2', '1', '9', '1', '4', '1', '12', '8', '8', '2', '1', '1', '1', '1', '1', '9', '2', '9', '8', '9', '5', '9', '1', '8', '2', '4', '1', '1', '1', '9', '12', '4', '1', '1', '9', '8', '4', '1', '8', '9', '8', '8', '9', '9', '9', '9', '9', '9', '1', '3', '4', '2', '2', '8', '2', '9', '2', '3', '8', '3', '1', '7', '2', '1', '1', '4', '2', '9', '2', '1', '3', '2', '9', '2', '9', '3', '8', '1', '9', '1', '1', '2', '8', '8', '8', '2', '3', '9', '9', '8', '2', '4', '1', '5', '1', '1', '9', '9', '2', '1', '1', '1', '9', '9', '1', '3', '9', '2', '2', '9', '9', '8', '3', '9', '9', '8', '2', '2', '8', '9', '2', '2', '8', '9', '10', '10', '2', '8', '7', '9', '9', '4', '2', '4', '11', '2', '1', '7', '8', '8', '8', '9', '4', '9', '8', '8', '2', '1', '1', '9', '8', '3', '3', '8', '8', '7', '9', '1', '3', '4', '8', '10', '2', '4', '4', '8', '2', '8', '2', '8', '9', '2', '1', '2', '2', '3', '1', '9', '4', '5', '9', '9', '1', '1', '1', '1', '1', '1', '7', '1', '1', '1', '8', '1', '1', '1', '1', '1', '9', '1', '2', '1', '12', '9', '3', '9', '9', '4', '1', '9', '1', '1', '2', '11', '9', '3', '2', '2', '2', '4', '2', '3', '2', '7', '7', '8', '1', '10', '9', '5', '7', '1', '10', '8', '1', '2', '7', '1', '12', '5', '8', '4', '2', '9', '9', '8', '2', '9', '8', '8', '10', '2', '3', '2', '7', '2', '8', '10', '3', '2', '9', '9', '9', '1', '1', '1', '1', '8', '2', '10', '9', '8', '1', '2', '2', '2', '1', '2', '1', '3', '9', '12', '2', '2', '4', '8', '1', '3', '9', '4', '9', '1', '9', '9', '2', '1', '1', '9', '7', '10', '3', '10', '2', '9', '2', '9', '2', '8', '2', '1', '2', '3', '1', '1', '1', '8', '5', '8', '3', '1', '1', '4', '2', '2', '1', '3', '8', '2', '1', '1', '1', '2', '9', '4', '1', '8', '1', '2', '8', '2', '9', '2', '2', '5', '5', '2', '5', '2', '1', '8', '8', '1', '3', '12', '2', '4', '3', '2', '2', '2', '6', '2', '3', '3', '1', '8', '2', '2', '1', '1', '8', '8', '10', '1', '9', '2', '9', '12', '0', '2', '1', '3', '2', '2', '1', '2', '1', '9', '9', '2', '9', '8', '8', '1', '7', '8', '1', '2', '6', '9', '8', '10', '1', '12', '2', '2', '1', '8', '2', '1', '12', '7', '4', '2', '10', '2', '2', '1', '2', '1', '2', '1', '1', '2', '1', '9', '8', '10', '2', '10', '9', '9', '12', '5', '3', '2', '8', '2', '8', '3', '3', '1', '4', '9', '10', '2', '4', '8', '1', '2', '10', '12', '12', '3', '1', '1', '2', '9', '6', '6', '1', '9', '1', '8', '1', '8', '8', '8', '8', '2', '2', '2', '8', '8', '2', '1', '1', '1', '1', '1', '8', '8', '5', '8', '8', '1', '8', '9', '9', '1', '7', '12', '9', '9', '7', '2', '2', '7', '8', '9', '8', '2', '9', '6', '3', '4', '8', '4', '1', '2', '9', '8', '8', '8', '2', '1', '1', '8', '8', '3', '1', '9', '9', '7', '1', '2', '2', '2', '9', '4', '1', '2', '12', '1', '8', '2', '1', '9', '1', '8', '9', '9', '2', '5', '4', '2', '1', '2', '8', '12', '8', '10', '3', '8', '8', '8', '2', '2', '8', '2', '8', '2', '4', '8', '10', '2', '7', '2', '1', '8', '8', '9', '5', '8', '1', '12', '7', '10', '2', '2', '2', '1', '4', '8', '12', '1', '8', '2', '2', '9', '1', '8', '3', '1', '1', '5', '10', '2', '1', '2', '4', '8', '9', '3', '2', '8', '7', '4', '4', '9', '9', '2', '8', '1', '1', '2', '4', '7', '1', '9', '1', '1', '1', '8', '4', '1', '3', '1', '2', '2', '9', '1', '8', '8', '2', '2', '9', '9', '1', '8', '8', '1', '9', '8', '8', '2', '2', '2', '2', '1', '1', '1', '7', '4', '1', '1', '4', '2', '3', '3', '2', '3', '9', '2', '8', '2', '1', '5', '1', '2', '2', '1', '3', '5', '2', '9', '1', '1', '9', '8', '1', '4', '8', '1', '8', '2', '4', '8', '4', '7', '9', '8', '1', '2', '2', '4', '4', '11', '3', '11', '1', '3', '8', '1', '9', '6', '8', '9', '3', '9', '8', '9', '2', '3', '3', '7', '1', '7', '2', '9', '9', '1', '8', '8', '1', '5', '5', '8', '8', '1', '2', '1', '3', '9', '9', '5', '9', '8', '1', '4', '3', '8', '1', '1', '2', '1', '8', '9', '8', '2', '2', '2', '1', '2', '4', '10', '8', '1', '9', '1', '8', '1', '1', '9', '9', '8', '11', '1', '8', '2', '1', '2', '8', '2', '8', '8', '2', '9', '6', '10', '1', '8', '6', '4', '5', '11', '2', '2', '8', '1', '8', '1', '8', '1', '4', '8', '1', '1', '1', '8', '3', '7', '3', '3', '7', '1', '1', '4', '8', '1', '6', '4', '2', '2', '1', '1', '1', '1', '6', '2', '2', '8', '1', '8', '5', '5', '2', '1', '1', '2', '3', '7', '1', '1', '6', '2', '9', '3', '9', '8', '9', '2', '1', '4', '2', '9', '13', '9', '2', '8', '12', '4', '1', '1', '9', '5', '1', '1', '1', '1', '8', '2', '1', '1', '2', '9', '0', '12', '1', '1', '8', '9', '8', '3', '2', '2', '12', '8', '2', '2', '2', '1', '10', '1', '9', '8', '9', '5', '2', '2', '9', '2', '8', '4', '8', '3', '7', '9', '2', '2', '7', '2', '2', '8', '1', '1', '9', '10', '9', '8', '2', '9', '0', '7', '1', '1', '1', '9', '8', '2', '8', '12', '8', '3', '2', '8', '8', '7', '7', '8', '12', '2', '1', '3', '9', '2', '2', '8', '2', '8', '8', '12', '8', '8', '1', '8', '9', '2', '4', '4', '7', '2', '1', '1', '9', '3', '7', '1', '3', '9', '8', '2', '2', '3', '1', '3', '10', '2', '3', '7', '1', '8', '1', '1', '10', '3', '10', '8', '4', '1', '2', '4', '3', '10', '9', '3', '9', '5', '8', '7', '2', '9', '3', '3', '2', '11', '8', '1', '1', '7', '2', '7', '2', '9', '4', '8', '1', '12', '8', '7', '9', '1', '0', '2', '6', '4', '9', '3', '2', '4', '12', '9', '8', '5', '9', '2', '8', '4', '6', '8', '9', '3', '1', '8', '7', '3', '1', '1', '1', '8', '9', '1', '8', '7', '9', '8', '2', '1', '2', '2', '3', '2', '2', '1', '1', '4', '9', '7', '3', '2', '8', '8', '9', '1', '9', '7', '4', '2', '2', '8', '5', '9', '1', '5', '7', '1', '4', '6', '9', '10', '1', '8', '2', '9', '1', '9', '2', '9', '8', '2', '9', '9', '11', '2', '12', '2', '2', '9', '4', '8', '1', '9', '9', '2', '8', '9', '1', '2', '9', '3', '8', '8', '1', '7', '8', '8', '2', '3', '2', '2', '2', '2', '9', '8', '10', '2', '2', '1', '9', '11', '1', '8', '8', '2', '4', '2', '1', '2', '8', '8', '9', '2', '8', '9', '2', '3', '1', '2', '1', '3', '10', '8', '1', '2', '4', '1', '9', '8', '9', '3', '3', '9', '8', '2', '11', '3', '7', '1', '8', '7', '1', '8', '8', '8', '1', '4', '8', '8', '1', '9', '3', '1', '1', '2', '8', '8', '8', '2', '1', '8', '10', '2', '12', '8', '8', '9', '1', '8', '1', '3', '2', '2', '10', '7', '2', '2', '1', '1', '8', '2', '9', '1', '2', '9', '3', '3', '10', '1', '4', '7', '10', '12', '2', '4', '10', '8', '6', '2', '9', '1', '8', '3', '1', '2', '8', '8', '2', '12', '10', '2', '10', '9', '1', '2', '5', '8', '12', '8', '8', '7', '8', '8', '1', '8', '2', '2', '9', '1', '8', '10', '5', '5', '5', '8', '1', '9', '1', '13', '1', '2', '2', '1', '2', '4', '9', '2', '1', '9', '13', '11', '1', '9', '9', '3', '10', '8', '7', '8', '8', '4', '2', '1', '9', '4', '3', '1', '1', '10', '9', '2', '8', '6', '2', '1', '1', '3', '2', '8', '1', '1', '1', '1', '2', '4', '1', '1', '10', '1', '8', '1', '1', '9', '1', '7', '1', '1', '9', '9', '2', '12', '9', '1', '8', '1', '12', '1', '8', '3', '8', '1', '8', '1', '2', '10', '1', '1', '2', '2', '9', '3', '9', '12', '1', '9', '1', '2', '8', '7', '2', '1', '1', '3', '2', '8', '4', '4', '5', '12', '7', '8', '9', '6', '1', '8', '9', '2', '1', '9', '1', '4', '1', '8', '2', '1', '1', '8', '7', '4', '9', '1', '8', '4', '2', '7', '5', '1', '3', '9', '2', '8', '2', '1', '2', '9', '1', '1', '2', '3', '4', '3', '1', '2', '2', '10', '2', '2', '8', '1', '1', '2', '4', '9', '2', '11', '9', '8', '9', '2', '1', '10', '4', '1', '1', '10', '7', '2', '10', '8', '1', '10', '8', '8', '8', '8', '8', '8', '2', '2', '2', '8', '3', '4', '1', '1', '1', '2', '1', '8', '8', '2', '8', '3', '8', '3', '1', '2', '3', '1', '1', '10', '1', '4', '4', '3', '3', '1', '9', '1', '1', '2', '1', '1', '6', '1', '1', '1', '9', '1', '4', '1', '1', '1', '1', '1', '1', '2', '12', '8', '8', '1', '9', '12', '10', '8', '2', '12', '1', '1', '2', '1', '7', '10', '7', '1', '2', '1', '10', '8', '2', '2', '8', '2', '2', '1', '9', '8', '8', '8', '8', '3', '1', '4', '2', '8', '3', '10', '10', '1', '9', '1', '9', '1', '9', '1', '8', '3', '1', '1', '8', '8', '8', '6', '3', '4', '1', '1', '1', '9', '9', '5', '2', '10', '2', '6', '2', '2', '7', '1', '8', '2', '11', '1', '1', '1', '2', '1', '9', '4', '4', '2', '8', '4', '6', '8', '8', '10', '10', '2', '1', '2', '9', '1', '8', '8', '3', '4', '8', '8', '2', '8', '8', '1', '2', '1', '2', '9', '4', '1', '3', '3', '3', '8', '8', '8', '2', '6', '4', '3', '3', '12', '3', '4', '9', '6', '8', '7', '7', '8', '6', '4', '1', '10', '6', '3', '3', '8', '1', '1', '8', '1', '11', '8', '1', '1', '2', '3', '11', '7', '2', '1', '1', '1', '9', '1', '2', '1', '1', '9', '8', '1', '1', '8', '0', '4', '1', '1', '2', '1', '1', '2', '2', '9', '1', '10', '1', '3', '8', '9', '3', '3', '1', '10', '1', '9', '1', '1', '7', '4', '1', '8', '10', '1', '2', '10', '1', '1', '2', '3', '3', '9', '1', '8', '10', '1', '7', '9', '1', '9', '6', '3', '9', '2', '8', '4', '12', '12', '3', '1', '1', '9', '1', '9', '2', '1', '1', '1', '1', '1', '9', '2', '1', '2', '10', '9', '8', '8', '10', '8', '2', '3', '2', '1', '12', '8', '12', '2', '9', '1', '3', '2', '5', '2', '2', '8', '2', '2', '4', '10', '9', '10', '9', '4', '2', '4', '12', '8', '2', '1', '8', '3', '9', '2', '9', '4', '1', '1', '1', '1', '1', '9', '6', '2', '9', '2', '1', '3', '2', '2', '5', '8', '9', '1', '6', '3', '1', '2', '2', '2', '6', '1', '8', '10', '8', '3', '3', '13', '2', '9', '9', '10', '6', '12', '8', '2', '9', '2', '3', '8', '8', '1', '2', '2', '3', '2', '10', '1', '8', '9', '1', '8', '9', '8', '8', '2', '10', '2', '1', '1', '1', '9', '9', '12', '1', '1', '4', '8', '3', '2', '2', '4', '4', '2', '2', '1', '1', '1', '9', '2', '2', '4', '1', '1', '8', '8', '8', '9', '2', '13', '9', '9', '4', '1', '9', '7', '10', '10', '8', '1', '9', '1', '1', '1', '8', '12', '4', '1', '2', '1', '1', '1', '7', '8', '3', '3', '1', '2', '2', '7', '2', '2', '10', '1', '8', '4', '0', '1', '9', '4', '1', '1', '2', '10', '10', '7', '9', '3', '12', '11', '2', '8', '10', '8', '9', '4', '3', '4', '10', '1', '1', '9', '3', '10', '1', '5', '3', '3', '2', '2', '1', '1', '6', '1', '7', '1', '1', '12', '1', '1', '1', '10', '1', '8', '8', '1', '3', '8', '3', '1', '2', '8', '3', '4', '2', '8', '8', '6', '1', '1', '4', '1', '2', '9', '5', '1', '7', '8', '9', '2', '4', '9', '8', '5', '9', '9', '9', '1', '2', '1', '3', '8', '9', '2', '9', '8', '9', '1', '8', '13', '9', '3', '8', '1', '4', '1', '3', '1', '6', '4', '10', '2', '2', '7', '8', '8', '12', '8', '9', '9', '9', '10', '3', '12', '12', '2', '9', '10', '9', '7', '2', '3', '4', '10', '10', '2', '7', '7', '1', '7', '3', '9', '1', '9', '8', '2', '9', '1', '8', '1', '10', '8', '8', '2', '7', '8', '1', '8', '9', '8', '1', '4', '1', '8', '8', '8', '1', '1', '10', '3', '2', '2', '6', '8', '9', '8', '2', '1', '1', '3', '8', '8', '1', '4', '4', '3', '9', '10', '4', '2', '4', '2', '9', '2', '2', '9', '1', '1', '10', '1', '1', '2', '1', '1', '1', '9', '1', '8', '8', '8', '2', '3', '8', '1', '6', '7', '3', '1', '9', '1', '2', '1', '6', '1', '2', '10', '1', '4', '2', '3', '2', '9', '9', '4', '9', '9', '1', '2', '8', '4', '7', '8', '7', '13', '9', '1', '2', '1', '9', '3', '3', '6', '8', '9', '10', '9', '2', '2', '3', '12', '1', '1', '1', '13', '2', '1', '7', '10', '8', '9', '1', '2', '9', '1', '1', '1', '2', '1', '12', '5', '6', '9', '10', '3', '1', '10', '1', '3', '2', '8', '9', '2', '10', '8', '8', '9', '2', '8', '9', '2', '2', '2', '8', '8', '2', '3', '1', '4', '8', '1', '9', '9', '2', '8', '10', '9', '1', '2', '12', '8', '8', '0', '2', '1', '1', '9', '10', '1', '6', '2', '2', '10', '2', '2', '4', '2', '2', '12', '1', '6', '7', '8', '9', '1', '3', '5', '7', '3', '1', '3', '1', '1', '3', '2', '6', '3', '2', '1', '8', '3', '5', '2', '1', '1', '8', '1', '1', '0', '2', '3', '5', '3', '1', '1', '9', '8', '8', '8', '8', '4', '7', '2', '9', '12', '5', '9', '3', '12', '3', '9', '1', '9', '1', '7', '3', '9', '3', '10', '1', '2', '1', '4', '9', '8', '2', '9', '11', '2', '8', '9', '4', '1', '1', '1', '1', '1', '1', '1', '9', '1', '9', '1', '1', '1', '12', '3', '10', '3', '6', '1', '7', '10', '8', '3', '7', '1', '1', '2', '6', '1', '3', '9', '2', '8', '1', '8', '8', '1', '8', '10', '12', '1', '10', '12', '10', '1', '8', '8', '1', '4', '7', '9', '2', '1', '9', '3', '8', '12', '10', '6', '1', '6', '8', '3', '1', '3', '1', '10', '10', '9', '8', '1', '2', '9', '8', '8', '1', '9', '8', '8', '3', '1', '3', '9', '12', '1', '8', '1', '5', '8', '11', '5', '5', '1', '2', '1', '3', '1', '1', '1', '9', '8', '8', '1', '9', '10', '8', '4', '1', '1', '8', '10', '8', '9', '1', '10', '1', '7', '2', '1', '1', '9', '6', '2', '9', '8', '1', '1', '1', '9', '8', '8', '1', '9', '9', '1', '1', '2', '7', '9', '2', '9', '9', '8', '7', '6', '8', '7', '8', '2', '9', '2', '1', '8', '8', '8', '2', '6', '3', '1', '8', '12', '12', '8', '7', '1', '2', '2', '7', '1', '9', '8', '1', '4', '1', '1', '8', '5', '8', '1', '8', '8', '8', '8', '1', '2', '11', '1', '1', '1', '2', '7', '1', '9', '2', '2', '2', '9', '8', '9', '2', '9', '1', '8', '1', '7', '11', '13', '1', '9', '8', '2', '2', '1', '2', '3', '10', '3', '10', '4', '3', '9', '1', '8', '1', '1', '13', '1', '1', '3', '8', '1', '2', '2', '2', '9', '4', '1', '8', '1', '9', '3', '6', '8', '5', '2', '8', '2', '8', '1', '9', '2', '2', '8', '1', '1', '4', '8', '9', '2', '2', '7', '1', '8', '12', '2', '12', '8', '2', '9', '9', '3', '8', '1', '1', '9', '9', '9', '2', '8', '1', '9', '1', '8', '9', '8', '2', '2', '4', '1', '9', '10', '10', '8', '8', '1', '1', '2', '1', '4', '1', '2', '1', '1', '3', '12', '8', '8', '8', '8', '10', '2', '8', '12', '1', '9', '9', '8', '9', '1', '2', '1', '8', '8', '8', '2', '8', '8', '3', '10', '8', '9', '1', '1', '1', '3', '4', '8', '10', '6', '3', '1', '3', '2', '8', '9', '3', '5', '4', '1', '5', '2', '9', '9', '9', '1', '1', '1', '9', '9', '6', '10', '9', '8', '12', '2', '9', '1', '2', '1', '1', '1', '8', '1', '12', '1', '12', '1', '9', '1', '8', '1', '2', '1', '2', '8', '8', '10', '1', '9', '1', '2', '2', '8', '8', '3', '12', '1', '11', '2', '10', '12', '11', '2', '1', '10', '1', '1', '9', '3', '10', '2', '7', '1', '8', '1', '9', '9', '2', '1', '3', '12', '5', '9', '9', '8', '1', '8', '8', '8', '1', '1', '3', '9', '8', '9', '8', '3', '10', '3', '7', '8', '9', '2', '3', '1', '9', '10', '9', '2', '8', '2', '6', '4', '1', '1', '3', '8', '2', '3', '9', '1', '2', '1', '8', '1', '2', '2', '2', '2', '9', '9', '4', '8', '9', '1', '1', '2', '9', '1', '5', '2', '9', '8', '10', '1', '2', '10', '8', '5', '6', '4', '8', '1', '1', '1', '8', '9', '2', '8', '8', '8', '12', '2', '2', '1', '8', '1', '2', '1', '9', '8', '8', '9', '1', '1', '8', '7', '1', '8', '0', '2', '3', '8', '1', '1', '8', '1', '1', '12', '3', '2', '10', '8', '10', '6', '1', '8', '8', '10', '8', '8', '1', '4', '8', '1', '2', '9', '8', '1', '1', '3', '3', '5', '1', '1', '2', '1', '2', '1', '9', '8', '3', '9', '8', '2', '10', '9', '1', '8', '1', '10', '9', '10', '1', '2', '3', '8', '1', '3', '1', '9', '9', '8', '1', '9', '8', '8', '9', '1', '9', '2', '8', '8', '2', '13', '8', '10', '8', '9', '3', '2', '1', '3', '1', '12', '10', '10', '1', '11', '1', '2', '4', '2', '10', '1', '8', '11', '1', '1', '1', '2', '2', '8', '2', '9', '8', '9', '9', '12', '8', '1', '9', '2', '4', '1', '8', '3', '9', '1', '6', '5', '8', '9', '3', '3', '2', '1', '1', '1', '8', '11', '1', '8', '7', '1', '11', '9', '1', '7', '9', '9', '8', '8', '8', '12', '8', '1', '2', '9', '8', '8', '2', '10', '9', '8', '8', '4', '10', '9', '8', '2', '10', '10', '2', '1', '2', '1', '2', '1', '4', '1', '7', '1', '1', '3', '12', '9', '8', '5', '9', '10', '1', '9', '9', '2', '7', '4', '1', '1', '8', '12', '8', '2', '2', '9', '4', '1', '1', '5', '9', '1', '8', '12', '8', '1', '1', '4', '9', '10', '2', '3', '1', '3', '3', '3', '8', '1', '2', '1', '2', '1', '2', '9', '1', '2', '8', '9', '4', '8', '8', '10', '2', '12', '5', '5', '1', '8', '1', '7', '8', '1', '1', '8', '9', '12', '4', '9', '3', '1', '2', '5', '9', '2', '1', '2', '2', '1', '2', '8', '1', '8', '8', '2', '2', '1', '8', '4', '1', '2', '10', '9', '10', '8', '1', '10', '4', '8', '8', '10', '2', '8', '1', '1', '3', '10', '1', '4', '2', '8', '3', '3', '2', '9', '8', '5', '5', '9', '3', '10', '8', '1', '2', '1', '2', '9', '1', '1', '2', '9', '9', '8', '8', '11', '9', '4', '1', '8', '2', '7', '1', '2', '8', '2', '9', '9', '8', '2', '9', '9', '8', '8', '1', '2', '8', '1', '1', '2', '9', '8', '1', '4', '9', '1', '8', '12', '1', '12', '2', '8', '2', '9', '1', '12', '1', '2', '9', '1', '1', '9', '3', '2', '9', '9', '2', '11', '8', '7', '9', '9', '9', '8', '8', '1', '2', '8', '1', '1', '4', '1', '2', '10', '1', '9', '2', '1', '1', '1', '9', '1', '13', '4', '3', '2', '1', '1', '2', '2', '7', '8', '7', '1', '1', '2', '8', '8', '3', '4', '8', '2', '10', '13', '8', '8', '9', '2', '2', '1', '1', '10', '4', '2', '7', '9', '8', '2', '1', '9', '8', '1', '1', '9', '8', '9', '8', '2', '8', '2', '2', '9', '2', '1', '9', '1', '2', '4', '8', '2', '1', '1', '8', '1', '4', '1', '9', '3', '9', '2', '7', '1', '9', '6', '1', '10', '2', '2', '2', '2', '2', '10', '10', '10', '9', '9', '9', '3', '12', '10', '1', '1', '10', '1', '1', '2', '2', '1', '2', '3', '12', '8', '8', '1', '2', '9', '9', '1', '1', '10', '8', '9', '8', '3', '1', '1', '3', '1', '10', '1', '1', '1', '4', '1', '1', '1', '7', '10', '2', '1', '8', '1', '9', '3', '1', '10', '8', '8', '1', '5', '9', '1', '2', '1', '8', '8', '1', '3', '1', '10', '2', '1', '1', '2', '8', '2', '5', '3', '5', '2', '1', '1', '9', '1', '7', '9', '9', '2', '8', '1', '12', '4', '1', '4', '9', '9', '4', '8', '1', '10', '10', '8', '8', '8', '9', '2', '3', '5', '1', '8', '1', '1', '1', '7', '10', '1', '4', '12', '3', '2', '2', '9', '1', '1', '1', '2', '8', '4', '1', '7', '3', '12', '8', '6', '2', '2', '7', '9', '11', '1', '10', '9', '1', '8', '12', '2', '11', '1', '2', '3', '1', '1', '1', '8', '9', '2', '2', '2', '2', '3', '3', '8', '10', '4', '1', '2', '9', '8', '2', '2', '9', '1', '8', '11', '4', '2', '8', '7', '1', '1', '7', '3', '9', '9', '4', '5', '8', '10', '2', '9', '9', '9', '8', '1', '9', '2', '2', '2', '1', '2', '9', '4', '3', '12', '4', '3', '2', '3', '8', '3', '10', '1', '10', '1', '1', '8', '10', '6', '8', '9', '9', '1', '2', '9', '2', '9', '3', '8', '1', '1', '12', '1', '5', '1', '10', '8', '10', '2', '7', '1', '1', '1', '1', '3', '2', '3', '5', '9', '1', '2', '1', '9', '8', '10', '9', '8', '1', '8', '1', '8', '2', '8', '1', '2', '8', '1', '2', '5', '8', '12', '2', '1', '1', '1', '1', '1', '8', '8', '2', '1', '4', '2', '8', '8', '8', '2', '9', '9', '11', '1', '2', '9', '3', '1', '1', '8', '9', '2', '4', '10', '1', '9', '8', '4', '9', '8', '10', '8', '8', '9', '2', '8', '9', '3', '3', '3', '4', '3', '2', '2', '10', '8', '2', '1', '5', '1', '9', '1', '1', '2', '1', '2', '3', '1', '1', '8', '8', '1', '10', '8', '8', '1', '6', '4', '8', '2', '5', '8', '1', '3', '8', '2', '8', '9', '1', '1', '8', '12', '10', '1', '5', '8', '2', '8', '10', '9', '9', '1', '6', '9', '1', '10', '8', '2', '9', '10', '1', '2', '1', '1', '8', '8', '3', '9', '8', '1', '10', '2', '8', '1', '10', '1', '5', '1', '1', '1', '5', '1', '1', '1', '4', '1', '1', '1', '8', '2', '8', '1', '8', '3', '8', '9', '1', '8', '1', '8', '1', '1', '2', '2', '1', '12', '9', '9', '1', '8', '2', '1', '2', '8', '1', '9', '1', '1', '2', '1', '2', '2', '2', '1', '9', '9', '8', '8', '1', '1', '9', '10', '8', '4', '3', '3', '4', '9', '1', '3', '3', '10', '2', '2', '1', '8', '4', '1', '9', '4', '1', '1', '9', '8', '10', '4', '9', '9', '8', '2', '1', '3', '4', '9', '3', '8', '9', '1', '1', '2', '1', '7', '8', '2', '9', '1', '9', '2', '6', '9', '10', '8', '1', '1', '5', '9', '5', '1', '9', '10', '9', '3', '3', '8', '8', '2', '9', '2', '8', '2', '1', '8', '5', '8', '3', '10', '1', '1', '2', '1', '8', '1', '4', '3', '2', '9', '4', '9', '8', '8', '8', '1', '3', '9', '1', '9', '9', '1', '2', '1', '9', '8', '9', '8', '1', '1', '1', '4', '2', '2', '9', '9', '1', '1', '1', '3', '6', '1', '2', '3', '1', '4', '2', '9', '1', '9', '9', '1', '8', '2', '1', '9', '1', '8', '1', '8', '8', '2', '1', '9', '9', '6', '8', '8', '9', '8', '10', '8', '9', '5', '1', '1', '1', '1', '8', '1', '8', '12', '8', '1', '12', '9', '2', '8', '1', '1', '2', '1', '8', '6', '9', '9', '8', '1', '8', '9', '7', '7', '8', '9', '9', '1', '8', '3', '8', '1', '3', '3', '8', '9', '8', '2', '8', '1', '2', '9', '8', '1', '1', '1', '1', '9', '1', '9', '8', '12', '3', '8', '8', '1', '10', '10', '10', '2', '2', '1', '10', '2', '2', '10', '8', '12', '8', '1', '1', '1', '2', '1', '1', '3', '8', '1', '2', '2', '2', '1', '1', '6', '2', '8', '8', '8', '1', '1', '1', '9', '9', '2', '8', '10', '2', '8', '2', '1', '2', '9', '9', '2', '1', '1', '8', '8', '1', '3', '1', '8', '9', '1', '10', '9', '1', '1', '1', '1', '7', '2', '2', '1', '2', '8', '8', '1', '2', '1', '3', '2', '1', '8', '3', '9', '2', '10', '2', '10', '1', '2', '1', '1', '13', '9', '10', '8', '2', '2', '1', '1', '8', '2', '2', '1', '9', '1', '1', '8', '9', '9', '2', '9', '2', '2', '1', '1', '1', '8', '9', '4', '9', '9', '1', '8', '2', '1', '1', '8', '2', '2', '2', '8', '1', '1', '8', '2', '10', '2', '9', '9', '7', '1', '1', '2', '2', '8', '3', '1', '2', '1', '1', '8', '1', '1', '1', '9', '1', '1', '8', '8', '1', '9', '6', '1', '9', '1', '8', '1', '8', '8', '8', '1', '3', '8', '8', '2', '6', '3', '2', '1', '10', '1', '8', '2', '1', '6', '1', '1', '1', '8', '1', '10', '1', '1', '8', '9', '2', '6', '9', '2', '9', '4', '1', '8', '1', '3', '7', '8', '8', '5', '8', '7', '1', '1', '1', '13', '8', '3', '1', '1', '1', '1', '2', '8', '2', '12', '8', '1', '2', '5', '1', '8', '2', '10', '8', '2', '10', '1', '5', '2', '1', '1', '5', '2', '2', '7', '8', '2', '9', '9', '1', '10', '1', '8', '2', '5', '8', '1', '5', '1', '10', '1', '1', '9', '8', '6', '1', '8', '9', '8', '1', '8', '2', '2', '8', '3', '9', '1', '9', '3', '8', '4', '4', '3', '3', '1', '1', '1', '8', '9', '1', '1', '8', '3', '1', '2', '8', '2', '1', '1', '10', '1', '2', '2', '1', '1', '8', '2', '8', '2', '10', '2', '2', '10', '8', '8', '8', '13', '2', '2', '9', '2', '5', '9', '2', '1', '8', '8', '8', '1', '12', '12', '2', '9', '2', '1', '8', '2', '1', '8', '1', '9', '9', '2', '7', '1', '1', '7', '1', '4', '3', '1', '10', '3', '10', '9', '4', '6', '7', '5', '2', '1', '8', '8', '1', '4', '9', '2', '1', '8', '1', '1', '1', '1', '1', '8', '9', '8', '8', '8', '8', '9', '8', '10', '1', '8', '2', '7', '8', '9', '1', '2', '5', '9', '10', '8', '9', '12', '1', '6', '1', '2', '10', '9', '4', '1', '8', '10', '10', '1', '1', '1', '1', '1', '2', '3', '8', '1', '2', '10', '2', '1', '2', '4', '2', '2', '9', '1', '4', '2', '1', '1', '12', '12', '9', '9', '7', '1', '8', '4', '9', '9', '8', '1', '7', '3', '2', '1', '1', '4', '8', '1', '5', '8', '2', '8', '1', '14', '9', '12', '1', '10', '2', '1', '1', '8', '8', '6', '6', '3', '1', '2', '8', '1', '3', '2', '1', '8', '8', '1', '10', '8', '2', '8', '2', '5', '9', '1', '8', '8', '1', '1', '8', '8', '13', '1', '3', '8', '4', '1', '9', '7', '8', '8', '8', '3', '8', '1', '4', '2', '8', '8', '8', '7', '1', '1', '2', '8', '8', '9', '8', '8', '2', '3', '9', '9', '2', '1', '2', '2', '1', '8', '2', '6', '2', '1', '1', '9', '10', '3', '8', '2', '3', '2', '8', '8', '1', '8', '8', '8', '9', '1', '4', '1', '9', '2', '1', '8', '0', '1', '8', '1', '1', '8', '9', '9', '1', '3', '1', '2', '8', '8', '7', '1', '1', '2', '9', '2', '10', '2', '8', '2', '2', '2', '7', '1', '9', '8', '1', '3', '9', '2', '1', '10', '8', '1', '4', '1', '8', '5', '9', '8', '8', '1', '2', '2', '1', '8', '8', '6', '8', '1', '1', '8', '3', '2', '2', '1', '8', '1', '7', '6', '8', '1', '1', '1', '9', '1', '1', '1', '2', '1', '11', '10', '10', '11', '9', '']\n",
            "['8', '1', '8', '2', '8', '8', '8', '9', '7', '8', '1', '2', '1', '1', '8', '4', '8', '8', '12', '3', '3', '7', '3', '12', '1', '8', '8', '8', '8', '8', '8', '8', '1', '9', '5', '9', '9', '9', '11', '8', '8', '8', '4', '8', '8', '8', '8', '1', '3', '9', '3', '7', '1', '2', '9', '9', '7', '8', '8', '1', '10', '7', '8', '8', '9', '8', '7', '9', '9', '12', '7', '2', '8', '1', '11', '11', '1', '7', '7', '12', '1', '9', '8', '10', '12', '7', '8', '2', '8', '9', '9', '1', '8', '9', '1', '7', '12', '10', '10', '10', '8', '3', '7', '9', '8', '9', '1', '8', '8', '2', '7', '2', '9', '9', '11', '8', '8', '12', '12', '7', '8', '12', '4', '9', '3', '1', '12', '1', '1', '8', '8', '3', '8', '8', '8', '8', '9', '1', '8', '8', '10', '1', '8', '2', '8', '8', '7', '3', '8', '2', '4', '4', '9', '8', '10', '12', '12', '1', '1', '9', '1', '1', '1', '8', '2', '2', '8', '1', '1', '2', '2', '1', '2', '8', '1', '9', '9', '8', '8', '4', '2', '9', '9', '8', '3', '4', '3', '1', '8', '8', '2', '1', '9', '7', '8', '8', '1', '12', '3', '8', '2', '4', '2', '9', '12', '1', '4', '1', '8', '8', '8', '2', '2', '8', '9', '8', '8', '8', '10', '9', '8', '7', '9', '1', '1', '9', '4', '2', '4', '2', '2', '1', '7', '8', '11', '11', '3', '9', '2', '4', '8', '9', '1', '8', '1', '1', '4', '9', '1', '1', '8', '8', '2', '1', '8', '4', '2', '8', '9', '8', '8', '2', '8', '8', '8', '7', '1', '1', '1', '2', '1', '1', '8', '7', '8', '8', '12', '2', '12', '12', '8', '10', '12', '8', '3', '3', '12', '10', '1', '8', '12', '1', '8', '8', '2', '8', '4', '7', '8', '7', '10', '8', '10', '9', '8', '12', '12', '1', '8', '8', '3', '8', '8', '8', '8', '8', '1', '9', '8', '11', '1', '1', '1', '9', '8', '1', '9', '2', '3', '11', '8', '9', '9', '9', '2', '1', '8', '8', '9', '7', '1', '4', '9', '4', '8', '8', '4', '8', '12', '9', '4', '8', '2', '10', '10', '10', '8', '9', '9', '8', '8', '12', '7', '1', '8', '8', '8', '4', '1', '1', '1', '1', '1', '1', '8', '1', '9', '8', '9', '9', '4', '8', '12', '9', '8', '8', '2', '8', '8', '8', '6', '9', '8', '3', '7', '8', '8', '4', '12', '8', '8', '9', '12', '12', '9', '8', '2', '9', '2', '3', '1', '12', '8', '10', '9', '9', '9', '10', '10', '3', '8', '12', '1', '4', '2', '1', '10', '8', '2', '8', '4', '8', '9', '1', '9', '9', '10', '10', '1', '4', '9', '2', '4', '9', '1', '1', '3', '10', '3', '3', '8', '7', '3', '8', '9', '9', '12', '4', '8', '12', '2', '2', '4', '1', '9', '9', '4', '1', '4', '2', '8', '12', '2', '3', '10', '10', '9', '8', '9', '9', '1', '12', '8', '8', '8', '12', '4', '1', '8', '8', '1', '9', '8', '8', '2', '1', '8', '9', '8', '3', '3', '3', '1', '8', '8', '9', '1', '10', '9', '9', '9', '9', '5', '9', '9', '8', '8', '8', '8', '8', '8', '8', '8', '11', '12', '8', '8', '1', '8', '9', '11', '2', '2', '2', '2', '3', '1', '2', '2', '8', '2', '4', '9', '1', '2', '9', '8', '2', '8', '9', '9', '3', '10', '9', '9', '2', '8', '9', '8', '12', '12', '1', '3', '8', '8', '8', '2', '7', '7', '7', '7', '3', '9', '1', '9', '8', '9', '9', '1', '1', '1', '2', '9', '9', '9', '11', '1', '8', '8', '9', '1', '9', '8', '8', '8', '1', '1', '8', '7', '1', '1', '8', '8', '9', '4', '4', '8', '2', '2', '8', '8', '8', '8', '8', '8', '11', '8', '2', '9', '4', '9', '3', '9', '9', '1', '3', '9', '3', '1', '12', '8', '9', '12', '1', '8', '4', '2', '1', '4', '8', '3', '3', '8', '2', '8', '9', '7', '8', '8', '8', '5', '8', '3', '9', '8', '8', '13', '12', '1', '1', '2', '8', '4', '1', '9', '9', '12', '8', '9', '12', '9', '1', '9', '9', '9', '9', '3', '2', '9', '9', '4', '8', '12', '2', '4', '9', '3', '1', '9', '7', '8', '9', '9', '8', '4', '8', '8', '7', '9', '10', '3', '8', '8', '8', '1', '1', '1', '1', '8', '8', '4', '1', '10', '1', '5', '7', '7', '1', '8', '9', '3', '7', '2', '7', '7', '2', '4', '8', '12', '7', '4', '2', '9', '9', '12', '6', '10', '8', '2', '4', '12', '9', '9', '3', '8', '8', '1', '2', '10', '9', '9', '8', '4', '12', '2', '1', '8', '8', '8', '12', '10', '10', '9', '3', '8', '8', '9', '2', '8', '10', '1', '1', '1', '1', '2', '1', '1', '1', '1', '9', '8', '12', '9', '4', '8', '8', '9', '1', '9', '3', '9', '8', '8', '1', '7', '7', '10', '1', '8', '8', '1', '9', '8', '10', '3', '1', '7', '1', '8', '8', '12', '8', '8', '1', '8', '7', '1', '7', '7', '8', '2', '1', '8', '8', '2', '10', '8', '8', '8', '8', '8', '10', '1', '8', '8', '12', '8', '3', '3', '2', '2', '2', '10', '8', '8', '8', '2', '9', '1', '8', '9', '3', '2', '8', '10', '8', '6', '1', '1', '8', '4', '1', '9', '10', '8', '1', '7', '1', '2', '8', '1', '1', '1', '12', '1', '9', '12', '8', '12', '12', '12', '8', '8', '12', '4', '8', '8', '8', '8', '9', '9', '1', '3', '3', '3', '3', '1', '12', '12', '9', '10', '8', '8', '1', '9', '2', '2', '13', '9', '8', '9', '2', '1', '9', '1', '8', '8', '8', '4', '8', '1', '1', '1', '12', '12', '7', '2', '2', '2', '8', '3', '8', '9', '2', '10', '7', '8', '9', '2', '1', '2', '12', '12', '8', '8', '9', '2', '2', '9', '11', '1', '8', '1', '10', '9', '2', '1', '4', '7', '7', '7', '7', '7', '12', '8', '8', '8', '1', '1', '10', '1', '12', '1', '8', '2', '1', '1', '12', '8', '7', '9', '12', '8', '9', '3', '9', '8', '8', '8', '8', '3', '11', '2', '2', '9', '8', '8', '10', '8', '2', '7', '3', '1', '4', '7', '8', '8', '1', '8', '3', '7', '12', '8', '10', '9', '9', '8', '8', '2', '8', '9', '9', '1', '2', '8', '8', '9', '8', '3', '8', '1', '8', '10', '9', '8', '9', '9', '12', '4', '4', '8', '9', '9', '8', '2', '10', '1', '2', '8', '9', '1', '9', '9', '9', '7', '12', '12', '8', '1', '1', '1', '1', '8', '3', '1', '1', '8', '1', '8', '8', '7', '8', '8', '8', '8', '3', '2', '2', '10', '10', '10', '7', '8', '1', '2', '12', '7', '9', '8', '7', '8', '12', '2', '8', '9', '2', '6', '6', '7', '9', '8', '1', '8', '9', '8', '1', '12', '1', '2', '8', '7', '7', '7', '8', '2', '2', '8', '1', '2', '2', '9', '9', '1', '8', '8', '4', '3', '3', '1', '6', '3', '3', '12', '3', '8', '9', '1', '4', '3', '1', '8', '3', '9', '2', '8', '2', '8', '8', '8', '1', '1', '1', '9', '9', '8', '1', '9', '8', '1', '1', '3', '10', '8', '1', '1', '3', '9', '1', '4', '4', '1', '8', '9', '9', '2', '0', '0', '1', '8', '3', '1', '8', '8', '9', '8', '8', '1', '1', '8', '9', '8', '8', '8', '7', '9', '8', '8', '8', '10', '9', '8', '1', '2', '6', '1', '9', '9', '8', '12', '12', '12', '8', '8', '2', '8', '1', '2', '2', '2', '1', '9', '8', '2', '12', '2', '8', '12', '8', '9', '8', '8', '9', '7', '1', '1', '1', '1', '1', '8', '8', '1', '8', '8', '1', '1', '3', '2', '8', '8', '9', '10', '10', '2', '2', '1', '9', '2', '9', '9', '4', '12', '12', '12', '10', '7', '3', '3', '4', '2', '2', '9', '2', '8', '4', '2', '4', '1', '10', '9', '7', '8', '7', '1', '1', '3', '3', '1', '1', '3', '3', '3', '1', '1', '1', '1', '8', '2', '3', '1', '1', '2', '8', '8', '12', '8', '8', '8', '8', '11', '9', '1', '8', '9', '2', '8', '8', '8', '3', '9', '1', '9', '2', '7', '2', '8', '2', '8', '10', '8', '1', '10', '1', '1', '9', '9', '8', '8', '1', '8', '8', '8', '12', '8', '8', '8', '1', '8', '8', '8', '1', '9', '1', '1', '8', '1', '8', '9', '8', '2', '12', '9', '9', '0', '1', '8', '8', '1', '8', '12', '8', '8', '10', '8', '8', '8', '7', '8', '1', '8', '7', '3', '10', '1', '8', '9', '1', '8', '8', '8', '10', '1', '10', '3', '9', '1', '8', '9', '2', '8', '3', '3', '9', '9', '7', '9', '1', '1', '9', '2', '1', '1', '1', '7', '1', '1', '8', '8', '1', '1', '8', '1', '8', '3', '12', '9', '3', '3', '8', '8', '8', '8', '3', '1', '3', '3', '1', '11', '0', '8', '8', '7', '8', '12', '1', '8', '9', '8', '9', '8', '8', '3', '8', '8', '1', '1', '1', '9', '2', '2', '2', '8', '7', '12', '8', '8', '9', '10', '10', '7', '8', '1', '9', '8', '7', '3', '1', '3', '8', '2', '2', '3', '9', '8', '4', '4', '8', '9', '2', '1', '1', '7', '8', '9', '9', '7', '8', '7', '7', '8', '2', '2', '8', '4', '9', '7', '10', '0', '9', '8', '3', '7', '8', '1', '1', '8', '9', '9', '2', '2', '10', '1', '9', '10', '10', '10', '8', '3', '2', '12', '9', '9', '10', '12', '9', '12', '12', '9', '1', '2', '4', '12', '12', '7', '8', '9', '7', '7', '7', '3', '9', '8', '9', '1', '12', '8', '9', '4', '1', '3', '12', '12', '12', '12', '8', '8', '2', '1', '1', '2', '1', '1', '1', '12', '12', '8', '12', '2', '2', '12', '3', '3', '12', '8', '2', '8', '8', '12', '2', '1', '10', '3', '2', '8', '7', '1', '8', '1', '3', '7', '8', '9', '8', '3', '1', '1', '7', '8', '8', '9', '8', '2', '9', '2', '2', '9', '8', '1', '8', '8', '1', '3', '3', '1', '1', '10', '1', '2', '8', '1', '1', '1', '1', '9', '1', '4', '1', '7', '7', '7', '7', '2', '2', '8', '8', '12', '1', '9', '1', '7', '3', '3', '1', '8', '8', '10', '8', '9', '2', '9', '1', '3', '8', '8', '3', '12', '2', '8', '12', '2', '9', '1', '3', '3', '3', '3', '2', '8', '7', '9', '8', '3', '3', '1', '7', '8', '3', '1', '1', '12', '8', '9', '1', '2', '3', '8', '1', '1', '3', '3', '9', '1', '1', '1', '12', '1', '7', '3', '3', '1', '8', '8', '8', '1', '2', '4', '8', '1', '10', '2', '5', '3', '3', '12', '10', '9', '9', '12', '9', '0', '2', '8', '8', '9', '9', '9', '8', '1', '3', '1', '1', '4', '8', '1', '10', '8', '7', '2', '8', '2', '8', '4', '7', '8', '1', '9', '1', '9', '8', '2', '8', '2', '7', '9', '2', '2', '9', '1', '8', '12', '1', '8', '1', '4', '1', '9', '9', '1', '10', '12', '4', '8', '1', '7', '3', '9', '2', '12', '7', '8', '8', '2', '1', '12', '9', '8', '1', '2', '2', '8', '10', '2', '1', '7', '7', '7', '12', '3', '3', '8', '3', '8', '8', '3', '9', '8', '9', '1', '1', '8', '7', '9', '3', '3', '8', '1', '0', '9', '9', '9', '1', '8', '9', '9', '10', '1', '8', '7', '8', '8', '8', '9', '9', '8', '9', '10', '4', '9', '3', '7', '12', '1', '9', '9', '8', '9', '1', '9', '3', '1', '8', '4', '12', '10', '9', '8', '7', '10', '8', '12', '12', '3', '10', '8', '12', '2', '1', '2', '3', '9', '8', '8', '7', '1', '1', '2', '2', '1', '2', '2', '7', '1', '3', '9', '9', '3', '8', '8', '8', '8', '8', '7', '8', '8', '10', '8', '1', '8', '2', '8', '2', '2', '2', '2', '2', '1', '7', '12', '10', '1', '2', '8', '1', '4', '7', '8', '1', '12', '8', '7', '9', '2', '2', '2', '8', '1', '8', '1', '1', '0', '2', '2', '6', '1', '8', '2', '1', '8', '1', '1', '2', '2', '3', '2', '2', '10', '10', '9', '1', '7', '7', '8', '8', '1', '12', '10', '12', '3', '8', '8', '8', '3', '8', '3', '10', '2', '2', '2', '1', '2', '2', '1', '0', '1', '8', '9', '1', '1', '1', '2', '2', '2', '2', '2', '1', '1', '1', '1', '12', '9', '9', '1', '10', '11', '12', '9', '1', '1', '8', '8', '1', '2', '12', '8', '3', '7', '2', '10', '9', '7', '8', '1', '2', '2', '7', '9', '12', '2', '2', '1', '8', '9', '2', '3', '2', '2', '2', '9', '9', '8', '9', '2', '3', '8', '8', '9', '7', '3', '10', '8', '8', '9', '1', '1', '2', '8', '1', '10', '8', '8', '9', '7', '1', '7', '12', '8', '1', '7', '2', '1', '3', '9', '2', '8', '8', '1', '2', '3', '9', '3', '8', '9', '7', '7', '1', '10', '2', '8', '8', '8', '1', '8', '2', '8', '8', '1', '2', '8', '10', '2', '1', '0', '8', '3', '8', '10', '10', '12', '3', '8', '9', '3', '10', '8', '8', '8', '1', '8', '8', '2', '2', '2', '2', '2', '2', '1', '1', '10', '1', '10', '8', '2', '1', '2', '2', '8', '3', '3', '2', '2', '1', '8', '1', '3', '1', '9', '2', '2', '1', '3', '2', '2', '3', '2', '2', '3', '2', '1', '1', '2', '1', '2', '7', '8', '2', '1', '12', '12', '3', '1', '9', '9', '7', '8', '7', '2', '7', '9', '2', '2', '2', '8', '9', '7', '9', '2', '8', '9', '9', '2', '9', '1', '1', '9', '8', '3', '3', '8', '2', '7', '8', '11', '2', '8', '9', '1', '8', '3', '1', '2', '1', '2', '2', '2', '2', '8', '3', '9', '2', '10', '9', '9', '2', '7', '7', '7', '1', '3', '8', '8', '1', '1', '8', '7', '1', '8', '8', '9', '3', '3', '3', '2', '4', '8', '12', '8', '1', '12', '9', '2', '8', '2', '12', '2', '1', '3', '8', '12', '12', '11', '8', '10', '9', '2', '9', '4', '10', '1', '12', '12', '9', '3', '8', '9', '12', '9', '9', '2', '2', '2', '3', '5', '1', '1', '1', '7', '7', '8', '2', '2', '8', '2', '1', '7', '3', '2', '9', '8', '9', '1', '1', '9', '8', '9', '3', '3', '2', '9', '4', '9', '8', '8', '1', '8', '10', '2', '0', '3', '8', '8', '8', '9', '12', '9', '8', '2', '2', '8', '10', '9', '8', '2', '2', '4', '1', '9', '9', '8', '8', '10', '3', '1', '1', '2', '8', '7', '8', '2', '1', '1', '8', '12', '2', '9', '4', '8', '3', '3', '3', '8', '2', '12', '12', '8', '9', '12', '8', '2', '12', '7', '9', '2', '2', '1', '3', '1', '3', '11', '9', '8', '1', '10', '2', '12', '3', '8', '4', '1', '2', '8', '8', '3', '9', '3', '8', '7', '1', '1', '3', '1', '8', '2', '9', '8', '12', '1', '1', '1', '9', '9', '1', '8', '8', '8', '2', '2', '12', '3', '1', '1', '1', '9', '9', '3', '2', '1', '1', '9', '9', '1', '9', '9', '1', '9', '3', '9', '1', '8', '1', '9', '8', '3', '8', '8', '9', '9', '8', '2', '8', '1', '2', '2', '10', '1', '2', '1', '1', '9', '1', '9', '3', '7', '7', '2', '10', '3', '3', '1', '1', '9', '1', '2', '1', '3', '2', '1', '1', '1', '1', '1', '1', '1', '1', '1', '2', '8', '8', '7', '1', '1', '8', '8', '2', '1', '12', '1', '8', '1', '9', '8', '8', '2', '1', '9', '9', '2', '2', '8', '1', '7', '7', '9', '8', '1', '6', '8', '8', '2', '2', '8', '3', '8', '3', '2', '3', '8', '1', '2', '2', '2', '8', '9', '9', '9', '12', '1', '9', '2', '1', '9', '2', '9', '9', '10', '8', '1', '1', '9', '2', '1', '7', '1', '3', '7', '1', '1', '1', '3', '8', '8', '8', '9', '9', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '1', '0', '1', '1', '1', '2', '2', '3', '1', '1', '2', '3', '3', '3', '7', '3', '1', '9', '3', '9', '1', '4', '1', '1', '9', '10', '9', '8', '2', '3', '10', '9', '3', '10', '12', '8', '9', '8', '2', '1', '9', '8', '1', '7', '8', '1', '9', '1', '2', '10', '7', '7', '8', '1', '3', '3', '9', '3', '3', '1', '1', '1', '9', '1', '3', '8', '8', '8', '1', '8', '1', '9', '7', '8', '8', '2', '1', '3', '8', '2', '1', '6', '1', '8', '8', '2', '8', '2', '2', '9', '8', '1', '1', '6', '8', '6', '10', '2', '1', '1', '8', '9', '3', '3', '3', '1', '3', '9', '6', '8', '1', '8', '3', '1', '2', '2', '12', '7', '1', '1', '1', '1', '12', '1', '8', '8', '3', '2', '1', '9', '2', '3', '3', '2', '2', '7', '2', '2', '2', '3', '1', '9', '8', '7', '1', '1', '3', '1', '9', '4', '3', '9', '2', '1', '1', '9', '8', '8', '7', '2', '1', '3', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '2', '10', '1', '1', '8', '2', '8', '1', '8', '1', '3', '1', '2', '1', '1', '3', '1', '2', '1', '1', '1', '1', '1', '1', '1', '2', '1', '8', '1', '9', '8', '8', '1', '10', '8', '8', '1', '3', '9', '8', '9', '8', '2', '1', '3', '4', '1', '12', '1', '9', '8', '2', '7', '2', '1', '2', '9', '2', '1', '3', '8', '1', '2', '3', '8', '2', '10', '10', '10', '9', '9', '9', '3', '8', '3', '1', '1', '1', '1', '1', '1', '2', '1', '9', '2', '7', '9', '8', '7', '9', '1', '1', '1', '1', '8', '2', '2', '3', '3', '2', '12', '8', '3', '1', '1', '12', '9', '8', '2', '2', '9', '1', '2', '2', '9', '1', '1', '2', '4', '8', '1', '8', '10', '2', '9', '9', '9', '1', '2', '1', '1', '2', '2', '2', '12', '1', '2', '8', '3', '9', '9', '3', '2', '9', '9', '7', '2', '8', '8', '9', '2', '9', '1', '1', '1', '1', '1', '2', '8', '2', '1', '2', '2', '8', '3', '1', '1', '9', '1', '3', '10', '7', '9', '8', '8', '2', '2', '9', '2', '2', '2', '2', '2', '2', '9', '1', '3', '6', '4', '2', '2', '9', '1', '9', '8', '1', '1', '1', '7', '3', '2', '9', '9', '1', '9', '7', '2', '8', '9', '9', '12', '10', '8', '10', '2', '3', '1', '2', '2', '12', '12', '3', '12', '1', '2', '1', '2', '1', '2', '8', '2', '3', '12', '12', '8', '2', '2', '9', '2', '9', '2', '3', '3', '1', '1', '1', '12', '3', '7', '1', '3', '1', '2', '9', '2', '7', '2', '1', '8', '3', '7', '8', '1', '1', '3', '8', '3', '2', '9', '3', '1', '1', '1', '1', '11', '1', '2', '6', '1', '2', '3', '8', '9', '9', '2', '9', '2', '1', '1', '3', '1', '12', '9', '1', '1', '9', '1', '9', '9', '8', '9', '8', '1', '2', '7', '1', '0', '9', '8', '2', '3', '2', '4', '1', '1', '1', '8', '1', '12', '1', '2', '3', '3', '9', '9', '9', '9', '9', '7', '9', '3', '9', '1', '7', '3', '3', '3', '3', '7', '8', '2', '2', '2', '8', '2', '3', '9', '9', '3', '1', '8', '9', '12', '8', '8', '3', '2', '1', '6', '1', '9', '1', '1', '1', '1', '1', '1', '2', '2', '1', '2', '2', '2', '2', '9', '2', '5', '3', '2', '2', '8', '2', '1', '8', '3', '3', '8', '3', '1', '3', '9', '2', '8', '9', '4', '4', '9', '8', '8', '7', '7', '3', '1', '8', '8', '3', '2', '2', '10', '3', '3', '2', '2', '2', '2', '12', '3', '1', '2', '7', '7', '12', '2', '1', '2', '1', '1', '2', '1', '3', '3', '3', '1', '3', '9', '4', '8', '8', '4', '2', '4', '2', '8', '9', '1', '1', '2', '2', '2', '8', '7', '2', '8', '2', '9', '1', '8', '1', '1', '2', '1', '1', '1', '2', '9', '9', '8', '1', '8', '1', '4', '8', '1', '2', '7', '9', '2', '1', '8', '9', '8', '9', '1', '1', '2', '3', '2', '12', '7', '1', '2', '4', '4', '4', '2', '2', '2', '9', '8', '8', '1', '12', '3', '3', '2', '2', '3', '12', '1', '5', '1', '1', '3', '2', '8', '8', '2', '2', '2', '3', '9', '8', '8', '9', '8', '8', '11', '8', '2', '9', '2', '3', '1', '7', '1', '8', '2', '8', '1', '1', '1', '8', '1', '1', '1', '7', '8', '8', '2', '3', '2', '1', '8', '8', '1', '8', '4', '8', '9', '8', '9', '8', '1', '2', '3', '2', '1', '2', '2', '3', '1', '9', '4', '8', '1', '1', '9', '1', '1', '8', '2', '2', '2', '1', '7', '3', '3', '1', '3', '3', '12', '3', '1', '1', '3', '1', '1', '4', '8', '4', '4', '3', '3', '3', '4', '8', '1', '9', '9', '9', '4', '1', '7', '4', '8', '4', '9', '8', '1', '1', '3', '8', '9', '1', '9', '7', '1', '1', '9', '1', '8', '8', '8', '1', '1', '8', '2', '2', '2', '9', '4', '4', '1', '2', '1', '2', '9', '1', '1', '8', '5', '5', '5', '9', '12', '4', '1', '2', '1', '8', '8', '1', '9', '9', '2', '12', '1', '1', '7', '8', '8', '4', '1', '2', '8', '11', '2', '3', '2', '2', '1', '11', '2', '2', '2', '2', '9', '9', '2', '2', '9', '3', '1', '9', '1', '1', '1', '7', '8', '1', '8', '9', '1', '2', '9', '8', '1', '2', '9', '8', '12', '2', '2', '10', '8', '2', '2', '8', '8', '1', '2', '6', '1', '1', '7', '7', '2', '3', '2', '1', '1', '8', '1', '10', '8', '12', '6', '4', '1', '4', '1', '2', '9', '8', '9', '4', '8', '8', '10', '8', '8', '8', '8', '8', '2', '2', '8', '4', '9', '3', '3', '3', '3', '3', '9', '2', '1', '1', '2', '3', '2', '1', '2', '3', '3', '3', '2', '2', '3', '3', '2', '1', '2', '3', '3', '3', '3', '12', '9', '3', '2', '2', '3', '1', '5', '1', '1', '3', '2', '2', '1', '2', '9', '3', '9', '8', '1', '8', '7', '7', '9', '1', '1', '7', '9', '10', '1', '7', '1', '4', '3', '8', '9', '4', '9', '9', '2', '9', '2', '8', '4', '2', '2', '9', '5', '2', '1', '10', '9', '3', '1', '1', '1', '2', '1', '2', '9', '1', '7', '9', '9', '8', '2', '2', '8', '9', '1', '9', '8', '2', '3', '2', '2', '1', '2', '2', '2', '1', '1', '8', '1', '9', '4', '12', '8', '2', '7', '2', '2', '1', '9', '3', '1', '8', '12', '1', '1', '2', '1', '10', '2', '6', '12', '12', '4', '9', '1', '9', '10', '1', '2', '8', '2', '1', '8', '6', '12', '9', '9', '2', '1', '7', '8', '9', '5', '8', '1', '8', '8', '3', '1', '8', '2', '8', '2', '2', '1', '2', '9', '2', '8', '1', '12', '8', '2', '7', '3', '3', '12', '2', '3', '3', '9', '9', '3', '3', '3', '3', '3', '2', '1', '4', '1', '4', '8', '8', '2', '1', '2', '9', '12', '1', '10', '12', '1', '2', '9', '4', '8', '10', '3', '2', '8', '7', '4', '8', '9', '2', '7', '1', '10', '2', '2', '2', '8', '4', '2', '2', '1', '1', '8', '7', '9', '2', '1', '9', '8', '9', '12', '4', '8', '2', '8', '7', '7', '1', '9', '2', '1', '1', '9', '1', '8', '2', '3', '9', '9', '1', '3', '2', '9', '2', '2', '8', '1', '1', '2', '1', '2', '1', '9', '8', '2', '2', '4', '8', '8', '9', '5', '5', '9', '8', '9', '6', '2', '1', '12', '2', '3', '8', '8', '9', '2', '2', '1', '3', '2', '10', '9', '10', '7', '2', '8', '4', '2', '8', '9', '8', '1', '3', '8', '8', '1', '8', '3', '8', '8', '1', '10', '3', '1', '5', '8', '8', '9', '2', '2', '9', '2', '1', '9', '1', '4', '1', '12', '8', '8', '2', '1', '1', '1', '1', '1', '9', '2', '9', '8', '9', '5', '9', '1', '8', '2', '4', '1', '1', '1', '9', '12', '4', '1', '1', '9', '8', '4', '1', '8', '9', '8', '8', '9', '9', '9', '9', '9', '9', '1', '3', '4', '2', '2', '8', '2', '9', '2', '3', '8', '3', '1', '7', '2', '1', '1', '4', '2', '9', '2', '1', '3', '2', '9', '2', '9', '3', '8', '1', '9', '1', '1', '2', '8', '8', '8', '2', '3', '9', '9', '8', '2', '4', '1', '5', '1', '1', '9', '9', '2', '1', '1', '1', '9', '9', '1', '3', '9', '2', '2', '9', '9', '8', '3', '9', '9', '8', '2', '2', '8', '9', '2', '2', '8', '9', '10', '10', '2', '8', '7', '9', '9', '4', '2', '4', '11', '2', '1', '7', '8', '8', '8', '9', '4', '9', '8', '8', '2', '1', '1', '9', '8', '3', '3', '8', '8', '7', '9', '1', '3', '4', '8', '10', '2', '4', '4', '8', '2', '8', '2', '8', '9', '2', '1', '2', '2', '3', '1', '9', '4', '5', '9', '9', '1', '1', '1', '1', '1', '1', '7', '1', '1', '1', '8', '1', '1', '1', '1', '1', '9', '1', '2', '1', '12', '9', '3', '9', '9', '4', '1', '9', '1', '1', '2', '11', '9', '3', '2', '2', '2', '4', '2', '3', '2', '7', '7', '8', '1', '10', '9', '5', '7', '1', '10', '8', '1', '2', '7', '1', '12', '5', '8', '4', '2', '9', '9', '8', '2', '9', '8', '8', '10', '2', '3', '2', '7', '2', '8', '10', '3', '2', '9', '9', '9', '1', '1', '1', '1', '8', '2', '10', '9', '8', '1', '2', '2', '2', '1', '2', '1', '3', '9', '12', '2', '2', '4', '8', '1', '3', '9', '4', '9', '1', '9', '9', '2', '1', '1', '9', '7', '10', '3', '10', '2', '9', '2', '9', '2', '8', '2', '1', '2', '3', '1', '1', '1', '8', '5', '8', '3', '1', '1', '4', '2', '2', '1', '3', '8', '2', '1', '1', '1', '2', '9', '4', '1', '8', '1', '2', '8', '2', '9', '2', '2', '5', '5', '2', '5', '2', '1', '8', '8', '1', '3', '12', '2', '4', '3', '2', '2', '2', '6', '2', '3', '3', '1', '8', '2', '2', '1', '1', '8', '8', '10', '1', '9', '2', '9', '12', '0', '2', '1', '3', '2', '2', '1', '2', '1', '9', '9', '2', '9', '8', '8', '1', '7', '8', '1', '2', '6', '9', '8', '10', '1', '12', '2', '2', '1', '8', '2', '1', '12', '7', '4', '2', '10', '2', '2', '1', '2', '1', '2', '1', '1', '2', '1', '9', '8', '10', '2', '10', '9', '9', '12', '5', '3', '2', '8', '2', '8', '3', '3', '1', '4', '9', '10', '2', '4', '8', '1', '2', '10', '12', '12', '3', '1', '1', '2', '9', '6', '6', '1', '9', '1', '8', '1', '8', '8', '8', '8', '2', '2', '2', '8', '8', '2', '1', '1', '1', '1', '1', '8', '8', '5', '8', '8', '1', '8', '9', '9', '1', '7', '12', '9', '9', '7', '2', '2', '7', '8', '9', '8', '2', '9', '6', '3', '4', '8', '4', '1', '2', '9', '8', '8', '8', '2', '1', '1', '8', '8', '3', '1', '9', '9', '7', '1', '2', '2', '2', '9', '4', '1', '2', '12', '1', '8', '2', '1', '9', '1', '8', '9', '9', '2', '5', '4', '2', '1', '2', '8', '12', '8', '10', '3', '8', '8', '8', '2', '2', '8', '2', '8', '2', '4', '8', '10', '2', '7', '2', '1', '8', '8', '9', '5', '8', '1', '12', '7', '10', '2', '2', '2', '1', '4', '8', '12', '1', '8', '2', '2', '9', '1', '8', '3', '1', '1', '5', '10', '2', '1', '2', '4', '8', '9', '3', '2', '8', '7', '4', '4', '9', '9', '2', '8', '1', '1', '2', '4', '7', '1', '9', '1', '1', '1', '8', '4', '1', '3', '1', '2', '2', '9', '1', '8', '8', '2', '2', '9', '9', '1', '8', '8', '1', '9', '8', '8', '2', '2', '2', '2', '1', '1', '1', '7', '4', '1', '1', '4', '2', '3', '3', '2', '3', '9', '2', '8', '2', '1', '5', '1', '2', '2', '1', '3', '5', '2', '9', '1', '1', '9', '8', '1', '4', '8', '1', '8', '2', '4', '8', '4', '7', '9', '8', '1', '2', '2', '4', '4', '11', '3', '11', '1', '3', '8', '1', '9', '6', '8', '9', '3', '9', '8', '9', '2', '3', '3', '7', '1', '7', '2', '9', '9', '1', '8', '8', '1', '5', '5', '8', '8', '1', '2', '1', '3', '9', '9', '5', '9', '8', '1', '4', '3', '8', '1', '1', '2', '1', '8', '9', '8', '2', '2', '2', '1', '2', '4', '10', '8', '1', '9', '1', '8', '1', '1', '9', '9', '8', '11', '1', '8', '2', '1', '2', '8', '2', '8', '8', '2', '9', '6', '10', '1', '8', '6', '4', '5', '11', '2', '2', '8', '1', '8', '1', '8', '1', '4', '8', '1', '1', '1', '8', '3', '7', '3', '3', '7', '1', '1', '4', '8', '1', '6', '4', '2', '2', '1', '1', '1', '1', '6', '2', '2', '8', '1', '8', '5', '5', '2', '1', '1', '2', '3', '7', '1', '1', '6', '2', '9', '3', '9', '8', '9', '2', '1', '4', '2', '9', '13', '9', '2', '8', '12', '4', '1', '1', '9', '5', '1', '1', '1', '1', '8', '2', '1', '1', '2', '9', '0', '12', '1', '1', '8', '9', '8', '3', '2', '2', '12', '8', '2', '2', '2', '1', '10', '1', '9', '8', '9', '5', '2', '2', '9', '2', '8', '4', '8', '3', '7', '9', '2', '2', '7', '2', '2', '8', '1', '1', '9', '10', '9', '8', '2', '9', '0', '7', '1', '1', '1', '9', '8', '2', '8', '12', '8', '3', '2', '8', '8', '7', '7', '8', '12', '2', '1', '3', '9', '2', '2', '8', '2', '8', '8', '12', '8', '8', '1', '8', '9', '2', '4', '4', '7', '2', '1', '1', '9', '3', '7', '1', '3', '9', '8', '2', '2', '3', '1', '3', '10', '2', '3', '7', '1', '8', '1', '1', '10', '3', '10', '8', '4', '1', '2', '4', '3', '10', '9', '3', '9', '5', '8', '7', '2', '9', '3', '3', '2', '11', '8', '1', '1', '7', '2', '7', '2', '9', '4', '8', '1', '12', '8', '7', '9', '1', '0', '2', '6', '4', '9', '3', '2', '4', '12', '9', '8', '5', '9', '2', '8', '4', '6', '8', '9', '3', '1', '8', '7', '3', '1', '1', '1', '8', '9', '1', '8', '7', '9', '8', '2', '1', '2', '2', '3', '2', '2', '1', '1', '4', '9', '7', '3', '2', '8', '8', '9', '1', '9', '7', '4', '2', '2', '8', '5', '9', '1', '5', '7', '1', '4', '6', '9', '10', '1', '8', '2', '9', '1', '9', '2', '9', '8', '2', '9', '9', '11', '2', '12', '2', '2', '9', '4', '8', '1', '9', '9', '2', '8', '9', '1', '2', '9', '3', '8', '8', '1', '7', '8', '8', '2', '3', '2', '2', '2', '2', '9', '8', '10', '2', '2', '1', '9', '11', '1', '8', '8', '2', '4', '2', '1', '2', '8', '8', '9', '2', '8', '9', '2', '3', '1', '2', '1', '3', '10', '8', '1', '2', '4', '1', '9', '8', '9', '3', '3', '9', '8', '2', '11', '3', '7', '1', '8', '7', '1', '8', '8', '8', '1', '4', '8', '8', '1', '9', '3', '1', '1', '2', '8', '8', '8', '2', '1', '8', '10', '2', '12', '8', '8', '9', '1', '8', '1', '3', '2', '2', '10', '7', '2', '2', '1', '1', '8', '2', '9', '1', '2', '9', '3', '3', '10', '1', '4', '7', '10', '12', '2', '4', '10', '8', '6', '2', '9', '1', '8', '3', '1', '2', '8', '8', '2', '12', '10', '2', '10', '9', '1', '2', '5', '8', '12', '8', '8', '7', '8', '8', '1', '8', '2', '2', '9', '1', '8', '10', '5', '5', '5', '8', '1', '9', '1', '13', '1', '2', '2', '1', '2', '4', '9', '2', '1', '9', '13', '11', '1', '9', '9', '3', '10', '8', '7', '8', '8', '4', '2', '1', '9', '4', '3', '1', '1', '10', '9', '2', '8', '6', '2', '1', '1', '3', '2', '8', '1', '1', '1', '1', '2', '4', '1', '1', '10', '1', '8', '1', '1', '9', '1', '7', '1', '1', '9', '9', '2', '12', '9', '1', '8', '1', '12', '1', '8', '3', '8', '1', '8', '1', '2', '10', '1', '1', '2', '2', '9', '3', '9', '12', '1', '9', '1', '2', '8', '7', '2', '1', '1', '3', '2', '8', '4', '4', '5', '12', '7', '8', '9', '6', '1', '8', '9', '2', '1', '9', '1', '4', '1', '8', '2', '1', '1', '8', '7', '4', '9', '1', '8', '4', '2', '7', '5', '1', '3', '9', '2', '8', '2', '1', '2', '9', '1', '1', '2', '3', '4', '3', '1', '2', '2', '10', '2', '2', '8', '1', '1', '2', '4', '9', '2', '11', '9', '8', '9', '2', '1', '10', '4', '1', '1', '10', '7', '2', '10', '8', '1', '10', '8', '8', '8', '8', '8', '8', '2', '2', '2', '8', '3', '4', '1', '1', '1', '2', '1', '8', '8', '2', '8', '3', '8', '3', '1', '2', '3', '1', '1', '10', '1', '4', '4', '3', '3', '1', '9', '1', '1', '2', '1', '1', '6', '1', '1', '1', '9', '1', '4', '1', '1', '1', '1', '1', '1', '2', '12', '8', '8', '1', '9', '12', '10', '8', '2', '12', '1', '1', '2', '1', '7', '10', '7', '1', '2', '1', '10', '8', '2', '2', '8', '2', '2', '1', '9', '8', '8', '8', '8', '3', '1', '4', '2', '8', '3', '10', '10', '1', '9', '1', '9', '1', '9', '1', '8', '3', '1', '1', '8', '8', '8', '6', '3', '4', '1', '1', '1', '9', '9', '5', '2', '10', '2', '6', '2', '2', '7', '1', '8', '2', '11', '1', '1', '1', '2', '1', '9', '4', '4', '2', '8', '4', '6', '8', '8', '10', '10', '2', '1', '2', '9', '1', '8', '8', '3', '4', '8', '8', '2', '8', '8', '1', '2', '1', '2', '9', '4', '1', '3', '3', '3', '8', '8', '8', '2', '6', '4', '3', '3', '12', '3', '4', '9', '6', '8', '7', '7', '8', '6', '4', '1', '10', '6', '3', '3', '8', '1', '1', '8', '1', '11', '8', '1', '1', '2', '3', '11', '7', '2', '1', '1', '1', '9', '1', '2', '1', '1', '9', '8', '1', '1', '8', '0', '4', '1', '1', '2', '1', '1', '2', '2', '9', '1', '10', '1', '3', '8', '9', '3', '3', '1', '10', '1', '9', '1', '1', '7', '4', '1', '8', '10', '1', '2', '10', '1', '1', '2', '3', '3', '9', '1', '8', '10', '1', '7', '9', '1', '9', '6', '3', '9', '2', '8', '4', '12', '12', '3', '1', '1', '9', '1', '9', '2', '1', '1', '1', '1', '1', '9', '2', '1', '2', '10', '9', '8', '8', '10', '8', '2', '3', '2', '1', '12', '8', '12', '2', '9', '1', '3', '2', '5', '2', '2', '8', '2', '2', '4', '10', '9', '10', '9', '4', '2', '4', '12', '8', '2', '1', '8', '3', '9', '2', '9', '4', '1', '1', '1', '1', '1', '9', '6', '2', '9', '2', '1', '3', '2', '2', '5', '8', '9', '1', '6', '3', '1', '2', '2', '2', '6', '1', '8', '10', '8', '3', '3', '13', '2', '9', '9', '10', '6', '12', '8', '2', '9', '2', '3', '8', '8', '1', '2', '2', '3', '2', '10', '1', '8', '9', '1', '8', '9', '8', '8', '2', '10', '2', '1', '1', '1', '9', '9', '12', '1', '1', '4', '8', '3', '2', '2', '4', '4', '2', '2', '1', '1', '1', '9', '2', '2', '4', '1', '1', '8', '8', '8', '9', '2', '13', '9', '9', '4', '1', '9', '7', '10', '10', '8', '1', '9', '1', '1', '1', '8', '12', '4', '1', '2', '1', '1', '1', '7', '8', '3', '3', '1', '2', '2', '7', '2', '2', '10', '1', '8', '4', '0', '1', '9', '4', '1', '1', '2', '10', '10', '7', '9', '3', '12', '11', '2', '8', '10', '8', '9', '4', '3', '4', '10', '1', '1', '9', '3', '10', '1', '5', '3', '3', '2', '2', '1', '1', '6', '1', '7', '1', '1', '12', '1', '1', '1', '10', '1', '8', '8', '1', '3', '8', '3', '1', '2', '8', '3', '4', '2', '8', '8', '6', '1', '1', '4', '1', '2', '9', '5', '1', '7', '8', '9', '2', '4', '9', '8', '5', '9', '9', '9', '1', '2', '1', '3', '8', '9', '2', '9', '8', '9', '1', '8', '13', '9', '3', '8', '1', '4', '1', '3', '1', '6', '4', '10', '2', '2', '7', '8', '8', '12', '8', '9', '9', '9', '10', '3', '12', '12', '2', '9', '10', '9', '7', '2', '3', '4', '10', '10', '2', '7', '7', '1', '7', '3', '9', '1', '9', '8', '2', '9', '1', '8', '1', '10', '8', '8', '2', '7', '8', '1', '8', '9', '8', '1', '4', '1', '8', '8', '8', '1', '1', '10', '3', '2', '2', '6', '8', '9', '8', '2', '1', '1', '3', '8', '8', '1', '4', '4', '3', '9', '10', '4', '2', '4', '2', '9', '2', '2', '9', '1', '1', '10', '1', '1', '2', '1', '1', '1', '9', '1', '8', '8', '8', '2', '3', '8', '1', '6', '7', '3', '1', '9', '1', '2', '1', '6', '1', '2', '10', '1', '4', '2', '3', '2', '9', '9', '4', '9', '9', '1', '2', '8', '4', '7', '8', '7', '13', '9', '1', '2', '1', '9', '3', '3', '6', '8', '9', '10', '9', '2', '2', '3', '12', '1', '1', '1', '13', '2', '1', '7', '10', '8', '9', '1', '2', '9', '1', '1', '1', '2', '1', '12', '5', '6', '9', '10', '3', '1', '10', '1', '3', '2', '8', '9', '2', '10', '8', '8', '9', '2', '8', '9', '2', '2', '2', '8', '8', '2', '3', '1', '4', '8', '1', '9', '9', '2', '8', '10', '9', '1', '2', '12', '8', '8', '0', '2', '1', '1', '9', '10', '1', '6', '2', '2', '10', '2', '2', '4', '2', '2', '12', '1', '6', '7', '8', '9', '1', '3', '5', '7', '3', '1', '3', '1', '1', '3', '2', '6', '3', '2', '1', '8', '3', '5', '2', '1', '1', '8', '1', '1', '0', '2', '3', '5', '3', '1', '1', '9', '8', '8', '8', '8', '4', '7', '2', '9', '12', '5', '9', '3', '12', '3', '9', '1', '9', '1', '7', '3', '9', '3', '10', '1', '2', '1', '4', '9', '8', '2', '9', '11', '2', '8', '9', '4', '1', '1', '1', '1', '1', '1', '1', '9', '1', '9', '1', '1', '1', '12', '3', '10', '3', '6', '1', '7', '10', '8', '3', '7', '1', '1', '2', '6', '1', '3', '9', '2', '8', '1', '8', '8', '1', '8', '10', '12', '1', '10', '12', '10', '1', '8', '8', '1', '4', '7', '9', '2', '1', '9', '3', '8', '12', '10', '6', '1', '6', '8', '3', '1', '3', '1', '10', '10', '9', '8', '1', '2', '9', '8', '8', '1', '9', '8', '8', '3', '1', '3', '9', '12', '1', '8', '1', '5', '8', '11', '5', '5', '1', '2', '1', '3', '1', '1', '1', '9', '8', '8', '1', '9', '10', '8', '4', '1', '1', '8', '10', '8', '9', '1', '10', '1', '7', '2', '1', '1', '9', '6', '2', '9', '8', '1', '1', '1', '9', '8', '8', '1', '9', '9', '1', '1', '2', '7', '9', '2', '9', '9', '8', '7', '6', '8', '7', '8', '2', '9', '2', '1', '8', '8', '8', '2', '6', '3', '1', '8', '12', '12', '8', '7', '1', '2', '2', '7', '1', '9', '8', '1', '4', '1', '1', '8', '5', '8', '1', '8', '8', '8', '8', '1', '2', '11', '1', '1', '1', '2', '7', '1', '9', '2', '2', '2', '9', '8', '9', '2', '9', '1', '8', '1', '7', '11', '13', '1', '9', '8', '2', '2', '1', '2', '3', '10', '3', '10', '4', '3', '9', '1', '8', '1', '1', '13', '1', '1', '3', '8', '1', '2', '2', '2', '9', '4', '1', '8', '1', '9', '3', '6', '8', '5', '2', '8', '2', '8', '1', '9', '2', '2', '8', '1', '1', '4', '8', '9', '2', '2', '7', '1', '8', '12', '2', '12', '8', '2', '9', '9', '3', '8', '1', '1', '9', '9', '9', '2', '8', '1', '9', '1', '8', '9', '8', '2', '2', '4', '1', '9', '10', '10', '8', '8', '1', '1', '2', '1', '4', '1', '2', '1', '1', '3', '12', '8', '8', '8', '8', '10', '2', '8', '12', '1', '9', '9', '8', '9', '1', '2', '1', '8', '8', '8', '2', '8', '8', '3', '10', '8', '9', '1', '1', '1', '3', '4', '8', '10', '6', '3', '1', '3', '2', '8', '9', '3', '5', '4', '1', '5', '2', '9', '9', '9', '1', '1', '1', '9', '9', '6', '10', '9', '8', '12', '2', '9', '1', '2', '1', '1', '1', '8', '1', '12', '1', '12', '1', '9', '1', '8', '1', '2', '1', '2', '8', '8', '10', '1', '9', '1', '2', '2', '8', '8', '3', '12', '1', '11', '2', '10', '12', '11', '2', '1', '10', '1', '1', '9', '3', '10', '2', '7', '1', '8', '1', '9', '9', '2', '1', '3', '12', '5', '9', '9', '8', '1', '8', '8', '8', '1', '1', '3', '9', '8', '9', '8', '3', '10', '3', '7', '8', '9', '2', '3', '1', '9', '10', '9', '2', '8', '2', '6', '4', '1', '1', '3', '8', '2', '3', '9', '1', '2', '1', '8', '1', '2', '2', '2', '2', '9', '9', '4', '8', '9', '1', '1', '2', '9', '1', '5', '2', '9', '8', '10', '1', '2', '10', '8', '5', '6', '4', '8', '1', '1', '1', '8', '9', '2', '8', '8', '8', '12', '2', '2', '1', '8', '1', '2', '1', '9', '8', '8', '9', '1', '1', '8', '7', '1', '8', '0', '2', '3', '8', '1', '1', '8', '1', '1', '12', '3', '2', '10', '8', '10', '6', '1', '8', '8', '10', '8', '8', '1', '4', '8', '1', '2', '9', '8', '1', '1', '3', '3', '5', '1', '1', '2', '1', '2', '1', '9', '8', '3', '9', '8', '2', '10', '9', '1', '8', '1', '10', '9', '10', '1', '2', '3', '8', '1', '3', '1', '9', '9', '8', '1', '9', '8', '8', '9', '1', '9', '2', '8', '8', '2', '13', '8', '10', '8', '9', '3', '2', '1', '3', '1', '12', '10', '10', '1', '11', '1', '2', '4', '2', '10', '1', '8', '11', '1', '1', '1', '2', '2', '8', '2', '9', '8', '9', '9', '12', '8', '1', '9', '2', '4', '1', '8', '3', '9', '1', '6', '5', '8', '9', '3', '3', '2', '1', '1', '1', '8', '11', '1', '8', '7', '1', '11', '9', '1', '7', '9', '9', '8', '8', '8', '12', '8', '1', '2', '9', '8', '8', '2', '10', '9', '8', '8', '4', '10', '9', '8', '2', '10', '10', '2', '1', '2', '1', '2', '1', '4', '1', '7', '1', '1', '3', '12', '9', '8', '5', '9', '10', '1', '9', '9', '2', '7', '4', '1', '1', '8', '12', '8', '2', '2', '9', '4', '1', '1', '5', '9', '1', '8', '12', '8', '1', '1', '4', '9', '10', '2', '3', '1', '3', '3', '3', '8', '1', '2', '1', '2', '1', '2', '9', '1', '2', '8', '9', '4', '8', '8', '10', '2', '12', '5', '5', '1', '8', '1', '7', '8', '1', '1', '8', '9', '12', '4', '9', '3', '1', '2', '5', '9', '2', '1', '2', '2', '1', '2', '8', '1', '8', '8', '2', '2', '1', '8', '4', '1', '2', '10', '9', '10', '8', '1', '10', '4', '8', '8', '10', '2', '8', '1', '1', '3', '10', '1', '4', '2', '8', '3', '3', '2', '9', '8', '5', '5', '9', '3', '10', '8', '1', '2', '1', '2', '9', '1', '1', '2', '9', '9', '8', '8', '11', '9', '4', '1', '8', '2', '7', '1', '2', '8', '2', '9', '9', '8', '2', '9', '9', '8', '8', '1', '2', '8', '1', '1', '2', '9', '8', '1', '4', '9', '1', '8', '12', '1', '12', '2', '8', '2', '9', '1', '12', '1', '2', '9', '1', '1', '9', '3', '2', '9', '9', '2', '11', '8', '7', '9', '9', '9', '8', '8', '1', '2', '8', '1', '1', '4', '1', '2', '10', '1', '9', '2', '1', '1', '1', '9', '1', '13', '4', '3', '2', '1', '1', '2', '2', '7', '8', '7', '1', '1', '2', '8', '8', '3', '4', '8', '2', '10', '13', '8', '8', '9', '2', '2', '1', '1', '10', '4', '2', '7', '9', '8', '2', '1', '9', '8', '1', '1', '9', '8', '9', '8', '2', '8', '2', '2', '9', '2', '1', '9', '1', '2', '4', '8', '2', '1', '1', '8', '1', '4', '1', '9', '3', '9', '2', '7', '1', '9', '6', '1', '10', '2', '2', '2', '2', '2', '10', '10', '10', '9', '9', '9', '3', '12', '10', '1', '1', '10', '1', '1', '2', '2', '1', '2', '3', '12', '8', '8', '1', '2', '9', '9', '1', '1', '10', '8', '9', '8', '3', '1', '1', '3', '1', '10', '1', '1', '1', '4', '1', '1', '1', '7', '10', '2', '1', '8', '1', '9', '3', '1', '10', '8', '8', '1', '5', '9', '1', '2', '1', '8', '8', '1', '3', '1', '10', '2', '1', '1', '2', '8', '2', '5', '3', '5', '2', '1', '1', '9', '1', '7', '9', '9', '2', '8', '1', '12', '4', '1', '4', '9', '9', '4', '8', '1', '10', '10', '8', '8', '8', '9', '2', '3', '5', '1', '8', '1', '1', '1', '7', '10', '1', '4', '12', '3', '2', '2', '9', '1', '1', '1', '2', '8', '4', '1', '7', '3', '12', '8', '6', '2', '2', '7', '9', '11', '1', '10', '9', '1', '8', '12', '2', '11', '1', '2', '3', '1', '1', '1', '8', '9', '2', '2', '2', '2', '3', '3', '8', '10', '4', '1', '2', '9', '8', '2', '2', '9', '1', '8', '11', '4', '2', '8', '7', '1', '1', '7', '3', '9', '9', '4', '5', '8', '10', '2', '9', '9', '9', '8', '1', '9', '2', '2', '2', '1', '2', '9', '4', '3', '12', '4', '3', '2', '3', '8', '3', '10', '1', '10', '1', '1', '8', '10', '6', '8', '9', '9', '1', '2', '9', '2', '9', '3', '8', '1', '1', '12', '1', '5', '1', '10', '8', '10', '2', '7', '1', '1', '1', '1', '3', '2', '3', '5', '9', '1', '2', '1', '9', '8', '10', '9', '8', '1', '8', '1', '8', '2', '8', '1', '2', '8', '1', '2', '5', '8', '12', '2', '1', '1', '1', '1', '1', '8', '8', '2', '1', '4', '2', '8', '8', '8', '2', '9', '9', '11', '1', '2', '9', '3', '1', '1', '8', '9', '2', '4', '10', '1', '9', '8', '4', '9', '8', '10', '8', '8', '9', '2', '8', '9', '3', '3', '3', '4', '3', '2', '2', '10', '8', '2', '1', '5', '1', '9', '1', '1', '2', '1', '2', '3', '1', '1', '8', '8', '1', '10', '8', '8', '1', '6', '4', '8', '2', '5', '8', '1', '3', '8', '2', '8', '9', '1', '1', '8', '12', '10', '1', '5', '8', '2', '8', '10', '9', '9', '1', '6', '9', '1', '10', '8', '2', '9', '10', '1', '2', '1', '1', '8', '8', '3', '9', '8', '1', '10', '2', '8', '1', '10', '1', '5', '1', '1', '1', '5', '1', '1', '1', '4', '1', '1', '1', '8', '2', '8', '1', '8', '3', '8', '9', '1', '8', '1', '8', '1', '1', '2', '2', '1', '12', '9', '9', '1', '8', '2', '1', '2', '8', '1', '9', '1', '1', '2', '1', '2', '2', '2', '1', '9', '9', '8', '8', '1', '1', '9', '10', '8', '4', '3', '3', '4', '9', '1', '3', '3', '10', '2', '2', '1', '8', '4', '1', '9', '4', '1', '1', '9', '8', '10', '4', '9', '9', '8', '2', '1', '3', '4', '9', '3', '8', '9', '1', '1', '2', '1', '7', '8', '2', '9', '1', '9', '2', '6', '9', '10', '8', '1', '1', '5', '9', '5', '1', '9', '10', '9', '3', '3', '8', '8', '2', '9', '2', '8', '2', '1', '8', '5', '8', '3', '10', '1', '1', '2', '1', '8', '1', '4', '3', '2', '9', '4', '9', '8', '8', '8', '1', '3', '9', '1', '9', '9', '1', '2', '1', '9', '8', '9', '8', '1', '1', '1', '4', '2', '2', '9', '9', '1', '1', '1', '3', '6', '1', '2', '3', '1', '4', '2', '9', '1', '9', '9', '1', '8', '2', '1', '9', '1', '8', '1', '8', '8', '2', '1', '9', '9', '6', '8', '8', '9', '8', '10', '8', '9', '5', '1', '1', '1', '1', '8', '1', '8', '12', '8', '1', '12', '9', '2', '8', '1', '1', '2', '1', '8', '6', '9', '9', '8', '1', '8', '9', '7', '7', '8', '9', '9', '1', '8', '3', '8', '1', '3', '3', '8', '9', '8', '2', '8', '1', '2', '9', '8', '1', '1', '1', '1', '9', '1', '9', '8', '12', '3', '8', '8', '1', '10', '10', '10', '2', '2', '1', '10', '2', '2', '10', '8', '12', '8', '1', '1', '1', '2', '1', '1', '3', '8', '1', '2', '2', '2', '1', '1', '6', '2', '8', '8', '8', '1', '1', '1', '9', '9', '2', '8', '10', '2', '8', '2', '1', '2', '9', '9', '2', '1', '1', '8', '8', '1', '3', '1', '8', '9', '1', '10', '9', '1', '1', '1', '1', '7', '2', '2', '1', '2', '8', '8', '1', '2', '1', '3', '2', '1', '8', '3', '9', '2', '10', '2', '10', '1', '2', '1', '1', '13', '9', '10', '8', '2', '2', '1', '1', '8', '2', '2', '1', '9', '1', '1', '8', '9', '9', '2', '9', '2', '2', '1', '1', '1', '8', '9', '4', '9', '9', '1', '8', '2', '1', '1', '8', '2', '2', '2', '8', '1', '1', '8', '2', '10', '2', '9', '9', '7', '1', '1', '2', '2', '8', '3', '1', '2', '1', '1', '8', '1', '1', '1', '9', '1', '1', '8', '8', '1', '9', '6', '1', '9', '1', '8', '1', '8', '8', '8', '1', '3', '8', '8', '2', '6', '3', '2', '1', '10', '1', '8', '2', '1', '6', '1', '1', '1', '8', '1', '10', '1', '1', '8', '9', '2', '6', '9', '2', '9', '4', '1', '8', '1', '3', '7', '8', '8', '5', '8', '7', '1', '1', '1', '13', '8', '3', '1', '1', '1', '1', '2', '8', '2', '12', '8', '1', '2', '5', '1', '8', '2', '10', '8', '2', '10', '1', '5', '2', '1', '1', '5', '2', '2', '7', '8', '2', '9', '9', '1', '10', '1', '8', '2', '5', '8', '1', '5', '1', '10', '1', '1', '9', '8', '6', '1', '8', '9', '8', '1', '8', '2', '2', '8', '3', '9', '1', '9', '3', '8', '4', '4', '3', '3', '1', '1', '1', '8', '9', '1', '1', '8', '3', '1', '2', '8', '2', '1', '1', '10', '1', '2', '2', '1', '1', '8', '2', '8', '2', '10', '2', '2', '10', '8', '8', '8', '13', '2', '2', '9', '2', '5', '9', '2', '1', '8', '8', '8', '1', '12', '12', '2', '9', '2', '1', '8', '2', '1', '8', '1', '9', '9', '2', '7', '1', '1', '7', '1', '4', '3', '1', '10', '3', '10', '9', '4', '6', '7', '5', '2', '1', '8', '8', '1', '4', '9', '2', '1', '8', '1', '1', '1', '1', '1', '8', '9', '8', '8', '8', '8', '9', '8', '10', '1', '8', '2', '7', '8', '9', '1', '2', '5', '9', '10', '8', '9', '12', '1', '6', '1', '2', '10', '9', '4', '1', '8', '10', '10', '1', '1', '1', '1', '1', '2', '3', '8', '1', '2', '10', '2', '1', '2', '4', '2', '2', '9', '1', '4', '2', '1', '1', '12', '12', '9', '9', '7', '1', '8', '4', '9', '9', '8', '1', '7', '3', '2', '1', '1', '4', '8', '1', '5', '8', '2', '8', '1', '14', '9', '12', '1', '10', '2', '1', '1', '8', '8', '6', '6', '3', '1', '2', '8', '1', '3', '2', '1', '8', '8', '1', '10', '8', '2', '8', '2', '5', '9', '1', '8', '8', '1', '1', '8', '8', '13', '1', '3', '8', '4', '1', '9', '7', '8', '8', '8', '3', '8', '1', '4', '2', '8', '8', '8', '7', '1', '1', '2', '8', '8', '9', '8', '8', '2', '3', '9', '9', '2', '1', '2', '2', '1', '8', '2', '6', '2', '1', '1', '9', '10', '3', '8', '2', '3', '2', '8', '8', '1', '8', '8', '8', '9', '1', '4', '1', '9', '2', '1', '8', '0', '1', '8', '1', '1', '8', '9', '9', '1', '3', '1', '2', '8', '8', '7', '1', '1', '2', '9', '2', '10', '2', '8', '2', '2', '2', '7', '1', '9', '8', '1', '3', '9', '2', '1', '10', '8', '1', '4', '1', '8', '5', '9', '8', '8', '1', '2', '2', '1', '8', '8', '6', '8', '1', '1', '8', '3', '2', '2', '1', '8', '1', '7', '6', '8', '1', '1', '1', '9', '1', '1', '1', '2', '1', '11', '10', '10', '11', '9']\n",
            "     filenum                                               text  label\n",
            "0          1  scope of what is meant by the equivalent of an...      8\n",
            "1          2  time during which the silence has endured, can...      1\n",
            "2          4  on original Indian title were held to be outsi...      2\n",
            "3          6  who framed it. Every word appears to have been...      8\n",
            "4          7  in of the Act, the general policy declarations...      8\n",
            "...      ...                                                ...    ...\n",
            "5270    8411  of life without the possibility of parole. In ...      1\n",
            "5271    8412  habeas relief. We reverse. In Ylst, we said th...      1\n",
            "5272    8413  existence. Novelty in procedural requirements ...      2\n",
            "5273    8414  circumstances marked 'yes' in Section II outwe...      1\n",
            "5274    8417  it, the States' claims of ownership prior to t...     10\n",
            "\n",
            "[5275 rows x 3 columns]\n"
          ]
        }
      ],
      "source": [
        "fh = open('sc_model_4.txt')\n",
        "filenums=[]\n",
        "records=[]\n",
        "for record in fh:\n",
        "    record=record.split(\"---\")\n",
        "    filenum=record[0]\n",
        "    modelnum=record[1]\n",
        "    text=record[2]\n",
        "    filenums.append(filenum)\n",
        "    records.append(text)\n",
        "    \n",
        "print(len(filenums))\n",
        "print(len(records))\n",
        "\n",
        "summarized_data = pd.DataFrame(list(zip(filenums, records)),columns =['filenum','text'])\n",
        "len_list = [len(ele.split()) for ele in records]\n",
        "res = 0 if len(len_list) == 0 else (float(sum(len_list)) / len(len_list))\n",
        "print(\"Average Length %s\" % res)\n",
        "\n",
        "temp_file = open(\"labels_sc.txt\", \"r\")\n",
        "#temp_file = open(\"labels_sc_279.txt\", \"r\")\n",
        "\n",
        "data = temp_file.read()\n",
        "\n",
        "label_list = data.split(\"\\n\")\n",
        "print(label_list)\n",
        "label_list = label_list[0:-1]\n",
        "print(label_list)\n",
        "label_list = [int(i) for i in label_list]\n",
        "temp_file.close()\n",
        "\n",
        "\n",
        "new_label_list=[]\n",
        "for num in filenums:\n",
        "  new_label_list.append(label_list[int(num)-1])\n",
        "summarized_data['label'] = new_label_list\n",
        "\n",
        "print(summarized_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kenM-hJwR4q",
        "outputId": "1d3e2cde-3a51-4dcc-e61d-2f50c6b36e4c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_1[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_roberta_model[0][0]']       \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 15)           7695        ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 8  1  2 ...  2  1 10]\n",
            "[   1    2    4 ... 8413 8414 8417]\n",
            "[6622   35 6713 4989 8233 7444 5061 8344 5464  317 4515 3001 6991 2875\n",
            " 3885 6647 4857 4504  214 8340 2969 3847 4609 1346 5993 7358 2752 7651\n",
            " 8188 7073 6069 2346 6483 4952 5072 1967 4071 7590 5030 7297  626 5463\n",
            " 6029 6302 7256 3993 4263 3835 5801 4293 8129 5866 5777 4837 3843 1418\n",
            " 5274 6205 2356 5997 6685 3310 7721 5928 2360 7197 4252 8343  909 4354\n",
            " 4752  649 3488 2496 5765 6612 4004 4052 1403 6697 5268 6293 6380 4602\n",
            " 6057 4577 4236 7978 5083 6552  361 7706 4186 8351  812 5108 6600 8264\n",
            " 8064 7121 8279 5556 5119 7947 3350 7152 5199 4282 5025 5051 1927 3615\n",
            " 1221 8062 1489 2031 7351  234 8015 7416 7921 6574 6257 8396  941 4520\n",
            " 3300 5640  240 7810 7425 1321 6580 5404 8273 6742   94    9 7861 7975\n",
            " 1018 2140 1415 7061 7767 4990 8124 5927 6051 3269 6916 2846 6818 7052\n",
            " 7775 7582 8116 4767 5248 1812 6055 8331  998 1564 7326  292 3065 6764\n",
            " 6854 4915 3583 5902 4229 2144 7565 3712 7224 4502 1406 1651 3827 6525\n",
            " 6318 2686 5369 8235 1704 1011 3351 8021  102  224 8294 7707 2985 5146\n",
            " 3709 6824 2281 6837 5436 2649 4886  966 5476 1199 8387 5449 1028 2226\n",
            " 6166 4387  350 6123 5236 5041  734 3412 6085  587  489 4982 5115 5575\n",
            " 3839 6743 7173 5724 7689 7787 8302 3150 2247 8128 6660 3505 4610 2866\n",
            " 4161 6905 7096  768 1062 4411 6087 2096 4903 7546 6630 2928 4640 7125\n",
            " 1792 2063 1700 7928 8095  737 5849  922 2359 6015 4489 4361 6081 6346\n",
            " 5014 5167 1023 8009  268 4777 5221 4323 6168 5702 6129 7181 1860 7192\n",
            " 5467 6579   67 2693 6134 4270 4611 6746 7696 3466 3950 6125 8370 6142\n",
            " 6131  995 5548 1721 5067 4015 7092 3156   25 1749 4878 8248 6836 5310\n",
            " 6960 2339 1650 6227 4104 5130 7089 6993 1289 4305 5585 7933 6284 7057\n",
            " 3807 5790 8347 5280 2841 7693 8090 4171 5260  133 7451 3957 1719 4743\n",
            " 4746 5435 6216 4858 4417 8355 4492 2648 5104 5278 7308  158 7543 2168\n",
            " 5913 7111 3414 7592   39 7157 5647 4555 3956 3617 4630 4384 6596 4539\n",
            " 6749 7202 2413 4749 5122 5929  252 5565  336 6008 4564 5580 7237 5958\n",
            " 7657 4327 5091 6918 6487 2989 7448 4715 3382  467 3784 4419 1117 7954\n",
            " 7122 3174 1534 4164 7526  498 5944 4572 2307 2929 7480 4412 4563 7692\n",
            " 1869 2472 6966 6549 1594 2276 7269 2545 2216  605 6587 6315 5297 5282\n",
            " 1619 5504 6399 3917 6342 5721 7909 6126 8382 4383 5502 7130  485 4936\n",
            " 7674 5372 7109 4605 5211 5462 8065   50 4080 3334 8167 8378 5595 1873\n",
            " 8207 7076 4106 3719 4627  231 2405 6973 3480 3246 5931 8208 4825 6203\n",
            " 3019 5822   63 5854 6543 5787 4764 6701 6562 3822 4861 1315 8318 8169\n",
            " 8218 7549  707 1076 2465 4636   80 7025 7127 4026 4961 7531  744 6684\n",
            " 4297 7939 7384  446 6675 7752 5279 8196 3020 4192 4253 2887 2556 5729\n",
            " 1898 3208 5973 7033 1431 5915 8312 5799 7105  511 6404 2852 1561]\n"
          ]
        }
      ],
      "source": [
        "gc.collect()\n",
        "tf.keras.backend.clear_session()\n",
        "dbert_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "dbert_model = TFRobertaModel.from_pretrained('roberta-base')\n",
        "max_len=512\n",
        "sentences=summarized_data['text']\n",
        "labels=summarized_data['label']\n",
        "filenumbers=summarized_data['filenum']\n",
        "len(sentences),len(labels),len(filenumbers)\n",
        "\n",
        "model_4=create_model()\n",
        "input_ids=[]\n",
        "attention_masks=[]\n",
        "\n",
        "for sent in sentences:\n",
        "  dbert_inps=dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n",
        "  input_ids.append(dbert_inps['input_ids'])\n",
        "  attention_masks.append(dbert_inps['attention_mask'])\n",
        "input_ids=np.asarray(input_ids)\n",
        "\n",
        "attention_masks=np.array(attention_masks)\n",
        "labels=np.array(labels)\n",
        "filenumbers=[int(i) for i in filenumbers]\n",
        "filenumbers=np.array(filenumbers)\n",
        "print(labels)\n",
        "print(filenumbers)\n",
        "\n",
        "#train_inp,val_inp,train_label,val_label,train_mask,val_mask,train_fnum,val_fnum=train_test_split(input_ids,labels,attention_masks,filenumbers,test_size=0.1,random_state=42)\n",
        "\n",
        "new_train_inp_4=[]\n",
        "new_val_inp_4=[]\n",
        "new_train_label_4=[]\n",
        "new_val_label_4=[]\n",
        "new_train_mask_4=[]\n",
        "new_train_fnum_4=[]\n",
        "new_val_fnum_4=[]\n",
        "new_val_mask_4=[]\n",
        "for i in range(len(train_fnum)):\n",
        "  if train_fnum[i] in filenumbers:\n",
        "    new_train_inp_4.append(train_inp[i])\n",
        "    #new_val_inp.append(val_inp[i])\n",
        "    new_train_label_4.append(train_label[i])\n",
        "    #new_val_label.append(val_label[i])\n",
        "    new_train_mask_4.append(train_mask[i])\n",
        "    #new_val_mask.append(val_mask[i])\n",
        "    new_train_fnum_4.append(train_fnum[i])\n",
        "\n",
        "for i in range(len(val_fnum)):\n",
        "  if val_fnum[i] in filenumbers:\n",
        "    #new_train_inp.append(train_inp[i])\n",
        "    new_val_inp_4.append(val_inp[i])\n",
        "    #new_train_label.append(train_label[i])\n",
        "    new_val_label_4.append(val_label[i])\n",
        "    #new_train_mask.append(train_mask[i])\n",
        "    new_val_mask_4.append(val_mask[i])\n",
        "    new_val_fnum_4.append(val_fnum[i])\n",
        "\n",
        "# print(new_train_label)\n",
        "# print(len(new_train_label))\n",
        "# print(new_val_label)\n",
        "# print(len(new_val_label))\n",
        "new_train_inp_4=np.array(new_train_inp_4)\n",
        "new_val_inp_4=np.array(new_val_inp_4)\n",
        "new_train_label_4=np.array(new_train_label_4)\n",
        "new_val_label_4=np.array(new_val_label_4)\n",
        "new_train_mask_4=np.array(new_train_mask_4)\n",
        "new_train_fnum_4=np.array(new_train_fnum_4)\n",
        "new_val_fnum_4=np.array(new_val_fnum_4)\n",
        "new_val_mask_4=np.array(new_val_mask_4)\n",
        "\n",
        "print(new_val_fnum_4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "qAa4P7OtO0QK",
        "outputId": "85f74def-7989-40e3-dc03-ef954b3f1971"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_1[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_roberta_model[0][0]']       \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 15)           7695        ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 8  1  2 ...  2  1 10]\n",
            "[   1    2    4 ... 8413 8414 8417]\n",
            "[6622   35 6713 4989 8233 7444 5061 8344 5464  317 4515 3001 6991 2875\n",
            " 3885 6647 4857 4504  214 8340 2969 3847 4609 1346 5993 7358 2752 7651\n",
            " 8188 7073 6069 2346 6483 4952 5072 1967 4071 7590 5030 7297  626 5463\n",
            " 6029 6302 7256 3993 4263 3835 5801 4293 8129 5866 5777 4837 3843 1418\n",
            " 5274 6205 2356 5997 6685 3310 7721 5928 2360 7197 4252 8343  909 4354\n",
            " 4752  649 3488 2496 5765 6612 4004 4052 1403 6697 5268 6293 6380 4602\n",
            " 6057 4577 4236 7978 5083 6552  361 7706 4186 8351  812 5108 6600 8264\n",
            " 8064 7121 8279 5556 5119 7947 3350 7152 5199 4282 5025 5051 1927 3615\n",
            " 1221 8062 1489 2031 7351  234 8015 7416 7921 6574 6257 8396  941 4520\n",
            " 3300 5640  240 7810 7425 1321 6580 5404 8273 6742   94    9 7861 7975\n",
            " 1018 2140 1415 7061 7767 4990 8124 5927 6051 3269 6916 2846 6818 7052\n",
            " 7775 7582 8116 4767 5248 1812 6055 8331  998 1564 7326  292 3065 6764\n",
            " 6854 4915 3583 5902 4229 2144 7565 3712 7224 4502 1406 1651 3827 6525\n",
            " 6318 2686 5369 8235 1704 1011 3351 8021  102  224 8294 7707 2985 5146\n",
            " 3709 6824 2281 6837 5436 2649 4886  966 5476 1199 8387 5449 1028 2226\n",
            " 6166 4387  350 6123 5236 5041  734 3412 6085  587  489 4982 5115 5575\n",
            " 3839 6743 7173 5724 7689 7787 8302 3150 2247 8128 6660 3505 4610 2866\n",
            " 4161 6905 7096  768 1062 4411 6087 2096 4903 7546 6630 2928 4640 7125\n",
            " 1792 2063 1700 7928 8095  737 5849  922 2359 6015 4489 4361 6081 6346\n",
            " 5014 5167 1023 8009  268 4777 5221 4323 6168 5702 6129 7181 1860 7192\n",
            " 5467 6579   67 2693 6134 4270 4611 6746 7696 3466 3950 6125 8370 6142\n",
            " 6131  995 5548 1721 5067 4015 7092 3156   25 1749 4878 8248 6836 5310\n",
            " 6960 2339 1650 6227 4104 5130 7089 6993 1289 4305 5585 7933 6284 7057\n",
            " 3807 5790 8347 5280 2841 7693 8090 4171 5260  133 7451 3957 1719 4743\n",
            " 4746 5435 6216 4858 4417 8355 4492 2648 5104 5278 7308  158 7543 2168\n",
            " 5913 7111 3414 7592   39 7157 5647 4555 3956 3617 4630 4384 6596 4539\n",
            " 6749 7202 2413 4749 5122 5929  252 5565  336 6008 4564 5580 7237 5958\n",
            " 7657 4327 5091 6918 6487 2989 7448 4715 3382  467 3784 4419 1117 7954\n",
            " 7122 3174 1534 4164 7526  498 5944 4572 2307 2929 7480 4412 4563 7692\n",
            " 1869 2472 6966 6549 1594 2276 7269 2545 2216  605 6587 6315 5297 5282\n",
            " 1619 5504 6399 3917 6342 5721 7909 6126 8382 4383 5502 7130  485 4936\n",
            " 7674 5372 7109 4605 5211 5462 8065   50 4080 3334 8167 8378 5595 1873\n",
            " 8207 7076 4106 3719 4627  231 2405 6973 3480 3246 5931 8208 4825 6203\n",
            " 3019 5822   63 5854 6543 5787 4764 6701 6562 3822 4861 1315 8318 8169\n",
            " 8218 7549  707 1076 2465 4636   80 7025 7127 4026 4961 7531  744 6684\n",
            " 4297 7939 7384  446 6675 7752 5279 8196 3020 4192 4253 2887 2556 5729\n",
            " 1898 3208 5973 7033 1431 5915 8312 5799 7105  511 6404 2852 1561]\n",
            "Sun Jun  5 16:14:43 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P0    37W / 300W |  15237MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "595/595 [==============================] - 222s 347ms/step - loss: 7.6703 - accuracy: 0.5555 - val_loss: 6.9577 - val_accuracy: 0.7234\n",
            "Epoch 2/5\n",
            "595/595 [==============================] - 207s 347ms/step - loss: 6.7326 - accuracy: 0.7497 - val_loss: 6.5508 - val_accuracy: 0.7447\n",
            "Epoch 3/5\n",
            "595/595 [==============================] - 203s 341ms/step - loss: 6.2918 - accuracy: 0.7995 - val_loss: 6.3881 - val_accuracy: 0.7311\n",
            "Epoch 4/5\n",
            "595/595 [==============================] - 206s 347ms/step - loss: 5.8946 - accuracy: 0.8512 - val_loss: 5.9974 - val_accuracy: 0.7602\n",
            "Epoch 5/5\n",
            "595/595 [==============================] - 202s 340ms/step - loss: 5.5269 - accuracy: 0.8894 - val_loss: 5.9988 - val_accuracy: 0.7466\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_3[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_roberta_model[1][0]']       \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 15)           7695        ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.7292161520190024\n",
            "Weighted F1: 0.7286083085079803\n",
            "Micro F1: 0.7292161520190024\n",
            "Weighted Precision: 0.7347144180268118\n",
            "Micro Precision: 0.7292161520190024\n",
            "Weighted Recall: 0.7292161520190024\n",
            "Micro Recall: 0.7292161520190024\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_1[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_roberta_model[0][0]']       \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 15)           7695        ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 8  1  2 ...  2  1 10]\n",
            "[   1    2    4 ... 8413 8414 8417]\n",
            "[6622   35 6713 4989 8233 7444 5061 8344 5464  317 4515 3001 6991 2875\n",
            " 3885 6647 4857 4504  214 8340 2969 3847 4609 1346 5993 7358 2752 7651\n",
            " 8188 7073 6069 2346 6483 4952 5072 1967 4071 7590 5030 7297  626 5463\n",
            " 6029 6302 7256 3993 4263 3835 5801 4293 8129 5866 5777 4837 3843 1418\n",
            " 5274 6205 2356 5997 6685 3310 7721 5928 2360 7197 4252 8343  909 4354\n",
            " 4752  649 3488 2496 5765 6612 4004 4052 1403 6697 5268 6293 6380 4602\n",
            " 6057 4577 4236 7978 5083 6552  361 7706 4186 8351  812 5108 6600 8264\n",
            " 8064 7121 8279 5556 5119 7947 3350 7152 5199 4282 5025 5051 1927 3615\n",
            " 1221 8062 1489 2031 7351  234 8015 7416 7921 6574 6257 8396  941 4520\n",
            " 3300 5640  240 7810 7425 1321 6580 5404 8273 6742   94    9 7861 7975\n",
            " 1018 2140 1415 7061 7767 4990 8124 5927 6051 3269 6916 2846 6818 7052\n",
            " 7775 7582 8116 4767 5248 1812 6055 8331  998 1564 7326  292 3065 6764\n",
            " 6854 4915 3583 5902 4229 2144 7565 3712 7224 4502 1406 1651 3827 6525\n",
            " 6318 2686 5369 8235 1704 1011 3351 8021  102  224 8294 7707 2985 5146\n",
            " 3709 6824 2281 6837 5436 2649 4886  966 5476 1199 8387 5449 1028 2226\n",
            " 6166 4387  350 6123 5236 5041  734 3412 6085  587  489 4982 5115 5575\n",
            " 3839 6743 7173 5724 7689 7787 8302 3150 2247 8128 6660 3505 4610 2866\n",
            " 4161 6905 7096  768 1062 4411 6087 2096 4903 7546 6630 2928 4640 7125\n",
            " 1792 2063 1700 7928 8095  737 5849  922 2359 6015 4489 4361 6081 6346\n",
            " 5014 5167 1023 8009  268 4777 5221 4323 6168 5702 6129 7181 1860 7192\n",
            " 5467 6579   67 2693 6134 4270 4611 6746 7696 3466 3950 6125 8370 6142\n",
            " 6131  995 5548 1721 5067 4015 7092 3156   25 1749 4878 8248 6836 5310\n",
            " 6960 2339 1650 6227 4104 5130 7089 6993 1289 4305 5585 7933 6284 7057\n",
            " 3807 5790 8347 5280 2841 7693 8090 4171 5260  133 7451 3957 1719 4743\n",
            " 4746 5435 6216 4858 4417 8355 4492 2648 5104 5278 7308  158 7543 2168\n",
            " 5913 7111 3414 7592   39 7157 5647 4555 3956 3617 4630 4384 6596 4539\n",
            " 6749 7202 2413 4749 5122 5929  252 5565  336 6008 4564 5580 7237 5958\n",
            " 7657 4327 5091 6918 6487 2989 7448 4715 3382  467 3784 4419 1117 7954\n",
            " 7122 3174 1534 4164 7526  498 5944 4572 2307 2929 7480 4412 4563 7692\n",
            " 1869 2472 6966 6549 1594 2276 7269 2545 2216  605 6587 6315 5297 5282\n",
            " 1619 5504 6399 3917 6342 5721 7909 6126 8382 4383 5502 7130  485 4936\n",
            " 7674 5372 7109 4605 5211 5462 8065   50 4080 3334 8167 8378 5595 1873\n",
            " 8207 7076 4106 3719 4627  231 2405 6973 3480 3246 5931 8208 4825 6203\n",
            " 3019 5822   63 5854 6543 5787 4764 6701 6562 3822 4861 1315 8318 8169\n",
            " 8218 7549  707 1076 2465 4636   80 7025 7127 4026 4961 7531  744 6684\n",
            " 4297 7939 7384  446 6675 7752 5279 8196 3020 4192 4253 2887 2556 5729\n",
            " 1898 3208 5973 7033 1431 5915 8312 5799 7105  511 6404 2852 1561]\n",
            "Sun Jun  5 16:33:49 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P0    37W / 300W |  15237MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "595/595 [==============================] - 221s 347ms/step - loss: 7.6438 - accuracy: 0.5450 - val_loss: 6.9453 - val_accuracy: 0.7021\n",
            "Epoch 2/5\n",
            "595/595 [==============================] - 206s 347ms/step - loss: 6.6560 - accuracy: 0.7598 - val_loss: 6.5436 - val_accuracy: 0.7369\n",
            "Epoch 3/5\n",
            "595/595 [==============================] - 202s 340ms/step - loss: 6.1783 - accuracy: 0.8182 - val_loss: 6.3172 - val_accuracy: 0.7292\n",
            "Epoch 4/5\n",
            "595/595 [==============================] - 202s 340ms/step - loss: 5.7751 - accuracy: 0.8590 - val_loss: 6.0340 - val_accuracy: 0.7311\n",
            "Epoch 5/5\n",
            "595/595 [==============================] - 206s 346ms/step - loss: 5.3952 - accuracy: 0.9058 - val_loss: 5.9976 - val_accuracy: 0.7447\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_3[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_roberta_model[1][0]']       \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 15)           7695        ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.7161520190023754\n",
            "Weighted F1: 0.7060741156912325\n",
            "Micro F1: 0.7161520190023754\n",
            "Weighted Precision: 0.7150347814427972\n",
            "Micro Precision: 0.7161520190023754\n",
            "Weighted Recall: 0.7161520190023754\n",
            "Micro Recall: 0.7161520190023754\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_1[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_roberta_model[0][0]']       \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 15)           7695        ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 8  1  2 ...  2  1 10]\n",
            "[   1    2    4 ... 8413 8414 8417]\n",
            "[6622   35 6713 4989 8233 7444 5061 8344 5464  317 4515 3001 6991 2875\n",
            " 3885 6647 4857 4504  214 8340 2969 3847 4609 1346 5993 7358 2752 7651\n",
            " 8188 7073 6069 2346 6483 4952 5072 1967 4071 7590 5030 7297  626 5463\n",
            " 6029 6302 7256 3993 4263 3835 5801 4293 8129 5866 5777 4837 3843 1418\n",
            " 5274 6205 2356 5997 6685 3310 7721 5928 2360 7197 4252 8343  909 4354\n",
            " 4752  649 3488 2496 5765 6612 4004 4052 1403 6697 5268 6293 6380 4602\n",
            " 6057 4577 4236 7978 5083 6552  361 7706 4186 8351  812 5108 6600 8264\n",
            " 8064 7121 8279 5556 5119 7947 3350 7152 5199 4282 5025 5051 1927 3615\n",
            " 1221 8062 1489 2031 7351  234 8015 7416 7921 6574 6257 8396  941 4520\n",
            " 3300 5640  240 7810 7425 1321 6580 5404 8273 6742   94    9 7861 7975\n",
            " 1018 2140 1415 7061 7767 4990 8124 5927 6051 3269 6916 2846 6818 7052\n",
            " 7775 7582 8116 4767 5248 1812 6055 8331  998 1564 7326  292 3065 6764\n",
            " 6854 4915 3583 5902 4229 2144 7565 3712 7224 4502 1406 1651 3827 6525\n",
            " 6318 2686 5369 8235 1704 1011 3351 8021  102  224 8294 7707 2985 5146\n",
            " 3709 6824 2281 6837 5436 2649 4886  966 5476 1199 8387 5449 1028 2226\n",
            " 6166 4387  350 6123 5236 5041  734 3412 6085  587  489 4982 5115 5575\n",
            " 3839 6743 7173 5724 7689 7787 8302 3150 2247 8128 6660 3505 4610 2866\n",
            " 4161 6905 7096  768 1062 4411 6087 2096 4903 7546 6630 2928 4640 7125\n",
            " 1792 2063 1700 7928 8095  737 5849  922 2359 6015 4489 4361 6081 6346\n",
            " 5014 5167 1023 8009  268 4777 5221 4323 6168 5702 6129 7181 1860 7192\n",
            " 5467 6579   67 2693 6134 4270 4611 6746 7696 3466 3950 6125 8370 6142\n",
            " 6131  995 5548 1721 5067 4015 7092 3156   25 1749 4878 8248 6836 5310\n",
            " 6960 2339 1650 6227 4104 5130 7089 6993 1289 4305 5585 7933 6284 7057\n",
            " 3807 5790 8347 5280 2841 7693 8090 4171 5260  133 7451 3957 1719 4743\n",
            " 4746 5435 6216 4858 4417 8355 4492 2648 5104 5278 7308  158 7543 2168\n",
            " 5913 7111 3414 7592   39 7157 5647 4555 3956 3617 4630 4384 6596 4539\n",
            " 6749 7202 2413 4749 5122 5929  252 5565  336 6008 4564 5580 7237 5958\n",
            " 7657 4327 5091 6918 6487 2989 7448 4715 3382  467 3784 4419 1117 7954\n",
            " 7122 3174 1534 4164 7526  498 5944 4572 2307 2929 7480 4412 4563 7692\n",
            " 1869 2472 6966 6549 1594 2276 7269 2545 2216  605 6587 6315 5297 5282\n",
            " 1619 5504 6399 3917 6342 5721 7909 6126 8382 4383 5502 7130  485 4936\n",
            " 7674 5372 7109 4605 5211 5462 8065   50 4080 3334 8167 8378 5595 1873\n",
            " 8207 7076 4106 3719 4627  231 2405 6973 3480 3246 5931 8208 4825 6203\n",
            " 3019 5822   63 5854 6543 5787 4764 6701 6562 3822 4861 1315 8318 8169\n",
            " 8218 7549  707 1076 2465 4636   80 7025 7127 4026 4961 7531  744 6684\n",
            " 4297 7939 7384  446 6675 7752 5279 8196 3020 4192 4253 2887 2556 5729\n",
            " 1898 3208 5973 7033 1431 5915 8312 5799 7105  511 6404 2852 1561]\n",
            "Sun Jun  5 16:52:47 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P0    37W / 300W |  15237MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "595/595 [==============================] - 223s 348ms/step - loss: 7.6734 - accuracy: 0.5433 - val_loss: 6.9393 - val_accuracy: 0.7176\n",
            "Epoch 2/5\n",
            "595/595 [==============================] - 206s 347ms/step - loss: 6.6843 - accuracy: 0.7512 - val_loss: 6.5736 - val_accuracy: 0.7253\n",
            "Epoch 3/5\n",
            "595/595 [==============================] - 206s 346ms/step - loss: 6.2175 - accuracy: 0.8090 - val_loss: 6.3694 - val_accuracy: 0.7273\n",
            "Epoch 4/5\n",
            "595/595 [==============================] - 206s 346ms/step - loss: 5.8446 - accuracy: 0.8459 - val_loss: 6.0258 - val_accuracy: 0.7582\n",
            "Epoch 5/5\n",
            "595/595 [==============================] - 202s 340ms/step - loss: 5.4625 - accuracy: 0.8886 - val_loss: 5.9438 - val_accuracy: 0.7466\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_3[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_roberta_model[1][0]']       \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 15)           7695        ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.7315914489311164\n",
            "Weighted F1: 0.7242153114071438\n",
            "Micro F1: 0.7315914489311163\n",
            "Weighted Precision: 0.7261471108110715\n",
            "Micro Precision: 0.7315914489311164\n",
            "Weighted Recall: 0.7315914489311164\n",
            "Micro Recall: 0.7315914489311164\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_1[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_roberta_model[0][0]']       \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 15)           7695        ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 8  1  2 ...  2  1 10]\n",
            "[   1    2    4 ... 8413 8414 8417]\n",
            "[6622   35 6713 4989 8233 7444 5061 8344 5464  317 4515 3001 6991 2875\n",
            " 3885 6647 4857 4504  214 8340 2969 3847 4609 1346 5993 7358 2752 7651\n",
            " 8188 7073 6069 2346 6483 4952 5072 1967 4071 7590 5030 7297  626 5463\n",
            " 6029 6302 7256 3993 4263 3835 5801 4293 8129 5866 5777 4837 3843 1418\n",
            " 5274 6205 2356 5997 6685 3310 7721 5928 2360 7197 4252 8343  909 4354\n",
            " 4752  649 3488 2496 5765 6612 4004 4052 1403 6697 5268 6293 6380 4602\n",
            " 6057 4577 4236 7978 5083 6552  361 7706 4186 8351  812 5108 6600 8264\n",
            " 8064 7121 8279 5556 5119 7947 3350 7152 5199 4282 5025 5051 1927 3615\n",
            " 1221 8062 1489 2031 7351  234 8015 7416 7921 6574 6257 8396  941 4520\n",
            " 3300 5640  240 7810 7425 1321 6580 5404 8273 6742   94    9 7861 7975\n",
            " 1018 2140 1415 7061 7767 4990 8124 5927 6051 3269 6916 2846 6818 7052\n",
            " 7775 7582 8116 4767 5248 1812 6055 8331  998 1564 7326  292 3065 6764\n",
            " 6854 4915 3583 5902 4229 2144 7565 3712 7224 4502 1406 1651 3827 6525\n",
            " 6318 2686 5369 8235 1704 1011 3351 8021  102  224 8294 7707 2985 5146\n",
            " 3709 6824 2281 6837 5436 2649 4886  966 5476 1199 8387 5449 1028 2226\n",
            " 6166 4387  350 6123 5236 5041  734 3412 6085  587  489 4982 5115 5575\n",
            " 3839 6743 7173 5724 7689 7787 8302 3150 2247 8128 6660 3505 4610 2866\n",
            " 4161 6905 7096  768 1062 4411 6087 2096 4903 7546 6630 2928 4640 7125\n",
            " 1792 2063 1700 7928 8095  737 5849  922 2359 6015 4489 4361 6081 6346\n",
            " 5014 5167 1023 8009  268 4777 5221 4323 6168 5702 6129 7181 1860 7192\n",
            " 5467 6579   67 2693 6134 4270 4611 6746 7696 3466 3950 6125 8370 6142\n",
            " 6131  995 5548 1721 5067 4015 7092 3156   25 1749 4878 8248 6836 5310\n",
            " 6960 2339 1650 6227 4104 5130 7089 6993 1289 4305 5585 7933 6284 7057\n",
            " 3807 5790 8347 5280 2841 7693 8090 4171 5260  133 7451 3957 1719 4743\n",
            " 4746 5435 6216 4858 4417 8355 4492 2648 5104 5278 7308  158 7543 2168\n",
            " 5913 7111 3414 7592   39 7157 5647 4555 3956 3617 4630 4384 6596 4539\n",
            " 6749 7202 2413 4749 5122 5929  252 5565  336 6008 4564 5580 7237 5958\n",
            " 7657 4327 5091 6918 6487 2989 7448 4715 3382  467 3784 4419 1117 7954\n",
            " 7122 3174 1534 4164 7526  498 5944 4572 2307 2929 7480 4412 4563 7692\n",
            " 1869 2472 6966 6549 1594 2276 7269 2545 2216  605 6587 6315 5297 5282\n",
            " 1619 5504 6399 3917 6342 5721 7909 6126 8382 4383 5502 7130  485 4936\n",
            " 7674 5372 7109 4605 5211 5462 8065   50 4080 3334 8167 8378 5595 1873\n",
            " 8207 7076 4106 3719 4627  231 2405 6973 3480 3246 5931 8208 4825 6203\n",
            " 3019 5822   63 5854 6543 5787 4764 6701 6562 3822 4861 1315 8318 8169\n",
            " 8218 7549  707 1076 2465 4636   80 7025 7127 4026 4961 7531  744 6684\n",
            " 4297 7939 7384  446 6675 7752 5279 8196 3020 4192 4253 2887 2556 5729\n",
            " 1898 3208 5973 7033 1431 5915 8312 5799 7105  511 6404 2852 1561]\n",
            "Sun Jun  5 17:11:55 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P0    37W / 300W |  15237MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "595/595 [==============================] - 222s 348ms/step - loss: 7.6605 - accuracy: 0.5618 - val_loss: 6.9335 - val_accuracy: 0.7157\n",
            "Epoch 2/5\n",
            "595/595 [==============================] - 206s 347ms/step - loss: 6.7209 - accuracy: 0.7509 - val_loss: 6.6048 - val_accuracy: 0.7524\n",
            "Epoch 3/5\n",
            "595/595 [==============================] - 202s 340ms/step - loss: 6.2527 - accuracy: 0.8184 - val_loss: 6.3686 - val_accuracy: 0.7389\n",
            "Epoch 4/5\n",
            "595/595 [==============================] - 202s 340ms/step - loss: 5.8654 - accuracy: 0.8581 - val_loss: 6.1472 - val_accuracy: 0.7389\n",
            "Epoch 5/5\n",
            "595/595 [==============================] - 202s 340ms/step - loss: 5.5053 - accuracy: 0.8937 - val_loss: 6.0494 - val_accuracy: 0.7408\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_3[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_roberta_model[1][0]']       \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 15)           7695        ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.7054631828978623\n",
            "Weighted F1: 0.6989787058875981\n",
            "Micro F1: 0.7054631828978623\n",
            "Weighted Precision: 0.7029985113884637\n",
            "Micro Precision: 0.7054631828978623\n",
            "Weighted Recall: 0.7054631828978623\n",
            "Micro Recall: 0.7054631828978623\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_1[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_roberta_model[0][0]']       \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 15)           7695        ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 8  1  2 ...  2  1 10]\n",
            "[   1    2    4 ... 8413 8414 8417]\n",
            "[6622   35 6713 4989 8233 7444 5061 8344 5464  317 4515 3001 6991 2875\n",
            " 3885 6647 4857 4504  214 8340 2969 3847 4609 1346 5993 7358 2752 7651\n",
            " 8188 7073 6069 2346 6483 4952 5072 1967 4071 7590 5030 7297  626 5463\n",
            " 6029 6302 7256 3993 4263 3835 5801 4293 8129 5866 5777 4837 3843 1418\n",
            " 5274 6205 2356 5997 6685 3310 7721 5928 2360 7197 4252 8343  909 4354\n",
            " 4752  649 3488 2496 5765 6612 4004 4052 1403 6697 5268 6293 6380 4602\n",
            " 6057 4577 4236 7978 5083 6552  361 7706 4186 8351  812 5108 6600 8264\n",
            " 8064 7121 8279 5556 5119 7947 3350 7152 5199 4282 5025 5051 1927 3615\n",
            " 1221 8062 1489 2031 7351  234 8015 7416 7921 6574 6257 8396  941 4520\n",
            " 3300 5640  240 7810 7425 1321 6580 5404 8273 6742   94    9 7861 7975\n",
            " 1018 2140 1415 7061 7767 4990 8124 5927 6051 3269 6916 2846 6818 7052\n",
            " 7775 7582 8116 4767 5248 1812 6055 8331  998 1564 7326  292 3065 6764\n",
            " 6854 4915 3583 5902 4229 2144 7565 3712 7224 4502 1406 1651 3827 6525\n",
            " 6318 2686 5369 8235 1704 1011 3351 8021  102  224 8294 7707 2985 5146\n",
            " 3709 6824 2281 6837 5436 2649 4886  966 5476 1199 8387 5449 1028 2226\n",
            " 6166 4387  350 6123 5236 5041  734 3412 6085  587  489 4982 5115 5575\n",
            " 3839 6743 7173 5724 7689 7787 8302 3150 2247 8128 6660 3505 4610 2866\n",
            " 4161 6905 7096  768 1062 4411 6087 2096 4903 7546 6630 2928 4640 7125\n",
            " 1792 2063 1700 7928 8095  737 5849  922 2359 6015 4489 4361 6081 6346\n",
            " 5014 5167 1023 8009  268 4777 5221 4323 6168 5702 6129 7181 1860 7192\n",
            " 5467 6579   67 2693 6134 4270 4611 6746 7696 3466 3950 6125 8370 6142\n",
            " 6131  995 5548 1721 5067 4015 7092 3156   25 1749 4878 8248 6836 5310\n",
            " 6960 2339 1650 6227 4104 5130 7089 6993 1289 4305 5585 7933 6284 7057\n",
            " 3807 5790 8347 5280 2841 7693 8090 4171 5260  133 7451 3957 1719 4743\n",
            " 4746 5435 6216 4858 4417 8355 4492 2648 5104 5278 7308  158 7543 2168\n",
            " 5913 7111 3414 7592   39 7157 5647 4555 3956 3617 4630 4384 6596 4539\n",
            " 6749 7202 2413 4749 5122 5929  252 5565  336 6008 4564 5580 7237 5958\n",
            " 7657 4327 5091 6918 6487 2989 7448 4715 3382  467 3784 4419 1117 7954\n",
            " 7122 3174 1534 4164 7526  498 5944 4572 2307 2929 7480 4412 4563 7692\n",
            " 1869 2472 6966 6549 1594 2276 7269 2545 2216  605 6587 6315 5297 5282\n",
            " 1619 5504 6399 3917 6342 5721 7909 6126 8382 4383 5502 7130  485 4936\n",
            " 7674 5372 7109 4605 5211 5462 8065   50 4080 3334 8167 8378 5595 1873\n",
            " 8207 7076 4106 3719 4627  231 2405 6973 3480 3246 5931 8208 4825 6203\n",
            " 3019 5822   63 5854 6543 5787 4764 6701 6562 3822 4861 1315 8318 8169\n",
            " 8218 7549  707 1076 2465 4636   80 7025 7127 4026 4961 7531  744 6684\n",
            " 4297 7939 7384  446 6675 7752 5279 8196 3020 4192 4253 2887 2556 5729\n",
            " 1898 3208 5973 7033 1431 5915 8312 5799 7105  511 6404 2852 1561]\n",
            "Sun Jun  5 17:30:54 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P0    37W / 300W |  15237MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "595/595 [==============================] - 222s 348ms/step - loss: 7.6629 - accuracy: 0.5357 - val_loss: 6.8687 - val_accuracy: 0.7292\n",
            "Epoch 2/5\n",
            "595/595 [==============================] - 206s 347ms/step - loss: 6.6282 - accuracy: 0.7545 - val_loss: 6.4827 - val_accuracy: 0.7389\n",
            "Epoch 3/5\n",
            "595/595 [==============================] - 202s 340ms/step - loss: 6.1608 - accuracy: 0.8121 - val_loss: 6.2440 - val_accuracy: 0.7253\n",
            "Epoch 4/5\n",
            "595/595 [==============================] - 206s 346ms/step - loss: 5.7656 - accuracy: 0.8560 - val_loss: 6.0163 - val_accuracy: 0.7447\n",
            "Epoch 5/5\n",
            "595/595 [==============================] - 202s 340ms/step - loss: 5.4053 - accuracy: 0.8894 - val_loss: 5.8815 - val_accuracy: 0.7311\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_3[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_roberta_model[1][0]']       \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 15)           7695        ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.7232779097387173\n",
            "Weighted F1: 0.7193859455047584\n",
            "Micro F1: 0.7232779097387173\n",
            "Weighted Precision: 0.7248016582766308\n",
            "Micro Precision: 0.7232779097387173\n",
            "Weighted Recall: 0.7232779097387173\n",
            "Micro Recall: 0.7232779097387173\n",
            "Average Accuracy: 0.7211401425178147\n",
            "Average Weighted F1: 0.7154524773997426\n",
            "Average Micro F1: 0.7211401425178147\n",
            "Average Weighted Precision: 0.720739295989155\n",
            "Average Micro Precision: 0.7211401425178147\n",
            "Average Weighted Recall: 0.7211401425178147\n",
            "Average Micro Recall: 0.7211401425178147\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "total_accuracy=0\n",
        "total_weighted_f1=0\n",
        "total_micro_f1=0\n",
        "total_weighted_precision=0\n",
        "total_micro_precision=0\n",
        "total_weighted_recall=0\n",
        "total_micro_recall=0\n",
        "\n",
        "for f in range(5):\n",
        "  gc.collect()\n",
        "  tf.keras.backend.clear_session()\n",
        "  dbert_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "  dbert_model = TFRobertaModel.from_pretrained('roberta-base')\n",
        "  max_len=512\n",
        "  sentences=summarized_data['text']\n",
        "  labels=summarized_data['label']\n",
        "  filenumbers=summarized_data['filenum']\n",
        "  len(sentences),len(labels),len(filenumbers)\n",
        "  \n",
        "  model_4=create_model()\n",
        "  input_ids=[]\n",
        "  attention_masks=[]\n",
        "\n",
        "  for sent in sentences:\n",
        "    dbert_inps=dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n",
        "    input_ids.append(dbert_inps['input_ids'])\n",
        "    attention_masks.append(dbert_inps['attention_mask'])\n",
        "  input_ids=np.asarray(input_ids)\n",
        "\n",
        "  attention_masks=np.array(attention_masks)\n",
        "  labels=np.array(labels)\n",
        "  filenumbers=[int(i) for i in filenumbers]\n",
        "  filenumbers=np.array(filenumbers)\n",
        "  print(labels)\n",
        "  print(filenumbers)\n",
        "\n",
        "  #train_inp,val_inp,train_label,val_label,train_mask,val_mask,train_fnum,val_fnum=train_test_split(input_ids,labels,attention_masks,filenumbers,test_size=0.1,random_state=42)\n",
        "\n",
        "  new_train_inp_4=[]\n",
        "  new_val_inp_4=[]\n",
        "  new_train_label_4=[]\n",
        "  new_val_label_4=[]\n",
        "  new_train_mask_4=[]\n",
        "  new_train_fnum_4=[]\n",
        "  new_val_fnum_4=[]\n",
        "  new_val_mask_4=[]\n",
        "  for i in range(len(train_fnum)):\n",
        "    if train_fnum[i] in filenumbers:\n",
        "      new_train_inp_4.append(train_inp[i])\n",
        "      #new_val_inp.append(val_inp[i])\n",
        "      new_train_label_4.append(train_label[i])\n",
        "      #new_val_label.append(val_label[i])\n",
        "      new_train_mask_4.append(train_mask[i])\n",
        "      #new_val_mask.append(val_mask[i])\n",
        "      new_train_fnum_4.append(train_fnum[i])\n",
        "\n",
        "  for i in range(len(val_fnum)):\n",
        "    if val_fnum[i] in filenumbers:\n",
        "      #new_train_inp.append(train_inp[i])\n",
        "      new_val_inp_4.append(val_inp[i])\n",
        "      #new_train_label.append(train_label[i])\n",
        "      new_val_label_4.append(val_label[i])\n",
        "      #new_train_mask.append(train_mask[i])\n",
        "      new_val_mask_4.append(val_mask[i])\n",
        "      new_val_fnum_4.append(val_fnum[i])\n",
        "\n",
        "  # print(new_train_label)\n",
        "  # print(len(new_train_label))\n",
        "  # print(new_val_label)\n",
        "  # print(len(new_val_label))\n",
        "  new_train_inp_4=np.array(new_train_inp_4)\n",
        "  new_val_inp_4=np.array(new_val_inp_4)\n",
        "  new_train_label_4=np.array(new_train_label_4)\n",
        "  new_val_label_4=np.array(new_val_label_4)\n",
        "  new_train_mask_4=np.array(new_train_mask_4)\n",
        "  new_train_fnum_4=np.array(new_train_fnum_4)\n",
        "  new_val_fnum_4=np.array(new_val_fnum_4)\n",
        "  new_val_mask_4=np.array(new_val_mask_4)\n",
        "\n",
        "  print(new_val_fnum_4)\n",
        "  \n",
        "  log_dir='dbert_model'\n",
        "\n",
        "  model_save_path='./drive/MyDrive/Ensemble/roberta-ensemble-512-model_4-'+str(f)+'-15labels.h5'\n",
        "\n",
        "  loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "  accuracy = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
        "  callbacks= [tf.keras.callbacks.ModelCheckpoint(filepath=model_save_path,monitor='val_accuracy',mode='max',save_best_only=True,save_weights_only=True),keras.callbacks.TensorBoard(log_dir=log_dir)]\n",
        "  model_4.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])\n",
        "  gpu_info = !nvidia-smi\n",
        "  gpu_info = '\\n'.join(gpu_info)\n",
        "  if gpu_info.find('failed') >= 0:\n",
        "    print('Not connected to a GPU')\n",
        "  else:\n",
        "    print(gpu_info)\n",
        "  \n",
        "  history=model_4.fit([new_train_inp_4,new_train_mask_4],new_train_label_4,batch_size=8,epochs=5,validation_data=([new_val_inp_4,new_val_mask_4],new_val_label_4),callbacks=callbacks)\n",
        "\n",
        "  pred_labels=[]\n",
        "\n",
        "  model_saved_4= create_model()\n",
        "  model_saved_4.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])\n",
        "  model_saved_4.load_weights('./drive/MyDrive/Ensemble/roberta-ensemble-512-model_4-'+str(f)+'-15labels.h5')\n",
        "\n",
        "  for i in range(0,len(val_inp)):\n",
        "    pred=model_saved_4.predict([val_inp[i].reshape(1,512),val_mask[i].reshape(1,512)])\n",
        "    pred_label = pred.argmax(axis=1)\n",
        "    pred_labels.append(pred_label)\n",
        "  accuracy=accuracy_score(val_label, pred_labels)\n",
        "  print(\"Accuracy: \"+str(accuracy))\n",
        "  total_accuracy=total_accuracy+accuracy\n",
        "  \n",
        "  weighted_f1=f1_score(val_label,pred_labels, average='weighted')\n",
        "  print(\"Weighted F1: \"+ str(weighted_f1))\n",
        "  total_weighted_f1=total_weighted_f1+weighted_f1\n",
        "  micro_f1=f1_score(val_label,pred_labels, average='micro')\n",
        "  print(\"Micro F1: \"+ str(micro_f1))\n",
        "  total_micro_f1=total_micro_f1+micro_f1\n",
        "\n",
        "  weighted_precision=precision_score(val_label, pred_labels, average='weighted')\n",
        "  print(\"Weighted Precision: \" + str(weighted_precision))\n",
        "  total_weighted_precision=total_weighted_precision+weighted_precision\n",
        "  micro_precision=precision_score(val_label, pred_labels, average='micro')\n",
        "  print(\"Micro Precision: \" + str(micro_precision))\n",
        "  total_micro_precision=total_micro_precision+micro_precision\n",
        "\n",
        "  weighted_recall=recall_score(val_label, pred_labels, average='weighted')\n",
        "  print(\"Weighted Recall: \" + str(weighted_recall))\n",
        "  total_weighted_recall=total_weighted_recall+weighted_recall\n",
        "  micro_recall=recall_score(val_label, pred_labels, average='micro')\n",
        "  print(\"Micro Recall: \" + str(micro_recall))\n",
        "  total_micro_recall=total_micro_recall+micro_recall\n",
        "\n",
        "\n",
        "print(\"Average Accuracy: \"+str(total_accuracy/5))\n",
        "print(\"Average Weighted F1: \"+str(total_weighted_f1/5))\n",
        "print(\"Average Micro F1: \"+str(total_micro_f1/5))\n",
        "print(\"Average Weighted Precision: \"+str(total_weighted_precision/5))\n",
        "print(\"Average Micro Precision: \"+str(total_micro_precision/5))\n",
        "print(\"Average Weighted Recall: \"+str(total_weighted_recall/5))\n",
        "print(\"Average Micro Recall: \"+str(total_micro_recall/5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "maHyXaSvPmP9",
        "outputId": "870ab9c8-d2af-42ed-dc90-96de063271e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4504\n",
            "4504\n",
            "Average Length 511.5863676731794\n",
            "['8', '1', '8', '2', '8', '8', '8', '9', '7', '8', '1', '2', '1', '1', '8', '4', '8', '8', '12', '3', '3', '7', '3', '12', '1', '8', '8', '8', '8', '8', '8', '8', '1', '9', '5', '9', '9', '9', '11', '8', '8', '8', '4', '8', '8', '8', '8', '1', '3', '9', '3', '7', '1', '2', '9', '9', '7', '8', '8', '1', '10', '7', '8', '8', '9', '8', '7', '9', '9', '12', '7', '2', '8', '1', '11', '11', '1', '7', '7', '12', '1', '9', '8', '10', '12', '7', '8', '2', '8', '9', '9', '1', '8', '9', '1', '7', '12', '10', '10', '10', '8', '3', '7', '9', '8', '9', '1', '8', '8', '2', '7', '2', '9', '9', '11', '8', '8', '12', '12', '7', '8', '12', '4', '9', '3', '1', '12', '1', '1', '8', '8', '3', '8', '8', '8', '8', '9', '1', '8', '8', '10', '1', '8', '2', '8', '8', '7', '3', '8', '2', '4', '4', '9', '8', '10', '12', '12', '1', '1', '9', '1', '1', '1', '8', '2', '2', '8', '1', '1', '2', '2', '1', '2', '8', '1', '9', '9', '8', '8', '4', '2', '9', '9', '8', '3', '4', '3', '1', '8', '8', '2', '1', '9', '7', '8', '8', '1', '12', '3', '8', '2', '4', '2', '9', '12', '1', '4', '1', '8', '8', '8', '2', '2', '8', '9', '8', '8', '8', '10', '9', '8', '7', '9', '1', '1', '9', '4', '2', '4', '2', '2', '1', '7', '8', '11', '11', '3', '9', '2', '4', '8', '9', '1', '8', '1', '1', '4', '9', '1', '1', '8', '8', '2', '1', '8', '4', '2', '8', '9', '8', '8', '2', '8', '8', '8', '7', '1', '1', '1', '2', '1', '1', '8', '7', '8', '8', '12', '2', '12', '12', '8', '10', '12', '8', '3', '3', '12', '10', '1', '8', '12', '1', '8', '8', '2', '8', '4', '7', '8', '7', '10', '8', '10', '9', '8', '12', '12', '1', '8', '8', '3', '8', '8', '8', '8', '8', '1', '9', '8', '11', '1', '1', '1', '9', '8', '1', '9', '2', '3', '11', '8', '9', '9', '9', '2', '1', '8', '8', '9', '7', '1', '4', '9', '4', '8', '8', '4', '8', '12', '9', '4', '8', '2', '10', '10', '10', '8', '9', '9', '8', '8', '12', '7', '1', '8', '8', '8', '4', '1', '1', '1', '1', '1', '1', '8', '1', '9', '8', '9', '9', '4', '8', '12', '9', '8', '8', '2', '8', '8', '8', '6', '9', '8', '3', '7', '8', '8', '4', '12', '8', '8', '9', '12', '12', '9', '8', '2', '9', '2', '3', '1', '12', '8', '10', '9', '9', '9', '10', '10', '3', '8', '12', '1', '4', '2', '1', '10', '8', '2', '8', '4', '8', '9', '1', '9', '9', '10', '10', '1', '4', '9', '2', '4', '9', '1', '1', '3', '10', '3', '3', '8', '7', '3', '8', '9', '9', '12', '4', '8', '12', '2', '2', '4', '1', '9', '9', '4', '1', '4', '2', '8', '12', '2', '3', '10', '10', '9', '8', '9', '9', '1', '12', '8', '8', '8', '12', '4', '1', '8', '8', '1', '9', '8', '8', '2', '1', '8', '9', '8', '3', '3', '3', '1', '8', '8', '9', '1', '10', '9', '9', '9', '9', '5', '9', '9', '8', '8', '8', '8', '8', '8', '8', '8', '11', '12', '8', '8', '1', '8', '9', '11', '2', '2', '2', '2', '3', '1', '2', '2', '8', '2', '4', '9', '1', '2', '9', '8', '2', '8', '9', '9', '3', '10', '9', '9', '2', '8', '9', '8', '12', '12', '1', '3', '8', '8', '8', '2', '7', '7', '7', '7', '3', '9', '1', '9', '8', '9', '9', '1', '1', '1', '2', '9', '9', '9', '11', '1', '8', '8', '9', '1', '9', '8', '8', '8', '1', '1', '8', '7', '1', '1', '8', '8', '9', '4', '4', '8', '2', '2', '8', '8', '8', '8', '8', '8', '11', '8', '2', '9', '4', '9', '3', '9', '9', '1', '3', '9', '3', '1', '12', '8', '9', '12', '1', '8', '4', '2', '1', '4', '8', '3', '3', '8', '2', '8', '9', '7', '8', '8', '8', '5', '8', '3', '9', '8', '8', '13', '12', '1', '1', '2', '8', '4', '1', '9', '9', '12', '8', '9', '12', '9', '1', '9', '9', '9', '9', '3', '2', '9', '9', '4', '8', '12', '2', '4', '9', '3', '1', '9', '7', '8', '9', '9', '8', '4', '8', '8', '7', '9', '10', '3', '8', '8', '8', '1', '1', '1', '1', '8', '8', '4', '1', '10', '1', '5', '7', '7', '1', '8', '9', '3', '7', '2', '7', '7', '2', '4', '8', '12', '7', '4', '2', '9', '9', '12', '6', '10', '8', '2', '4', '12', '9', '9', '3', '8', '8', '1', '2', '10', '9', '9', '8', '4', '12', '2', '1', '8', '8', '8', '12', '10', '10', '9', '3', '8', '8', '9', '2', '8', '10', '1', '1', '1', '1', '2', '1', '1', '1', '1', '9', '8', '12', '9', '4', '8', '8', '9', '1', '9', '3', '9', '8', '8', '1', '7', '7', '10', '1', '8', '8', '1', '9', '8', '10', '3', '1', '7', '1', '8', '8', '12', '8', '8', '1', '8', '7', '1', '7', '7', '8', '2', '1', '8', '8', '2', '10', '8', '8', '8', '8', '8', '10', '1', '8', '8', '12', '8', '3', '3', '2', '2', '2', '10', '8', '8', '8', '2', '9', '1', '8', '9', '3', '2', '8', '10', '8', '6', '1', '1', '8', '4', '1', '9', '10', '8', '1', '7', '1', '2', '8', '1', '1', '1', '12', '1', '9', '12', '8', '12', '12', '12', '8', '8', '12', '4', '8', '8', '8', '8', '9', '9', '1', '3', '3', '3', '3', '1', '12', '12', '9', '10', '8', '8', '1', '9', '2', '2', '13', '9', '8', '9', '2', '1', '9', '1', '8', '8', '8', '4', '8', '1', '1', '1', '12', '12', '7', '2', '2', '2', '8', '3', '8', '9', '2', '10', '7', '8', '9', '2', '1', '2', '12', '12', '8', '8', '9', '2', '2', '9', '11', '1', '8', '1', '10', '9', '2', '1', '4', '7', '7', '7', '7', '7', '12', '8', '8', '8', '1', '1', '10', '1', '12', '1', '8', '2', '1', '1', '12', '8', '7', '9', '12', '8', '9', '3', '9', '8', '8', '8', '8', '3', '11', '2', '2', '9', '8', '8', '10', '8', '2', '7', '3', '1', '4', '7', '8', '8', '1', '8', '3', '7', '12', '8', '10', '9', '9', '8', '8', '2', '8', '9', '9', '1', '2', '8', '8', '9', '8', '3', '8', '1', '8', '10', '9', '8', '9', '9', '12', '4', '4', '8', '9', '9', '8', '2', '10', '1', '2', '8', '9', '1', '9', '9', '9', '7', '12', '12', '8', '1', '1', '1', '1', '8', '3', '1', '1', '8', '1', '8', '8', '7', '8', '8', '8', '8', '3', '2', '2', '10', '10', '10', '7', '8', '1', '2', '12', '7', '9', '8', '7', '8', '12', '2', '8', '9', '2', '6', '6', '7', '9', '8', '1', '8', '9', '8', '1', '12', '1', '2', '8', '7', '7', '7', '8', '2', '2', '8', '1', '2', '2', '9', '9', '1', '8', '8', '4', '3', '3', '1', '6', '3', '3', '12', '3', '8', '9', '1', '4', '3', '1', '8', '3', '9', '2', '8', '2', '8', '8', '8', '1', '1', '1', '9', '9', '8', '1', '9', '8', '1', '1', '3', '10', '8', '1', '1', '3', '9', '1', '4', '4', '1', '8', '9', '9', '2', '0', '0', '1', '8', '3', '1', '8', '8', '9', '8', '8', '1', '1', '8', '9', '8', '8', '8', '7', '9', '8', '8', '8', '10', '9', '8', '1', '2', '6', '1', '9', '9', '8', '12', '12', '12', '8', '8', '2', '8', '1', '2', '2', '2', '1', '9', '8', '2', '12', '2', '8', '12', '8', '9', '8', '8', '9', '7', '1', '1', '1', '1', '1', '8', '8', '1', '8', '8', '1', '1', '3', '2', '8', '8', '9', '10', '10', '2', '2', '1', '9', '2', '9', '9', '4', '12', '12', '12', '10', '7', '3', '3', '4', '2', '2', '9', '2', '8', '4', '2', '4', '1', '10', '9', '7', '8', '7', '1', '1', '3', '3', '1', '1', '3', '3', '3', '1', '1', '1', '1', '8', '2', '3', '1', '1', '2', '8', '8', '12', '8', '8', '8', '8', '11', '9', '1', '8', '9', '2', '8', '8', '8', '3', '9', '1', '9', '2', '7', '2', '8', '2', '8', '10', '8', '1', '10', '1', '1', '9', '9', '8', '8', '1', '8', '8', '8', '12', '8', '8', '8', '1', '8', '8', '8', '1', '9', '1', '1', '8', '1', '8', '9', '8', '2', '12', '9', '9', '0', '1', '8', '8', '1', '8', '12', '8', '8', '10', '8', '8', '8', '7', '8', '1', '8', '7', '3', '10', '1', '8', '9', '1', '8', '8', '8', '10', '1', '10', '3', '9', '1', '8', '9', '2', '8', '3', '3', '9', '9', '7', '9', '1', '1', '9', '2', '1', '1', '1', '7', '1', '1', '8', '8', '1', '1', '8', '1', '8', '3', '12', '9', '3', '3', '8', '8', '8', '8', '3', '1', '3', '3', '1', '11', '0', '8', '8', '7', '8', '12', '1', '8', '9', '8', '9', '8', '8', '3', '8', '8', '1', '1', '1', '9', '2', '2', '2', '8', '7', '12', '8', '8', '9', '10', '10', '7', '8', '1', '9', '8', '7', '3', '1', '3', '8', '2', '2', '3', '9', '8', '4', '4', '8', '9', '2', '1', '1', '7', '8', '9', '9', '7', '8', '7', '7', '8', '2', '2', '8', '4', '9', '7', '10', '0', '9', '8', '3', '7', '8', '1', '1', '8', '9', '9', '2', '2', '10', '1', '9', '10', '10', '10', '8', '3', '2', '12', '9', '9', '10', '12', '9', '12', '12', '9', '1', '2', '4', '12', '12', '7', '8', '9', '7', '7', '7', '3', '9', '8', '9', '1', '12', '8', '9', '4', '1', '3', '12', '12', '12', '12', '8', '8', '2', '1', '1', '2', '1', '1', '1', '12', '12', '8', '12', '2', '2', '12', '3', '3', '12', '8', '2', '8', '8', '12', '2', '1', '10', '3', '2', '8', '7', '1', '8', '1', '3', '7', '8', '9', '8', '3', '1', '1', '7', '8', '8', '9', '8', '2', '9', '2', '2', '9', '8', '1', '8', '8', '1', '3', '3', '1', '1', '10', '1', '2', '8', '1', '1', '1', '1', '9', '1', '4', '1', '7', '7', '7', '7', '2', '2', '8', '8', '12', '1', '9', '1', '7', '3', '3', '1', '8', '8', '10', '8', '9', '2', '9', '1', '3', '8', '8', '3', '12', '2', '8', '12', '2', '9', '1', '3', '3', '3', '3', '2', '8', '7', '9', '8', '3', '3', '1', '7', '8', '3', '1', '1', '12', '8', '9', '1', '2', '3', '8', '1', '1', '3', '3', '9', '1', '1', '1', '12', '1', '7', '3', '3', '1', '8', '8', '8', '1', '2', '4', '8', '1', '10', '2', '5', '3', '3', '12', '10', '9', '9', '12', '9', '0', '2', '8', '8', '9', '9', '9', '8', '1', '3', '1', '1', '4', '8', '1', '10', '8', '7', '2', '8', '2', '8', '4', '7', '8', '1', '9', '1', '9', '8', '2', '8', '2', '7', '9', '2', '2', '9', '1', '8', '12', '1', '8', '1', '4', '1', '9', '9', '1', '10', '12', '4', '8', '1', '7', '3', '9', '2', '12', '7', '8', '8', '2', '1', '12', '9', '8', '1', '2', '2', '8', '10', '2', '1', '7', '7', '7', '12', '3', '3', '8', '3', '8', '8', '3', '9', '8', '9', '1', '1', '8', '7', '9', '3', '3', '8', '1', '0', '9', '9', '9', '1', '8', '9', '9', '10', '1', '8', '7', '8', '8', '8', '9', '9', '8', '9', '10', '4', '9', '3', '7', '12', '1', '9', '9', '8', '9', '1', '9', '3', '1', '8', '4', '12', '10', '9', '8', '7', '10', '8', '12', '12', '3', '10', '8', '12', '2', '1', '2', '3', '9', '8', '8', '7', '1', '1', '2', '2', '1', '2', '2', '7', '1', '3', '9', '9', '3', '8', '8', '8', '8', '8', '7', '8', '8', '10', '8', '1', '8', '2', '8', '2', '2', '2', '2', '2', '1', '7', '12', '10', '1', '2', '8', '1', '4', '7', '8', '1', '12', '8', '7', '9', '2', '2', '2', '8', '1', '8', '1', '1', '0', '2', '2', '6', '1', '8', '2', '1', '8', '1', '1', '2', '2', '3', '2', '2', '10', '10', '9', '1', '7', '7', '8', '8', '1', '12', '10', '12', '3', '8', '8', '8', '3', '8', '3', '10', '2', '2', '2', '1', '2', '2', '1', '0', '1', '8', '9', '1', '1', '1', '2', '2', '2', '2', '2', '1', '1', '1', '1', '12', '9', '9', '1', '10', '11', '12', '9', '1', '1', '8', '8', '1', '2', '12', '8', '3', '7', '2', '10', '9', '7', '8', '1', '2', '2', '7', '9', '12', '2', '2', '1', '8', '9', '2', '3', '2', '2', '2', '9', '9', '8', '9', '2', '3', '8', '8', '9', '7', '3', '10', '8', '8', '9', '1', '1', '2', '8', '1', '10', '8', '8', '9', '7', '1', '7', '12', '8', '1', '7', '2', '1', '3', '9', '2', '8', '8', '1', '2', '3', '9', '3', '8', '9', '7', '7', '1', '10', '2', '8', '8', '8', '1', '8', '2', '8', '8', '1', '2', '8', '10', '2', '1', '0', '8', '3', '8', '10', '10', '12', '3', '8', '9', '3', '10', '8', '8', '8', '1', '8', '8', '2', '2', '2', '2', '2', '2', '1', '1', '10', '1', '10', '8', '2', '1', '2', '2', '8', '3', '3', '2', '2', '1', '8', '1', '3', '1', '9', '2', '2', '1', '3', '2', '2', '3', '2', '2', '3', '2', '1', '1', '2', '1', '2', '7', '8', '2', '1', '12', '12', '3', '1', '9', '9', '7', '8', '7', '2', '7', '9', '2', '2', '2', '8', '9', '7', '9', '2', '8', '9', '9', '2', '9', '1', '1', '9', '8', '3', '3', '8', '2', '7', '8', '11', '2', '8', '9', '1', '8', '3', '1', '2', '1', '2', '2', '2', '2', '8', '3', '9', '2', '10', '9', '9', '2', '7', '7', '7', '1', '3', '8', '8', '1', '1', '8', '7', '1', '8', '8', '9', '3', '3', '3', '2', '4', '8', '12', '8', '1', '12', '9', '2', '8', '2', '12', '2', '1', '3', '8', '12', '12', '11', '8', '10', '9', '2', '9', '4', '10', '1', '12', '12', '9', '3', '8', '9', '12', '9', '9', '2', '2', '2', '3', '5', '1', '1', '1', '7', '7', '8', '2', '2', '8', '2', '1', '7', '3', '2', '9', '8', '9', '1', '1', '9', '8', '9', '3', '3', '2', '9', '4', '9', '8', '8', '1', '8', '10', '2', '0', '3', '8', '8', '8', '9', '12', '9', '8', '2', '2', '8', '10', '9', '8', '2', '2', '4', '1', '9', '9', '8', '8', '10', '3', '1', '1', '2', '8', '7', '8', '2', '1', '1', '8', '12', '2', '9', '4', '8', '3', '3', '3', '8', '2', '12', '12', '8', '9', '12', '8', '2', '12', '7', '9', '2', '2', '1', '3', '1', '3', '11', '9', '8', '1', '10', '2', '12', '3', '8', '4', '1', '2', '8', '8', '3', '9', '3', '8', '7', '1', '1', '3', '1', '8', '2', '9', '8', '12', '1', '1', '1', '9', '9', '1', '8', '8', '8', '2', '2', '12', '3', '1', '1', '1', '9', '9', '3', '2', '1', '1', '9', '9', '1', '9', '9', '1', '9', '3', '9', '1', '8', '1', '9', '8', '3', '8', '8', '9', '9', '8', '2', '8', '1', '2', '2', '10', '1', '2', '1', '1', '9', '1', '9', '3', '7', '7', '2', '10', '3', '3', '1', '1', '9', '1', '2', '1', '3', '2', '1', '1', '1', '1', '1', '1', '1', '1', '1', '2', '8', '8', '7', '1', '1', '8', '8', '2', '1', '12', '1', '8', '1', '9', '8', '8', '2', '1', '9', '9', '2', '2', '8', '1', '7', '7', '9', '8', '1', '6', '8', '8', '2', '2', '8', '3', '8', '3', '2', '3', '8', '1', '2', '2', '2', '8', '9', '9', '9', '12', '1', '9', '2', '1', '9', '2', '9', '9', '10', '8', '1', '1', '9', '2', '1', '7', '1', '3', '7', '1', '1', '1', '3', '8', '8', '8', '9', '9', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '1', '0', '1', '1', '1', '2', '2', '3', '1', '1', '2', '3', '3', '3', '7', '3', '1', '9', '3', '9', '1', '4', '1', '1', '9', '10', '9', '8', '2', '3', '10', '9', '3', '10', '12', '8', '9', '8', '2', '1', '9', '8', '1', '7', '8', '1', '9', '1', '2', '10', '7', '7', '8', '1', '3', '3', '9', '3', '3', '1', '1', '1', '9', '1', '3', '8', '8', '8', '1', '8', '1', '9', '7', '8', '8', '2', '1', '3', '8', '2', '1', '6', '1', '8', '8', '2', '8', '2', '2', '9', '8', '1', '1', '6', '8', '6', '10', '2', '1', '1', '8', '9', '3', '3', '3', '1', '3', '9', '6', '8', '1', '8', '3', '1', '2', '2', '12', '7', '1', '1', '1', '1', '12', '1', '8', '8', '3', '2', '1', '9', '2', '3', '3', '2', '2', '7', '2', '2', '2', '3', '1', '9', '8', '7', '1', '1', '3', '1', '9', '4', '3', '9', '2', '1', '1', '9', '8', '8', '7', '2', '1', '3', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '2', '10', '1', '1', '8', '2', '8', '1', '8', '1', '3', '1', '2', '1', '1', '3', '1', '2', '1', '1', '1', '1', '1', '1', '1', '2', '1', '8', '1', '9', '8', '8', '1', '10', '8', '8', '1', '3', '9', '8', '9', '8', '2', '1', '3', '4', '1', '12', '1', '9', '8', '2', '7', '2', '1', '2', '9', '2', '1', '3', '8', '1', '2', '3', '8', '2', '10', '10', '10', '9', '9', '9', '3', '8', '3', '1', '1', '1', '1', '1', '1', '2', '1', '9', '2', '7', '9', '8', '7', '9', '1', '1', '1', '1', '8', '2', '2', '3', '3', '2', '12', '8', '3', '1', '1', '12', '9', '8', '2', '2', '9', '1', '2', '2', '9', '1', '1', '2', '4', '8', '1', '8', '10', '2', '9', '9', '9', '1', '2', '1', '1', '2', '2', '2', '12', '1', '2', '8', '3', '9', '9', '3', '2', '9', '9', '7', '2', '8', '8', '9', '2', '9', '1', '1', '1', '1', '1', '2', '8', '2', '1', '2', '2', '8', '3', '1', '1', '9', '1', '3', '10', '7', '9', '8', '8', '2', '2', '9', '2', '2', '2', '2', '2', '2', '9', '1', '3', '6', '4', '2', '2', '9', '1', '9', '8', '1', '1', '1', '7', '3', '2', '9', '9', '1', '9', '7', '2', '8', '9', '9', '12', '10', '8', '10', '2', '3', '1', '2', '2', '12', '12', '3', '12', '1', '2', '1', '2', '1', '2', '8', '2', '3', '12', '12', '8', '2', '2', '9', '2', '9', '2', '3', '3', '1', '1', '1', '12', '3', '7', '1', '3', '1', '2', '9', '2', '7', '2', '1', '8', '3', '7', '8', '1', '1', '3', '8', '3', '2', '9', '3', '1', '1', '1', '1', '11', '1', '2', '6', '1', '2', '3', '8', '9', '9', '2', '9', '2', '1', '1', '3', '1', '12', '9', '1', '1', '9', '1', '9', '9', '8', '9', '8', '1', '2', '7', '1', '0', '9', '8', '2', '3', '2', '4', '1', '1', '1', '8', '1', '12', '1', '2', '3', '3', '9', '9', '9', '9', '9', '7', '9', '3', '9', '1', '7', '3', '3', '3', '3', '7', '8', '2', '2', '2', '8', '2', '3', '9', '9', '3', '1', '8', '9', '12', '8', '8', '3', '2', '1', '6', '1', '9', '1', '1', '1', '1', '1', '1', '2', '2', '1', '2', '2', '2', '2', '9', '2', '5', '3', '2', '2', '8', '2', '1', '8', '3', '3', '8', '3', '1', '3', '9', '2', '8', '9', '4', '4', '9', '8', '8', '7', '7', '3', '1', '8', '8', '3', '2', '2', '10', '3', '3', '2', '2', '2', '2', '12', '3', '1', '2', '7', '7', '12', '2', '1', '2', '1', '1', '2', '1', '3', '3', '3', '1', '3', '9', '4', '8', '8', '4', '2', '4', '2', '8', '9', '1', '1', '2', '2', '2', '8', '7', '2', '8', '2', '9', '1', '8', '1', '1', '2', '1', '1', '1', '2', '9', '9', '8', '1', '8', '1', '4', '8', '1', '2', '7', '9', '2', '1', '8', '9', '8', '9', '1', '1', '2', '3', '2', '12', '7', '1', '2', '4', '4', '4', '2', '2', '2', '9', '8', '8', '1', '12', '3', '3', '2', '2', '3', '12', '1', '5', '1', '1', '3', '2', '8', '8', '2', '2', '2', '3', '9', '8', '8', '9', '8', '8', '11', '8', '2', '9', '2', '3', '1', '7', '1', '8', '2', '8', '1', '1', '1', '8', '1', '1', '1', '7', '8', '8', '2', '3', '2', '1', '8', '8', '1', '8', '4', '8', '9', '8', '9', '8', '1', '2', '3', '2', '1', '2', '2', '3', '1', '9', '4', '8', '1', '1', '9', '1', '1', '8', '2', '2', '2', '1', '7', '3', '3', '1', '3', '3', '12', '3', '1', '1', '3', '1', '1', '4', '8', '4', '4', '3', '3', '3', '4', '8', '1', '9', '9', '9', '4', '1', '7', '4', '8', '4', '9', '8', '1', '1', '3', '8', '9', '1', '9', '7', '1', '1', '9', '1', '8', '8', '8', '1', '1', '8', '2', '2', '2', '9', '4', '4', '1', '2', '1', '2', '9', '1', '1', '8', '5', '5', '5', '9', '12', '4', '1', '2', '1', '8', '8', '1', '9', '9', '2', '12', '1', '1', '7', '8', '8', '4', '1', '2', '8', '11', '2', '3', '2', '2', '1', '11', '2', '2', '2', '2', '9', '9', '2', '2', '9', '3', '1', '9', '1', '1', '1', '7', '8', '1', '8', '9', '1', '2', '9', '8', '1', '2', '9', '8', '12', '2', '2', '10', '8', '2', '2', '8', '8', '1', '2', '6', '1', '1', '7', '7', '2', '3', '2', '1', '1', '8', '1', '10', '8', '12', '6', '4', '1', '4', '1', '2', '9', '8', '9', '4', '8', '8', '10', '8', '8', '8', '8', '8', '2', '2', '8', '4', '9', '3', '3', '3', '3', '3', '9', '2', '1', '1', '2', '3', '2', '1', '2', '3', '3', '3', '2', '2', '3', '3', '2', '1', '2', '3', '3', '3', '3', '12', '9', '3', '2', '2', '3', '1', '5', '1', '1', '3', '2', '2', '1', '2', '9', '3', '9', '8', '1', '8', '7', '7', '9', '1', '1', '7', '9', '10', '1', '7', '1', '4', '3', '8', '9', '4', '9', '9', '2', '9', '2', '8', '4', '2', '2', '9', '5', '2', '1', '10', '9', '3', '1', '1', '1', '2', '1', '2', '9', '1', '7', '9', '9', '8', '2', '2', '8', '9', '1', '9', '8', '2', '3', '2', '2', '1', '2', '2', '2', '1', '1', '8', '1', '9', '4', '12', '8', '2', '7', '2', '2', '1', '9', '3', '1', '8', '12', '1', '1', '2', '1', '10', '2', '6', '12', '12', '4', '9', '1', '9', '10', '1', '2', '8', '2', '1', '8', '6', '12', '9', '9', '2', '1', '7', '8', '9', '5', '8', '1', '8', '8', '3', '1', '8', '2', '8', '2', '2', '1', '2', '9', '2', '8', '1', '12', '8', '2', '7', '3', '3', '12', '2', '3', '3', '9', '9', '3', '3', '3', '3', '3', '2', '1', '4', '1', '4', '8', '8', '2', '1', '2', '9', '12', '1', '10', '12', '1', '2', '9', '4', '8', '10', '3', '2', '8', '7', '4', '8', '9', '2', '7', '1', '10', '2', '2', '2', '8', '4', '2', '2', '1', '1', '8', '7', '9', '2', '1', '9', '8', '9', '12', '4', '8', '2', '8', '7', '7', '1', '9', '2', '1', '1', '9', '1', '8', '2', '3', '9', '9', '1', '3', '2', '9', '2', '2', '8', '1', '1', '2', '1', '2', '1', '9', '8', '2', '2', '4', '8', '8', '9', '5', '5', '9', '8', '9', '6', '2', '1', '12', '2', '3', '8', '8', '9', '2', '2', '1', '3', '2', '10', '9', '10', '7', '2', '8', '4', '2', '8', '9', '8', '1', '3', '8', '8', '1', '8', '3', '8', '8', '1', '10', '3', '1', '5', '8', '8', '9', '2', '2', '9', '2', '1', '9', '1', '4', '1', '12', '8', '8', '2', '1', '1', '1', '1', '1', '9', '2', '9', '8', '9', '5', '9', '1', '8', '2', '4', '1', '1', '1', '9', '12', '4', '1', '1', '9', '8', '4', '1', '8', '9', '8', '8', '9', '9', '9', '9', '9', '9', '1', '3', '4', '2', '2', '8', '2', '9', '2', '3', '8', '3', '1', '7', '2', '1', '1', '4', '2', '9', '2', '1', '3', '2', '9', '2', '9', '3', '8', '1', '9', '1', '1', '2', '8', '8', '8', '2', '3', '9', '9', '8', '2', '4', '1', '5', '1', '1', '9', '9', '2', '1', '1', '1', '9', '9', '1', '3', '9', '2', '2', '9', '9', '8', '3', '9', '9', '8', '2', '2', '8', '9', '2', '2', '8', '9', '10', '10', '2', '8', '7', '9', '9', '4', '2', '4', '11', '2', '1', '7', '8', '8', '8', '9', '4', '9', '8', '8', '2', '1', '1', '9', '8', '3', '3', '8', '8', '7', '9', '1', '3', '4', '8', '10', '2', '4', '4', '8', '2', '8', '2', '8', '9', '2', '1', '2', '2', '3', '1', '9', '4', '5', '9', '9', '1', '1', '1', '1', '1', '1', '7', '1', '1', '1', '8', '1', '1', '1', '1', '1', '9', '1', '2', '1', '12', '9', '3', '9', '9', '4', '1', '9', '1', '1', '2', '11', '9', '3', '2', '2', '2', '4', '2', '3', '2', '7', '7', '8', '1', '10', '9', '5', '7', '1', '10', '8', '1', '2', '7', '1', '12', '5', '8', '4', '2', '9', '9', '8', '2', '9', '8', '8', '10', '2', '3', '2', '7', '2', '8', '10', '3', '2', '9', '9', '9', '1', '1', '1', '1', '8', '2', '10', '9', '8', '1', '2', '2', '2', '1', '2', '1', '3', '9', '12', '2', '2', '4', '8', '1', '3', '9', '4', '9', '1', '9', '9', '2', '1', '1', '9', '7', '10', '3', '10', '2', '9', '2', '9', '2', '8', '2', '1', '2', '3', '1', '1', '1', '8', '5', '8', '3', '1', '1', '4', '2', '2', '1', '3', '8', '2', '1', '1', '1', '2', '9', '4', '1', '8', '1', '2', '8', '2', '9', '2', '2', '5', '5', '2', '5', '2', '1', '8', '8', '1', '3', '12', '2', '4', '3', '2', '2', '2', '6', '2', '3', '3', '1', '8', '2', '2', '1', '1', '8', '8', '10', '1', '9', '2', '9', '12', '0', '2', '1', '3', '2', '2', '1', '2', '1', '9', '9', '2', '9', '8', '8', '1', '7', '8', '1', '2', '6', '9', '8', '10', '1', '12', '2', '2', '1', '8', '2', '1', '12', '7', '4', '2', '10', '2', '2', '1', '2', '1', '2', '1', '1', '2', '1', '9', '8', '10', '2', '10', '9', '9', '12', '5', '3', '2', '8', '2', '8', '3', '3', '1', '4', '9', '10', '2', '4', '8', '1', '2', '10', '12', '12', '3', '1', '1', '2', '9', '6', '6', '1', '9', '1', '8', '1', '8', '8', '8', '8', '2', '2', '2', '8', '8', '2', '1', '1', '1', '1', '1', '8', '8', '5', '8', '8', '1', '8', '9', '9', '1', '7', '12', '9', '9', '7', '2', '2', '7', '8', '9', '8', '2', '9', '6', '3', '4', '8', '4', '1', '2', '9', '8', '8', '8', '2', '1', '1', '8', '8', '3', '1', '9', '9', '7', '1', '2', '2', '2', '9', '4', '1', '2', '12', '1', '8', '2', '1', '9', '1', '8', '9', '9', '2', '5', '4', '2', '1', '2', '8', '12', '8', '10', '3', '8', '8', '8', '2', '2', '8', '2', '8', '2', '4', '8', '10', '2', '7', '2', '1', '8', '8', '9', '5', '8', '1', '12', '7', '10', '2', '2', '2', '1', '4', '8', '12', '1', '8', '2', '2', '9', '1', '8', '3', '1', '1', '5', '10', '2', '1', '2', '4', '8', '9', '3', '2', '8', '7', '4', '4', '9', '9', '2', '8', '1', '1', '2', '4', '7', '1', '9', '1', '1', '1', '8', '4', '1', '3', '1', '2', '2', '9', '1', '8', '8', '2', '2', '9', '9', '1', '8', '8', '1', '9', '8', '8', '2', '2', '2', '2', '1', '1', '1', '7', '4', '1', '1', '4', '2', '3', '3', '2', '3', '9', '2', '8', '2', '1', '5', '1', '2', '2', '1', '3', '5', '2', '9', '1', '1', '9', '8', '1', '4', '8', '1', '8', '2', '4', '8', '4', '7', '9', '8', '1', '2', '2', '4', '4', '11', '3', '11', '1', '3', '8', '1', '9', '6', '8', '9', '3', '9', '8', '9', '2', '3', '3', '7', '1', '7', '2', '9', '9', '1', '8', '8', '1', '5', '5', '8', '8', '1', '2', '1', '3', '9', '9', '5', '9', '8', '1', '4', '3', '8', '1', '1', '2', '1', '8', '9', '8', '2', '2', '2', '1', '2', '4', '10', '8', '1', '9', '1', '8', '1', '1', '9', '9', '8', '11', '1', '8', '2', '1', '2', '8', '2', '8', '8', '2', '9', '6', '10', '1', '8', '6', '4', '5', '11', '2', '2', '8', '1', '8', '1', '8', '1', '4', '8', '1', '1', '1', '8', '3', '7', '3', '3', '7', '1', '1', '4', '8', '1', '6', '4', '2', '2', '1', '1', '1', '1', '6', '2', '2', '8', '1', '8', '5', '5', '2', '1', '1', '2', '3', '7', '1', '1', '6', '2', '9', '3', '9', '8', '9', '2', '1', '4', '2', '9', '13', '9', '2', '8', '12', '4', '1', '1', '9', '5', '1', '1', '1', '1', '8', '2', '1', '1', '2', '9', '0', '12', '1', '1', '8', '9', '8', '3', '2', '2', '12', '8', '2', '2', '2', '1', '10', '1', '9', '8', '9', '5', '2', '2', '9', '2', '8', '4', '8', '3', '7', '9', '2', '2', '7', '2', '2', '8', '1', '1', '9', '10', '9', '8', '2', '9', '0', '7', '1', '1', '1', '9', '8', '2', '8', '12', '8', '3', '2', '8', '8', '7', '7', '8', '12', '2', '1', '3', '9', '2', '2', '8', '2', '8', '8', '12', '8', '8', '1', '8', '9', '2', '4', '4', '7', '2', '1', '1', '9', '3', '7', '1', '3', '9', '8', '2', '2', '3', '1', '3', '10', '2', '3', '7', '1', '8', '1', '1', '10', '3', '10', '8', '4', '1', '2', '4', '3', '10', '9', '3', '9', '5', '8', '7', '2', '9', '3', '3', '2', '11', '8', '1', '1', '7', '2', '7', '2', '9', '4', '8', '1', '12', '8', '7', '9', '1', '0', '2', '6', '4', '9', '3', '2', '4', '12', '9', '8', '5', '9', '2', '8', '4', '6', '8', '9', '3', '1', '8', '7', '3', '1', '1', '1', '8', '9', '1', '8', '7', '9', '8', '2', '1', '2', '2', '3', '2', '2', '1', '1', '4', '9', '7', '3', '2', '8', '8', '9', '1', '9', '7', '4', '2', '2', '8', '5', '9', '1', '5', '7', '1', '4', '6', '9', '10', '1', '8', '2', '9', '1', '9', '2', '9', '8', '2', '9', '9', '11', '2', '12', '2', '2', '9', '4', '8', '1', '9', '9', '2', '8', '9', '1', '2', '9', '3', '8', '8', '1', '7', '8', '8', '2', '3', '2', '2', '2', '2', '9', '8', '10', '2', '2', '1', '9', '11', '1', '8', '8', '2', '4', '2', '1', '2', '8', '8', '9', '2', '8', '9', '2', '3', '1', '2', '1', '3', '10', '8', '1', '2', '4', '1', '9', '8', '9', '3', '3', '9', '8', '2', '11', '3', '7', '1', '8', '7', '1', '8', '8', '8', '1', '4', '8', '8', '1', '9', '3', '1', '1', '2', '8', '8', '8', '2', '1', '8', '10', '2', '12', '8', '8', '9', '1', '8', '1', '3', '2', '2', '10', '7', '2', '2', '1', '1', '8', '2', '9', '1', '2', '9', '3', '3', '10', '1', '4', '7', '10', '12', '2', '4', '10', '8', '6', '2', '9', '1', '8', '3', '1', '2', '8', '8', '2', '12', '10', '2', '10', '9', '1', '2', '5', '8', '12', '8', '8', '7', '8', '8', '1', '8', '2', '2', '9', '1', '8', '10', '5', '5', '5', '8', '1', '9', '1', '13', '1', '2', '2', '1', '2', '4', '9', '2', '1', '9', '13', '11', '1', '9', '9', '3', '10', '8', '7', '8', '8', '4', '2', '1', '9', '4', '3', '1', '1', '10', '9', '2', '8', '6', '2', '1', '1', '3', '2', '8', '1', '1', '1', '1', '2', '4', '1', '1', '10', '1', '8', '1', '1', '9', '1', '7', '1', '1', '9', '9', '2', '12', '9', '1', '8', '1', '12', '1', '8', '3', '8', '1', '8', '1', '2', '10', '1', '1', '2', '2', '9', '3', '9', '12', '1', '9', '1', '2', '8', '7', '2', '1', '1', '3', '2', '8', '4', '4', '5', '12', '7', '8', '9', '6', '1', '8', '9', '2', '1', '9', '1', '4', '1', '8', '2', '1', '1', '8', '7', '4', '9', '1', '8', '4', '2', '7', '5', '1', '3', '9', '2', '8', '2', '1', '2', '9', '1', '1', '2', '3', '4', '3', '1', '2', '2', '10', '2', '2', '8', '1', '1', '2', '4', '9', '2', '11', '9', '8', '9', '2', '1', '10', '4', '1', '1', '10', '7', '2', '10', '8', '1', '10', '8', '8', '8', '8', '8', '8', '2', '2', '2', '8', '3', '4', '1', '1', '1', '2', '1', '8', '8', '2', '8', '3', '8', '3', '1', '2', '3', '1', '1', '10', '1', '4', '4', '3', '3', '1', '9', '1', '1', '2', '1', '1', '6', '1', '1', '1', '9', '1', '4', '1', '1', '1', '1', '1', '1', '2', '12', '8', '8', '1', '9', '12', '10', '8', '2', '12', '1', '1', '2', '1', '7', '10', '7', '1', '2', '1', '10', '8', '2', '2', '8', '2', '2', '1', '9', '8', '8', '8', '8', '3', '1', '4', '2', '8', '3', '10', '10', '1', '9', '1', '9', '1', '9', '1', '8', '3', '1', '1', '8', '8', '8', '6', '3', '4', '1', '1', '1', '9', '9', '5', '2', '10', '2', '6', '2', '2', '7', '1', '8', '2', '11', '1', '1', '1', '2', '1', '9', '4', '4', '2', '8', '4', '6', '8', '8', '10', '10', '2', '1', '2', '9', '1', '8', '8', '3', '4', '8', '8', '2', '8', '8', '1', '2', '1', '2', '9', '4', '1', '3', '3', '3', '8', '8', '8', '2', '6', '4', '3', '3', '12', '3', '4', '9', '6', '8', '7', '7', '8', '6', '4', '1', '10', '6', '3', '3', '8', '1', '1', '8', '1', '11', '8', '1', '1', '2', '3', '11', '7', '2', '1', '1', '1', '9', '1', '2', '1', '1', '9', '8', '1', '1', '8', '0', '4', '1', '1', '2', '1', '1', '2', '2', '9', '1', '10', '1', '3', '8', '9', '3', '3', '1', '10', '1', '9', '1', '1', '7', '4', '1', '8', '10', '1', '2', '10', '1', '1', '2', '3', '3', '9', '1', '8', '10', '1', '7', '9', '1', '9', '6', '3', '9', '2', '8', '4', '12', '12', '3', '1', '1', '9', '1', '9', '2', '1', '1', '1', '1', '1', '9', '2', '1', '2', '10', '9', '8', '8', '10', '8', '2', '3', '2', '1', '12', '8', '12', '2', '9', '1', '3', '2', '5', '2', '2', '8', '2', '2', '4', '10', '9', '10', '9', '4', '2', '4', '12', '8', '2', '1', '8', '3', '9', '2', '9', '4', '1', '1', '1', '1', '1', '9', '6', '2', '9', '2', '1', '3', '2', '2', '5', '8', '9', '1', '6', '3', '1', '2', '2', '2', '6', '1', '8', '10', '8', '3', '3', '13', '2', '9', '9', '10', '6', '12', '8', '2', '9', '2', '3', '8', '8', '1', '2', '2', '3', '2', '10', '1', '8', '9', '1', '8', '9', '8', '8', '2', '10', '2', '1', '1', '1', '9', '9', '12', '1', '1', '4', '8', '3', '2', '2', '4', '4', '2', '2', '1', '1', '1', '9', '2', '2', '4', '1', '1', '8', '8', '8', '9', '2', '13', '9', '9', '4', '1', '9', '7', '10', '10', '8', '1', '9', '1', '1', '1', '8', '12', '4', '1', '2', '1', '1', '1', '7', '8', '3', '3', '1', '2', '2', '7', '2', '2', '10', '1', '8', '4', '0', '1', '9', '4', '1', '1', '2', '10', '10', '7', '9', '3', '12', '11', '2', '8', '10', '8', '9', '4', '3', '4', '10', '1', '1', '9', '3', '10', '1', '5', '3', '3', '2', '2', '1', '1', '6', '1', '7', '1', '1', '12', '1', '1', '1', '10', '1', '8', '8', '1', '3', '8', '3', '1', '2', '8', '3', '4', '2', '8', '8', '6', '1', '1', '4', '1', '2', '9', '5', '1', '7', '8', '9', '2', '4', '9', '8', '5', '9', '9', '9', '1', '2', '1', '3', '8', '9', '2', '9', '8', '9', '1', '8', '13', '9', '3', '8', '1', '4', '1', '3', '1', '6', '4', '10', '2', '2', '7', '8', '8', '12', '8', '9', '9', '9', '10', '3', '12', '12', '2', '9', '10', '9', '7', '2', '3', '4', '10', '10', '2', '7', '7', '1', '7', '3', '9', '1', '9', '8', '2', '9', '1', '8', '1', '10', '8', '8', '2', '7', '8', '1', '8', '9', '8', '1', '4', '1', '8', '8', '8', '1', '1', '10', '3', '2', '2', '6', '8', '9', '8', '2', '1', '1', '3', '8', '8', '1', '4', '4', '3', '9', '10', '4', '2', '4', '2', '9', '2', '2', '9', '1', '1', '10', '1', '1', '2', '1', '1', '1', '9', '1', '8', '8', '8', '2', '3', '8', '1', '6', '7', '3', '1', '9', '1', '2', '1', '6', '1', '2', '10', '1', '4', '2', '3', '2', '9', '9', '4', '9', '9', '1', '2', '8', '4', '7', '8', '7', '13', '9', '1', '2', '1', '9', '3', '3', '6', '8', '9', '10', '9', '2', '2', '3', '12', '1', '1', '1', '13', '2', '1', '7', '10', '8', '9', '1', '2', '9', '1', '1', '1', '2', '1', '12', '5', '6', '9', '10', '3', '1', '10', '1', '3', '2', '8', '9', '2', '10', '8', '8', '9', '2', '8', '9', '2', '2', '2', '8', '8', '2', '3', '1', '4', '8', '1', '9', '9', '2', '8', '10', '9', '1', '2', '12', '8', '8', '0', '2', '1', '1', '9', '10', '1', '6', '2', '2', '10', '2', '2', '4', '2', '2', '12', '1', '6', '7', '8', '9', '1', '3', '5', '7', '3', '1', '3', '1', '1', '3', '2', '6', '3', '2', '1', '8', '3', '5', '2', '1', '1', '8', '1', '1', '0', '2', '3', '5', '3', '1', '1', '9', '8', '8', '8', '8', '4', '7', '2', '9', '12', '5', '9', '3', '12', '3', '9', '1', '9', '1', '7', '3', '9', '3', '10', '1', '2', '1', '4', '9', '8', '2', '9', '11', '2', '8', '9', '4', '1', '1', '1', '1', '1', '1', '1', '9', '1', '9', '1', '1', '1', '12', '3', '10', '3', '6', '1', '7', '10', '8', '3', '7', '1', '1', '2', '6', '1', '3', '9', '2', '8', '1', '8', '8', '1', '8', '10', '12', '1', '10', '12', '10', '1', '8', '8', '1', '4', '7', '9', '2', '1', '9', '3', '8', '12', '10', '6', '1', '6', '8', '3', '1', '3', '1', '10', '10', '9', '8', '1', '2', '9', '8', '8', '1', '9', '8', '8', '3', '1', '3', '9', '12', '1', '8', '1', '5', '8', '11', '5', '5', '1', '2', '1', '3', '1', '1', '1', '9', '8', '8', '1', '9', '10', '8', '4', '1', '1', '8', '10', '8', '9', '1', '10', '1', '7', '2', '1', '1', '9', '6', '2', '9', '8', '1', '1', '1', '9', '8', '8', '1', '9', '9', '1', '1', '2', '7', '9', '2', '9', '9', '8', '7', '6', '8', '7', '8', '2', '9', '2', '1', '8', '8', '8', '2', '6', '3', '1', '8', '12', '12', '8', '7', '1', '2', '2', '7', '1', '9', '8', '1', '4', '1', '1', '8', '5', '8', '1', '8', '8', '8', '8', '1', '2', '11', '1', '1', '1', '2', '7', '1', '9', '2', '2', '2', '9', '8', '9', '2', '9', '1', '8', '1', '7', '11', '13', '1', '9', '8', '2', '2', '1', '2', '3', '10', '3', '10', '4', '3', '9', '1', '8', '1', '1', '13', '1', '1', '3', '8', '1', '2', '2', '2', '9', '4', '1', '8', '1', '9', '3', '6', '8', '5', '2', '8', '2', '8', '1', '9', '2', '2', '8', '1', '1', '4', '8', '9', '2', '2', '7', '1', '8', '12', '2', '12', '8', '2', '9', '9', '3', '8', '1', '1', '9', '9', '9', '2', '8', '1', '9', '1', '8', '9', '8', '2', '2', '4', '1', '9', '10', '10', '8', '8', '1', '1', '2', '1', '4', '1', '2', '1', '1', '3', '12', '8', '8', '8', '8', '10', '2', '8', '12', '1', '9', '9', '8', '9', '1', '2', '1', '8', '8', '8', '2', '8', '8', '3', '10', '8', '9', '1', '1', '1', '3', '4', '8', '10', '6', '3', '1', '3', '2', '8', '9', '3', '5', '4', '1', '5', '2', '9', '9', '9', '1', '1', '1', '9', '9', '6', '10', '9', '8', '12', '2', '9', '1', '2', '1', '1', '1', '8', '1', '12', '1', '12', '1', '9', '1', '8', '1', '2', '1', '2', '8', '8', '10', '1', '9', '1', '2', '2', '8', '8', '3', '12', '1', '11', '2', '10', '12', '11', '2', '1', '10', '1', '1', '9', '3', '10', '2', '7', '1', '8', '1', '9', '9', '2', '1', '3', '12', '5', '9', '9', '8', '1', '8', '8', '8', '1', '1', '3', '9', '8', '9', '8', '3', '10', '3', '7', '8', '9', '2', '3', '1', '9', '10', '9', '2', '8', '2', '6', '4', '1', '1', '3', '8', '2', '3', '9', '1', '2', '1', '8', '1', '2', '2', '2', '2', '9', '9', '4', '8', '9', '1', '1', '2', '9', '1', '5', '2', '9', '8', '10', '1', '2', '10', '8', '5', '6', '4', '8', '1', '1', '1', '8', '9', '2', '8', '8', '8', '12', '2', '2', '1', '8', '1', '2', '1', '9', '8', '8', '9', '1', '1', '8', '7', '1', '8', '0', '2', '3', '8', '1', '1', '8', '1', '1', '12', '3', '2', '10', '8', '10', '6', '1', '8', '8', '10', '8', '8', '1', '4', '8', '1', '2', '9', '8', '1', '1', '3', '3', '5', '1', '1', '2', '1', '2', '1', '9', '8', '3', '9', '8', '2', '10', '9', '1', '8', '1', '10', '9', '10', '1', '2', '3', '8', '1', '3', '1', '9', '9', '8', '1', '9', '8', '8', '9', '1', '9', '2', '8', '8', '2', '13', '8', '10', '8', '9', '3', '2', '1', '3', '1', '12', '10', '10', '1', '11', '1', '2', '4', '2', '10', '1', '8', '11', '1', '1', '1', '2', '2', '8', '2', '9', '8', '9', '9', '12', '8', '1', '9', '2', '4', '1', '8', '3', '9', '1', '6', '5', '8', '9', '3', '3', '2', '1', '1', '1', '8', '11', '1', '8', '7', '1', '11', '9', '1', '7', '9', '9', '8', '8', '8', '12', '8', '1', '2', '9', '8', '8', '2', '10', '9', '8', '8', '4', '10', '9', '8', '2', '10', '10', '2', '1', '2', '1', '2', '1', '4', '1', '7', '1', '1', '3', '12', '9', '8', '5', '9', '10', '1', '9', '9', '2', '7', '4', '1', '1', '8', '12', '8', '2', '2', '9', '4', '1', '1', '5', '9', '1', '8', '12', '8', '1', '1', '4', '9', '10', '2', '3', '1', '3', '3', '3', '8', '1', '2', '1', '2', '1', '2', '9', '1', '2', '8', '9', '4', '8', '8', '10', '2', '12', '5', '5', '1', '8', '1', '7', '8', '1', '1', '8', '9', '12', '4', '9', '3', '1', '2', '5', '9', '2', '1', '2', '2', '1', '2', '8', '1', '8', '8', '2', '2', '1', '8', '4', '1', '2', '10', '9', '10', '8', '1', '10', '4', '8', '8', '10', '2', '8', '1', '1', '3', '10', '1', '4', '2', '8', '3', '3', '2', '9', '8', '5', '5', '9', '3', '10', '8', '1', '2', '1', '2', '9', '1', '1', '2', '9', '9', '8', '8', '11', '9', '4', '1', '8', '2', '7', '1', '2', '8', '2', '9', '9', '8', '2', '9', '9', '8', '8', '1', '2', '8', '1', '1', '2', '9', '8', '1', '4', '9', '1', '8', '12', '1', '12', '2', '8', '2', '9', '1', '12', '1', '2', '9', '1', '1', '9', '3', '2', '9', '9', '2', '11', '8', '7', '9', '9', '9', '8', '8', '1', '2', '8', '1', '1', '4', '1', '2', '10', '1', '9', '2', '1', '1', '1', '9', '1', '13', '4', '3', '2', '1', '1', '2', '2', '7', '8', '7', '1', '1', '2', '8', '8', '3', '4', '8', '2', '10', '13', '8', '8', '9', '2', '2', '1', '1', '10', '4', '2', '7', '9', '8', '2', '1', '9', '8', '1', '1', '9', '8', '9', '8', '2', '8', '2', '2', '9', '2', '1', '9', '1', '2', '4', '8', '2', '1', '1', '8', '1', '4', '1', '9', '3', '9', '2', '7', '1', '9', '6', '1', '10', '2', '2', '2', '2', '2', '10', '10', '10', '9', '9', '9', '3', '12', '10', '1', '1', '10', '1', '1', '2', '2', '1', '2', '3', '12', '8', '8', '1', '2', '9', '9', '1', '1', '10', '8', '9', '8', '3', '1', '1', '3', '1', '10', '1', '1', '1', '4', '1', '1', '1', '7', '10', '2', '1', '8', '1', '9', '3', '1', '10', '8', '8', '1', '5', '9', '1', '2', '1', '8', '8', '1', '3', '1', '10', '2', '1', '1', '2', '8', '2', '5', '3', '5', '2', '1', '1', '9', '1', '7', '9', '9', '2', '8', '1', '12', '4', '1', '4', '9', '9', '4', '8', '1', '10', '10', '8', '8', '8', '9', '2', '3', '5', '1', '8', '1', '1', '1', '7', '10', '1', '4', '12', '3', '2', '2', '9', '1', '1', '1', '2', '8', '4', '1', '7', '3', '12', '8', '6', '2', '2', '7', '9', '11', '1', '10', '9', '1', '8', '12', '2', '11', '1', '2', '3', '1', '1', '1', '8', '9', '2', '2', '2', '2', '3', '3', '8', '10', '4', '1', '2', '9', '8', '2', '2', '9', '1', '8', '11', '4', '2', '8', '7', '1', '1', '7', '3', '9', '9', '4', '5', '8', '10', '2', '9', '9', '9', '8', '1', '9', '2', '2', '2', '1', '2', '9', '4', '3', '12', '4', '3', '2', '3', '8', '3', '10', '1', '10', '1', '1', '8', '10', '6', '8', '9', '9', '1', '2', '9', '2', '9', '3', '8', '1', '1', '12', '1', '5', '1', '10', '8', '10', '2', '7', '1', '1', '1', '1', '3', '2', '3', '5', '9', '1', '2', '1', '9', '8', '10', '9', '8', '1', '8', '1', '8', '2', '8', '1', '2', '8', '1', '2', '5', '8', '12', '2', '1', '1', '1', '1', '1', '8', '8', '2', '1', '4', '2', '8', '8', '8', '2', '9', '9', '11', '1', '2', '9', '3', '1', '1', '8', '9', '2', '4', '10', '1', '9', '8', '4', '9', '8', '10', '8', '8', '9', '2', '8', '9', '3', '3', '3', '4', '3', '2', '2', '10', '8', '2', '1', '5', '1', '9', '1', '1', '2', '1', '2', '3', '1', '1', '8', '8', '1', '10', '8', '8', '1', '6', '4', '8', '2', '5', '8', '1', '3', '8', '2', '8', '9', '1', '1', '8', '12', '10', '1', '5', '8', '2', '8', '10', '9', '9', '1', '6', '9', '1', '10', '8', '2', '9', '10', '1', '2', '1', '1', '8', '8', '3', '9', '8', '1', '10', '2', '8', '1', '10', '1', '5', '1', '1', '1', '5', '1', '1', '1', '4', '1', '1', '1', '8', '2', '8', '1', '8', '3', '8', '9', '1', '8', '1', '8', '1', '1', '2', '2', '1', '12', '9', '9', '1', '8', '2', '1', '2', '8', '1', '9', '1', '1', '2', '1', '2', '2', '2', '1', '9', '9', '8', '8', '1', '1', '9', '10', '8', '4', '3', '3', '4', '9', '1', '3', '3', '10', '2', '2', '1', '8', '4', '1', '9', '4', '1', '1', '9', '8', '10', '4', '9', '9', '8', '2', '1', '3', '4', '9', '3', '8', '9', '1', '1', '2', '1', '7', '8', '2', '9', '1', '9', '2', '6', '9', '10', '8', '1', '1', '5', '9', '5', '1', '9', '10', '9', '3', '3', '8', '8', '2', '9', '2', '8', '2', '1', '8', '5', '8', '3', '10', '1', '1', '2', '1', '8', '1', '4', '3', '2', '9', '4', '9', '8', '8', '8', '1', '3', '9', '1', '9', '9', '1', '2', '1', '9', '8', '9', '8', '1', '1', '1', '4', '2', '2', '9', '9', '1', '1', '1', '3', '6', '1', '2', '3', '1', '4', '2', '9', '1', '9', '9', '1', '8', '2', '1', '9', '1', '8', '1', '8', '8', '2', '1', '9', '9', '6', '8', '8', '9', '8', '10', '8', '9', '5', '1', '1', '1', '1', '8', '1', '8', '12', '8', '1', '12', '9', '2', '8', '1', '1', '2', '1', '8', '6', '9', '9', '8', '1', '8', '9', '7', '7', '8', '9', '9', '1', '8', '3', '8', '1', '3', '3', '8', '9', '8', '2', '8', '1', '2', '9', '8', '1', '1', '1', '1', '9', '1', '9', '8', '12', '3', '8', '8', '1', '10', '10', '10', '2', '2', '1', '10', '2', '2', '10', '8', '12', '8', '1', '1', '1', '2', '1', '1', '3', '8', '1', '2', '2', '2', '1', '1', '6', '2', '8', '8', '8', '1', '1', '1', '9', '9', '2', '8', '10', '2', '8', '2', '1', '2', '9', '9', '2', '1', '1', '8', '8', '1', '3', '1', '8', '9', '1', '10', '9', '1', '1', '1', '1', '7', '2', '2', '1', '2', '8', '8', '1', '2', '1', '3', '2', '1', '8', '3', '9', '2', '10', '2', '10', '1', '2', '1', '1', '13', '9', '10', '8', '2', '2', '1', '1', '8', '2', '2', '1', '9', '1', '1', '8', '9', '9', '2', '9', '2', '2', '1', '1', '1', '8', '9', '4', '9', '9', '1', '8', '2', '1', '1', '8', '2', '2', '2', '8', '1', '1', '8', '2', '10', '2', '9', '9', '7', '1', '1', '2', '2', '8', '3', '1', '2', '1', '1', '8', '1', '1', '1', '9', '1', '1', '8', '8', '1', '9', '6', '1', '9', '1', '8', '1', '8', '8', '8', '1', '3', '8', '8', '2', '6', '3', '2', '1', '10', '1', '8', '2', '1', '6', '1', '1', '1', '8', '1', '10', '1', '1', '8', '9', '2', '6', '9', '2', '9', '4', '1', '8', '1', '3', '7', '8', '8', '5', '8', '7', '1', '1', '1', '13', '8', '3', '1', '1', '1', '1', '2', '8', '2', '12', '8', '1', '2', '5', '1', '8', '2', '10', '8', '2', '10', '1', '5', '2', '1', '1', '5', '2', '2', '7', '8', '2', '9', '9', '1', '10', '1', '8', '2', '5', '8', '1', '5', '1', '10', '1', '1', '9', '8', '6', '1', '8', '9', '8', '1', '8', '2', '2', '8', '3', '9', '1', '9', '3', '8', '4', '4', '3', '3', '1', '1', '1', '8', '9', '1', '1', '8', '3', '1', '2', '8', '2', '1', '1', '10', '1', '2', '2', '1', '1', '8', '2', '8', '2', '10', '2', '2', '10', '8', '8', '8', '13', '2', '2', '9', '2', '5', '9', '2', '1', '8', '8', '8', '1', '12', '12', '2', '9', '2', '1', '8', '2', '1', '8', '1', '9', '9', '2', '7', '1', '1', '7', '1', '4', '3', '1', '10', '3', '10', '9', '4', '6', '7', '5', '2', '1', '8', '8', '1', '4', '9', '2', '1', '8', '1', '1', '1', '1', '1', '8', '9', '8', '8', '8', '8', '9', '8', '10', '1', '8', '2', '7', '8', '9', '1', '2', '5', '9', '10', '8', '9', '12', '1', '6', '1', '2', '10', '9', '4', '1', '8', '10', '10', '1', '1', '1', '1', '1', '2', '3', '8', '1', '2', '10', '2', '1', '2', '4', '2', '2', '9', '1', '4', '2', '1', '1', '12', '12', '9', '9', '7', '1', '8', '4', '9', '9', '8', '1', '7', '3', '2', '1', '1', '4', '8', '1', '5', '8', '2', '8', '1', '14', '9', '12', '1', '10', '2', '1', '1', '8', '8', '6', '6', '3', '1', '2', '8', '1', '3', '2', '1', '8', '8', '1', '10', '8', '2', '8', '2', '5', '9', '1', '8', '8', '1', '1', '8', '8', '13', '1', '3', '8', '4', '1', '9', '7', '8', '8', '8', '3', '8', '1', '4', '2', '8', '8', '8', '7', '1', '1', '2', '8', '8', '9', '8', '8', '2', '3', '9', '9', '2', '1', '2', '2', '1', '8', '2', '6', '2', '1', '1', '9', '10', '3', '8', '2', '3', '2', '8', '8', '1', '8', '8', '8', '9', '1', '4', '1', '9', '2', '1', '8', '0', '1', '8', '1', '1', '8', '9', '9', '1', '3', '1', '2', '8', '8', '7', '1', '1', '2', '9', '2', '10', '2', '8', '2', '2', '2', '7', '1', '9', '8', '1', '3', '9', '2', '1', '10', '8', '1', '4', '1', '8', '5', '9', '8', '8', '1', '2', '2', '1', '8', '8', '6', '8', '1', '1', '8', '3', '2', '2', '1', '8', '1', '7', '6', '8', '1', '1', '1', '9', '1', '1', '1', '2', '1', '11', '10', '10', '11', '9', '']\n",
            "['8', '1', '8', '2', '8', '8', '8', '9', '7', '8', '1', '2', '1', '1', '8', '4', '8', '8', '12', '3', '3', '7', '3', '12', '1', '8', '8', '8', '8', '8', '8', '8', '1', '9', '5', '9', '9', '9', '11', '8', '8', '8', '4', '8', '8', '8', '8', '1', '3', '9', '3', '7', '1', '2', '9', '9', '7', '8', '8', '1', '10', '7', '8', '8', '9', '8', '7', '9', '9', '12', '7', '2', '8', '1', '11', '11', '1', '7', '7', '12', '1', '9', '8', '10', '12', '7', '8', '2', '8', '9', '9', '1', '8', '9', '1', '7', '12', '10', '10', '10', '8', '3', '7', '9', '8', '9', '1', '8', '8', '2', '7', '2', '9', '9', '11', '8', '8', '12', '12', '7', '8', '12', '4', '9', '3', '1', '12', '1', '1', '8', '8', '3', '8', '8', '8', '8', '9', '1', '8', '8', '10', '1', '8', '2', '8', '8', '7', '3', '8', '2', '4', '4', '9', '8', '10', '12', '12', '1', '1', '9', '1', '1', '1', '8', '2', '2', '8', '1', '1', '2', '2', '1', '2', '8', '1', '9', '9', '8', '8', '4', '2', '9', '9', '8', '3', '4', '3', '1', '8', '8', '2', '1', '9', '7', '8', '8', '1', '12', '3', '8', '2', '4', '2', '9', '12', '1', '4', '1', '8', '8', '8', '2', '2', '8', '9', '8', '8', '8', '10', '9', '8', '7', '9', '1', '1', '9', '4', '2', '4', '2', '2', '1', '7', '8', '11', '11', '3', '9', '2', '4', '8', '9', '1', '8', '1', '1', '4', '9', '1', '1', '8', '8', '2', '1', '8', '4', '2', '8', '9', '8', '8', '2', '8', '8', '8', '7', '1', '1', '1', '2', '1', '1', '8', '7', '8', '8', '12', '2', '12', '12', '8', '10', '12', '8', '3', '3', '12', '10', '1', '8', '12', '1', '8', '8', '2', '8', '4', '7', '8', '7', '10', '8', '10', '9', '8', '12', '12', '1', '8', '8', '3', '8', '8', '8', '8', '8', '1', '9', '8', '11', '1', '1', '1', '9', '8', '1', '9', '2', '3', '11', '8', '9', '9', '9', '2', '1', '8', '8', '9', '7', '1', '4', '9', '4', '8', '8', '4', '8', '12', '9', '4', '8', '2', '10', '10', '10', '8', '9', '9', '8', '8', '12', '7', '1', '8', '8', '8', '4', '1', '1', '1', '1', '1', '1', '8', '1', '9', '8', '9', '9', '4', '8', '12', '9', '8', '8', '2', '8', '8', '8', '6', '9', '8', '3', '7', '8', '8', '4', '12', '8', '8', '9', '12', '12', '9', '8', '2', '9', '2', '3', '1', '12', '8', '10', '9', '9', '9', '10', '10', '3', '8', '12', '1', '4', '2', '1', '10', '8', '2', '8', '4', '8', '9', '1', '9', '9', '10', '10', '1', '4', '9', '2', '4', '9', '1', '1', '3', '10', '3', '3', '8', '7', '3', '8', '9', '9', '12', '4', '8', '12', '2', '2', '4', '1', '9', '9', '4', '1', '4', '2', '8', '12', '2', '3', '10', '10', '9', '8', '9', '9', '1', '12', '8', '8', '8', '12', '4', '1', '8', '8', '1', '9', '8', '8', '2', '1', '8', '9', '8', '3', '3', '3', '1', '8', '8', '9', '1', '10', '9', '9', '9', '9', '5', '9', '9', '8', '8', '8', '8', '8', '8', '8', '8', '11', '12', '8', '8', '1', '8', '9', '11', '2', '2', '2', '2', '3', '1', '2', '2', '8', '2', '4', '9', '1', '2', '9', '8', '2', '8', '9', '9', '3', '10', '9', '9', '2', '8', '9', '8', '12', '12', '1', '3', '8', '8', '8', '2', '7', '7', '7', '7', '3', '9', '1', '9', '8', '9', '9', '1', '1', '1', '2', '9', '9', '9', '11', '1', '8', '8', '9', '1', '9', '8', '8', '8', '1', '1', '8', '7', '1', '1', '8', '8', '9', '4', '4', '8', '2', '2', '8', '8', '8', '8', '8', '8', '11', '8', '2', '9', '4', '9', '3', '9', '9', '1', '3', '9', '3', '1', '12', '8', '9', '12', '1', '8', '4', '2', '1', '4', '8', '3', '3', '8', '2', '8', '9', '7', '8', '8', '8', '5', '8', '3', '9', '8', '8', '13', '12', '1', '1', '2', '8', '4', '1', '9', '9', '12', '8', '9', '12', '9', '1', '9', '9', '9', '9', '3', '2', '9', '9', '4', '8', '12', '2', '4', '9', '3', '1', '9', '7', '8', '9', '9', '8', '4', '8', '8', '7', '9', '10', '3', '8', '8', '8', '1', '1', '1', '1', '8', '8', '4', '1', '10', '1', '5', '7', '7', '1', '8', '9', '3', '7', '2', '7', '7', '2', '4', '8', '12', '7', '4', '2', '9', '9', '12', '6', '10', '8', '2', '4', '12', '9', '9', '3', '8', '8', '1', '2', '10', '9', '9', '8', '4', '12', '2', '1', '8', '8', '8', '12', '10', '10', '9', '3', '8', '8', '9', '2', '8', '10', '1', '1', '1', '1', '2', '1', '1', '1', '1', '9', '8', '12', '9', '4', '8', '8', '9', '1', '9', '3', '9', '8', '8', '1', '7', '7', '10', '1', '8', '8', '1', '9', '8', '10', '3', '1', '7', '1', '8', '8', '12', '8', '8', '1', '8', '7', '1', '7', '7', '8', '2', '1', '8', '8', '2', '10', '8', '8', '8', '8', '8', '10', '1', '8', '8', '12', '8', '3', '3', '2', '2', '2', '10', '8', '8', '8', '2', '9', '1', '8', '9', '3', '2', '8', '10', '8', '6', '1', '1', '8', '4', '1', '9', '10', '8', '1', '7', '1', '2', '8', '1', '1', '1', '12', '1', '9', '12', '8', '12', '12', '12', '8', '8', '12', '4', '8', '8', '8', '8', '9', '9', '1', '3', '3', '3', '3', '1', '12', '12', '9', '10', '8', '8', '1', '9', '2', '2', '13', '9', '8', '9', '2', '1', '9', '1', '8', '8', '8', '4', '8', '1', '1', '1', '12', '12', '7', '2', '2', '2', '8', '3', '8', '9', '2', '10', '7', '8', '9', '2', '1', '2', '12', '12', '8', '8', '9', '2', '2', '9', '11', '1', '8', '1', '10', '9', '2', '1', '4', '7', '7', '7', '7', '7', '12', '8', '8', '8', '1', '1', '10', '1', '12', '1', '8', '2', '1', '1', '12', '8', '7', '9', '12', '8', '9', '3', '9', '8', '8', '8', '8', '3', '11', '2', '2', '9', '8', '8', '10', '8', '2', '7', '3', '1', '4', '7', '8', '8', '1', '8', '3', '7', '12', '8', '10', '9', '9', '8', '8', '2', '8', '9', '9', '1', '2', '8', '8', '9', '8', '3', '8', '1', '8', '10', '9', '8', '9', '9', '12', '4', '4', '8', '9', '9', '8', '2', '10', '1', '2', '8', '9', '1', '9', '9', '9', '7', '12', '12', '8', '1', '1', '1', '1', '8', '3', '1', '1', '8', '1', '8', '8', '7', '8', '8', '8', '8', '3', '2', '2', '10', '10', '10', '7', '8', '1', '2', '12', '7', '9', '8', '7', '8', '12', '2', '8', '9', '2', '6', '6', '7', '9', '8', '1', '8', '9', '8', '1', '12', '1', '2', '8', '7', '7', '7', '8', '2', '2', '8', '1', '2', '2', '9', '9', '1', '8', '8', '4', '3', '3', '1', '6', '3', '3', '12', '3', '8', '9', '1', '4', '3', '1', '8', '3', '9', '2', '8', '2', '8', '8', '8', '1', '1', '1', '9', '9', '8', '1', '9', '8', '1', '1', '3', '10', '8', '1', '1', '3', '9', '1', '4', '4', '1', '8', '9', '9', '2', '0', '0', '1', '8', '3', '1', '8', '8', '9', '8', '8', '1', '1', '8', '9', '8', '8', '8', '7', '9', '8', '8', '8', '10', '9', '8', '1', '2', '6', '1', '9', '9', '8', '12', '12', '12', '8', '8', '2', '8', '1', '2', '2', '2', '1', '9', '8', '2', '12', '2', '8', '12', '8', '9', '8', '8', '9', '7', '1', '1', '1', '1', '1', '8', '8', '1', '8', '8', '1', '1', '3', '2', '8', '8', '9', '10', '10', '2', '2', '1', '9', '2', '9', '9', '4', '12', '12', '12', '10', '7', '3', '3', '4', '2', '2', '9', '2', '8', '4', '2', '4', '1', '10', '9', '7', '8', '7', '1', '1', '3', '3', '1', '1', '3', '3', '3', '1', '1', '1', '1', '8', '2', '3', '1', '1', '2', '8', '8', '12', '8', '8', '8', '8', '11', '9', '1', '8', '9', '2', '8', '8', '8', '3', '9', '1', '9', '2', '7', '2', '8', '2', '8', '10', '8', '1', '10', '1', '1', '9', '9', '8', '8', '1', '8', '8', '8', '12', '8', '8', '8', '1', '8', '8', '8', '1', '9', '1', '1', '8', '1', '8', '9', '8', '2', '12', '9', '9', '0', '1', '8', '8', '1', '8', '12', '8', '8', '10', '8', '8', '8', '7', '8', '1', '8', '7', '3', '10', '1', '8', '9', '1', '8', '8', '8', '10', '1', '10', '3', '9', '1', '8', '9', '2', '8', '3', '3', '9', '9', '7', '9', '1', '1', '9', '2', '1', '1', '1', '7', '1', '1', '8', '8', '1', '1', '8', '1', '8', '3', '12', '9', '3', '3', '8', '8', '8', '8', '3', '1', '3', '3', '1', '11', '0', '8', '8', '7', '8', '12', '1', '8', '9', '8', '9', '8', '8', '3', '8', '8', '1', '1', '1', '9', '2', '2', '2', '8', '7', '12', '8', '8', '9', '10', '10', '7', '8', '1', '9', '8', '7', '3', '1', '3', '8', '2', '2', '3', '9', '8', '4', '4', '8', '9', '2', '1', '1', '7', '8', '9', '9', '7', '8', '7', '7', '8', '2', '2', '8', '4', '9', '7', '10', '0', '9', '8', '3', '7', '8', '1', '1', '8', '9', '9', '2', '2', '10', '1', '9', '10', '10', '10', '8', '3', '2', '12', '9', '9', '10', '12', '9', '12', '12', '9', '1', '2', '4', '12', '12', '7', '8', '9', '7', '7', '7', '3', '9', '8', '9', '1', '12', '8', '9', '4', '1', '3', '12', '12', '12', '12', '8', '8', '2', '1', '1', '2', '1', '1', '1', '12', '12', '8', '12', '2', '2', '12', '3', '3', '12', '8', '2', '8', '8', '12', '2', '1', '10', '3', '2', '8', '7', '1', '8', '1', '3', '7', '8', '9', '8', '3', '1', '1', '7', '8', '8', '9', '8', '2', '9', '2', '2', '9', '8', '1', '8', '8', '1', '3', '3', '1', '1', '10', '1', '2', '8', '1', '1', '1', '1', '9', '1', '4', '1', '7', '7', '7', '7', '2', '2', '8', '8', '12', '1', '9', '1', '7', '3', '3', '1', '8', '8', '10', '8', '9', '2', '9', '1', '3', '8', '8', '3', '12', '2', '8', '12', '2', '9', '1', '3', '3', '3', '3', '2', '8', '7', '9', '8', '3', '3', '1', '7', '8', '3', '1', '1', '12', '8', '9', '1', '2', '3', '8', '1', '1', '3', '3', '9', '1', '1', '1', '12', '1', '7', '3', '3', '1', '8', '8', '8', '1', '2', '4', '8', '1', '10', '2', '5', '3', '3', '12', '10', '9', '9', '12', '9', '0', '2', '8', '8', '9', '9', '9', '8', '1', '3', '1', '1', '4', '8', '1', '10', '8', '7', '2', '8', '2', '8', '4', '7', '8', '1', '9', '1', '9', '8', '2', '8', '2', '7', '9', '2', '2', '9', '1', '8', '12', '1', '8', '1', '4', '1', '9', '9', '1', '10', '12', '4', '8', '1', '7', '3', '9', '2', '12', '7', '8', '8', '2', '1', '12', '9', '8', '1', '2', '2', '8', '10', '2', '1', '7', '7', '7', '12', '3', '3', '8', '3', '8', '8', '3', '9', '8', '9', '1', '1', '8', '7', '9', '3', '3', '8', '1', '0', '9', '9', '9', '1', '8', '9', '9', '10', '1', '8', '7', '8', '8', '8', '9', '9', '8', '9', '10', '4', '9', '3', '7', '12', '1', '9', '9', '8', '9', '1', '9', '3', '1', '8', '4', '12', '10', '9', '8', '7', '10', '8', '12', '12', '3', '10', '8', '12', '2', '1', '2', '3', '9', '8', '8', '7', '1', '1', '2', '2', '1', '2', '2', '7', '1', '3', '9', '9', '3', '8', '8', '8', '8', '8', '7', '8', '8', '10', '8', '1', '8', '2', '8', '2', '2', '2', '2', '2', '1', '7', '12', '10', '1', '2', '8', '1', '4', '7', '8', '1', '12', '8', '7', '9', '2', '2', '2', '8', '1', '8', '1', '1', '0', '2', '2', '6', '1', '8', '2', '1', '8', '1', '1', '2', '2', '3', '2', '2', '10', '10', '9', '1', '7', '7', '8', '8', '1', '12', '10', '12', '3', '8', '8', '8', '3', '8', '3', '10', '2', '2', '2', '1', '2', '2', '1', '0', '1', '8', '9', '1', '1', '1', '2', '2', '2', '2', '2', '1', '1', '1', '1', '12', '9', '9', '1', '10', '11', '12', '9', '1', '1', '8', '8', '1', '2', '12', '8', '3', '7', '2', '10', '9', '7', '8', '1', '2', '2', '7', '9', '12', '2', '2', '1', '8', '9', '2', '3', '2', '2', '2', '9', '9', '8', '9', '2', '3', '8', '8', '9', '7', '3', '10', '8', '8', '9', '1', '1', '2', '8', '1', '10', '8', '8', '9', '7', '1', '7', '12', '8', '1', '7', '2', '1', '3', '9', '2', '8', '8', '1', '2', '3', '9', '3', '8', '9', '7', '7', '1', '10', '2', '8', '8', '8', '1', '8', '2', '8', '8', '1', '2', '8', '10', '2', '1', '0', '8', '3', '8', '10', '10', '12', '3', '8', '9', '3', '10', '8', '8', '8', '1', '8', '8', '2', '2', '2', '2', '2', '2', '1', '1', '10', '1', '10', '8', '2', '1', '2', '2', '8', '3', '3', '2', '2', '1', '8', '1', '3', '1', '9', '2', '2', '1', '3', '2', '2', '3', '2', '2', '3', '2', '1', '1', '2', '1', '2', '7', '8', '2', '1', '12', '12', '3', '1', '9', '9', '7', '8', '7', '2', '7', '9', '2', '2', '2', '8', '9', '7', '9', '2', '8', '9', '9', '2', '9', '1', '1', '9', '8', '3', '3', '8', '2', '7', '8', '11', '2', '8', '9', '1', '8', '3', '1', '2', '1', '2', '2', '2', '2', '8', '3', '9', '2', '10', '9', '9', '2', '7', '7', '7', '1', '3', '8', '8', '1', '1', '8', '7', '1', '8', '8', '9', '3', '3', '3', '2', '4', '8', '12', '8', '1', '12', '9', '2', '8', '2', '12', '2', '1', '3', '8', '12', '12', '11', '8', '10', '9', '2', '9', '4', '10', '1', '12', '12', '9', '3', '8', '9', '12', '9', '9', '2', '2', '2', '3', '5', '1', '1', '1', '7', '7', '8', '2', '2', '8', '2', '1', '7', '3', '2', '9', '8', '9', '1', '1', '9', '8', '9', '3', '3', '2', '9', '4', '9', '8', '8', '1', '8', '10', '2', '0', '3', '8', '8', '8', '9', '12', '9', '8', '2', '2', '8', '10', '9', '8', '2', '2', '4', '1', '9', '9', '8', '8', '10', '3', '1', '1', '2', '8', '7', '8', '2', '1', '1', '8', '12', '2', '9', '4', '8', '3', '3', '3', '8', '2', '12', '12', '8', '9', '12', '8', '2', '12', '7', '9', '2', '2', '1', '3', '1', '3', '11', '9', '8', '1', '10', '2', '12', '3', '8', '4', '1', '2', '8', '8', '3', '9', '3', '8', '7', '1', '1', '3', '1', '8', '2', '9', '8', '12', '1', '1', '1', '9', '9', '1', '8', '8', '8', '2', '2', '12', '3', '1', '1', '1', '9', '9', '3', '2', '1', '1', '9', '9', '1', '9', '9', '1', '9', '3', '9', '1', '8', '1', '9', '8', '3', '8', '8', '9', '9', '8', '2', '8', '1', '2', '2', '10', '1', '2', '1', '1', '9', '1', '9', '3', '7', '7', '2', '10', '3', '3', '1', '1', '9', '1', '2', '1', '3', '2', '1', '1', '1', '1', '1', '1', '1', '1', '1', '2', '8', '8', '7', '1', '1', '8', '8', '2', '1', '12', '1', '8', '1', '9', '8', '8', '2', '1', '9', '9', '2', '2', '8', '1', '7', '7', '9', '8', '1', '6', '8', '8', '2', '2', '8', '3', '8', '3', '2', '3', '8', '1', '2', '2', '2', '8', '9', '9', '9', '12', '1', '9', '2', '1', '9', '2', '9', '9', '10', '8', '1', '1', '9', '2', '1', '7', '1', '3', '7', '1', '1', '1', '3', '8', '8', '8', '9', '9', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '1', '0', '1', '1', '1', '2', '2', '3', '1', '1', '2', '3', '3', '3', '7', '3', '1', '9', '3', '9', '1', '4', '1', '1', '9', '10', '9', '8', '2', '3', '10', '9', '3', '10', '12', '8', '9', '8', '2', '1', '9', '8', '1', '7', '8', '1', '9', '1', '2', '10', '7', '7', '8', '1', '3', '3', '9', '3', '3', '1', '1', '1', '9', '1', '3', '8', '8', '8', '1', '8', '1', '9', '7', '8', '8', '2', '1', '3', '8', '2', '1', '6', '1', '8', '8', '2', '8', '2', '2', '9', '8', '1', '1', '6', '8', '6', '10', '2', '1', '1', '8', '9', '3', '3', '3', '1', '3', '9', '6', '8', '1', '8', '3', '1', '2', '2', '12', '7', '1', '1', '1', '1', '12', '1', '8', '8', '3', '2', '1', '9', '2', '3', '3', '2', '2', '7', '2', '2', '2', '3', '1', '9', '8', '7', '1', '1', '3', '1', '9', '4', '3', '9', '2', '1', '1', '9', '8', '8', '7', '2', '1', '3', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '2', '10', '1', '1', '8', '2', '8', '1', '8', '1', '3', '1', '2', '1', '1', '3', '1', '2', '1', '1', '1', '1', '1', '1', '1', '2', '1', '8', '1', '9', '8', '8', '1', '10', '8', '8', '1', '3', '9', '8', '9', '8', '2', '1', '3', '4', '1', '12', '1', '9', '8', '2', '7', '2', '1', '2', '9', '2', '1', '3', '8', '1', '2', '3', '8', '2', '10', '10', '10', '9', '9', '9', '3', '8', '3', '1', '1', '1', '1', '1', '1', '2', '1', '9', '2', '7', '9', '8', '7', '9', '1', '1', '1', '1', '8', '2', '2', '3', '3', '2', '12', '8', '3', '1', '1', '12', '9', '8', '2', '2', '9', '1', '2', '2', '9', '1', '1', '2', '4', '8', '1', '8', '10', '2', '9', '9', '9', '1', '2', '1', '1', '2', '2', '2', '12', '1', '2', '8', '3', '9', '9', '3', '2', '9', '9', '7', '2', '8', '8', '9', '2', '9', '1', '1', '1', '1', '1', '2', '8', '2', '1', '2', '2', '8', '3', '1', '1', '9', '1', '3', '10', '7', '9', '8', '8', '2', '2', '9', '2', '2', '2', '2', '2', '2', '9', '1', '3', '6', '4', '2', '2', '9', '1', '9', '8', '1', '1', '1', '7', '3', '2', '9', '9', '1', '9', '7', '2', '8', '9', '9', '12', '10', '8', '10', '2', '3', '1', '2', '2', '12', '12', '3', '12', '1', '2', '1', '2', '1', '2', '8', '2', '3', '12', '12', '8', '2', '2', '9', '2', '9', '2', '3', '3', '1', '1', '1', '12', '3', '7', '1', '3', '1', '2', '9', '2', '7', '2', '1', '8', '3', '7', '8', '1', '1', '3', '8', '3', '2', '9', '3', '1', '1', '1', '1', '11', '1', '2', '6', '1', '2', '3', '8', '9', '9', '2', '9', '2', '1', '1', '3', '1', '12', '9', '1', '1', '9', '1', '9', '9', '8', '9', '8', '1', '2', '7', '1', '0', '9', '8', '2', '3', '2', '4', '1', '1', '1', '8', '1', '12', '1', '2', '3', '3', '9', '9', '9', '9', '9', '7', '9', '3', '9', '1', '7', '3', '3', '3', '3', '7', '8', '2', '2', '2', '8', '2', '3', '9', '9', '3', '1', '8', '9', '12', '8', '8', '3', '2', '1', '6', '1', '9', '1', '1', '1', '1', '1', '1', '2', '2', '1', '2', '2', '2', '2', '9', '2', '5', '3', '2', '2', '8', '2', '1', '8', '3', '3', '8', '3', '1', '3', '9', '2', '8', '9', '4', '4', '9', '8', '8', '7', '7', '3', '1', '8', '8', '3', '2', '2', '10', '3', '3', '2', '2', '2', '2', '12', '3', '1', '2', '7', '7', '12', '2', '1', '2', '1', '1', '2', '1', '3', '3', '3', '1', '3', '9', '4', '8', '8', '4', '2', '4', '2', '8', '9', '1', '1', '2', '2', '2', '8', '7', '2', '8', '2', '9', '1', '8', '1', '1', '2', '1', '1', '1', '2', '9', '9', '8', '1', '8', '1', '4', '8', '1', '2', '7', '9', '2', '1', '8', '9', '8', '9', '1', '1', '2', '3', '2', '12', '7', '1', '2', '4', '4', '4', '2', '2', '2', '9', '8', '8', '1', '12', '3', '3', '2', '2', '3', '12', '1', '5', '1', '1', '3', '2', '8', '8', '2', '2', '2', '3', '9', '8', '8', '9', '8', '8', '11', '8', '2', '9', '2', '3', '1', '7', '1', '8', '2', '8', '1', '1', '1', '8', '1', '1', '1', '7', '8', '8', '2', '3', '2', '1', '8', '8', '1', '8', '4', '8', '9', '8', '9', '8', '1', '2', '3', '2', '1', '2', '2', '3', '1', '9', '4', '8', '1', '1', '9', '1', '1', '8', '2', '2', '2', '1', '7', '3', '3', '1', '3', '3', '12', '3', '1', '1', '3', '1', '1', '4', '8', '4', '4', '3', '3', '3', '4', '8', '1', '9', '9', '9', '4', '1', '7', '4', '8', '4', '9', '8', '1', '1', '3', '8', '9', '1', '9', '7', '1', '1', '9', '1', '8', '8', '8', '1', '1', '8', '2', '2', '2', '9', '4', '4', '1', '2', '1', '2', '9', '1', '1', '8', '5', '5', '5', '9', '12', '4', '1', '2', '1', '8', '8', '1', '9', '9', '2', '12', '1', '1', '7', '8', '8', '4', '1', '2', '8', '11', '2', '3', '2', '2', '1', '11', '2', '2', '2', '2', '9', '9', '2', '2', '9', '3', '1', '9', '1', '1', '1', '7', '8', '1', '8', '9', '1', '2', '9', '8', '1', '2', '9', '8', '12', '2', '2', '10', '8', '2', '2', '8', '8', '1', '2', '6', '1', '1', '7', '7', '2', '3', '2', '1', '1', '8', '1', '10', '8', '12', '6', '4', '1', '4', '1', '2', '9', '8', '9', '4', '8', '8', '10', '8', '8', '8', '8', '8', '2', '2', '8', '4', '9', '3', '3', '3', '3', '3', '9', '2', '1', '1', '2', '3', '2', '1', '2', '3', '3', '3', '2', '2', '3', '3', '2', '1', '2', '3', '3', '3', '3', '12', '9', '3', '2', '2', '3', '1', '5', '1', '1', '3', '2', '2', '1', '2', '9', '3', '9', '8', '1', '8', '7', '7', '9', '1', '1', '7', '9', '10', '1', '7', '1', '4', '3', '8', '9', '4', '9', '9', '2', '9', '2', '8', '4', '2', '2', '9', '5', '2', '1', '10', '9', '3', '1', '1', '1', '2', '1', '2', '9', '1', '7', '9', '9', '8', '2', '2', '8', '9', '1', '9', '8', '2', '3', '2', '2', '1', '2', '2', '2', '1', '1', '8', '1', '9', '4', '12', '8', '2', '7', '2', '2', '1', '9', '3', '1', '8', '12', '1', '1', '2', '1', '10', '2', '6', '12', '12', '4', '9', '1', '9', '10', '1', '2', '8', '2', '1', '8', '6', '12', '9', '9', '2', '1', '7', '8', '9', '5', '8', '1', '8', '8', '3', '1', '8', '2', '8', '2', '2', '1', '2', '9', '2', '8', '1', '12', '8', '2', '7', '3', '3', '12', '2', '3', '3', '9', '9', '3', '3', '3', '3', '3', '2', '1', '4', '1', '4', '8', '8', '2', '1', '2', '9', '12', '1', '10', '12', '1', '2', '9', '4', '8', '10', '3', '2', '8', '7', '4', '8', '9', '2', '7', '1', '10', '2', '2', '2', '8', '4', '2', '2', '1', '1', '8', '7', '9', '2', '1', '9', '8', '9', '12', '4', '8', '2', '8', '7', '7', '1', '9', '2', '1', '1', '9', '1', '8', '2', '3', '9', '9', '1', '3', '2', '9', '2', '2', '8', '1', '1', '2', '1', '2', '1', '9', '8', '2', '2', '4', '8', '8', '9', '5', '5', '9', '8', '9', '6', '2', '1', '12', '2', '3', '8', '8', '9', '2', '2', '1', '3', '2', '10', '9', '10', '7', '2', '8', '4', '2', '8', '9', '8', '1', '3', '8', '8', '1', '8', '3', '8', '8', '1', '10', '3', '1', '5', '8', '8', '9', '2', '2', '9', '2', '1', '9', '1', '4', '1', '12', '8', '8', '2', '1', '1', '1', '1', '1', '9', '2', '9', '8', '9', '5', '9', '1', '8', '2', '4', '1', '1', '1', '9', '12', '4', '1', '1', '9', '8', '4', '1', '8', '9', '8', '8', '9', '9', '9', '9', '9', '9', '1', '3', '4', '2', '2', '8', '2', '9', '2', '3', '8', '3', '1', '7', '2', '1', '1', '4', '2', '9', '2', '1', '3', '2', '9', '2', '9', '3', '8', '1', '9', '1', '1', '2', '8', '8', '8', '2', '3', '9', '9', '8', '2', '4', '1', '5', '1', '1', '9', '9', '2', '1', '1', '1', '9', '9', '1', '3', '9', '2', '2', '9', '9', '8', '3', '9', '9', '8', '2', '2', '8', '9', '2', '2', '8', '9', '10', '10', '2', '8', '7', '9', '9', '4', '2', '4', '11', '2', '1', '7', '8', '8', '8', '9', '4', '9', '8', '8', '2', '1', '1', '9', '8', '3', '3', '8', '8', '7', '9', '1', '3', '4', '8', '10', '2', '4', '4', '8', '2', '8', '2', '8', '9', '2', '1', '2', '2', '3', '1', '9', '4', '5', '9', '9', '1', '1', '1', '1', '1', '1', '7', '1', '1', '1', '8', '1', '1', '1', '1', '1', '9', '1', '2', '1', '12', '9', '3', '9', '9', '4', '1', '9', '1', '1', '2', '11', '9', '3', '2', '2', '2', '4', '2', '3', '2', '7', '7', '8', '1', '10', '9', '5', '7', '1', '10', '8', '1', '2', '7', '1', '12', '5', '8', '4', '2', '9', '9', '8', '2', '9', '8', '8', '10', '2', '3', '2', '7', '2', '8', '10', '3', '2', '9', '9', '9', '1', '1', '1', '1', '8', '2', '10', '9', '8', '1', '2', '2', '2', '1', '2', '1', '3', '9', '12', '2', '2', '4', '8', '1', '3', '9', '4', '9', '1', '9', '9', '2', '1', '1', '9', '7', '10', '3', '10', '2', '9', '2', '9', '2', '8', '2', '1', '2', '3', '1', '1', '1', '8', '5', '8', '3', '1', '1', '4', '2', '2', '1', '3', '8', '2', '1', '1', '1', '2', '9', '4', '1', '8', '1', '2', '8', '2', '9', '2', '2', '5', '5', '2', '5', '2', '1', '8', '8', '1', '3', '12', '2', '4', '3', '2', '2', '2', '6', '2', '3', '3', '1', '8', '2', '2', '1', '1', '8', '8', '10', '1', '9', '2', '9', '12', '0', '2', '1', '3', '2', '2', '1', '2', '1', '9', '9', '2', '9', '8', '8', '1', '7', '8', '1', '2', '6', '9', '8', '10', '1', '12', '2', '2', '1', '8', '2', '1', '12', '7', '4', '2', '10', '2', '2', '1', '2', '1', '2', '1', '1', '2', '1', '9', '8', '10', '2', '10', '9', '9', '12', '5', '3', '2', '8', '2', '8', '3', '3', '1', '4', '9', '10', '2', '4', '8', '1', '2', '10', '12', '12', '3', '1', '1', '2', '9', '6', '6', '1', '9', '1', '8', '1', '8', '8', '8', '8', '2', '2', '2', '8', '8', '2', '1', '1', '1', '1', '1', '8', '8', '5', '8', '8', '1', '8', '9', '9', '1', '7', '12', '9', '9', '7', '2', '2', '7', '8', '9', '8', '2', '9', '6', '3', '4', '8', '4', '1', '2', '9', '8', '8', '8', '2', '1', '1', '8', '8', '3', '1', '9', '9', '7', '1', '2', '2', '2', '9', '4', '1', '2', '12', '1', '8', '2', '1', '9', '1', '8', '9', '9', '2', '5', '4', '2', '1', '2', '8', '12', '8', '10', '3', '8', '8', '8', '2', '2', '8', '2', '8', '2', '4', '8', '10', '2', '7', '2', '1', '8', '8', '9', '5', '8', '1', '12', '7', '10', '2', '2', '2', '1', '4', '8', '12', '1', '8', '2', '2', '9', '1', '8', '3', '1', '1', '5', '10', '2', '1', '2', '4', '8', '9', '3', '2', '8', '7', '4', '4', '9', '9', '2', '8', '1', '1', '2', '4', '7', '1', '9', '1', '1', '1', '8', '4', '1', '3', '1', '2', '2', '9', '1', '8', '8', '2', '2', '9', '9', '1', '8', '8', '1', '9', '8', '8', '2', '2', '2', '2', '1', '1', '1', '7', '4', '1', '1', '4', '2', '3', '3', '2', '3', '9', '2', '8', '2', '1', '5', '1', '2', '2', '1', '3', '5', '2', '9', '1', '1', '9', '8', '1', '4', '8', '1', '8', '2', '4', '8', '4', '7', '9', '8', '1', '2', '2', '4', '4', '11', '3', '11', '1', '3', '8', '1', '9', '6', '8', '9', '3', '9', '8', '9', '2', '3', '3', '7', '1', '7', '2', '9', '9', '1', '8', '8', '1', '5', '5', '8', '8', '1', '2', '1', '3', '9', '9', '5', '9', '8', '1', '4', '3', '8', '1', '1', '2', '1', '8', '9', '8', '2', '2', '2', '1', '2', '4', '10', '8', '1', '9', '1', '8', '1', '1', '9', '9', '8', '11', '1', '8', '2', '1', '2', '8', '2', '8', '8', '2', '9', '6', '10', '1', '8', '6', '4', '5', '11', '2', '2', '8', '1', '8', '1', '8', '1', '4', '8', '1', '1', '1', '8', '3', '7', '3', '3', '7', '1', '1', '4', '8', '1', '6', '4', '2', '2', '1', '1', '1', '1', '6', '2', '2', '8', '1', '8', '5', '5', '2', '1', '1', '2', '3', '7', '1', '1', '6', '2', '9', '3', '9', '8', '9', '2', '1', '4', '2', '9', '13', '9', '2', '8', '12', '4', '1', '1', '9', '5', '1', '1', '1', '1', '8', '2', '1', '1', '2', '9', '0', '12', '1', '1', '8', '9', '8', '3', '2', '2', '12', '8', '2', '2', '2', '1', '10', '1', '9', '8', '9', '5', '2', '2', '9', '2', '8', '4', '8', '3', '7', '9', '2', '2', '7', '2', '2', '8', '1', '1', '9', '10', '9', '8', '2', '9', '0', '7', '1', '1', '1', '9', '8', '2', '8', '12', '8', '3', '2', '8', '8', '7', '7', '8', '12', '2', '1', '3', '9', '2', '2', '8', '2', '8', '8', '12', '8', '8', '1', '8', '9', '2', '4', '4', '7', '2', '1', '1', '9', '3', '7', '1', '3', '9', '8', '2', '2', '3', '1', '3', '10', '2', '3', '7', '1', '8', '1', '1', '10', '3', '10', '8', '4', '1', '2', '4', '3', '10', '9', '3', '9', '5', '8', '7', '2', '9', '3', '3', '2', '11', '8', '1', '1', '7', '2', '7', '2', '9', '4', '8', '1', '12', '8', '7', '9', '1', '0', '2', '6', '4', '9', '3', '2', '4', '12', '9', '8', '5', '9', '2', '8', '4', '6', '8', '9', '3', '1', '8', '7', '3', '1', '1', '1', '8', '9', '1', '8', '7', '9', '8', '2', '1', '2', '2', '3', '2', '2', '1', '1', '4', '9', '7', '3', '2', '8', '8', '9', '1', '9', '7', '4', '2', '2', '8', '5', '9', '1', '5', '7', '1', '4', '6', '9', '10', '1', '8', '2', '9', '1', '9', '2', '9', '8', '2', '9', '9', '11', '2', '12', '2', '2', '9', '4', '8', '1', '9', '9', '2', '8', '9', '1', '2', '9', '3', '8', '8', '1', '7', '8', '8', '2', '3', '2', '2', '2', '2', '9', '8', '10', '2', '2', '1', '9', '11', '1', '8', '8', '2', '4', '2', '1', '2', '8', '8', '9', '2', '8', '9', '2', '3', '1', '2', '1', '3', '10', '8', '1', '2', '4', '1', '9', '8', '9', '3', '3', '9', '8', '2', '11', '3', '7', '1', '8', '7', '1', '8', '8', '8', '1', '4', '8', '8', '1', '9', '3', '1', '1', '2', '8', '8', '8', '2', '1', '8', '10', '2', '12', '8', '8', '9', '1', '8', '1', '3', '2', '2', '10', '7', '2', '2', '1', '1', '8', '2', '9', '1', '2', '9', '3', '3', '10', '1', '4', '7', '10', '12', '2', '4', '10', '8', '6', '2', '9', '1', '8', '3', '1', '2', '8', '8', '2', '12', '10', '2', '10', '9', '1', '2', '5', '8', '12', '8', '8', '7', '8', '8', '1', '8', '2', '2', '9', '1', '8', '10', '5', '5', '5', '8', '1', '9', '1', '13', '1', '2', '2', '1', '2', '4', '9', '2', '1', '9', '13', '11', '1', '9', '9', '3', '10', '8', '7', '8', '8', '4', '2', '1', '9', '4', '3', '1', '1', '10', '9', '2', '8', '6', '2', '1', '1', '3', '2', '8', '1', '1', '1', '1', '2', '4', '1', '1', '10', '1', '8', '1', '1', '9', '1', '7', '1', '1', '9', '9', '2', '12', '9', '1', '8', '1', '12', '1', '8', '3', '8', '1', '8', '1', '2', '10', '1', '1', '2', '2', '9', '3', '9', '12', '1', '9', '1', '2', '8', '7', '2', '1', '1', '3', '2', '8', '4', '4', '5', '12', '7', '8', '9', '6', '1', '8', '9', '2', '1', '9', '1', '4', '1', '8', '2', '1', '1', '8', '7', '4', '9', '1', '8', '4', '2', '7', '5', '1', '3', '9', '2', '8', '2', '1', '2', '9', '1', '1', '2', '3', '4', '3', '1', '2', '2', '10', '2', '2', '8', '1', '1', '2', '4', '9', '2', '11', '9', '8', '9', '2', '1', '10', '4', '1', '1', '10', '7', '2', '10', '8', '1', '10', '8', '8', '8', '8', '8', '8', '2', '2', '2', '8', '3', '4', '1', '1', '1', '2', '1', '8', '8', '2', '8', '3', '8', '3', '1', '2', '3', '1', '1', '10', '1', '4', '4', '3', '3', '1', '9', '1', '1', '2', '1', '1', '6', '1', '1', '1', '9', '1', '4', '1', '1', '1', '1', '1', '1', '2', '12', '8', '8', '1', '9', '12', '10', '8', '2', '12', '1', '1', '2', '1', '7', '10', '7', '1', '2', '1', '10', '8', '2', '2', '8', '2', '2', '1', '9', '8', '8', '8', '8', '3', '1', '4', '2', '8', '3', '10', '10', '1', '9', '1', '9', '1', '9', '1', '8', '3', '1', '1', '8', '8', '8', '6', '3', '4', '1', '1', '1', '9', '9', '5', '2', '10', '2', '6', '2', '2', '7', '1', '8', '2', '11', '1', '1', '1', '2', '1', '9', '4', '4', '2', '8', '4', '6', '8', '8', '10', '10', '2', '1', '2', '9', '1', '8', '8', '3', '4', '8', '8', '2', '8', '8', '1', '2', '1', '2', '9', '4', '1', '3', '3', '3', '8', '8', '8', '2', '6', '4', '3', '3', '12', '3', '4', '9', '6', '8', '7', '7', '8', '6', '4', '1', '10', '6', '3', '3', '8', '1', '1', '8', '1', '11', '8', '1', '1', '2', '3', '11', '7', '2', '1', '1', '1', '9', '1', '2', '1', '1', '9', '8', '1', '1', '8', '0', '4', '1', '1', '2', '1', '1', '2', '2', '9', '1', '10', '1', '3', '8', '9', '3', '3', '1', '10', '1', '9', '1', '1', '7', '4', '1', '8', '10', '1', '2', '10', '1', '1', '2', '3', '3', '9', '1', '8', '10', '1', '7', '9', '1', '9', '6', '3', '9', '2', '8', '4', '12', '12', '3', '1', '1', '9', '1', '9', '2', '1', '1', '1', '1', '1', '9', '2', '1', '2', '10', '9', '8', '8', '10', '8', '2', '3', '2', '1', '12', '8', '12', '2', '9', '1', '3', '2', '5', '2', '2', '8', '2', '2', '4', '10', '9', '10', '9', '4', '2', '4', '12', '8', '2', '1', '8', '3', '9', '2', '9', '4', '1', '1', '1', '1', '1', '9', '6', '2', '9', '2', '1', '3', '2', '2', '5', '8', '9', '1', '6', '3', '1', '2', '2', '2', '6', '1', '8', '10', '8', '3', '3', '13', '2', '9', '9', '10', '6', '12', '8', '2', '9', '2', '3', '8', '8', '1', '2', '2', '3', '2', '10', '1', '8', '9', '1', '8', '9', '8', '8', '2', '10', '2', '1', '1', '1', '9', '9', '12', '1', '1', '4', '8', '3', '2', '2', '4', '4', '2', '2', '1', '1', '1', '9', '2', '2', '4', '1', '1', '8', '8', '8', '9', '2', '13', '9', '9', '4', '1', '9', '7', '10', '10', '8', '1', '9', '1', '1', '1', '8', '12', '4', '1', '2', '1', '1', '1', '7', '8', '3', '3', '1', '2', '2', '7', '2', '2', '10', '1', '8', '4', '0', '1', '9', '4', '1', '1', '2', '10', '10', '7', '9', '3', '12', '11', '2', '8', '10', '8', '9', '4', '3', '4', '10', '1', '1', '9', '3', '10', '1', '5', '3', '3', '2', '2', '1', '1', '6', '1', '7', '1', '1', '12', '1', '1', '1', '10', '1', '8', '8', '1', '3', '8', '3', '1', '2', '8', '3', '4', '2', '8', '8', '6', '1', '1', '4', '1', '2', '9', '5', '1', '7', '8', '9', '2', '4', '9', '8', '5', '9', '9', '9', '1', '2', '1', '3', '8', '9', '2', '9', '8', '9', '1', '8', '13', '9', '3', '8', '1', '4', '1', '3', '1', '6', '4', '10', '2', '2', '7', '8', '8', '12', '8', '9', '9', '9', '10', '3', '12', '12', '2', '9', '10', '9', '7', '2', '3', '4', '10', '10', '2', '7', '7', '1', '7', '3', '9', '1', '9', '8', '2', '9', '1', '8', '1', '10', '8', '8', '2', '7', '8', '1', '8', '9', '8', '1', '4', '1', '8', '8', '8', '1', '1', '10', '3', '2', '2', '6', '8', '9', '8', '2', '1', '1', '3', '8', '8', '1', '4', '4', '3', '9', '10', '4', '2', '4', '2', '9', '2', '2', '9', '1', '1', '10', '1', '1', '2', '1', '1', '1', '9', '1', '8', '8', '8', '2', '3', '8', '1', '6', '7', '3', '1', '9', '1', '2', '1', '6', '1', '2', '10', '1', '4', '2', '3', '2', '9', '9', '4', '9', '9', '1', '2', '8', '4', '7', '8', '7', '13', '9', '1', '2', '1', '9', '3', '3', '6', '8', '9', '10', '9', '2', '2', '3', '12', '1', '1', '1', '13', '2', '1', '7', '10', '8', '9', '1', '2', '9', '1', '1', '1', '2', '1', '12', '5', '6', '9', '10', '3', '1', '10', '1', '3', '2', '8', '9', '2', '10', '8', '8', '9', '2', '8', '9', '2', '2', '2', '8', '8', '2', '3', '1', '4', '8', '1', '9', '9', '2', '8', '10', '9', '1', '2', '12', '8', '8', '0', '2', '1', '1', '9', '10', '1', '6', '2', '2', '10', '2', '2', '4', '2', '2', '12', '1', '6', '7', '8', '9', '1', '3', '5', '7', '3', '1', '3', '1', '1', '3', '2', '6', '3', '2', '1', '8', '3', '5', '2', '1', '1', '8', '1', '1', '0', '2', '3', '5', '3', '1', '1', '9', '8', '8', '8', '8', '4', '7', '2', '9', '12', '5', '9', '3', '12', '3', '9', '1', '9', '1', '7', '3', '9', '3', '10', '1', '2', '1', '4', '9', '8', '2', '9', '11', '2', '8', '9', '4', '1', '1', '1', '1', '1', '1', '1', '9', '1', '9', '1', '1', '1', '12', '3', '10', '3', '6', '1', '7', '10', '8', '3', '7', '1', '1', '2', '6', '1', '3', '9', '2', '8', '1', '8', '8', '1', '8', '10', '12', '1', '10', '12', '10', '1', '8', '8', '1', '4', '7', '9', '2', '1', '9', '3', '8', '12', '10', '6', '1', '6', '8', '3', '1', '3', '1', '10', '10', '9', '8', '1', '2', '9', '8', '8', '1', '9', '8', '8', '3', '1', '3', '9', '12', '1', '8', '1', '5', '8', '11', '5', '5', '1', '2', '1', '3', '1', '1', '1', '9', '8', '8', '1', '9', '10', '8', '4', '1', '1', '8', '10', '8', '9', '1', '10', '1', '7', '2', '1', '1', '9', '6', '2', '9', '8', '1', '1', '1', '9', '8', '8', '1', '9', '9', '1', '1', '2', '7', '9', '2', '9', '9', '8', '7', '6', '8', '7', '8', '2', '9', '2', '1', '8', '8', '8', '2', '6', '3', '1', '8', '12', '12', '8', '7', '1', '2', '2', '7', '1', '9', '8', '1', '4', '1', '1', '8', '5', '8', '1', '8', '8', '8', '8', '1', '2', '11', '1', '1', '1', '2', '7', '1', '9', '2', '2', '2', '9', '8', '9', '2', '9', '1', '8', '1', '7', '11', '13', '1', '9', '8', '2', '2', '1', '2', '3', '10', '3', '10', '4', '3', '9', '1', '8', '1', '1', '13', '1', '1', '3', '8', '1', '2', '2', '2', '9', '4', '1', '8', '1', '9', '3', '6', '8', '5', '2', '8', '2', '8', '1', '9', '2', '2', '8', '1', '1', '4', '8', '9', '2', '2', '7', '1', '8', '12', '2', '12', '8', '2', '9', '9', '3', '8', '1', '1', '9', '9', '9', '2', '8', '1', '9', '1', '8', '9', '8', '2', '2', '4', '1', '9', '10', '10', '8', '8', '1', '1', '2', '1', '4', '1', '2', '1', '1', '3', '12', '8', '8', '8', '8', '10', '2', '8', '12', '1', '9', '9', '8', '9', '1', '2', '1', '8', '8', '8', '2', '8', '8', '3', '10', '8', '9', '1', '1', '1', '3', '4', '8', '10', '6', '3', '1', '3', '2', '8', '9', '3', '5', '4', '1', '5', '2', '9', '9', '9', '1', '1', '1', '9', '9', '6', '10', '9', '8', '12', '2', '9', '1', '2', '1', '1', '1', '8', '1', '12', '1', '12', '1', '9', '1', '8', '1', '2', '1', '2', '8', '8', '10', '1', '9', '1', '2', '2', '8', '8', '3', '12', '1', '11', '2', '10', '12', '11', '2', '1', '10', '1', '1', '9', '3', '10', '2', '7', '1', '8', '1', '9', '9', '2', '1', '3', '12', '5', '9', '9', '8', '1', '8', '8', '8', '1', '1', '3', '9', '8', '9', '8', '3', '10', '3', '7', '8', '9', '2', '3', '1', '9', '10', '9', '2', '8', '2', '6', '4', '1', '1', '3', '8', '2', '3', '9', '1', '2', '1', '8', '1', '2', '2', '2', '2', '9', '9', '4', '8', '9', '1', '1', '2', '9', '1', '5', '2', '9', '8', '10', '1', '2', '10', '8', '5', '6', '4', '8', '1', '1', '1', '8', '9', '2', '8', '8', '8', '12', '2', '2', '1', '8', '1', '2', '1', '9', '8', '8', '9', '1', '1', '8', '7', '1', '8', '0', '2', '3', '8', '1', '1', '8', '1', '1', '12', '3', '2', '10', '8', '10', '6', '1', '8', '8', '10', '8', '8', '1', '4', '8', '1', '2', '9', '8', '1', '1', '3', '3', '5', '1', '1', '2', '1', '2', '1', '9', '8', '3', '9', '8', '2', '10', '9', '1', '8', '1', '10', '9', '10', '1', '2', '3', '8', '1', '3', '1', '9', '9', '8', '1', '9', '8', '8', '9', '1', '9', '2', '8', '8', '2', '13', '8', '10', '8', '9', '3', '2', '1', '3', '1', '12', '10', '10', '1', '11', '1', '2', '4', '2', '10', '1', '8', '11', '1', '1', '1', '2', '2', '8', '2', '9', '8', '9', '9', '12', '8', '1', '9', '2', '4', '1', '8', '3', '9', '1', '6', '5', '8', '9', '3', '3', '2', '1', '1', '1', '8', '11', '1', '8', '7', '1', '11', '9', '1', '7', '9', '9', '8', '8', '8', '12', '8', '1', '2', '9', '8', '8', '2', '10', '9', '8', '8', '4', '10', '9', '8', '2', '10', '10', '2', '1', '2', '1', '2', '1', '4', '1', '7', '1', '1', '3', '12', '9', '8', '5', '9', '10', '1', '9', '9', '2', '7', '4', '1', '1', '8', '12', '8', '2', '2', '9', '4', '1', '1', '5', '9', '1', '8', '12', '8', '1', '1', '4', '9', '10', '2', '3', '1', '3', '3', '3', '8', '1', '2', '1', '2', '1', '2', '9', '1', '2', '8', '9', '4', '8', '8', '10', '2', '12', '5', '5', '1', '8', '1', '7', '8', '1', '1', '8', '9', '12', '4', '9', '3', '1', '2', '5', '9', '2', '1', '2', '2', '1', '2', '8', '1', '8', '8', '2', '2', '1', '8', '4', '1', '2', '10', '9', '10', '8', '1', '10', '4', '8', '8', '10', '2', '8', '1', '1', '3', '10', '1', '4', '2', '8', '3', '3', '2', '9', '8', '5', '5', '9', '3', '10', '8', '1', '2', '1', '2', '9', '1', '1', '2', '9', '9', '8', '8', '11', '9', '4', '1', '8', '2', '7', '1', '2', '8', '2', '9', '9', '8', '2', '9', '9', '8', '8', '1', '2', '8', '1', '1', '2', '9', '8', '1', '4', '9', '1', '8', '12', '1', '12', '2', '8', '2', '9', '1', '12', '1', '2', '9', '1', '1', '9', '3', '2', '9', '9', '2', '11', '8', '7', '9', '9', '9', '8', '8', '1', '2', '8', '1', '1', '4', '1', '2', '10', '1', '9', '2', '1', '1', '1', '9', '1', '13', '4', '3', '2', '1', '1', '2', '2', '7', '8', '7', '1', '1', '2', '8', '8', '3', '4', '8', '2', '10', '13', '8', '8', '9', '2', '2', '1', '1', '10', '4', '2', '7', '9', '8', '2', '1', '9', '8', '1', '1', '9', '8', '9', '8', '2', '8', '2', '2', '9', '2', '1', '9', '1', '2', '4', '8', '2', '1', '1', '8', '1', '4', '1', '9', '3', '9', '2', '7', '1', '9', '6', '1', '10', '2', '2', '2', '2', '2', '10', '10', '10', '9', '9', '9', '3', '12', '10', '1', '1', '10', '1', '1', '2', '2', '1', '2', '3', '12', '8', '8', '1', '2', '9', '9', '1', '1', '10', '8', '9', '8', '3', '1', '1', '3', '1', '10', '1', '1', '1', '4', '1', '1', '1', '7', '10', '2', '1', '8', '1', '9', '3', '1', '10', '8', '8', '1', '5', '9', '1', '2', '1', '8', '8', '1', '3', '1', '10', '2', '1', '1', '2', '8', '2', '5', '3', '5', '2', '1', '1', '9', '1', '7', '9', '9', '2', '8', '1', '12', '4', '1', '4', '9', '9', '4', '8', '1', '10', '10', '8', '8', '8', '9', '2', '3', '5', '1', '8', '1', '1', '1', '7', '10', '1', '4', '12', '3', '2', '2', '9', '1', '1', '1', '2', '8', '4', '1', '7', '3', '12', '8', '6', '2', '2', '7', '9', '11', '1', '10', '9', '1', '8', '12', '2', '11', '1', '2', '3', '1', '1', '1', '8', '9', '2', '2', '2', '2', '3', '3', '8', '10', '4', '1', '2', '9', '8', '2', '2', '9', '1', '8', '11', '4', '2', '8', '7', '1', '1', '7', '3', '9', '9', '4', '5', '8', '10', '2', '9', '9', '9', '8', '1', '9', '2', '2', '2', '1', '2', '9', '4', '3', '12', '4', '3', '2', '3', '8', '3', '10', '1', '10', '1', '1', '8', '10', '6', '8', '9', '9', '1', '2', '9', '2', '9', '3', '8', '1', '1', '12', '1', '5', '1', '10', '8', '10', '2', '7', '1', '1', '1', '1', '3', '2', '3', '5', '9', '1', '2', '1', '9', '8', '10', '9', '8', '1', '8', '1', '8', '2', '8', '1', '2', '8', '1', '2', '5', '8', '12', '2', '1', '1', '1', '1', '1', '8', '8', '2', '1', '4', '2', '8', '8', '8', '2', '9', '9', '11', '1', '2', '9', '3', '1', '1', '8', '9', '2', '4', '10', '1', '9', '8', '4', '9', '8', '10', '8', '8', '9', '2', '8', '9', '3', '3', '3', '4', '3', '2', '2', '10', '8', '2', '1', '5', '1', '9', '1', '1', '2', '1', '2', '3', '1', '1', '8', '8', '1', '10', '8', '8', '1', '6', '4', '8', '2', '5', '8', '1', '3', '8', '2', '8', '9', '1', '1', '8', '12', '10', '1', '5', '8', '2', '8', '10', '9', '9', '1', '6', '9', '1', '10', '8', '2', '9', '10', '1', '2', '1', '1', '8', '8', '3', '9', '8', '1', '10', '2', '8', '1', '10', '1', '5', '1', '1', '1', '5', '1', '1', '1', '4', '1', '1', '1', '8', '2', '8', '1', '8', '3', '8', '9', '1', '8', '1', '8', '1', '1', '2', '2', '1', '12', '9', '9', '1', '8', '2', '1', '2', '8', '1', '9', '1', '1', '2', '1', '2', '2', '2', '1', '9', '9', '8', '8', '1', '1', '9', '10', '8', '4', '3', '3', '4', '9', '1', '3', '3', '10', '2', '2', '1', '8', '4', '1', '9', '4', '1', '1', '9', '8', '10', '4', '9', '9', '8', '2', '1', '3', '4', '9', '3', '8', '9', '1', '1', '2', '1', '7', '8', '2', '9', '1', '9', '2', '6', '9', '10', '8', '1', '1', '5', '9', '5', '1', '9', '10', '9', '3', '3', '8', '8', '2', '9', '2', '8', '2', '1', '8', '5', '8', '3', '10', '1', '1', '2', '1', '8', '1', '4', '3', '2', '9', '4', '9', '8', '8', '8', '1', '3', '9', '1', '9', '9', '1', '2', '1', '9', '8', '9', '8', '1', '1', '1', '4', '2', '2', '9', '9', '1', '1', '1', '3', '6', '1', '2', '3', '1', '4', '2', '9', '1', '9', '9', '1', '8', '2', '1', '9', '1', '8', '1', '8', '8', '2', '1', '9', '9', '6', '8', '8', '9', '8', '10', '8', '9', '5', '1', '1', '1', '1', '8', '1', '8', '12', '8', '1', '12', '9', '2', '8', '1', '1', '2', '1', '8', '6', '9', '9', '8', '1', '8', '9', '7', '7', '8', '9', '9', '1', '8', '3', '8', '1', '3', '3', '8', '9', '8', '2', '8', '1', '2', '9', '8', '1', '1', '1', '1', '9', '1', '9', '8', '12', '3', '8', '8', '1', '10', '10', '10', '2', '2', '1', '10', '2', '2', '10', '8', '12', '8', '1', '1', '1', '2', '1', '1', '3', '8', '1', '2', '2', '2', '1', '1', '6', '2', '8', '8', '8', '1', '1', '1', '9', '9', '2', '8', '10', '2', '8', '2', '1', '2', '9', '9', '2', '1', '1', '8', '8', '1', '3', '1', '8', '9', '1', '10', '9', '1', '1', '1', '1', '7', '2', '2', '1', '2', '8', '8', '1', '2', '1', '3', '2', '1', '8', '3', '9', '2', '10', '2', '10', '1', '2', '1', '1', '13', '9', '10', '8', '2', '2', '1', '1', '8', '2', '2', '1', '9', '1', '1', '8', '9', '9', '2', '9', '2', '2', '1', '1', '1', '8', '9', '4', '9', '9', '1', '8', '2', '1', '1', '8', '2', '2', '2', '8', '1', '1', '8', '2', '10', '2', '9', '9', '7', '1', '1', '2', '2', '8', '3', '1', '2', '1', '1', '8', '1', '1', '1', '9', '1', '1', '8', '8', '1', '9', '6', '1', '9', '1', '8', '1', '8', '8', '8', '1', '3', '8', '8', '2', '6', '3', '2', '1', '10', '1', '8', '2', '1', '6', '1', '1', '1', '8', '1', '10', '1', '1', '8', '9', '2', '6', '9', '2', '9', '4', '1', '8', '1', '3', '7', '8', '8', '5', '8', '7', '1', '1', '1', '13', '8', '3', '1', '1', '1', '1', '2', '8', '2', '12', '8', '1', '2', '5', '1', '8', '2', '10', '8', '2', '10', '1', '5', '2', '1', '1', '5', '2', '2', '7', '8', '2', '9', '9', '1', '10', '1', '8', '2', '5', '8', '1', '5', '1', '10', '1', '1', '9', '8', '6', '1', '8', '9', '8', '1', '8', '2', '2', '8', '3', '9', '1', '9', '3', '8', '4', '4', '3', '3', '1', '1', '1', '8', '9', '1', '1', '8', '3', '1', '2', '8', '2', '1', '1', '10', '1', '2', '2', '1', '1', '8', '2', '8', '2', '10', '2', '2', '10', '8', '8', '8', '13', '2', '2', '9', '2', '5', '9', '2', '1', '8', '8', '8', '1', '12', '12', '2', '9', '2', '1', '8', '2', '1', '8', '1', '9', '9', '2', '7', '1', '1', '7', '1', '4', '3', '1', '10', '3', '10', '9', '4', '6', '7', '5', '2', '1', '8', '8', '1', '4', '9', '2', '1', '8', '1', '1', '1', '1', '1', '8', '9', '8', '8', '8', '8', '9', '8', '10', '1', '8', '2', '7', '8', '9', '1', '2', '5', '9', '10', '8', '9', '12', '1', '6', '1', '2', '10', '9', '4', '1', '8', '10', '10', '1', '1', '1', '1', '1', '2', '3', '8', '1', '2', '10', '2', '1', '2', '4', '2', '2', '9', '1', '4', '2', '1', '1', '12', '12', '9', '9', '7', '1', '8', '4', '9', '9', '8', '1', '7', '3', '2', '1', '1', '4', '8', '1', '5', '8', '2', '8', '1', '14', '9', '12', '1', '10', '2', '1', '1', '8', '8', '6', '6', '3', '1', '2', '8', '1', '3', '2', '1', '8', '8', '1', '10', '8', '2', '8', '2', '5', '9', '1', '8', '8', '1', '1', '8', '8', '13', '1', '3', '8', '4', '1', '9', '7', '8', '8', '8', '3', '8', '1', '4', '2', '8', '8', '8', '7', '1', '1', '2', '8', '8', '9', '8', '8', '2', '3', '9', '9', '2', '1', '2', '2', '1', '8', '2', '6', '2', '1', '1', '9', '10', '3', '8', '2', '3', '2', '8', '8', '1', '8', '8', '8', '9', '1', '4', '1', '9', '2', '1', '8', '0', '1', '8', '1', '1', '8', '9', '9', '1', '3', '1', '2', '8', '8', '7', '1', '1', '2', '9', '2', '10', '2', '8', '2', '2', '2', '7', '1', '9', '8', '1', '3', '9', '2', '1', '10', '8', '1', '4', '1', '8', '5', '9', '8', '8', '1', '2', '2', '1', '8', '8', '6', '8', '1', '1', '8', '3', '2', '2', '1', '8', '1', '7', '6', '8', '1', '1', '1', '9', '1', '1', '1', '2', '1', '11', '10', '10', '11', '9']\n",
            "     filenum                                               text  label\n",
            "0          1  ent waves which Lehr and Wyatt recorded on the...      8\n",
            "1          2  many times by the writers of the Old Testament...      1\n",
            "2          4  sued, recovery may be had for an involuntary, ...      2\n",
            "3          6  of the manufacturer.' The same result was reac...      8\n",
            "4          7  in the interest of either the consumer or the ...      8\n",
            "...      ...                                                ...    ...\n",
            "4499    8411  Dixon, supra, at P. at And the California Supr...      1\n",
            "4500    8412  improper venue because it was filed in the wro...      1\n",
            "4501    8414  hearing and the arguments of counsel, you find...      1\n",
            "4502    8417  re-examination, they should be reaffirmed in a...     10\n",
            "4503    8406  (emphasis added). And those listed locations a...      1\n",
            "\n",
            "[4504 rows x 3 columns]\n"
          ]
        }
      ],
      "source": [
        "fh = open('sc_model_5.txt')\n",
        "filenums=[]\n",
        "records=[]\n",
        "for record in fh:\n",
        "    record=record.split(\"---\")\n",
        "    filenum=record[0]\n",
        "    modelnum=record[1]\n",
        "    text=record[2]\n",
        "    filenums.append(filenum)\n",
        "    records.append(text)\n",
        "    \n",
        "print(len(filenums))\n",
        "print(len(records))\n",
        "\n",
        "summarized_data = pd.DataFrame(list(zip(filenums, records)),columns =['filenum','text'])\n",
        "len_list = [len(ele.split()) for ele in records]\n",
        "res = 0 if len(len_list) == 0 else (float(sum(len_list)) / len(len_list))\n",
        "print(\"Average Length %s\" % res)\n",
        "\n",
        "temp_file = open(\"labels_sc.txt\", \"r\")\n",
        "#temp_file = open(\"labels_sc_279.txt\", \"r\")\n",
        "\n",
        "data = temp_file.read()\n",
        "\n",
        "label_list = data.split(\"\\n\")\n",
        "print(label_list)\n",
        "label_list = label_list[0:-1]\n",
        "print(label_list)\n",
        "label_list = [int(i) for i in label_list]\n",
        "temp_file.close()\n",
        "\n",
        "\n",
        "new_label_list=[]\n",
        "for num in filenums:\n",
        "  new_label_list.append(label_list[int(num)-1])\n",
        "summarized_data['label'] = new_label_list\n",
        "\n",
        "print(summarized_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FL0rlksFwxTx",
        "outputId": "756b35b3-d306-4ee7-aa8f-8a9c5085228f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_1[0][0]',                \n",
            "                                thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_bert_model[0][0]']          \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 15)           7695        ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,883,663\n",
            "Trainable params: 109,883,663\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 8  1  2 ...  1 10  1]\n",
            "[   1    2    4 ... 8414 8417 8406]\n"
          ]
        }
      ],
      "source": [
        "gc.collect()\n",
        "tf.keras.backend.clear_session()\n",
        "dbert_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "dbert_model = TFRobertaModel.from_pretrained('roberta-base')\n",
        "max_len=512\n",
        "sentences=summarized_data['text']\n",
        "labels=summarized_data['label']\n",
        "filenumbers=summarized_data['filenum']\n",
        "len(sentences),len(labels),len(filenumbers)\n",
        "\n",
        "model_5=create_model()\n",
        "input_ids=[]\n",
        "attention_masks=[]\n",
        "\n",
        "for sent in sentences:\n",
        "  dbert_inps=dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n",
        "  input_ids.append(dbert_inps['input_ids'])\n",
        "  attention_masks.append(dbert_inps['attention_mask'])\n",
        "input_ids=np.asarray(input_ids)\n",
        "\n",
        "attention_masks=np.array(attention_masks)\n",
        "labels=np.array(labels)\n",
        "filenumbers=[int(i) for i in filenumbers]\n",
        "filenumbers=np.array(filenumbers)\n",
        "print(labels)\n",
        "print(filenumbers)\n",
        "\n",
        "train_inp,val_inp,train_label,val_label,train_mask,val_mask,train_fnum,val_fnum=train_test_split(input_ids,labels,attention_masks,filenumbers,test_size=0.1,random_state=42)\n",
        "\n",
        "new_train_inp_5=[]\n",
        "new_val_inp_5=[]\n",
        "new_train_label_5=[]\n",
        "new_val_label_5=[]\n",
        "new_train_mask_5=[]\n",
        "new_train_fnum_5=[]\n",
        "new_val_fnum_5=[]\n",
        "new_val_mask_5=[]\n",
        "for i in range(len(train_fnum)):\n",
        "  if train_fnum[i] in filenumbers:\n",
        "    new_train_inp_5.append(train_inp[i])\n",
        "    #new_val_inp.append(val_inp[i])\n",
        "    new_train_label_5.append(train_label[i])\n",
        "    #new_val_label.append(val_label[i])\n",
        "    new_train_mask_5.append(train_mask[i])\n",
        "    #new_val_mask.append(val_mask[i])\n",
        "    new_train_fnum_5.append(train_fnum[i])\n",
        "\n",
        "for i in range(len(val_fnum)):\n",
        "  if val_fnum[i] in filenumbers:\n",
        "    #new_train_inp.append(train_inp[i])\n",
        "    new_val_inp_5.append(val_inp[i])\n",
        "    #new_train_label.append(train_label[i])\n",
        "    new_val_label_5.append(val_label[i])\n",
        "    #new_train_mask.append(train_mask[i])\n",
        "    new_val_mask_5.append(val_mask[i])\n",
        "    new_val_fnum_5.append(val_fnum[i])\n",
        "\n",
        "# print(new_train_label)\n",
        "# print(len(new_train_label))\n",
        "# print(new_val_label)\n",
        "# print(len(new_val_label))\n",
        "new_train_inp_5=np.array(new_train_inp_5)\n",
        "new_val_inp_5=np.array(new_val_inp_5)\n",
        "new_train_label_5=np.array(new_train_label_5)\n",
        "new_val_label_5=np.array(new_val_label_5)\n",
        "new_train_mask_5=np.array(new_train_mask_5)\n",
        "new_train_fnum_5=np.array(new_train_fnum_5)\n",
        "new_val_fnum_5=np.array(new_val_fnum_5)\n",
        "new_val_mask_5=np.array(new_val_mask_5)\n",
        "\n",
        "print(new_val_fnum_5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9x3SEyjQiEC",
        "outputId": "a1c72cb5-dd9b-4723-a847-29149e36b4ff"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_1[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_roberta_model[0][0]']       \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 15)           7695        ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 8  1  2 ...  1 10  1]\n",
            "[   1    2    4 ... 8414 8417 8406]\n",
            "[6622   35 6713 4989 8233 7444 5061 8344 5464  317 4515 3001 6991 2875\n",
            " 3885 6647 4857 4504  214 8340 2969 3847 4609 7358 2752 7651 8188 7073\n",
            " 6483 4952 5072 1967 4071 7590 5030 7297  626 6029 6302 7256 3993 4263\n",
            " 3835 4293 8129 4837 3843 1418 5274 6205 5997 6685 3310 7721 2360 7197\n",
            " 4252 8343  909 4354  649 3488 2496 5765 4004 4052 1403 6697 6293 6380\n",
            " 4602 6057 4236 7978 5083 6552  361 7706 4186 8351  812 6600 8264 8064\n",
            " 7121 8279 5556 5119 7947 3350 7152 4282 5025 5051 3615 8062 1489 7351\n",
            "  234 7416 7921 6574 6257 8396 4520 3300 5640  240 7810 1321 6580 5404\n",
            " 8273 6742 7861 7975 1018 2140 7061 7767 4990 8124 5927 6051 6916 2846\n",
            " 6818 7052 7775 7582 8116 4767 5248 6055 8331 1564 7326 3065 6764 6854\n",
            " 4915 3583 5902 4229 7565 3712 7224 4502 3827 6525 6318 2686 5369 8235\n",
            " 3351 8021  102  224 8294 7707 2985 5146 3709 6824 2281 6837 5436 2649\n",
            " 4886  966 5476 8387 6166 4387  350 6123 5236 5041  734 3412 6085  489\n",
            " 4982 5115 6743 7173 5724 7689 7787 8302 3150 2247 8128 6660 4610 2866\n",
            " 6905 7096  768 4411 6087 2096 4903 7546 6630 2928 4640 7125 1792 1700\n",
            " 7928 8095  737 5849 2359 6015 4361 6081 5014 5167 1023 8009  268 4777\n",
            " 5702 6129 7181 1860 7192 5467   67 6134 4270 4611 6746 7696 3466 3950\n",
            " 6125 8370 6142 6131 5548 1721 5067 4015 7092   25 1749 4878 8248 6836\n",
            " 5310 6960 2339 1650 6227 4104 5130 7089 6993 1289 4305 5585 7933 6284\n",
            " 7057 3807 8347 5280 2841 7693 8090 4171 5260  133 7451 3957 1719 5435\n",
            " 6216 4858 4417 8355 4492 2648 5278 7308  158 7543 2168 5913 7111 3414\n",
            " 7592   39 7157 5647 4555 3956 3617 4630 4384 6596 4539 6749 7202 2413\n",
            " 4749 5122 5929  252 5565  336 6008 4564 5580 7237 5958 6918 6487 2989\n",
            " 7448 3382  467 3784 4419 1117 7954 7122 3174 1534 4164 7526  498 5944\n",
            " 4572 2307 2929 7480 4412 4563 7692 1869 2472 6966 6549 1594 2276 7269\n",
            " 2545 2216 6587 6315 5297 5282 6399 3917 6342 5721 7909 8382 4383 7130\n",
            "  485 4936 7674 5372 7109 4605 5211 5462 8065   50 4080 3334 8167 8378\n",
            " 5595 1873 8207 7076 3719 4627  231 2405 6973 3246 5931 8208 4825 6203\n",
            " 3019 5822   63 5854 5787 4764 6701 3822 8318 8169 8218 7549  707 1076\n",
            " 2465 4636 7025 7127 4026 4961 7531  744 6684 4297 7939 7384  446 6675\n",
            " 7752 5279 8196 3020 4192 4253 2887 1898 3208 5973 7033 1431 5915 8312\n",
            " 5799 7105  511 6404 1561]\n",
            "Sun Jun  5 18:36:45 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P0    37W / 300W |   4723MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "509/509 [==============================] - 193s 348ms/step - loss: 7.7818 - accuracy: 0.5132 - val_loss: 7.0795 - val_accuracy: 0.6834\n",
            "Epoch 2/5\n",
            "509/509 [==============================] - 177s 348ms/step - loss: 6.8143 - accuracy: 0.7309 - val_loss: 6.6446 - val_accuracy: 0.7426\n",
            "Epoch 3/5\n",
            "509/509 [==============================] - 177s 348ms/step - loss: 6.3390 - accuracy: 0.8007 - val_loss: 6.3895 - val_accuracy: 0.7472\n",
            "Epoch 4/5\n",
            "509/509 [==============================] - 173s 340ms/step - loss: 5.9647 - accuracy: 0.8418 - val_loss: 6.3053 - val_accuracy: 0.7130\n",
            "Epoch 5/5\n",
            "509/509 [==============================] - 173s 340ms/step - loss: 5.6221 - accuracy: 0.8913 - val_loss: 6.0572 - val_accuracy: 0.7426\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_3[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_roberta_model[1][0]']       \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 15)           7695        ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.7292161520190024\n",
            "Weighted F1: 0.7286083085079803\n",
            "Micro F1: 0.7292161520190024\n",
            "Weighted Precision: 0.7347144180268118\n",
            "Micro Precision: 0.7292161520190024\n",
            "Weighted Recall: 0.7292161520190024\n",
            "Micro Recall: 0.7292161520190024\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_1[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_roberta_model[0][0]']       \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 15)           7695        ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 8  1  2 ...  1 10  1]\n",
            "[   1    2    4 ... 8414 8417 8406]\n",
            "[6622   35 6713 4989 8233 7444 5061 8344 5464  317 4515 3001 6991 2875\n",
            " 3885 6647 4857 4504  214 8340 2969 3847 4609 7358 2752 7651 8188 7073\n",
            " 6483 4952 5072 1967 4071 7590 5030 7297  626 6029 6302 7256 3993 4263\n",
            " 3835 4293 8129 4837 3843 1418 5274 6205 5997 6685 3310 7721 2360 7197\n",
            " 4252 8343  909 4354  649 3488 2496 5765 4004 4052 1403 6697 6293 6380\n",
            " 4602 6057 4236 7978 5083 6552  361 7706 4186 8351  812 6600 8264 8064\n",
            " 7121 8279 5556 5119 7947 3350 7152 4282 5025 5051 3615 8062 1489 7351\n",
            "  234 7416 7921 6574 6257 8396 4520 3300 5640  240 7810 1321 6580 5404\n",
            " 8273 6742 7861 7975 1018 2140 7061 7767 4990 8124 5927 6051 6916 2846\n",
            " 6818 7052 7775 7582 8116 4767 5248 6055 8331 1564 7326 3065 6764 6854\n",
            " 4915 3583 5902 4229 7565 3712 7224 4502 3827 6525 6318 2686 5369 8235\n",
            " 3351 8021  102  224 8294 7707 2985 5146 3709 6824 2281 6837 5436 2649\n",
            " 4886  966 5476 8387 6166 4387  350 6123 5236 5041  734 3412 6085  489\n",
            " 4982 5115 6743 7173 5724 7689 7787 8302 3150 2247 8128 6660 4610 2866\n",
            " 6905 7096  768 4411 6087 2096 4903 7546 6630 2928 4640 7125 1792 1700\n",
            " 7928 8095  737 5849 2359 6015 4361 6081 5014 5167 1023 8009  268 4777\n",
            " 5702 6129 7181 1860 7192 5467   67 6134 4270 4611 6746 7696 3466 3950\n",
            " 6125 8370 6142 6131 5548 1721 5067 4015 7092   25 1749 4878 8248 6836\n",
            " 5310 6960 2339 1650 6227 4104 5130 7089 6993 1289 4305 5585 7933 6284\n",
            " 7057 3807 8347 5280 2841 7693 8090 4171 5260  133 7451 3957 1719 5435\n",
            " 6216 4858 4417 8355 4492 2648 5278 7308  158 7543 2168 5913 7111 3414\n",
            " 7592   39 7157 5647 4555 3956 3617 4630 4384 6596 4539 6749 7202 2413\n",
            " 4749 5122 5929  252 5565  336 6008 4564 5580 7237 5958 6918 6487 2989\n",
            " 7448 3382  467 3784 4419 1117 7954 7122 3174 1534 4164 7526  498 5944\n",
            " 4572 2307 2929 7480 4412 4563 7692 1869 2472 6966 6549 1594 2276 7269\n",
            " 2545 2216 6587 6315 5297 5282 6399 3917 6342 5721 7909 8382 4383 7130\n",
            "  485 4936 7674 5372 7109 4605 5211 5462 8065   50 4080 3334 8167 8378\n",
            " 5595 1873 8207 7076 3719 4627  231 2405 6973 3246 5931 8208 4825 6203\n",
            " 3019 5822   63 5854 5787 4764 6701 3822 8318 8169 8218 7549  707 1076\n",
            " 2465 4636 7025 7127 4026 4961 7531  744 6684 4297 7939 7384  446 6675\n",
            " 7752 5279 8196 3020 4192 4253 2887 1898 3208 5973 7033 1431 5915 8312\n",
            " 5799 7105  511 6404 1561]\n",
            "Sun Jun  5 18:53:17 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P0    37W / 300W |  15237MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "509/509 [==============================] - 193s 348ms/step - loss: 7.9067 - accuracy: 0.4598 - val_loss: 7.1273 - val_accuracy: 0.6606\n",
            "Epoch 2/5\n",
            "509/509 [==============================] - 177s 348ms/step - loss: 6.8450 - accuracy: 0.7060 - val_loss: 6.6354 - val_accuracy: 0.7198\n",
            "Epoch 3/5\n",
            "509/509 [==============================] - 177s 347ms/step - loss: 6.3339 - accuracy: 0.7914 - val_loss: 6.4171 - val_accuracy: 0.7267\n",
            "Epoch 4/5\n",
            "509/509 [==============================] - 177s 348ms/step - loss: 5.9751 - accuracy: 0.8367 - val_loss: 6.2404 - val_accuracy: 0.7289\n",
            "Epoch 5/5\n",
            "509/509 [==============================] - 177s 348ms/step - loss: 5.6263 - accuracy: 0.8831 - val_loss: 6.0514 - val_accuracy: 0.7403\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_3[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_roberta_model[1][0]']       \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 15)           7695        ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.7161520190023754\n",
            "Weighted F1: 0.7060741156912325\n",
            "Micro F1: 0.7161520190023754\n",
            "Weighted Precision: 0.7150347814427972\n",
            "Micro Precision: 0.7161520190023754\n",
            "Weighted Recall: 0.7161520190023754\n",
            "Micro Recall: 0.7161520190023754\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_1[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_roberta_model[0][0]']       \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 15)           7695        ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 8  1  2 ...  1 10  1]\n",
            "[   1    2    4 ... 8414 8417 8406]\n",
            "[6622   35 6713 4989 8233 7444 5061 8344 5464  317 4515 3001 6991 2875\n",
            " 3885 6647 4857 4504  214 8340 2969 3847 4609 7358 2752 7651 8188 7073\n",
            " 6483 4952 5072 1967 4071 7590 5030 7297  626 6029 6302 7256 3993 4263\n",
            " 3835 4293 8129 4837 3843 1418 5274 6205 5997 6685 3310 7721 2360 7197\n",
            " 4252 8343  909 4354  649 3488 2496 5765 4004 4052 1403 6697 6293 6380\n",
            " 4602 6057 4236 7978 5083 6552  361 7706 4186 8351  812 6600 8264 8064\n",
            " 7121 8279 5556 5119 7947 3350 7152 4282 5025 5051 3615 8062 1489 7351\n",
            "  234 7416 7921 6574 6257 8396 4520 3300 5640  240 7810 1321 6580 5404\n",
            " 8273 6742 7861 7975 1018 2140 7061 7767 4990 8124 5927 6051 6916 2846\n",
            " 6818 7052 7775 7582 8116 4767 5248 6055 8331 1564 7326 3065 6764 6854\n",
            " 4915 3583 5902 4229 7565 3712 7224 4502 3827 6525 6318 2686 5369 8235\n",
            " 3351 8021  102  224 8294 7707 2985 5146 3709 6824 2281 6837 5436 2649\n",
            " 4886  966 5476 8387 6166 4387  350 6123 5236 5041  734 3412 6085  489\n",
            " 4982 5115 6743 7173 5724 7689 7787 8302 3150 2247 8128 6660 4610 2866\n",
            " 6905 7096  768 4411 6087 2096 4903 7546 6630 2928 4640 7125 1792 1700\n",
            " 7928 8095  737 5849 2359 6015 4361 6081 5014 5167 1023 8009  268 4777\n",
            " 5702 6129 7181 1860 7192 5467   67 6134 4270 4611 6746 7696 3466 3950\n",
            " 6125 8370 6142 6131 5548 1721 5067 4015 7092   25 1749 4878 8248 6836\n",
            " 5310 6960 2339 1650 6227 4104 5130 7089 6993 1289 4305 5585 7933 6284\n",
            " 7057 3807 8347 5280 2841 7693 8090 4171 5260  133 7451 3957 1719 5435\n",
            " 6216 4858 4417 8355 4492 2648 5278 7308  158 7543 2168 5913 7111 3414\n",
            " 7592   39 7157 5647 4555 3956 3617 4630 4384 6596 4539 6749 7202 2413\n",
            " 4749 5122 5929  252 5565  336 6008 4564 5580 7237 5958 6918 6487 2989\n",
            " 7448 3382  467 3784 4419 1117 7954 7122 3174 1534 4164 7526  498 5944\n",
            " 4572 2307 2929 7480 4412 4563 7692 1869 2472 6966 6549 1594 2276 7269\n",
            " 2545 2216 6587 6315 5297 5282 6399 3917 6342 5721 7909 8382 4383 7130\n",
            "  485 4936 7674 5372 7109 4605 5211 5462 8065   50 4080 3334 8167 8378\n",
            " 5595 1873 8207 7076 3719 4627  231 2405 6973 3246 5931 8208 4825 6203\n",
            " 3019 5822   63 5854 5787 4764 6701 3822 8318 8169 8218 7549  707 1076\n",
            " 2465 4636 7025 7127 4026 4961 7531  744 6684 4297 7939 7384  446 6675\n",
            " 7752 5279 8196 3020 4192 4253 2887 1898 3208 5973 7033 1431 5915 8312\n",
            " 5799 7105  511 6404 1561]\n",
            "Sun Jun  5 19:10:00 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P0    37W / 300W |  15237MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "509/509 [==============================] - 192s 348ms/step - loss: 7.7955 - accuracy: 0.5137 - val_loss: 7.0995 - val_accuracy: 0.6856\n",
            "Epoch 2/5\n",
            "509/509 [==============================] - 177s 348ms/step - loss: 6.8519 - accuracy: 0.7196 - val_loss: 6.6587 - val_accuracy: 0.7198\n",
            "Epoch 3/5\n",
            "509/509 [==============================] - 178s 349ms/step - loss: 6.3576 - accuracy: 0.8037 - val_loss: 6.4305 - val_accuracy: 0.7494\n",
            "Epoch 4/5\n",
            "509/509 [==============================] - 173s 340ms/step - loss: 5.9717 - accuracy: 0.8487 - val_loss: 6.3350 - val_accuracy: 0.7221\n",
            "Epoch 5/5\n",
            "509/509 [==============================] - 173s 340ms/step - loss: 5.6340 - accuracy: 0.8792 - val_loss: 6.1253 - val_accuracy: 0.7153\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_3[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_roberta_model[1][0]']       \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 15)           7695        ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.7315914489311164\n",
            "Weighted F1: 0.7242153114071438\n",
            "Micro F1: 0.7315914489311163\n",
            "Weighted Precision: 0.7261471108110715\n",
            "Micro Precision: 0.7315914489311164\n",
            "Weighted Recall: 0.7315914489311164\n",
            "Micro Recall: 0.7315914489311164\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_1[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_roberta_model[0][0]']       \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 15)           7695        ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 8  1  2 ...  1 10  1]\n",
            "[   1    2    4 ... 8414 8417 8406]\n",
            "[6622   35 6713 4989 8233 7444 5061 8344 5464  317 4515 3001 6991 2875\n",
            " 3885 6647 4857 4504  214 8340 2969 3847 4609 7358 2752 7651 8188 7073\n",
            " 6483 4952 5072 1967 4071 7590 5030 7297  626 6029 6302 7256 3993 4263\n",
            " 3835 4293 8129 4837 3843 1418 5274 6205 5997 6685 3310 7721 2360 7197\n",
            " 4252 8343  909 4354  649 3488 2496 5765 4004 4052 1403 6697 6293 6380\n",
            " 4602 6057 4236 7978 5083 6552  361 7706 4186 8351  812 6600 8264 8064\n",
            " 7121 8279 5556 5119 7947 3350 7152 4282 5025 5051 3615 8062 1489 7351\n",
            "  234 7416 7921 6574 6257 8396 4520 3300 5640  240 7810 1321 6580 5404\n",
            " 8273 6742 7861 7975 1018 2140 7061 7767 4990 8124 5927 6051 6916 2846\n",
            " 6818 7052 7775 7582 8116 4767 5248 6055 8331 1564 7326 3065 6764 6854\n",
            " 4915 3583 5902 4229 7565 3712 7224 4502 3827 6525 6318 2686 5369 8235\n",
            " 3351 8021  102  224 8294 7707 2985 5146 3709 6824 2281 6837 5436 2649\n",
            " 4886  966 5476 8387 6166 4387  350 6123 5236 5041  734 3412 6085  489\n",
            " 4982 5115 6743 7173 5724 7689 7787 8302 3150 2247 8128 6660 4610 2866\n",
            " 6905 7096  768 4411 6087 2096 4903 7546 6630 2928 4640 7125 1792 1700\n",
            " 7928 8095  737 5849 2359 6015 4361 6081 5014 5167 1023 8009  268 4777\n",
            " 5702 6129 7181 1860 7192 5467   67 6134 4270 4611 6746 7696 3466 3950\n",
            " 6125 8370 6142 6131 5548 1721 5067 4015 7092   25 1749 4878 8248 6836\n",
            " 5310 6960 2339 1650 6227 4104 5130 7089 6993 1289 4305 5585 7933 6284\n",
            " 7057 3807 8347 5280 2841 7693 8090 4171 5260  133 7451 3957 1719 5435\n",
            " 6216 4858 4417 8355 4492 2648 5278 7308  158 7543 2168 5913 7111 3414\n",
            " 7592   39 7157 5647 4555 3956 3617 4630 4384 6596 4539 6749 7202 2413\n",
            " 4749 5122 5929  252 5565  336 6008 4564 5580 7237 5958 6918 6487 2989\n",
            " 7448 3382  467 3784 4419 1117 7954 7122 3174 1534 4164 7526  498 5944\n",
            " 4572 2307 2929 7480 4412 4563 7692 1869 2472 6966 6549 1594 2276 7269\n",
            " 2545 2216 6587 6315 5297 5282 6399 3917 6342 5721 7909 8382 4383 7130\n",
            "  485 4936 7674 5372 7109 4605 5211 5462 8065   50 4080 3334 8167 8378\n",
            " 5595 1873 8207 7076 3719 4627  231 2405 6973 3246 5931 8208 4825 6203\n",
            " 3019 5822   63 5854 5787 4764 6701 3822 8318 8169 8218 7549  707 1076\n",
            " 2465 4636 7025 7127 4026 4961 7531  744 6684 4297 7939 7384  446 6675\n",
            " 7752 5279 8196 3020 4192 4253 2887 1898 3208 5973 7033 1431 5915 8312\n",
            " 5799 7105  511 6404 1561]\n",
            "Sun Jun  5 19:26:29 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P0    37W / 300W |  15237MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "509/509 [==============================] - 193s 348ms/step - loss: 7.7458 - accuracy: 0.5183 - val_loss: 7.0271 - val_accuracy: 0.6765\n",
            "Epoch 2/5\n",
            "509/509 [==============================] - 177s 347ms/step - loss: 6.7816 - accuracy: 0.7284 - val_loss: 6.6145 - val_accuracy: 0.7312\n",
            "Epoch 3/5\n",
            "509/509 [==============================] - 177s 348ms/step - loss: 6.2986 - accuracy: 0.8057 - val_loss: 6.4390 - val_accuracy: 0.7380\n",
            "Epoch 4/5\n",
            "509/509 [==============================] - 173s 340ms/step - loss: 5.9500 - accuracy: 0.8433 - val_loss: 6.1976 - val_accuracy: 0.7267\n",
            "Epoch 5/5\n",
            "509/509 [==============================] - 173s 340ms/step - loss: 5.6015 - accuracy: 0.8900 - val_loss: 6.1310 - val_accuracy: 0.7221\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_3[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_roberta_model[1][0]']       \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 15)           7695        ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.7054631828978623\n",
            "Weighted F1: 0.6989787058875981\n",
            "Micro F1: 0.7054631828978623\n",
            "Weighted Precision: 0.7029985113884637\n",
            "Micro Precision: 0.7054631828978623\n",
            "Weighted Recall: 0.7054631828978623\n",
            "Micro Recall: 0.7054631828978623\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_1[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_roberta_model[0][0]']       \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 15)           7695        ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 8  1  2 ...  1 10  1]\n",
            "[   1    2    4 ... 8414 8417 8406]\n",
            "[6622   35 6713 4989 8233 7444 5061 8344 5464  317 4515 3001 6991 2875\n",
            " 3885 6647 4857 4504  214 8340 2969 3847 4609 7358 2752 7651 8188 7073\n",
            " 6483 4952 5072 1967 4071 7590 5030 7297  626 6029 6302 7256 3993 4263\n",
            " 3835 4293 8129 4837 3843 1418 5274 6205 5997 6685 3310 7721 2360 7197\n",
            " 4252 8343  909 4354  649 3488 2496 5765 4004 4052 1403 6697 6293 6380\n",
            " 4602 6057 4236 7978 5083 6552  361 7706 4186 8351  812 6600 8264 8064\n",
            " 7121 8279 5556 5119 7947 3350 7152 4282 5025 5051 3615 8062 1489 7351\n",
            "  234 7416 7921 6574 6257 8396 4520 3300 5640  240 7810 1321 6580 5404\n",
            " 8273 6742 7861 7975 1018 2140 7061 7767 4990 8124 5927 6051 6916 2846\n",
            " 6818 7052 7775 7582 8116 4767 5248 6055 8331 1564 7326 3065 6764 6854\n",
            " 4915 3583 5902 4229 7565 3712 7224 4502 3827 6525 6318 2686 5369 8235\n",
            " 3351 8021  102  224 8294 7707 2985 5146 3709 6824 2281 6837 5436 2649\n",
            " 4886  966 5476 8387 6166 4387  350 6123 5236 5041  734 3412 6085  489\n",
            " 4982 5115 6743 7173 5724 7689 7787 8302 3150 2247 8128 6660 4610 2866\n",
            " 6905 7096  768 4411 6087 2096 4903 7546 6630 2928 4640 7125 1792 1700\n",
            " 7928 8095  737 5849 2359 6015 4361 6081 5014 5167 1023 8009  268 4777\n",
            " 5702 6129 7181 1860 7192 5467   67 6134 4270 4611 6746 7696 3466 3950\n",
            " 6125 8370 6142 6131 5548 1721 5067 4015 7092   25 1749 4878 8248 6836\n",
            " 5310 6960 2339 1650 6227 4104 5130 7089 6993 1289 4305 5585 7933 6284\n",
            " 7057 3807 8347 5280 2841 7693 8090 4171 5260  133 7451 3957 1719 5435\n",
            " 6216 4858 4417 8355 4492 2648 5278 7308  158 7543 2168 5913 7111 3414\n",
            " 7592   39 7157 5647 4555 3956 3617 4630 4384 6596 4539 6749 7202 2413\n",
            " 4749 5122 5929  252 5565  336 6008 4564 5580 7237 5958 6918 6487 2989\n",
            " 7448 3382  467 3784 4419 1117 7954 7122 3174 1534 4164 7526  498 5944\n",
            " 4572 2307 2929 7480 4412 4563 7692 1869 2472 6966 6549 1594 2276 7269\n",
            " 2545 2216 6587 6315 5297 5282 6399 3917 6342 5721 7909 8382 4383 7130\n",
            "  485 4936 7674 5372 7109 4605 5211 5462 8065   50 4080 3334 8167 8378\n",
            " 5595 1873 8207 7076 3719 4627  231 2405 6973 3246 5931 8208 4825 6203\n",
            " 3019 5822   63 5854 5787 4764 6701 3822 8318 8169 8218 7549  707 1076\n",
            " 2465 4636 7025 7127 4026 4961 7531  744 6684 4297 7939 7384  446 6675\n",
            " 7752 5279 8196 3020 4192 4253 2887 1898 3208 5973 7033 1431 5915 8312\n",
            " 5799 7105  511 6404 1561]\n",
            "Sun Jun  5 19:43:00 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P0    37W / 300W |  15237MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "509/509 [==============================] - 195s 350ms/step - loss: 7.7177 - accuracy: 0.5309 - val_loss: 7.0703 - val_accuracy: 0.6743\n",
            "Epoch 2/5\n",
            "509/509 [==============================] - 177s 349ms/step - loss: 6.7708 - accuracy: 0.7176 - val_loss: 6.6377 - val_accuracy: 0.7221\n",
            "Epoch 3/5\n",
            "509/509 [==============================] - 177s 348ms/step - loss: 6.2905 - accuracy: 0.8057 - val_loss: 6.4017 - val_accuracy: 0.7403\n",
            "Epoch 4/5\n",
            "509/509 [==============================] - 177s 348ms/step - loss: 5.9242 - accuracy: 0.8431 - val_loss: 6.1976 - val_accuracy: 0.7449\n",
            "Epoch 5/5\n",
            "509/509 [==============================] - 173s 340ms/step - loss: 5.5706 - accuracy: 0.8866 - val_loss: 6.0730 - val_accuracy: 0.7403\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_3[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_roberta_model[1][0]']       \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 15)           7695        ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.7232779097387173\n",
            "Weighted F1: 0.7193859455047584\n",
            "Micro F1: 0.7232779097387173\n",
            "Weighted Precision: 0.7248016582766308\n",
            "Micro Precision: 0.7232779097387173\n",
            "Weighted Recall: 0.7232779097387173\n",
            "Micro Recall: 0.7232779097387173\n",
            "Average Accuracy: 0.7211401425178147\n",
            "Average Weighted F1: 0.7154524773997426\n",
            "Average Micro F1: 0.7211401425178147\n",
            "Average Weighted Precision: 0.720739295989155\n",
            "Average Micro Precision: 0.7211401425178147\n",
            "Average Weighted Recall: 0.7211401425178147\n",
            "Average Micro Recall: 0.7211401425178147\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "total_accuracy=0\n",
        "total_weighted_f1=0\n",
        "total_micro_f1=0\n",
        "total_weighted_precision=0\n",
        "total_micro_precision=0\n",
        "total_weighted_recall=0\n",
        "total_micro_recall=0\n",
        "\n",
        "for f in range(5):\n",
        "  gc.collect()\n",
        "  tf.keras.backend.clear_session()\n",
        "  dbert_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "  dbert_model = TFRobertaModel.from_pretrained('roberta-base')\n",
        "  max_len=512\n",
        "  sentences=summarized_data['text']\n",
        "  labels=summarized_data['label']\n",
        "  filenumbers=summarized_data['filenum']\n",
        "  len(sentences),len(labels),len(filenumbers)\n",
        "  \n",
        "  model_5=create_model()\n",
        "  input_ids=[]\n",
        "  attention_masks=[]\n",
        "\n",
        "  for sent in sentences:\n",
        "    dbert_inps=dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n",
        "    input_ids.append(dbert_inps['input_ids'])\n",
        "    attention_masks.append(dbert_inps['attention_mask'])\n",
        "  input_ids=np.asarray(input_ids)\n",
        "\n",
        "  attention_masks=np.array(attention_masks)\n",
        "  labels=np.array(labels)\n",
        "  filenumbers=[int(i) for i in filenumbers]\n",
        "  filenumbers=np.array(filenumbers)\n",
        "  print(labels)\n",
        "  print(filenumbers)\n",
        "\n",
        "  #train_inp,val_inp,train_label,val_label,train_mask,val_mask,train_fnum,val_fnum=train_test_split(input_ids,labels,attention_masks,filenumbers,test_size=0.1,random_state=42)\n",
        "\n",
        "  new_train_inp_5=[]\n",
        "  new_val_inp_5=[]\n",
        "  new_train_label_5=[]\n",
        "  new_val_label_5=[]\n",
        "  new_train_mask_5=[]\n",
        "  new_train_fnum_5=[]\n",
        "  new_val_fnum_5=[]\n",
        "  new_val_mask_5=[]\n",
        "  for i in range(len(train_fnum)):\n",
        "    if train_fnum[i] in filenumbers:\n",
        "      new_train_inp_5.append(train_inp[i])\n",
        "      #new_val_inp.append(val_inp[i])\n",
        "      new_train_label_5.append(train_label[i])\n",
        "      #new_val_label.append(val_label[i])\n",
        "      new_train_mask_5.append(train_mask[i])\n",
        "      #new_val_mask.append(val_mask[i])\n",
        "      new_train_fnum_5.append(train_fnum[i])\n",
        "\n",
        "  for i in range(len(val_fnum)):\n",
        "    if val_fnum[i] in filenumbers:\n",
        "      #new_train_inp.append(train_inp[i])\n",
        "      new_val_inp_5.append(val_inp[i])\n",
        "      #new_train_label.append(train_label[i])\n",
        "      new_val_label_5.append(val_label[i])\n",
        "      #new_train_mask.append(train_mask[i])\n",
        "      new_val_mask_5.append(val_mask[i])\n",
        "      new_val_fnum_5.append(val_fnum[i])\n",
        "\n",
        "  # print(new_train_label)\n",
        "  # print(len(new_train_label))\n",
        "  # print(new_val_label)\n",
        "  # print(len(new_val_label))\n",
        "  new_train_inp_5=np.array(new_train_inp_5)\n",
        "  new_val_inp_5=np.array(new_val_inp_5)\n",
        "  new_train_label_5=np.array(new_train_label_5)\n",
        "  new_val_label_5=np.array(new_val_label_5)\n",
        "  new_train_mask_5=np.array(new_train_mask_5)\n",
        "  new_train_fnum_5=np.array(new_train_fnum_5)\n",
        "  new_val_fnum_5=np.array(new_val_fnum_5)\n",
        "  new_val_mask_5=np.array(new_val_mask_5)\n",
        "\n",
        "  print(new_val_fnum_5)\n",
        "  \n",
        "  log_dir='dbert_model'\n",
        "\n",
        "  model_save_path='./drive/MyDrive/Ensemble/roberta-ensemble-512-model_5-'+str(f)+'-15labels.h5'\n",
        "\n",
        "  loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "  accuracy = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
        "  callbacks= [tf.keras.callbacks.ModelCheckpoint(filepath=model_save_path,monitor='val_accuracy',mode='max',save_best_only=True,save_weights_only=True),keras.callbacks.TensorBoard(log_dir=log_dir)]\n",
        "  model_5.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])\n",
        "  gpu_info = !nvidia-smi\n",
        "  gpu_info = '\\n'.join(gpu_info)\n",
        "  if gpu_info.find('failed') >= 0:\n",
        "    print('Not connected to a GPU')\n",
        "  else:\n",
        "    print(gpu_info)\n",
        "  \n",
        "  history=model_5.fit([new_train_inp_5,new_train_mask_5],new_train_label_5,batch_size=8,epochs=5,validation_data=([new_val_inp_5,new_val_mask_5],new_val_label_5),callbacks=callbacks)\n",
        "\n",
        "  pred_labels=[]\n",
        "\n",
        "  model_saved_5= create_model()\n",
        "  model_saved_5.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])\n",
        "  model_saved_5.load_weights('./drive/MyDrive/Ensemble/roberta-ensemble-512-model_4-'+str(f)+'-15labels.h5')\n",
        "\n",
        "  for i in range(0,len(val_inp)):\n",
        "    pred=model_saved_5.predict([val_inp[i].reshape(1,512),val_mask[i].reshape(1,512)])\n",
        "    pred_label = pred.argmax(axis=1)\n",
        "    pred_labels.append(pred_label)\n",
        "  accuracy=accuracy_score(val_label, pred_labels)\n",
        "  print(\"Accuracy: \"+str(accuracy))\n",
        "  total_accuracy=total_accuracy+accuracy\n",
        "  \n",
        "  weighted_f1=f1_score(val_label,pred_labels, average='weighted')\n",
        "  print(\"Weighted F1: \"+ str(weighted_f1))\n",
        "  total_weighted_f1=total_weighted_f1+weighted_f1\n",
        "  micro_f1=f1_score(val_label,pred_labels, average='micro')\n",
        "  print(\"Micro F1: \"+ str(micro_f1))\n",
        "  total_micro_f1=total_micro_f1+micro_f1\n",
        "\n",
        "  weighted_precision=precision_score(val_label, pred_labels, average='weighted')\n",
        "  print(\"Weighted Precision: \" + str(weighted_precision))\n",
        "  total_weighted_precision=total_weighted_precision+weighted_precision\n",
        "  micro_precision=precision_score(val_label, pred_labels, average='micro')\n",
        "  print(\"Micro Precision: \" + str(micro_precision))\n",
        "  total_micro_precision=total_micro_precision+micro_precision\n",
        "\n",
        "  weighted_recall=recall_score(val_label, pred_labels, average='weighted')\n",
        "  print(\"Weighted Recall: \" + str(weighted_recall))\n",
        "  total_weighted_recall=total_weighted_recall+weighted_recall\n",
        "  micro_recall=recall_score(val_label, pred_labels, average='micro')\n",
        "  print(\"Micro Recall: \" + str(micro_recall))\n",
        "  total_micro_recall=total_micro_recall+micro_recall\n",
        "\n",
        "\n",
        "print(\"Average Accuracy: \"+str(total_accuracy/5))\n",
        "print(\"Average Weighted F1: \"+str(total_weighted_f1/5))\n",
        "print(\"Average Micro F1: \"+str(total_micro_f1/5))\n",
        "print(\"Average Weighted Precision: \"+str(total_weighted_precision/5))\n",
        "print(\"Average Micro Precision: \"+str(total_micro_precision/5))\n",
        "print(\"Average Weighted Recall: \"+str(total_weighted_recall/5))\n",
        "print(\"Average Micro Recall: \"+str(total_micro_recall/5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aiQQ3PLLiLdn"
      },
      "outputs": [],
      "source": [
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "accuracy = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
        "\n",
        "def load_model(loss,accuracy,optimizer,id):\n",
        "  model= create_model()\n",
        "  model.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])\n",
        "  model.load_weights('./drive/MyDrive/Ensemble/roberta-ensemble-512-model_'+str(id)+'-15labels.h5')\n",
        "  return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcfQEe8-lDZ7",
        "outputId": "f926a8dd-2957-4a45-e44e-324b06e79f48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[6622   35 2227 6713 4989 8233 7444 5061 3942 8344  454 5464  317 2883\n",
            " 4515  796 3001 3410 6991 2875 1125 1657  354 3885 6647 4857 4504  214\n",
            " 3230 8340 2969 3847 4609 5092 6110 1346 5993 7358 2752 7651 7301 8188\n",
            " 7073 4782 6069 2346 6445 6483  542 1663 4952 5072 1967 4071 7590  860\n",
            " 6418 1997 5030 7297 2570  626 5463 6029 6302 3819 7256 2950 5667 1235\n",
            " 3993 1439 1323 4263 3835 2134 3054 2624 5801 4293 8129  320 5866 3247\n",
            " 5777 2373 5518   19 6059 6520 4837 3843 5695 1666  858 1418 5409 5274\n",
            " 6205 3115 3167 2356 8418 5997 6685  223 3444 3310 7721 5928 6139 2360\n",
            " 7197 4252 8343  909 4093 4354 4752 1421  649  829 1916 1003 3488 2496\n",
            " 5765 6612 4004 4052 1403 6697 5268 1010 6293  264 6380 4602 6057 1480\n",
            " 1851 4577 4236 7978 1350 5256 5083 6552 1058  361 7706 4186 2590 4452\n",
            " 4396 5386 2522  749  870 8351 3834  812 5108  463 2274 1743 6600 8264\n",
            " 8064 7121 8279 5556 1845 5119 2516 4673 2652 7947 8215 1882 3350 7617\n",
            " 7152 5199 3965 1374 4282 7672 1092 3342 8003 5025 5051  535 6024  933\n",
            " 3969 1927 3615 1221 8062 4920 1489 3296 7442 5459 2031 7351 1633  234\n",
            " 2313 8015 7416 7921 6574 6257 3535 8396  941 4520 3300 5640  240 7810\n",
            " 7425 1321 6580  928 1170 5404 1920 8273 5649 1919 6742 7005   94 7459\n",
            "    9 7861 7975 1018 5029 2322 2140 1415 7061 7767 4990 6261 8124 5927\n",
            " 6051 1739 3269 6916 5959 1503 4906 2846 6818 7052 7775 7582 8116 4240\n",
            " 4767 5248  430 1812 2175 6055 8331 3916  998 7271 1564 7326 1077 1559\n",
            "  292 3065 6764 6854 4915 5552 3583 1917  868 5902 2581 4229 2593 2144\n",
            " 7565 3712 7224 4502 1406 1651 6256 3827 6525 6318  791 2686 3616 5369\n",
            " 8235 1704 1011 3351 8089 8021  102  224 5671 8294 5874 1510 2459 7707\n",
            " 2985 6624 5146 1428 7381 3709 6824 6308  701 2281 6837 5436 1741 3520\n",
            " 5995 2649 4886  966 2863 5476 4058 1246 2406 1199 8387  513 5449 1028\n",
            " 2226 6166 4387  350 6123  678 5236 5041  734 3743 3439 3412 6085  587\n",
            " 2477  489 5760 3163 3376 4982  321 5115 4655 5575 3839 6743 7173  624\n",
            " 5724 7689 7787 1748  520  713 3214 8302 3150 5713 2247 8128 3044 4607\n",
            " 2089 6660 3505  983 4610 2866 4161 6905 7096  768 1062 4411 6087 2096\n",
            " 3536   21 4903 7546 6630 2928 4640 4510 7125 2816 1792 2063 1700 2305\n",
            " 7928 8095  737 5849 2311  922 3423 2359 6015 4489 4361 3250 5076 6081\n",
            " 6346 5014 5167 1023 8009 3335  268 2211 4777 5221 4323 6168 2580  759\n",
            "  711 5702 6129 7181 1860 5385 4701 7192 1347 1692 8278 5467 3378 6579\n",
            "   67 2693 6134 4841 4270 3073 8397 4611 1057 2943 6746 7696 3466 2573\n",
            " 1983 3950 4693 6125 4033  474 8370 6142 6131 3938  995 5548 2186 2112\n",
            " 1721 4153 5067 4015 5587 7092 3156   25 1749 5554 4878 1197 8248  458\n",
            " 8035 6836 1006 5310 2698 6960 2339 1650 6227 4104 5130 7089 6993 2400\n",
            " 1144 1289 4305 5585 2524 7933 2587 6284 7057 5324 3807 5790 7373 8347\n",
            " 2390 4815 5280 2841 7220 1384 6246 4205 4408 7693 8090 2671 5077 4171\n",
            " 5260  133 7451 6248 3957 6793 1719 7379 4743 4746 5435 6216 4858 4417\n",
            " 8355 4492 2648 5104 5278 6539  743 7308 5704  158 1185 7543 2168 5913\n",
            " 7111  864 3414 5788 7592   39 7157 5647 4555 1671 3956 1045 3617 4630\n",
            " 4384 3033 6596   96 4539 6749 7202 4710 2886 2413 4749 5122 8415 5052\n",
            " 1598 5929  252 5565  336 6008 4564 5580 7237 1833 2803 1466 1568 5958\n",
            " 7657 4327 5091 6918 6487 4082 2989 7448 4715   81 3382  467 2689 3784\n",
            " 4624 2317  764 4419 1117 2962 2571 3283 7954 7122 1430 6392  505 3174\n",
            " 1534  323 4164 3659 7526  498 5944 3510 4572 2307 2929 7480 4412  731\n",
            " 4563 7692 1869 2472 6966 4238 4702 6549 1594 2276 7269 2545 2216  605\n",
            " 6587 6315 5297 5282 4816 1607 3200 1619 5504 2536 6399 3459 3917 1212\n",
            " 6342 5754 5721 7385 7909 6126  555 7382 8382 4383 1777 5502 7130  485\n",
            " 5862 4936 7674 3575 5372 3893 7109 4605 5211 1895 5462 8065   50 4080\n",
            " 3334 8167 8378 5595 1873 8207 7076 4106 3719 4627 2090  395 2685  231\n",
            " 2405 5419 6973 3480 3246 5414 3593 5931 6252 8208 4825 6203 3019 1105\n",
            " 5822 6725   63 5854 6543 5787 4764 6701  334 6562 3822 1627 4861 3051\n",
            " 1315 4805 1149  246 1484 2629 8318 3862 1089 8169 2778 2036 8218 2756\n",
            " 1299 7549 2639  707 1284 1076 2465 3815  540 4636 3188  697   80 1455\n",
            "  381 7025 7127 3992   52 4026 2777 2038 4961  383 7531  744 1453 6640\n",
            " 1047 6684 1502 4297 7939  150 7384  446 7738 3287 6675  170 7752 5279\n",
            " 8196 3020 1203 4192 1552 2085 4253 2887 2556 5729 4127 1898 8362 3208\n",
            " 5973 7033 1431 5915 8312  881 5799 7105  511 3116 6404 2852 1561 4150\n",
            " 4835 1178]\n",
            "[6622   35 6713 4989 8233 7444 5061 3942 8344  454 5464  317 2883 4515\n",
            " 3001 3410 6991 2875 1125 3885 6647 4857 4504  214 3230 8340 2969 3847\n",
            " 4609 5092 1346 5993 7358 2752 7651 7301 8188 7073 4782 6069 2346 6445\n",
            " 6483  542 1663 4952 5072 1967 4071 7590  860 6418 5030 7297  626 5463\n",
            " 6029 6302 3819 7256 2950 5667 1235 3993 1439 1323 4263 3835 2134 2624\n",
            " 5801 4293 8129  320 5866 3247 5777   19 6059 6520 4837 3843 5695 1666\n",
            "  858 1418 5409 5274 6205 3167 2356 8418 5997 6685  223 3444 3310 7721\n",
            " 5928 2360 7197 4252 8343  909 4093 4354 4752 1421  649  829 1916 1003\n",
            " 3488 2496 5765 6612 4004 4052 1403 6697 5268 1010 6293 6380 4602 6057\n",
            " 1851 4577 4236 7978 1350 5083 6552 1058  361 7706 4186 2590 4452 4396\n",
            "  749  870 8351 3834  812 5108  463 1743 6600 8264 8064 7121 8279 5556\n",
            " 1845 5119 2516 4673 7947 8215 1882 3350 7617 7152 5199 4282 1092 3342\n",
            " 8003 5025 5051  535  933 3969 1927 3615 1221 8062 1489 3296 7442 5459\n",
            " 2031 7351  234 2313 8015 7416 7921 6574 6257 8396  941 4520 3300 5640\n",
            "  240 7810 7425 1321 6580  928 1170 5404 8273 6742 7005   94 7459    9\n",
            " 7861 7975 1018 5029 2322 2140 1415 7061 7767 4990 8124 5927 6051 3269\n",
            " 6916 5959 2846 6818 7052 7775 7582 8116 4240 4767 5248  430 1812 2175\n",
            " 6055 8331 3916  998 7271 1564 7326 1077 1559  292 3065 6764 6854 4915\n",
            " 5552 3583 1917  868 5902 4229 2144 7565 3712 7224 4502 1406 1651 6256\n",
            " 3827 6525 6318 2686 3616 5369 8235 1704 1011 3351 8089 8021  102  224\n",
            " 5671 8294 1510 7707 2985 6624 5146 1428 7381 3709 6824 6308  701 2281\n",
            " 6837 5436 1741 3520 5995 2649 4886  966 2863 5476 4058 1246 2406 1199\n",
            " 8387  513 5449 1028 2226 6166 4387  350 6123  678 5236 5041  734 3412\n",
            " 6085  587  489 5760 3163 3376 4982 5115 4655 5575 3839 6743 7173 5724\n",
            " 7689 7787  520  713 3214 8302 3150 5713 2247 8128 4607 6660 3505 4610\n",
            " 2866 4161 6905 7096  768 1062 4411 6087 2096   21 4903 7546 6630 2928\n",
            " 4640 7125 1792 2063 1700 2305 7928 8095  737 5849  922 3423 2359 6015\n",
            " 4489 4361 6081 6346 5014 5167 1023 8009 3335  268 4777 5221 4323 6168\n",
            "  759  711 5702 6129 7181 1860 7192 1347 1692 5467 3378 6579   67 2693\n",
            " 6134 4841 4270 3073 8397 4611 2943 6746 7696 3466 3950 4693 6125 8370\n",
            " 6142 6131  995 5548 2186 2112 1721 5067 4015 5587 7092 3156   25 1749\n",
            " 5554 4878 1197 8248  458 8035 6836 1006 5310 2698 6960 2339 1650 6227\n",
            " 4104 5130 7089 6993 2400 1289 4305 5585 2524 7933 6284 7057 5324 3807\n",
            " 5790 8347 2390 5280 2841 7220 1384 7693 8090 2671 4171 5260  133 7451\n",
            " 3957 6793 1719 7379 4743 4746 5435 6216 4858 4417 8355 4492 2648 5104\n",
            " 5278 6539  743 7308  158 7543 2168 5913 7111  864 3414 5788 7592   39\n",
            " 7157 5647 4555 3956 1045 3617 4630 4384 6596   96 4539 6749 7202 4710\n",
            " 2413 4749 5122 5052 1598 5929  252 5565  336 6008 4564 5580 7237 1833\n",
            " 1466 1568 5958 7657 4327 5091 6918 6487 4082 2989 7448 4715   81 3382\n",
            "  467 3784 4624  764 4419 1117 2962 7954 7122 1430 6392  505 3174 1534\n",
            "  323 4164 3659 7526  498 5944 3510 4572 2307 2929 7480 4412  731 4563\n",
            " 7692 1869 2472 6966 4238 4702 6549 1594 2276 7269 2545 2216  605 6587\n",
            " 6315 5297 5282 4816 3200 1619 5504 2536 6399 3459 3917 6342 5754 5721\n",
            " 7385 7909 6126 7382 8382 4383 5502 7130  485 5862 4936 7674 3575 5372\n",
            " 3893 7109 4605 5211 5462 8065   50 4080 3334 8167 8378 5595 1873 8207\n",
            " 7076 4106 3719 4627  395 2685  231 2405 5419 6973 3480 3246 5414 3593\n",
            " 5931 6252 8208 4825 6203 3019 1105 5822   63 5854 6543 5787 4764 6701\n",
            "  334 6562 3822 4861 3051 1315 4805  246 8318 1089 8169 8218 2756 7549\n",
            "  707 1284 1076 2465 3815  540 4636 3188  697   80 1455  381 7025 7127\n",
            " 3992 4026 2777 4961  383 7531  744 6640 1047 6684 4297 7939  150 7384\n",
            "  446 7738 3287 6675 7752 5279 8196 3020 1203 4192 1552 4253 2887 2556\n",
            " 5729 1898 3208 5973 7033 1431 5915 8312 5799 7105  511 6404 2852 1561\n",
            " 4150 4835 1178]\n",
            "[6622   35 6713 4989 8233 7444 5061 3942 8344  454 5464  317 4515 3001\n",
            " 6991 2875 1125 3885 6647 4857 4504  214 8340 2969 3847 4609 5092 1346\n",
            " 5993 7358 2752 7651 7301 8188 7073 4782 6069 2346 6445 6483 1663 4952\n",
            " 5072 1967 4071 7590  860 6418 5030 7297  626 5463 6029 6302 3819 7256\n",
            " 5667 1235 3993 1439 1323 4263 3835 2624 5801 4293 8129  320 5866 3247\n",
            " 5777   19 6059 4837 3843 1666  858 1418 5409 5274 6205 3167 2356 5997\n",
            " 6685  223 3444 3310 7721 5928 2360 7197 4252 8343  909 4354 4752 1421\n",
            "  649  829 1916 3488 2496 5765 6612 4004 4052 1403 6697 5268 1010 6293\n",
            " 6380 4602 6057 1851 4577 4236 7978 5083 6552 1058  361 7706 4186 4452\n",
            " 4396  749 8351 3834  812 5108  463 1743 6600 8264 8064 7121 8279 5556\n",
            " 5119 4673 7947 1882 3350 7617 7152 5199 4282 1092 8003 5025 5051  933\n",
            " 3969 1927 3615 1221 8062 1489 3296 7442 5459 2031 7351  234 2313 8015\n",
            " 7416 7921 6574 6257 8396  941 4520 3300 5640  240 7810 7425 1321 6580\n",
            "  928 1170 5404 8273 6742 7005   94 7459    9 7861 7975 1018 5029 2140\n",
            " 1415 7061 7767 4990 8124 5927 6051 3269 6916 5959 2846 6818 7052 7775\n",
            " 7582 8116 4240 4767 5248  430 1812 2175 6055 8331 3916  998 1564 7326\n",
            " 1559  292 3065 6764 6854 4915 5552 3583 1917 5902 4229 2144 7565 3712\n",
            " 7224 4502 1406 1651 6256 3827 6525 6318 2686 3616 5369 8235 1704 1011\n",
            " 3351 8089 8021  102  224 5671 8294 7707 2985 6624 5146 1428 7381 3709\n",
            " 6824 6308  701 2281 6837 5436 1741 3520 5995 2649 4886  966 5476 2406\n",
            " 1199 8387  513 5449 1028 2226 6166 4387  350 6123  678 5236 5041  734\n",
            " 3412 6085  587  489 5760 3376 4982 5115 4655 5575 3839 6743 7173 5724\n",
            " 7689 7787  520 8302 3150 5713 2247 8128 4607 6660 3505 4610 2866 4161\n",
            " 6905 7096  768 1062 4411 6087 2096 4903 7546 6630 2928 4640 7125 1792\n",
            " 2063 1700 2305 7928 8095  737 5849  922 2359 6015 4489 4361 6081 6346\n",
            " 5014 5167 1023 8009 3335  268 4777 5221 4323 6168  759  711 5702 6129\n",
            " 7181 1860 7192 1347 5467 3378 6579   67 2693 6134 4841 4270 4611 2943\n",
            " 6746 7696 3466 3950 4693 6125 8370 6142 6131  995 5548 2112 1721 5067\n",
            " 4015 7092 3156   25 1749 5554 4878 8248  458 6836 1006 5310 2698 6960\n",
            " 2339 1650 6227 4104 5130 7089 6993 2400 1289 4305 5585 7933 6284 7057\n",
            " 3807 5790 8347 2390 5280 2841 7220 1384 7693 8090 4171 5260  133 7451\n",
            " 3957 6793 1719 7379 4743 4746 5435 6216 4858 4417 8355 4492 2648 5104\n",
            " 5278 6539  743 7308  158 7543 2168 5913 7111 3414 5788 7592   39 7157\n",
            " 5647 4555 3956 1045 3617 4630 4384 6596   96 4539 6749 7202 2413 4749\n",
            " 5122 5052 1598 5929  252 5565  336 6008 4564 5580 7237 1466 1568 5958\n",
            " 7657 4327 5091 6918 6487 4082 2989 7448 4715 3382  467 3784 4624  764\n",
            " 4419 1117 7954 7122 6392  505 3174 1534  323 4164 3659 7526  498 5944\n",
            " 4572 2307 2929 7480 4412  731 4563 7692 1869 2472 6966 4238 6549 1594\n",
            " 2276 7269 2545 2216  605 6587 6315 5297 5282 4816 1619 5504 2536 6399\n",
            " 3917 6342 5754 5721 7385 7909 6126 7382 8382 4383 5502 7130  485 5862\n",
            " 4936 7674 3575 5372 3893 7109 4605 5211 5462 8065   50 4080 3334 8167\n",
            " 8378 5595 1873 8207 7076 4106 3719 4627  395  231 2405 5419 6973 3480\n",
            " 3246 5414 5931 8208 4825 6203 3019 5822   63 5854 6543 5787 4764 6701\n",
            " 6562 3822 4861 3051 1315 4805 8318 1089 8169 8218 7549  707 1284 1076\n",
            " 2465 3815 4636 3188  697   80 1455  381 7025 7127 3992 4026 2777 4961\n",
            " 7531  744 6640 1047 6684 4297 7939 7384  446 3287 6675 7752 5279 8196\n",
            " 3020 1203 4192 1552 4253 2887 2556 5729 1898 3208 5973 7033 1431 5915\n",
            " 8312 5799 7105  511 6404 2852 1561 4150 4835 1178]\n",
            "[6622   35 6713 4989 8233 7444 5061 3942 8344 5464  317 4515 3001 6991\n",
            " 2875 1125 3885 6647 4857 4504  214 8340 2969 3847 4609 1346 5993 7358\n",
            " 2752 7651 8188 7073 4782 6069 2346 6483 1663 4952 5072 1967 4071 7590\n",
            "  860 6418 5030 7297  626 5463 6029 6302 7256 5667 1235 3993 1323 4263\n",
            " 3835 2624 5801 4293 8129 5866 5777 6059 4837 3843 1418 5409 5274 6205\n",
            " 2356 5997 6685 3444 3310 7721 5928 2360 7197 4252 8343  909 4354 4752\n",
            "  649 1916 3488 2496 5765 6612 4004 4052 1403 6697 5268 6293 6380 4602\n",
            " 6057 1851 4577 4236 7978 5083 6552  361 7706 4186 4452 4396 8351 3834\n",
            "  812 5108  463 6600 8264 8064 7121 8279 5556 5119 7947 3350 7152 5199\n",
            " 4282 1092 5025 5051  933 1927 3615 1221 8062 1489 7442 5459 2031 7351\n",
            "  234 8015 7416 7921 6574 6257 8396  941 4520 3300 5640  240 7810 7425\n",
            " 1321 6580  928 1170 5404 8273 6742   94    9 7861 7975 1018 2140 1415\n",
            " 7061 7767 4990 8124 5927 6051 3269 6916 5959 2846 6818 7052 7775 7582\n",
            " 8116 4240 4767 5248 1812 2175 6055 8331  998 1564 7326 1559  292 3065\n",
            " 6764 6854 4915 5552 3583 5902 4229 2144 7565 3712 7224 4502 1406 1651\n",
            " 6256 3827 6525 6318 2686 3616 5369 8235 1704 1011 3351 8089 8021  102\n",
            "  224 5671 8294 7707 2985 6624 5146 7381 3709 6824 6308 2281 6837 5436\n",
            " 1741 3520 2649 4886  966 5476 1199 8387 5449 1028 2226 6166 4387  350\n",
            " 6123 5236 5041  734 3412 6085  587  489 5760 4982 5115 5575 3839 6743\n",
            " 7173 5724 7689 7787 8302 3150 2247 8128 6660 3505 4610 2866 4161 6905\n",
            " 7096  768 1062 4411 6087 2096 4903 7546 6630 2928 4640 7125 1792 2063\n",
            " 1700 7928 8095  737 5849  922 2359 6015 4489 4361 6081 6346 5014 5167\n",
            " 1023 8009 3335  268 4777 5221 4323 6168  711 5702 6129 7181 1860 7192\n",
            " 1347 5467 6579   67 2693 6134 4841 4270 4611 2943 6746 7696 3466 3950\n",
            " 6125 8370 6142 6131  995 5548 1721 5067 4015 7092 3156   25 1749 5554\n",
            " 4878 8248 6836 5310 2698 6960 2339 1650 6227 4104 5130 7089 6993 2400\n",
            " 1289 4305 5585 7933 6284 7057 3807 5790 8347 5280 2841 7220 1384 7693\n",
            " 8090 4171 5260  133 7451 3957 1719 7379 4743 4746 5435 6216 4858 4417\n",
            " 8355 4492 2648 5104 5278  743 7308  158 7543 2168 5913 7111 3414 7592\n",
            "   39 7157 5647 4555 3956 3617 4630 4384 6596   96 4539 6749 7202 2413\n",
            " 4749 5122 5052 5929  252 5565  336 6008 4564 5580 7237 1466 1568 5958\n",
            " 7657 4327 5091 6918 6487 4082 2989 7448 4715 3382  467 3784 4624 4419\n",
            " 1117 7954 7122 6392 3174 1534 4164 3659 7526  498 5944 4572 2307 2929\n",
            " 7480 4412  731 4563 7692 1869 2472 6966 4238 6549 1594 2276 7269 2545\n",
            " 2216  605 6587 6315 5297 5282 1619 5504 6399 3917 6342 5754 5721 7385\n",
            " 7909 6126 7382 8382 4383 5502 7130  485 4936 7674 5372 3893 7109 4605\n",
            " 5211 5462 8065   50 4080 3334 8167 8378 5595 1873 8207 7076 4106 3719\n",
            " 4627  395  231 2405 6973 3480 3246 5414 5931 8208 4825 6203 3019 5822\n",
            "   63 5854 6543 5787 4764 6701 6562 3822 4861 3051 1315 8318 1089 8169\n",
            " 8218 7549  707 1284 1076 2465 3815 4636   80 7025 7127 4026 2777 4961\n",
            " 7531  744 6640 6684 4297 7939 7384  446 3287 6675 7752 5279 8196 3020\n",
            " 1203 4192 4253 2887 2556 5729 1898 3208 5973 7033 1431 5915 8312 5799\n",
            " 7105  511 6404 2852 1561 4150 1178]\n",
            "[6622   35 6713 4989 8233 7444 5061 8344 5464  317 4515 3001 6991 2875\n",
            " 3885 6647 4857 4504  214 8340 2969 3847 4609 1346 5993 7358 2752 7651\n",
            " 8188 7073 6069 2346 6483 4952 5072 1967 4071 7590 5030 7297  626 5463\n",
            " 6029 6302 7256 3993 4263 3835 5801 4293 8129 5866 5777 4837 3843 1418\n",
            " 5274 6205 2356 5997 6685 3310 7721 5928 2360 7197 4252 8343  909 4354\n",
            " 4752  649 3488 2496 5765 6612 4004 4052 1403 6697 5268 6293 6380 4602\n",
            " 6057 4577 4236 7978 5083 6552  361 7706 4186 8351  812 5108 6600 8264\n",
            " 8064 7121 8279 5556 5119 7947 3350 7152 5199 4282 5025 5051 1927 3615\n",
            " 1221 8062 1489 2031 7351  234 8015 7416 7921 6574 6257 8396  941 4520\n",
            " 3300 5640  240 7810 7425 1321 6580 5404 8273 6742   94    9 7861 7975\n",
            " 1018 2140 1415 7061 7767 4990 8124 5927 6051 3269 6916 2846 6818 7052\n",
            " 7775 7582 8116 4767 5248 1812 6055 8331  998 1564 7326  292 3065 6764\n",
            " 6854 4915 3583 5902 4229 2144 7565 3712 7224 4502 1406 1651 3827 6525\n",
            " 6318 2686 5369 8235 1704 1011 3351 8021  102  224 8294 7707 2985 5146\n",
            " 3709 6824 2281 6837 5436 2649 4886  966 5476 1199 8387 5449 1028 2226\n",
            " 6166 4387  350 6123 5236 5041  734 3412 6085  587  489 4982 5115 5575\n",
            " 3839 6743 7173 5724 7689 7787 8302 3150 2247 8128 6660 3505 4610 2866\n",
            " 4161 6905 7096  768 1062 4411 6087 2096 4903 7546 6630 2928 4640 7125\n",
            " 1792 2063 1700 7928 8095  737 5849  922 2359 6015 4489 4361 6081 6346\n",
            " 5014 5167 1023 8009  268 4777 5221 4323 6168 5702 6129 7181 1860 7192\n",
            " 5467 6579   67 2693 6134 4270 4611 6746 7696 3466 3950 6125 8370 6142\n",
            " 6131  995 5548 1721 5067 4015 7092 3156   25 1749 4878 8248 6836 5310\n",
            " 6960 2339 1650 6227 4104 5130 7089 6993 1289 4305 5585 7933 6284 7057\n",
            " 3807 5790 8347 5280 2841 7693 8090 4171 5260  133 7451 3957 1719 4743\n",
            " 4746 5435 6216 4858 4417 8355 4492 2648 5104 5278 7308  158 7543 2168\n",
            " 5913 7111 3414 7592   39 7157 5647 4555 3956 3617 4630 4384 6596 4539\n",
            " 6749 7202 2413 4749 5122 5929  252 5565  336 6008 4564 5580 7237 5958\n",
            " 7657 4327 5091 6918 6487 2989 7448 4715 3382  467 3784 4419 1117 7954\n",
            " 7122 3174 1534 4164 7526  498 5944 4572 2307 2929 7480 4412 4563 7692\n",
            " 1869 2472 6966 6549 1594 2276 7269 2545 2216  605 6587 6315 5297 5282\n",
            " 1619 5504 6399 3917 6342 5721 7909 6126 8382 4383 5502 7130  485 4936\n",
            " 7674 5372 7109 4605 5211 5462 8065   50 4080 3334 8167 8378 5595 1873\n",
            " 8207 7076 4106 3719 4627  231 2405 6973 3480 3246 5931 8208 4825 6203\n",
            " 3019 5822   63 5854 6543 5787 4764 6701 6562 3822 4861 1315 8318 8169\n",
            " 8218 7549  707 1076 2465 4636   80 7025 7127 4026 4961 7531  744 6684\n",
            " 4297 7939 7384  446 6675 7752 5279 8196 3020 4192 4253 2887 2556 5729\n",
            " 1898 3208 5973 7033 1431 5915 8312 5799 7105  511 6404 2852 1561]\n",
            "[6622   35 6713 4989 8233 7444 5061 8344 5464  317 4515 3001 6991 2875\n",
            " 3885 6647 4857 4504  214 8340 2969 3847 4609 7358 2752 7651 8188 7073\n",
            " 6483 4952 5072 1967 4071 7590 5030 7297  626 6029 6302 7256 3993 4263\n",
            " 3835 4293 8129 4837 3843 1418 5274 6205 5997 6685 3310 7721 2360 7197\n",
            " 4252 8343  909 4354  649 3488 2496 5765 4004 4052 1403 6697 6293 6380\n",
            " 4602 6057 4236 7978 5083 6552  361 7706 4186 8351  812 6600 8264 8064\n",
            " 7121 8279 5556 5119 7947 3350 7152 4282 5025 5051 3615 8062 1489 7351\n",
            "  234 7416 7921 6574 6257 8396 4520 3300 5640  240 7810 1321 6580 5404\n",
            " 8273 6742 7861 7975 1018 2140 7061 7767 4990 8124 5927 6051 6916 2846\n",
            " 6818 7052 7775 7582 8116 4767 5248 6055 8331 1564 7326 3065 6764 6854\n",
            " 4915 3583 5902 4229 7565 3712 7224 4502 3827 6525 6318 2686 5369 8235\n",
            " 3351 8021  102  224 8294 7707 2985 5146 3709 6824 2281 6837 5436 2649\n",
            " 4886  966 5476 8387 6166 4387  350 6123 5236 5041  734 3412 6085  489\n",
            " 4982 5115 6743 7173 5724 7689 7787 8302 3150 2247 8128 6660 4610 2866\n",
            " 6905 7096  768 4411 6087 2096 4903 7546 6630 2928 4640 7125 1792 1700\n",
            " 7928 8095  737 5849 2359 6015 4361 6081 5014 5167 1023 8009  268 4777\n",
            " 5702 6129 7181 1860 7192 5467   67 6134 4270 4611 6746 7696 3466 3950\n",
            " 6125 8370 6142 6131 5548 1721 5067 4015 7092   25 1749 4878 8248 6836\n",
            " 5310 6960 2339 1650 6227 4104 5130 7089 6993 1289 4305 5585 7933 6284\n",
            " 7057 3807 8347 5280 2841 7693 8090 4171 5260  133 7451 3957 1719 5435\n",
            " 6216 4858 4417 8355 4492 2648 5278 7308  158 7543 2168 5913 7111 3414\n",
            " 7592   39 7157 5647 4555 3956 3617 4630 4384 6596 4539 6749 7202 2413\n",
            " 4749 5122 5929  252 5565  336 6008 4564 5580 7237 5958 6918 6487 2989\n",
            " 7448 3382  467 3784 4419 1117 7954 7122 3174 1534 4164 7526  498 5944\n",
            " 4572 2307 2929 7480 4412 4563 7692 1869 2472 6966 6549 1594 2276 7269\n",
            " 2545 2216 6587 6315 5297 5282 6399 3917 6342 5721 7909 8382 4383 7130\n",
            "  485 4936 7674 5372 7109 4605 5211 5462 8065   50 4080 3334 8167 8378\n",
            " 5595 1873 8207 7076 3719 4627  231 2405 6973 3246 5931 8208 4825 6203\n",
            " 3019 5822   63 5854 5787 4764 6701 3822 8318 8169 8218 7549  707 1076\n",
            " 2465 4636 7025 7127 4026 4961 7531  744 6684 4297 7939 7384  446 6675\n",
            " 7752 5279 8196 3020 4192 4253 2887 1898 3208 5973 7033 1431 5915 8312\n",
            " 5799 7105  511 6404 1561]\n",
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_5 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_6 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_5[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_6[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_2 (Sl  (None, 768)         0           ['tf_roberta_model[2][0]']       \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_2[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_39 (Dropout)           (None, 512)          0           ['dense_4[0][0]']                \n",
            "                                                                                                  \n",
            " dense_5 (Dense)                (None, 15)           7695        ['dropout_39[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_7 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_8 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_7[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_8[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_3 (Sl  (None, 768)         0           ['tf_roberta_model[3][0]']       \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_6 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_3[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_40 (Dropout)           (None, 512)          0           ['dense_6[0][0]']                \n",
            "                                                                                                  \n",
            " dense_7 (Dense)                (None, 15)           7695        ['dropout_40[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_9 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_10 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_9[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_10[0][0]']               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_4 (Sl  (None, 768)         0           ['tf_roberta_model[4][0]']       \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_8 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_4[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_41 (Dropout)           (None, 512)          0           ['dense_8[0][0]']                \n",
            "                                                                                                  \n",
            " dense_9 (Dense)                (None, 15)           7695        ['dropout_41[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model: \"model_5\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_11 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_12 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_11[0][0]',               \n",
            " el)                            thPoolingAndCrossAt               'input_12[0][0]']               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_5 (Sl  (None, 768)         0           ['tf_roberta_model[5][0]']       \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_10 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_5[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_42 (Dropout)           (None, 512)          0           ['dense_10[0][0]']               \n",
            "                                                                                                  \n",
            " dense_11 (Dense)               (None, 15)           7695        ['dropout_42[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model: \"model_6\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_13 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_14 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_13[0][0]',               \n",
            " el)                            thPoolingAndCrossAt               'input_14[0][0]']               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_6 (Sl  (None, 768)         0           ['tf_roberta_model[6][0]']       \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_12 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_6[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_43 (Dropout)           (None, 512)          0           ['dense_12[0][0]']               \n",
            "                                                                                                  \n",
            " dense_13 (Dense)               (None, 15)           7695        ['dropout_43[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model: \"model_7\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_15 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_16 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_15[0][0]',               \n",
            " el)                            thPoolingAndCrossAt               'input_16[0][0]']               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_7 (Sl  (None, 768)         0           ['tf_roberta_model[7][0]']       \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_14 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_7[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_44 (Dropout)           (None, 512)          0           ['dense_14[0][0]']               \n",
            "                                                                                                  \n",
            " dense_15 (Dense)               (None, 15)           7695        ['dropout_44[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "[1, 1, 3, 1, 2, 8, 3, 6, 2, 1, 8, 4, 1, 4, 1, 7, 2, 4, 1, 2, 8, 9, 9, 4, 2, 8, 2, 8, 1, 8, 12, 1, 8, 1, 8, 1, 1, 2, 2, 3, 2, 1, 1, 2, 2, 8, 1, 3, 4, 8, 3, 12, 1, 7, 2, 8, 9, 1, 9, 9, 3, 3, 6, 2, 1, 1, 8, 1, 10, 1, 3, 8, 7, 1, 2, 2, 9, 8, 1, 3, 1, 11, 10, 1, 7, 9, 9, 12, 8, 1, 2, 8, 1, 2, 1, 1, 2, 10, 8, 2, 7, 3, 11, 2, 1, 9, 10, 4, 8, 9, 6, 2, 1, 1, 10, 9, 3, 3, 9, 1, 8, 8, 7, 1, 2, 1, 3, 2, 8, 2, 9, 1, 8, 1, 9, 8, 4, 1, 1, 2, 1, 1, 8, 8, 1, 5, 8, 3, 8, 9, 1, 1, 3, 7, 4, 0, 3, 8, 8, 2, 9, 8, 4, 4, 12, 1, 8, 9, 4, 7, 3, 5, 10, 2, 8, 8, 3, 9, 12, 7, 3, 1, 8, 8, 9, 8, 8, 9, 12, 1, 1, 8, 10, 1, 2, 7, 10, 12, 1, 9, 3, 9, 1, 3, 8, 2, 8, 9, 2, 8, 8, 2, 2, 9, 1, 1, 2, 10, 2, 2, 8, 3, 8, 2, 1, 7, 3, 2, 1, 2, 2, 1, 9, 1, 8, 2, 8, 1, 2, 3, 1, 8, 8, 12, 1, 1, 7, 8, 1, 2, 2, 2, 12, 1, 9, 3, 1, 1, 9, 2, 8, 1, 1, 3, 6, 2, 2, 8, 8, 8, 2, 2, 8, 2, 7, 1, 8, 2, 2, 3, 7, 12, 2, 8, 1, 1, 9, 12, 1, 3, 2, 7, 3, 1, 8, 2, 1, 5, 1, 3, 2, 1, 2, 1, 8, 8, 2, 3, 5, 12, 7, 1, 8, 5, 1, 1, 1, 8, 9, 3, 9, 4, 4, 1, 1, 9, 1, 8, 2, 1, 3, 5, 1, 1, 1, 3, 1, 8, 2, 8, 1, 8, 7, 1, 1, 8, 8, 1, 2, 8, 1, 3, 8, 8, 1, 2, 1, 1, 9, 8, 1, 12, 3, 1, 1, 8, 4, 3, 1, 2, 9, 1, 2, 2, 2, 3, 6, 9, 1, 1, 10, 12, 8, 8, 1, 9, 10, 9, 12, 1, 9, 8, 8, 7, 9, 8, 8, 8, 8, 1, 8, 10, 1, 3, 6, 2, 3, 2, 2, 2, 10, 2, 1, 3, 1, 3, 1, 7, 5, 4, 1, 2, 8, 12, 1, 1, 2, 8, 8, 1, 2, 3, 9, 1, 8, 8, 8, 1, 12, 3, 1, 2, 4, 7, 9, 1, 2, 8, 2, 8, 3, 8, 8, 4, 8, 1, 8, 1, 1, 6, 8, 8, 7, 1, 4, 1, 2, 9, 5, 6, 12, 1, 3, 3, 4, 3, 1, 8, 1, 1, 9, 3, 3, 8, 1, 9, 8, 4, 8, 10, 1, 3, 1, 5, 2, 1, 3, 1, 2, 2, 10, 8, 1, 4, 1, 1, 4, 1, 1, 9, 10, 3, 12, 8, 1, 1, 2, 8, 2, 3, 2, 5, 2, 1, 3, 9, 1, 1, 7, 1, 1, 8, 8, 2, 8, 8, 1, 8, 1, 8, 8, 6, 1, 9, 1, 1, 8, 8, 2, 10, 8, 8, 2, 1, 8, 9, 8, 2, 5, 4, 7, 2, 8, 9, 8, 12, 8, 7, 9, 1, 3, 1, 3, 8, 1, 10, 1, 1, 8, 8, 9, 8, 1, 1, 2, 2, 1, 8, 12, 1, 2, 7, 9, 1, 1, 10, 2, 8, 1, 8, 11, 1, 3, 1, 3, 1, 8, 1, 8, 2, 3, 9, 10, 9, 1, 8, 10, 9, 9, 2, 8, 9, 3, 3, 4, 2, 8, 8, 2, 9, 1, 2, 3, 8, 8, 2, 8, 2, 1, 1, 3, 1, 8, 1, 8, 1, 1, 8, 1, 8, 3, 3, 8, 10, 2, 3, 10, 2, 8, 3, 12, 1, 1, 12, 12, 1, 7, 8, 11, 1, 3, 4, 3, 1, 8, 2, 4, 8, 2, 8, 4, 8, 4, 2, 8, 8, 6, 8, 2, 1, 1, 8, 8, 9, 5, 8, 1, 1, 2, 8, 7, 3, 1, 2, 1, 8, 8, 1, 1, 2, 8, 4, 3, 8, 1, 1, 1, 3, 8, 2, 8, 8, 9, 8, 4, 7, 1, 2, 1, 8, 9, 8, 8, 2, 4, 2, 1, 1, 2, 1, 1, 8, 1, 2, 8, 10, 10, 8, 8, 2, 9, 1, 8, 2, 8, 7, 8, 1, 8, 1, 1, 3, 1, 1, 8, 1, 8, 2, 8, 1, 1, 2, 3, 1, 1, 3, 1, 1, 8, 7, 2, 8, 12, 1, 12, 8, 2, 1, 7, 1, 8, 2, 2, 12, 1, 3, 8, 8, 1, 8, 8, 1, 1, 2, 10, 1, 9, 1, 8, 12, 8, 2, 1, 1, 2, 2, 9, 10, 2, 4, 1, 1, 9, 8, 2, 1, 10, 9, 8, 1, 2, 12, 8, 4, 8, 1, 7, 12, 12, 8, 2, 1]\n",
            "[1, 5, -1, 1, 2, 8, 3, 6, 2, 1, 8, 4, 1, 2, 1, -1, 2, 9, 1, 2, 8, -1, -1, 4, 2, 8, 2, 8, 1, 8, 8, 1, 8, 1, -1, 1, 1, 2, 2, 3, 2, 1, 10, 2, 10, 8, 1, 3, 4, 8, 3, 12, 1, 7, 2, 8, 9, -1, 9, 9, -1, 2, 6, 10, 1, 1, 2, 1, 8, 1, 3, 8, 7, 1, 2, 2, -1, 10, 1, 3, 1, 11, 10, 1, 7, -1, -1, 12, 10, 1, 2, 8, 1, 2, 1, 1, 2, 10, 8, -1, 7, 3, 10, 2, 1, 9, 10, 4, 8, 9, -1, 2, 1, 1, 10, 9, 3, 3, 9, 1, 8, 8, 7, 1, 2, 9, 3, 2, 8, 2, 9, 1, 8, 3, 9, -1, 1, 1, 1, -1, 1, 1, 10, 9, 1, -1, 8, 3, 8, 9, 2, 1, 3, 7, 4, -1, -1, 10, 8, 2, 9, 8, 4, 8, -1, 1, 8, 9, 4, 7, 3, 5, 10, 2, 8, 9, -1, 9, 12, 7, 2, 1, 9, 8, -1, -1, 8, -1, 12, 1, 1, 10, 10, 1, -1, 7, 10, 12, 1, 9, 3, -1, 1, 2, 8, 8, 8, 9, -1, 8, 8, 2, 2, 9, 1, 1, -1, 10, 2, 2, 8, 9, 8, 2, 1, 7, 3, 2, 1, 2, -1, 1, -1, -1, 8, 2, 9, 1, 8, 3, 1, 8, 8, 12, 1, 1, 7, 8, 1, -1, 2, 2, 12, -1, 9, 3, 1, -1, -1, 2, 8, 5, 1, 2, 2, 8, 2, 8, 8, 8, 2, 9, 8, 2, 7, 1, 8, 2, 2, 3, 7, 12, 2, 12, 1, 1, 9, 12, 1, 3, -1, 7, -1, 1, 8, 2, 1, 5, 1, 3, 9, 1, 2, 2, -1, 8, 2, 2, 5, 12, 7, 1, 10, 5, 1, 1, 1, 8, -1, 3, -1, 9, 2, 4, 9, 8, 1, 8, 2, 1, 7, 5, 1, 1, 1, 3, 1, 8, 2, 8, 1, 8, 7, 3, 1, 8, 8, 1, 2, 8, 1, 3, 3, 8, 1, 2, 2, 1, 9, -1, -1, 12, 3, 1, -1, 8, 4, 3, 1, 2, -1, 1, 2, 2, 2, 3, 4, -1, 1, 1, 10, -1, 8, 10, 1, 9, 10, 1, 12, 1, -1, 8, -1, 7, 3, -1, 8, 8, 8, 1, 9, 10, 1, 3, 6, 9, -1, 3, 2, 2, 10, 2, 1, -1, 1, -1, 1, 7, 5, 4, 1, 2, 8, 12, -1, 1, 2, 8, 8, 1, 2, -1, -1, 5, 8, 9, 8, 1, 12, 3, 1, -1, 8, 7, 9, 1, -1, 8, 2, 9, 8, 8, 8, -1, -1, 1, 8, 1, -1, 2, 8, 8, 7, 1, 4, 1, 2, 9, 5, 6, -1, 1, 3, 3, 2, -1, -1, 8, 8, 1, -1, -1, 9, 2, 1, -1, 8, 9, 8, 9, 1, -1, 1, 5, 2, 1, 3, 1, 2, 7, 10, 8, 2, 8, 1, 1, 4, 1, 1, 9, 7, 3, 12, 10, 1, 1, 2, 8, -1, 3, 2, 5, 8, 1, -1, 9, 1, 1, 7, 1, -1, 9, 8, -1, 8, 8, 1, 8, -1, -1, -1, 6, 1, 9, -1, 1, 8, 9, 2, -1, 8, 8, 2, 1, 8, 9, 8, 10, 5, 4, 7, 2, 8, 9, 8, 12, 8, 9, -1, 1, -1, 1, 3, 8, 1, 8, 1, 9, 8, 10, 9, 8, 1, -1, 2, 2, 5, 8, 12, -1, 2, 7, 9, 8, 1, 10, -1, 8, 1, 8, -1, 1, 3, 1, 3, 1, 8, 1, 8, 2, 3, 8, -1, 9, 1, 8, 10, 9, 9, 2, 8, 9, 9, 3, 2, 2, 8, 8, -1, 1, 1, -1, 3, 12, 8, 2, -1, -1, 1, 1, 3, 1, 8, 1, 8, 1, 1, 4, 1, 8, 3, 3, 8, 10, 2, 3, 10, 2, 8, 3, 12, 1, 1, 12, 12, 1, 7, 8, 11, 1, 3, 4, 3, 1, 8, 2, 4, -1, 2, 8, 2, 8, 4, 2, 8, -1, 6, 10, 2, 9, 1, 7, -1, 9, 5, 8, -1, 1, 2, 8, 7, 3, 1, 2, 1, 8, 8, 1, 1, -1, 8, 8, 3, 8, 1, 1, 1, 8, 8, 2, 8, 8, 9, 8, -1, 7, 1, 2, 1, 8, 9, 10, 8, 2, 4, 10, 1, 1, 2, 1, 1, 8, 1, -1, 8, 10, 10, 8, 8, 2, 9, 1, 8, -1, 8, 7, 8, 1, -1, 1, -1, -1, 1, -1, 8, 2, -1, -1, 9, 1, -1, 2, -1, 1, 1, 3, 3, 1, 8, 7, 2, 8, 12, 1, 8, 8, 2, 1, -1, 1, 8, -1, 2, 12, 1, 3, -1, 9, 1, 8, -1, 1, 1, 2, 10, 1, 9, 1, 10, -1, 9, 2, 1, 1, 2, 2, 9, -1, 2, 4, 1, 1, -1, 8, -1, 1, 7, 9, 8, 1, 2, -1, 8, 9, 8, -1, 7, 12, 12, 8, 2, 1]\n",
            "[1, 9, -1, 1, 2, 8, 3, 6, 2, 1, 8, 4, 1, -1, 1, -1, 2, -1, 1, 2, 8, -1, -1, 4, 2, 10, 2, 8, -1, 8, 8, 1, 8, 1, -1, 1, 1, 2, 2, 3, 2, 1, 8, 2, 2, 8, 1, 3, -1, 8, 3, 12, 1, 7, 2, 9, 9, -1, 9, 9, -1, 2, 6, 2, 1, 1, 2, -1, 10, 1, 3, 8, 7, 1, 2, -1, -1, 4, 1, 3, 1, 8, 10, 1, 7, -1, -1, 12, 10, -1, 2, 8, -1, 2, 1, 1, 2, 10, 8, -1, 7, 3, -1, 2, 1, 9, 10, 4, 8, 9, -1, 2, 1, 1, 10, 9, -1, 3, 9, 1, 8, 8, 7, -1, 2, 9, 3, 2, 8, 2, 9, 1, 8, 1, 9, -1, 1, 1, 1, -1, 1, 1, 10, 9, -1, -1, 8, 3, 8, 9, 2, 1, -1, 7, 4, -1, -1, 8, -1, 2, 4, 8, 4, 4, -1, 1, 8, 9, 4, 7, 3, 5, -1, 2, -1, 9, -1, 9, -1, 7, 2, 1, 9, 4, -1, -1, 8, -1, 12, -1, 1, 8, 10, -1, -1, 7, 10, 12, 1, 9, 3, -1, 1, 2, 8, 8, 8, 9, -1, 8, 8, 2, 2, 9, 1, 1, -1, 10, 2, 2, 8, 9, 8, 2, 1, 7, 3, 2, 4, 2, -1, 1, -1, -1, 8, 2, 9, 1, 7, 3, 1, 8, 8, -1, 1, 1, 7, 8, 1, -1, 2, 2, 12, -1, 9, 2, 1, -1, -1, 2, 8, 5, 1, 3, 9, 8, 2, 10, 8, 8, 2, 2, 8, 2, 7, -1, 8, 2, -1, 3, 7, 12, 2, 8, 1, 1, 9, 12, -1, 3, -1, 7, -1, 1, 8, 2, 1, 5, 1, 3, 2, 1, 2, 1, -1, 8, 2, 1, 8, 12, 10, 1, 10, 5, 1, 1, 1, 8, -1, -1, -1, 9, 4, 4, 1, 8, 1, 8, 2, 1, 7, 5, 1, 1, 1, 3, 9, 8, 2, 8, -1, 8, -1, -1, 1, 8, 8, 1, 2, 8, 1, 3, 3, 8, 1, 2, 1, 1, 9, -1, -1, 12, 3, 1, -1, 8, 4, -1, 1, 2, -1, 1, 2, 2, 2, 3, 4, -1, 1, 1, 10, -1, 8, -1, -1, 9, 10, 9, 12, 1, -1, 8, -1, 7, 9, -1, 8, 8, 8, 1, 8, 10, 1, 9, 6, 2, -1, -1, 2, 2, 10, 2, 1, -1, 1, -1, 1, 7, 5, 1, 8, 1, 8, 12, -1, 1, -1, 8, 8, 1, 2, -1, -1, 1, 8, 9, 10, 1, 12, 3, 1, -1, 8, 7, 9, 1, -1, 8, 2, 10, 8, 8, 8, -1, -1, 1, 8, -1, -1, 6, 8, 8, 7, 1, 4, 1, 2, -1, -1, 6, -1, 1, 3, 9, 2, -1, -1, 8, 8, 1, -1, -1, 9, 8, 1, -1, 8, 4, -1, 9, 1, -1, 1, 5, -1, 1, 3, 1, 2, 2, 10, -1, 1, 4, -1, 1, 4, 1, 1, 9, 10, 3, 12, 10, 1, 1, 2, 8, -1, 3, 2, 5, -1, 1, -1, 9, 1, -1, 10, 1, -1, 9, 8, -1, 8, 8, 1, 8, -1, -1, -1, 6, 1, -1, -1, 1, 8, 9, 2, -1, 8, 8, 2, 1, 8, 9, 8, 10, 5, 4, 7, 2, 8, 9, 8, 12, 8, 10, -1, 1, -1, 1, 3, 8, 1, -1, 1, 1, 8, 10, 9, 8, 1, -1, 2, 2, 1, 8, 12, -1, 2, 7, 2, 10, 1, -1, -1, 8, 1, 8, -1, 1, 1, 1, 3, 1, 8, 1, 8, 2, 3, -1, -1, 10, 1, 8, 8, 9, 9, 2, 8, 9, 8, 3, 4, -1, 8, 4, -1, 9, 1, -1, 3, 8, 8, -1, -1, -1, 1, 1, -1, 1, 8, 1, 8, 1, 1, 9, 1, 8, 3, -1, 8, 10, 2, 3, 10, 1, 8, 3, 12, 1, 1, 12, -1, 1, 7, 8, 11, 1, 3, 4, 2, 1, 8, 2, 4, -1, -1, 8, 4, 8, 4, -1, 8, -1, 6, 10, 2, 9, 1, 8, -1, 9, 5, 8, -1, 1, 2, 8, 7, 9, 1, 2, 1, 8, 8, 1, 1, -1, 8, 4, 3, 8, 1, 1, 1, 9, 8, 2, 8, 8, 8, 8, -1, 7, -1, 2, 1, 8, 9, 8, 8, 2, -1, 2, -1, 1, 2, 1, 1, -1, 1, -1, 8, 10, 10, 8, 8, 2, -1, 1, 8, -1, 8, 7, 8, 1, -1, -1, -1, -1, 1, -1, 8, 1, -1, -1, 10, -1, -1, 2, -1, 9, 1, 3, 2, 1, -1, 7, 2, 8, 8, 1, 8, 8, 2, 1, -1, 1, 8, -1, 2, -1, 1, 3, -1, 9, 1, 8, -1, 1, 1, -1, 10, 1, -1, 1, 10, -1, 10, 2, 1, 1, 2, 2, 9, -1, 2, 4, 1, 1, -1, 9, -1, 1, 10, 9, 8, 1, 9, -1, 8, 9, 8, -1, 7, 12, 12, 8, 2, 1]\n",
            "[1, 4, -1, 1, 2, 8, 3, 6, 2, 1, -1, 4, 1, -1, 1, -1, 2, -1, 9, 2, 8, -1, -1, 4, 2, 10, 2, 8, -1, 8, 8, 1, 8, -1, -1, 4, 1, 2, 2, 3, -1, 1, 10, 2, 10, 8, -1, 3, -1, 8, 3, 12, 1, 7, 2, 8, 9, -1, 9, 9, -1, 3, 6, 2, 1, -1, 2, -1, 10, 1, 3, -1, 7, 1, 2, -1, -1, 10, 1, 3, 1, -1, 10, -1, 7, -1, -1, -1, 10, -1, 2, 8, -1, -1, -1, 1, 2, 10, 8, -1, -1, 3, -1, 2, 1, -1, 10, 4, 9, 1, -1, 2, 1, 1, 10, 9, -1, 8, 2, -1, 8, -1, 7, -1, 2, 9, 3, 2, 8, 2, 9, 1, 8, -1, 9, -1, 4, 1, 1, -1, 9, 1, 10, 9, -1, -1, 8, 3, -1, 9, 2, 1, -1, 7, 2, -1, -1, -1, -1, 2, 4, 9, 4, 4, -1, -1, 8, 9, 4, 7, 3, 5, -1, 2, -1, -1, -1, 9, -1, -1, 2, -1, 9, 4, -1, -1, 8, -1, 12, -1, -1, 10, 10, -1, -1, 9, -1, 12, 1, 9, 3, -1, 1, -1, 8, 8, 8, 9, -1, 8, -1, 2, 2, 9, 1, 1, -1, 10, 2, 2, 8, 13, 4, 2, 1, 7, 3, 2, 1, 2, -1, 1, -1, -1, 8, -1, 9, -1, 10, 3, 1, 8, -1, -1, 1, 1, 7, 8, 1, -1, 9, 2, 12, -1, 9, 3, 1, -1, -1, 2, 8, 5, 1, 3, 9, 2, 2, 10, -1, 8, 2, 2, 8, -1, 7, -1, 8, 2, -1, 3, 7, 12, 2, 12, 1, 1, 9, -1, -1, 3, -1, 7, -1, 1, 8, 2, 1, 5, 1, 3, 2, 1, 2, 13, -1, 8, 2, 2, 5, 12, 10, 1, 10, 5, 1, 1, 1, 8, -1, -1, -1, 9, 2, 1, 9, -1, 1, 10, 2, 1, -1, 5, 1, 1, 1, 3, -1, 8, 2, 8, -1, 8, -1, -1, -1, 8, 6, -1, 2, 8, 1, 3, 8, 9, 1, -1, 1, 1, 9, -1, -1, 12, 3, 1, -1, 8, 4, -1, -1, 2, -1, 1, -1, 2, 2, 3, 4, -1, 1, 1, 10, -1, -1, -1, -1, 9, 10, -1, 12, 1, -1, -1, -1, 7, 9, -1, 8, 8, 8, 9, 9, 10, 1, 3, 6, 2, -1, -1, 2, 2, 10, 3, 1, -1, 1, -1, 1, 7, 5, -1, 1, 1, 10, 12, -1, 1, -1, 8, 8, 1, 2, -1, -1, 5, 8, 9, 10, 4, 12, 3, 1, -1, 4, 7, 9, 1, -1, -1, 2, 9, 8, 4, 8, -1, -1, 1, 8, -1, -1, 2, -1, 8, 7, 1, 4, 1, 2, -1, -1, 6, -1, 1, 3, 3, 4, -1, -1, 8, -1, 1, -1, -1, 13, 8, 1, -1, 8, 4, -1, -1, 1, -1, 1, 5, -1, 1, 3, 1, 2, 2, 10, -1, 1, -1, -1, 1, -1, 1, 1, 9, 10, 3, 12, 10, 1, 1, 2, 8, -1, 3, 2, 5, -1, 1, -1, 9, 1, -1, 7, 1, -1, 9, -1, -1, 8, 8, 1, 8, -1, -1, -1, 6, 1, -1, -1, 1, 8, 9, 2, -1, 8, -1, 2, 1, 8, 9, 8, 9, 5, 4, 7, 2, 9, 9, 8, -1, 8, 10, -1, 1, -1, 1, 3, 8, 1, -1, 9, -1, 8, 11, 9, 8, 1, -1, 2, -1, 1, 8, 12, -1, 2, 7, 2, 10, 1, -1, -1, 8, 1, 8, -1, 1, -1, 1, 8, 1, 4, 1, 8, 2, 3, -1, -1, 10, 1, 10, 10, 9, 2, 2, 8, 9, 8, 3, 2, -1, 8, 10, -1, 9, 1, -1, -1, 8, 8, -1, -1, -1, 1, 1, -1, 1, -1, 2, 9, -1, 1, 11, 1, 8, 3, -1, 8, 10, 2, 3, 10, 2, 8, 3, 12, 1, 1, 12, -1, 1, 7, 8, 11, 1, 3, 4, 2, 1, 8, 2, -1, -1, -1, 8, 2, -1, 4, -1, 8, -1, 6, 8, 2, 1, 1, 8, -1, 9, 5, 8, -1, 1, 2, 8, -1, 3, 1, -1, 1, 8, 8, 1, 1, -1, 8, 4, 3, 8, 1, 1, 1, 8, 8, 2, 8, 8, 8, 8, -1, 7, -1, 2, 1, -1, 9, 10, 8, 2, -1, 10, -1, 1, 2, 1, 1, -1, 1, -1, 8, 10, 10, 8, 9, 2, -1, 1, 8, -1, 2, 7, 8, -1, -1, -1, -1, -1, 1, -1, 8, 1, -1, -1, 10, -1, -1, 2, -1, 9, 1, 8, 2, 1, -1, 7, -1, -1, 8, -1, -1, 8, 2, -1, -1, 1, 8, -1, 2, -1, 1, 3, -1, 9, -1, 8, -1, 1, 1, -1, 10, 3, -1, 1, 10, -1, 10, 11, 1, 1, 2, 2, -1, -1, 3, 4, 1, 1, -1, 9, -1, 1, 10, 9, 7, 1, 9, -1, 8, 9, 8, -1, 7, 12, 12, 8, -1, 1]\n",
            "[1, 4, -1, 1, 2, 9, 3, 6, -1, 1, -1, 4, 1, -1, 1, -1, 2, -1, 1, 2, -1, -1, -1, 4, 2, 8, 2, 8, -1, 8, 8, 1, 8, -1, -1, 4, 1, 2, 2, 3, -1, 1, 10, -1, 10, 8, -1, 3, -1, -1, 3, 12, 1, 7, 2, -1, -1, -1, 9, 9, -1, 2, 6, 2, 1, -1, 2, -1, -1, -1, 3, -1, -1, 1, 2, -1, -1, -1, 1, 3, 1, -1, 10, -1, 7, -1, -1, -1, -1, -1, 2, 8, -1, -1, -1, 1, -1, 10, 8, -1, -1, 3, -1, 2, 1, -1, -1, 4, 8, 9, -1, 2, 1, 1, 8, 9, -1, 3, 2, -1, 8, -1, -1, -1, 2, 9, 3, 2, 8, 2, 9, 1, 8, -1, 9, -1, 1, 1, 1, -1, -1, 1, 10, 9, -1, -1, 8, 3, -1, 9, 2, 1, -1, -1, -1, -1, -1, -1, -1, 2, -1, 8, 4, -1, -1, -1, 8, 9, 4, 7, 3, 5, -1, 2, -1, -1, -1, 9, -1, -1, 3, -1, 9, 4, -1, -1, 8, -1, -1, -1, -1, 8, 8, -1, -1, -1, -1, 12, 1, 9, 3, -1, 1, -1, -1, -1, 8, 9, -1, 8, -1, 2, 2, 9, 1, 1, -1, 10, 1, 2, 8, 13, 4, 2, 1, 7, 3, -1, -1, 2, -1, 1, -1, -1, 8, -1, 9, -1, 2, 3, 1, 8, -1, -1, 1, 1, 7, 8, 1, -1, 9, 2, 3, -1, 9, 3, -1, -1, -1, 2, 8, 5, 1, 3, 2, -1, 2, 8, -1, 8, -1, 2, 8, -1, 7, -1, 8, 2, -1, -1, 7, 1, 2, 12, 1, -1, 9, -1, -1, 3, -1, 7, -1, 1, 8, 2, 4, 5, 1, 3, -1, 1, 2, 1, -1, 8, -1, 3, 8, 12, 10, 1, -1, 5, 1, 1, -1, 8, -1, -1, -1, 2, 2, -1, 1, -1, -1, 8, 2, -1, -1, 5, 1, 1, -1, -1, -1, 8, 2, 8, -1, 8, -1, -1, -1, 8, 6, -1, 2, 8, 1, 3, 3, 9, 1, -1, 1, 1, 9, -1, -1, 12, 3, 4, -1, 8, -1, -1, -1, 2, -1, 1, -1, 2, 2, 3, 6, -1, 1, 1, 10, -1, -1, -1, -1, 9, 8, -1, 12, 1, -1, -1, -1, 7, 9, -1, 8, 8, 8, 1, 8, 10, 1, 3, 6, 2, -1, -1, 2, 2, 10, 2, 1, -1, 1, -1, 1, 7, 5, -1, 8, 1, 10, 12, -1, 1, -1, 8, 8, 1, 2, -1, -1, 5, 8, 9, 8, 4, 12, -1, 2, -1, 8, 7, 9, 1, -1, -1, -1, 7, 8, 8, 8, -1, -1, 1, -1, -1, -1, 6, -1, 8, 7, 1, 4, -1, 2, -1, -1, 6, -1, -1, 3, 3, 4, -1, -1, 8, -1, 1, -1, -1, 13, 2, 1, -1, 8, 4, -1, -1, 1, -1, 4, 5, -1, 1, 3, 1, 2, -1, 10, -1, 1, -1, -1, 1, -1, 1, -1, 9, 10, 3, 12, 10, 1, 1, 2, -1, -1, 3, 2, 5, -1, 1, -1, 9, 1, -1, 8, 1, -1, 9, -1, -1, 8, 8, -1, -1, -1, -1, -1, 6, 1, -1, -1, 1, 8, 9, 2, -1, 8, -1, 2, -1, 8, 2, 8, 9, 5, 4, 7, 2, 8, 9, 8, -1, -1, 8, -1, 1, -1, 1, 3, 8, 1, -1, 1, -1, 8, 9, 9, 1, 1, -1, 2, -1, 1, 8, 12, -1, 4, -1, 9, 10, 1, -1, -1, 8, 1, 8, -1, -1, -1, 1, 3, 1, 1, 1, 8, 2, 3, -1, -1, -1, -1, 8, 10, 9, 9, 2, 8, -1, 8, 3, 2, -1, 8, 4, -1, 9, -1, -1, -1, 8, 8, -1, -1, -1, 1, 1, -1, -1, -1, 2, 8, -1, 1, -1, 1, 8, 3, -1, 8, 10, 2, 3, 10, -1, 8, 3, 12, 1, 1, -1, -1, 1, 7, 8, 11, 1, 3, 1, 3, 1, 8, 7, -1, -1, -1, 8, 4, -1, 4, -1, 8, -1, 6, -1, 2, -1, 1, 8, -1, -1, 5, 8, -1, 1, 2, 8, -1, 10, 1, -1, 1, -1, 8, 1, 1, -1, 8, 4, 3, 8, 1, 1, 1, 8, 8, 2, 8, 8, 9, 8, -1, -1, -1, 2, 1, -1, 9, 8, 8, -1, -1, 9, -1, 1, 2, 1, 1, -1, 1, -1, 8, 10, 10, 8, 8, 2, -1, 1, 8, -1, 8, -1, 8, -1, -1, -1, -1, -1, 1, -1, -1, 1, -1, -1, 9, -1, -1, 2, -1, 1, -1, 3, 1, -1, -1, 7, -1, -1, 8, -1, -1, 8, 2, -1, -1, 1, -1, -1, 2, -1, 1, 3, -1, -1, -1, 8, -1, 1, 1, -1, 10, 1, -1, -1, 8, -1, 10, 4, 1, 1, -1, 2, -1, -1, 2, 4, 1, 1, -1, 9, -1, 1, 9, 9, 8, 1, 2, -1, 8, 9, 8, -1, 7, 12, 12, -1, -1, -1]\n",
            "[1, 9, -1, 1, 2, 10, 3, 6, -1, 1, -1, 4, 1, -1, 10, -1, 2, -1, 1, 2, -1, -1, -1, 4, 2, 8, 2, 8, -1, 8, 12, 1, 8, -1, -1, -1, -1, 2, 2, 3, -1, 1, 10, -1, -1, -1, -1, 3, -1, -1, 3, 12, 1, 7, 2, -1, -1, -1, 9, 9, -1, 2, -1, 2, 1, -1, 2, -1, -1, -1, 3, -1, -1, 1, 2, -1, -1, -1, -1, 3, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 2, 8, -1, -1, -1, 1, -1, 10, 8, -1, -1, -1, -1, 9, 1, -1, -1, 4, 8, -1, -1, 2, 1, 1, 8, 9, -1, 3, -1, -1, 8, -1, -1, -1, 2, 9, 3, -1, 8, 9, 9, 1, -1, -1, 2, -1, 1, 1, 1, -1, -1, -1, 10, 9, -1, -1, 8, 3, -1, 9, 2, 1, -1, -1, -1, -1, -1, -1, -1, 2, -1, 8, -1, -1, -1, -1, 8, 9, 4, 7, 3, 5, -1, 2, -1, -1, -1, 9, -1, -1, 2, -1, 8, -1, -1, -1, 8, -1, -1, -1, -1, 8, 8, -1, -1, -1, -1, -1, 1, -1, 3, -1, 1, -1, -1, -1, -1, 9, -1, 8, -1, -1, 2, 9, 1, 1, -1, 8, -1, 2, 8, 9, 4, 2, -1, 7, 2, -1, -1, 2, -1, 1, -1, -1, 8, -1, -1, -1, -1, 3, 1, 8, -1, -1, 1, -1, 7, 8, 1, -1, 9, 2, 12, -1, -1, 3, -1, -1, -1, 2, 8, 5, 1, 3, 2, -1, 2, 8, -1, -1, -1, 9, 8, -1, -1, -1, 8, 2, -1, -1, -1, 12, 2, 8, 1, -1, 9, -1, -1, 3, -1, 7, -1, -1, 8, 2, 1, 5, -1, -1, -1, 1, 2, 1, -1, 8, -1, 2, 8, -1, -1, 1, -1, 5, 1, 1, -1, 8, -1, -1, -1, 9, 2, -1, 1, -1, -1, 8, 2, -1, -1, 5, 1, 1, -1, -1, -1, 4, 2, 8, -1, 8, -1, -1, -1, -1, 9, -1, -1, -1, -1, 3, 3, 8, 1, -1, 1, 1, 9, -1, -1, 12, 3, -1, -1, 8, -1, -1, -1, 2, -1, 1, -1, -1, -1, 3, 9, -1, 1, 1, 10, -1, -1, -1, -1, 9, 10, -1, 12, 1, -1, -1, -1, 7, -1, -1, 8, 8, -1, 1, 8, 2, -1, 3, 6, 2, -1, -1, 2, 2, 10, 2, 1, -1, 1, -1, 1, -1, 5, -1, 8, 1, 10, 12, -1, -1, -1, 8, 8, -1, 2, -1, -1, 5, -1, 9, 8, 1, 12, -1, 1, -1, 8, -1, -1, -1, -1, -1, -1, 7, 8, 8, 8, -1, -1, 1, -1, -1, -1, 6, -1, -1, 7, -1, 4, -1, 2, -1, -1, 6, -1, -1, 3, 4, 1, -1, -1, 8, -1, 1, -1, -1, 9, 2, 1, -1, -1, 8, -1, -1, 1, -1, 1, 5, -1, 1, -1, 1, 2, -1, 10, -1, 1, -1, -1, 1, -1, 1, -1, 9, 10, 3, 12, 10, 1, 1, 2, -1, -1, 3, 2, 5, -1, 1, -1, 9, 1, -1, 7, -1, -1, 9, -1, -1, 8, 8, -1, -1, -1, -1, -1, 6, 1, -1, -1, 1, 8, 9, 2, -1, 4, -1, 2, -1, -1, -1, 8, 2, 5, 4, 7, 2, 8, -1, 8, -1, -1, 9, -1, 1, -1, 1, 3, 8, 1, -1, 1, -1, 8, 8, 9, 1, 1, -1, 2, -1, 1, 8, 12, -1, 2, -1, 2, 10, 1, -1, -1, 8, 1, 8, -1, -1, -1, 1, 3, 1, 8, 1, 8, 2, 5, -1, -1, -1, -1, 8, -1, -1, -1, 2, 8, -1, 9, 3, -1, -1, 8, 4, -1, 9, -1, -1, -1, 8, 8, -1, -1, -1, 1, 1, -1, -1, -1, 2, 8, -1, 1, -1, 1, 8, 3, -1, 8, 10, 2, 3, 2, -1, 8, 3, 12, 1, 1, -1, -1, 1, 7, 8, 10, 1, 3, -1, 3, 1, 8, 2, -1, -1, -1, -1, -1, -1, 2, -1, 8, -1, 6, -1, 2, -1, 1, -1, -1, -1, 5, 8, -1, -1, 2, 8, -1, 2, 1, -1, 1, -1, 8, 1, 1, -1, 8, 4, 3, 8, 1, 1, 1, 9, 8, 2, 8, -1, 9, 8, -1, -1, -1, 2, 1, -1, 9, -1, 8, -1, -1, 9, -1, 1, 2, 1, 1, -1, 1, -1, 8, 10, -1, 8, 8, 2, -1, -1, 8, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, 1, -1, -1, 9, -1, -1, 2, -1, 1, -1, 3, 2, -1, -1, 7, -1, -1, -1, -1, -1, 8, 2, -1, -1, 1, -1, -1, 2, -1, 1, 3, -1, -1, -1, 8, -1, 1, 1, -1, 10, 3, -1, -1, 10, -1, 10, 9, 1, 1, -1, 2, -1, -1, 2, 9, -1, -1, -1, 9, -1, 1, 9, 9, 8, 1, 9, -1, 8, 9, 8, -1, 7, -1, 12, -1, -1, -1]\n",
            "0.7624703087885986\n",
            "[1, 1, 3, 1, 2, 8, 3, 6, 2, 1, 8, 4, 1, 4, 1, 7, 2, 4, 1, 2, 8, 9, 9, 4, 2, 8, 2, 8, 1, 8, 12, 1, 8, 1, 8, 1, 1, 2, 2, 3, 2, 1, 1, 2, 2, 8, 1, 3, 4, 8, 3, 12, 1, 7, 2, 8, 9, 1, 9, 9, 3, 3, 6, 2, 1, 1, 8, 1, 10, 1, 3, 8, 7, 1, 2, 2, 9, 8, 1, 3, 1, 11, 10, 1, 7, 9, 9, 12, 8, 1, 2, 8, 1, 2, 1, 1, 2, 10, 8, 2, 7, 3, 11, 2, 1, 9, 10, 4, 8, 9, 6, 2, 1, 1, 10, 9, 3, 3, 9, 1, 8, 8, 7, 1, 2, 1, 3, 2, 8, 2, 9, 1, 8, 1, 9, 8, 4, 1, 1, 2, 1, 1, 8, 8, 1, 5, 8, 3, 8, 9, 1, 1, 3, 7, 4, 0, 3, 8, 8, 2, 9, 8, 4, 4, 12, 1, 8, 9, 4, 7, 3, 5, 10, 2, 8, 8, 3, 9, 12, 7, 3, 1, 8, 8, 9, 8, 8, 9, 12, 1, 1, 8, 10, 1, 2, 7, 10, 12, 1, 9, 3, 9, 1, 3, 8, 2, 8, 9, 2, 8, 8, 2, 2, 9, 1, 1, 2, 10, 2, 2, 8, 3, 8, 2, 1, 7, 3, 2, 1, 2, 2, 1, 9, 1, 8, 2, 8, 1, 2, 3, 1, 8, 8, 12, 1, 1, 7, 8, 1, 2, 2, 2, 12, 1, 9, 3, 1, 1, 9, 2, 8, 1, 1, 3, 6, 2, 2, 8, 8, 8, 2, 2, 8, 2, 7, 1, 8, 2, 2, 3, 7, 12, 2, 8, 1, 1, 9, 12, 1, 3, 2, 7, 3, 1, 8, 2, 1, 5, 1, 3, 2, 1, 2, 1, 8, 8, 2, 3, 5, 12, 7, 1, 8, 5, 1, 1, 1, 8, 9, 3, 9, 4, 4, 1, 1, 9, 1, 8, 2, 1, 3, 5, 1, 1, 1, 3, 1, 8, 2, 8, 1, 8, 7, 1, 1, 8, 8, 1, 2, 8, 1, 3, 8, 8, 1, 2, 1, 1, 9, 8, 1, 12, 3, 1, 1, 8, 4, 3, 1, 2, 9, 1, 2, 2, 2, 3, 6, 9, 1, 1, 10, 12, 8, 8, 1, 9, 10, 9, 12, 1, 9, 8, 8, 7, 9, 8, 8, 8, 8, 1, 8, 10, 1, 3, 6, 2, 3, 2, 2, 2, 10, 2, 1, 3, 1, 3, 1, 7, 5, 4, 1, 2, 8, 12, 1, 1, 2, 8, 8, 1, 2, 3, 9, 1, 8, 8, 8, 1, 12, 3, 1, 2, 4, 7, 9, 1, 2, 8, 2, 8, 3, 8, 8, 4, 8, 1, 8, 1, 1, 6, 8, 8, 7, 1, 4, 1, 2, 9, 5, 6, 12, 1, 3, 3, 4, 3, 1, 8, 1, 1, 9, 3, 3, 8, 1, 9, 8, 4, 8, 10, 1, 3, 1, 5, 2, 1, 3, 1, 2, 2, 10, 8, 1, 4, 1, 1, 4, 1, 1, 9, 10, 3, 12, 8, 1, 1, 2, 8, 2, 3, 2, 5, 2, 1, 3, 9, 1, 1, 7, 1, 1, 8, 8, 2, 8, 8, 1, 8, 1, 8, 8, 6, 1, 9, 1, 1, 8, 8, 2, 10, 8, 8, 2, 1, 8, 9, 8, 2, 5, 4, 7, 2, 8, 9, 8, 12, 8, 7, 9, 1, 3, 1, 3, 8, 1, 10, 1, 1, 8, 8, 9, 8, 1, 1, 2, 2, 1, 8, 12, 1, 2, 7, 9, 1, 1, 10, 2, 8, 1, 8, 11, 1, 3, 1, 3, 1, 8, 1, 8, 2, 3, 9, 10, 9, 1, 8, 10, 9, 9, 2, 8, 9, 3, 3, 4, 2, 8, 8, 2, 9, 1, 2, 3, 8, 8, 2, 8, 2, 1, 1, 3, 1, 8, 1, 8, 1, 1, 8, 1, 8, 3, 3, 8, 10, 2, 3, 10, 2, 8, 3, 12, 1, 1, 12, 12, 1, 7, 8, 11, 1, 3, 4, 3, 1, 8, 2, 4, 8, 2, 8, 4, 8, 4, 2, 8, 8, 6, 8, 2, 1, 1, 8, 8, 9, 5, 8, 1, 1, 2, 8, 7, 3, 1, 2, 1, 8, 8, 1, 1, 2, 8, 4, 3, 8, 1, 1, 1, 3, 8, 2, 8, 8, 9, 8, 4, 7, 1, 2, 1, 8, 9, 8, 8, 2, 4, 2, 1, 1, 2, 1, 1, 8, 1, 2, 8, 10, 10, 8, 8, 2, 9, 1, 8, 2, 8, 7, 8, 1, 8, 1, 1, 3, 1, 1, 8, 1, 8, 2, 8, 1, 1, 2, 3, 1, 1, 3, 1, 1, 8, 7, 2, 8, 12, 1, 12, 8, 2, 1, 7, 1, 8, 2, 2, 12, 1, 3, 8, 8, 1, 8, 8, 1, 1, 2, 10, 1, 9, 1, 8, 12, 8, 2, 1, 1, 2, 2, 9, 10, 2, 4, 1, 1, 9, 8, 2, 1, 10, 9, 8, 1, 2, 12, 8, 4, 8, 1, 7, 12, 12, 8, 2, 1]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "final_pred_0=[]\n",
        "\n",
        "print(new_val_fnum_0)\n",
        "print(new_val_fnum_1)\n",
        "print(new_val_fnum_2)\n",
        "print(new_val_fnum_3)\n",
        "print(new_val_fnum_4)\n",
        "print(new_val_fnum_5)\n",
        "\n",
        "num_correct=0\n",
        "model_0_0=load_model(loss,accuracy,optimizer,'0-0')\n",
        "mod_labels_0=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_0:\n",
        "    pred_test_0_0=model_0_0.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_0_0 = pred_test_0_0.argmax(axis=1)\n",
        "    mod_labels_0.append(pred_labels_0_0[0])\n",
        "  else:\n",
        "    mod_labels_0.append(-1)\n",
        "\n",
        "model_1_0=load_model(loss,accuracy,optimizer,'1-0')\n",
        "mod_labels_1=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        " \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_1:\n",
        "    pred_test_1_0=model_1_0.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_1_0 = pred_test_1_0.argmax(axis=1)\n",
        "    mod_labels_1.append(pred_labels_1_0[0])\n",
        "  else:\n",
        "    mod_labels_1.append(-1)\n",
        "\n",
        "model_2_0=load_model(loss,accuracy,optimizer,'2-0')\n",
        "mod_labels_2=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_2:\n",
        "    pred_test_2_0=model_2_0.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_2_0 = pred_test_2_0.argmax(axis=1)\n",
        "    mod_labels_2.append(pred_labels_2_0[0])\n",
        "  else:\n",
        "    mod_labels_2.append(-1)\n",
        "\n",
        "model_3_0=load_model(loss,accuracy,optimizer,'3-0')\n",
        "mod_labels_3=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_3:\n",
        "    pred_test_3_0=model_3_0.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_3_0 = pred_test_3_0.argmax(axis=1)\n",
        "    mod_labels_3.append(pred_labels_3_0[0])\n",
        "  else:\n",
        "    mod_labels_3.append(-1)\n",
        "\n",
        "model_4_0=load_model(loss,accuracy,optimizer,'4-0')\n",
        "mod_labels_4=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_4:\n",
        "    pred_test_4_0=model_4_0.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_4_0 = pred_test_4_0.argmax(axis=1)\n",
        "    mod_labels_4.append(pred_labels_4_0[0])\n",
        "  else:\n",
        "    mod_labels_4.append(-1)\n",
        "\n",
        "model_5_0=load_model(loss,accuracy,optimizer,'5-0')\n",
        "mod_labels_5=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_5:\n",
        "    pred_test_5_0=model_5_0.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_5_0 = pred_test_5_0.argmax(axis=1)\n",
        "    mod_labels_5.append(pred_labels_5_0[0])\n",
        "  else:\n",
        "    mod_labels_5.append(-1)\n",
        "\n",
        "print(mod_labels_0)\n",
        "print(mod_labels_1)\n",
        "print(mod_labels_2)\n",
        "print(mod_labels_3)\n",
        "print(mod_labels_4)\n",
        "print(mod_labels_5)\n",
        "\n",
        "\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  fin_labels=[]\n",
        "  if mod_labels_0[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  if mod_labels_1[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  if mod_labels_2[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  if mod_labels_3[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  if mod_labels_4[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  if mod_labels_5[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  res=max(set(fin_labels), key = fin_labels.count)\n",
        "  final_pred_0.append(res)\n",
        "\n",
        "  if res==new_val_label_0[i]:\n",
        "    num_correct=num_correct+1\n",
        "\n",
        "accuracy=num_correct/len(new_val_label_0)\n",
        "print(accuracy)\n",
        "  \n",
        "print(final_pred_0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSrkJ8k6oBkG",
        "outputId": "77fe3e55-e852-4045-bbc3-aed4df2a4077"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.7624703087885986\n",
            "Weighted F1: 0.7540823069521467\n",
            "Micro F1: 0.7624703087885987\n",
            "Weighted Precision: 0.7642949996433699\n",
            "Micro Precision: 0.7624703087885986\n",
            "Weighted Recall: 0.7624703087885986\n",
            "Micro Recall: 0.7624703087885986\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "total_accuracy=0\n",
        "total_weighted_f1=0\n",
        "total_micro_f1=0\n",
        "total_weighted_precision=0\n",
        "total_micro_precision=0\n",
        "total_weighted_recall=0\n",
        "total_micro_recall=0\n",
        "\n",
        "accuracy=accuracy_score(new_val_label_0, final_pred_0)\n",
        "print(\"Accuracy: \"+str(accuracy))\n",
        "total_accuracy=total_accuracy+accuracy\n",
        "\n",
        "weighted_f1=f1_score(new_val_label_0,final_pred_0, average='weighted')\n",
        "print(\"Weighted F1: \"+ str(weighted_f1))\n",
        "total_weighted_f1=total_weighted_f1+weighted_f1\n",
        "micro_f1=f1_score(new_val_label_0,final_pred_0, average='micro')\n",
        "print(\"Micro F1: \"+ str(micro_f1))\n",
        "total_micro_f1=total_micro_f1+micro_f1\n",
        "\n",
        "weighted_precision=precision_score(new_val_label_0, final_pred_0, average='weighted')\n",
        "print(\"Weighted Precision: \" + str(weighted_precision))\n",
        "total_weighted_precision=total_weighted_precision+weighted_precision\n",
        "micro_precision=precision_score(new_val_label_0, final_pred_0, average='micro')\n",
        "print(\"Micro Precision: \" + str(micro_precision))\n",
        "total_micro_precision=total_micro_precision+micro_precision\n",
        "\n",
        "weighted_recall=recall_score(new_val_label_0, final_pred_0, average='weighted')\n",
        "print(\"Weighted Recall: \" + str(weighted_recall))\n",
        "total_weighted_recall=total_weighted_recall+weighted_recall\n",
        "micro_recall=recall_score(new_val_label_0, final_pred_0, average='micro')\n",
        "print(\"Micro Recall: \" + str(micro_recall))\n",
        "total_micro_recall=total_micro_recall+micro_recall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9SEXW7xUkmvH",
        "outputId": "6d897524-2bec-4368-954b-d25c348655a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_8\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_17 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_18 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_17[0][0]',               \n",
            " el)                            thPoolingAndCrossAt               'input_18[0][0]']               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_8 (Sl  (None, 768)         0           ['tf_roberta_model[8][0]']       \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_16 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_8[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_45 (Dropout)           (None, 512)          0           ['dense_16[0][0]']               \n",
            "                                                                                                  \n",
            " dense_17 (Dense)               (None, 15)           7695        ['dropout_45[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model: \"model_9\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_19 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_20 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_19[0][0]',               \n",
            " el)                            thPoolingAndCrossAt               'input_20[0][0]']               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_9 (Sl  (None, 768)         0           ['tf_roberta_model[9][0]']       \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_18 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_9[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_46 (Dropout)           (None, 512)          0           ['dense_18[0][0]']               \n",
            "                                                                                                  \n",
            " dense_19 (Dense)               (None, 15)           7695        ['dropout_46[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model: \"model_10\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_21 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_22 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_21[0][0]',               \n",
            " el)                            thPoolingAndCrossAt               'input_22[0][0]']               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_10 (S  (None, 768)         0           ['tf_roberta_model[10][0]']      \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " dense_20 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_10[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dropout_47 (Dropout)           (None, 512)          0           ['dense_20[0][0]']               \n",
            "                                                                                                  \n",
            " dense_21 (Dense)               (None, 15)           7695        ['dropout_47[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model: \"model_11\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_23 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_24 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_23[0][0]',               \n",
            " el)                            thPoolingAndCrossAt               'input_24[0][0]']               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_11 (S  (None, 768)         0           ['tf_roberta_model[11][0]']      \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " dense_22 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_11[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dropout_48 (Dropout)           (None, 512)          0           ['dense_22[0][0]']               \n",
            "                                                                                                  \n",
            " dense_23 (Dense)               (None, 15)           7695        ['dropout_48[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model: \"model_12\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_25 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_26 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_25[0][0]',               \n",
            " el)                            thPoolingAndCrossAt               'input_26[0][0]']               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_12 (S  (None, 768)         0           ['tf_roberta_model[12][0]']      \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " dense_24 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_12[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dropout_49 (Dropout)           (None, 512)          0           ['dense_24[0][0]']               \n",
            "                                                                                                  \n",
            " dense_25 (Dense)               (None, 15)           7695        ['dropout_49[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model: \"model_13\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_27 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_28 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_27[0][0]',               \n",
            " el)                            thPoolingAndCrossAt               'input_28[0][0]']               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_13 (S  (None, 768)         0           ['tf_roberta_model[13][0]']      \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " dense_26 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_13[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dropout_50 (Dropout)           (None, 512)          0           ['dense_26[0][0]']               \n",
            "                                                                                                  \n",
            " dense_27 (Dense)               (None, 15)           7695        ['dropout_50[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "[1, 9, 3, 1, 2, 8, 3, 6, 2, 1, 8, 4, 8, 4, 8, 7, 2, 4, 9, 2, 8, 9, 9, 4, 2, 8, 2, 8, 1, 8, 8, 1, 8, 1, 3, 1, 1, 2, 2, 3, 9, 1, 8, 2, 10, 8, 1, 3, 4, 8, 3, 12, 1, 7, 2, 8, 9, 1, 9, 9, 3, 2, 6, 2, 1, 1, 2, 1, 10, 1, 3, 8, 7, 1, 2, 2, 9, 12, 1, 3, 1, 11, 10, 1, 7, 9, 9, 12, 10, 1, 2, 8, 9, 2, 1, 1, 2, 10, 8, 2, 7, 3, 11, 2, 1, 9, 10, 4, 8, 9, 11, 2, 1, 1, 8, 9, 3, 3, 9, 1, 8, 8, 7, 1, 2, 9, 3, 2, 8, 9, 9, 1, 8, 1, 9, 8, 4, 1, 1, 2, 1, 1, 10, 9, 1, 9, 8, 3, 8, 9, 2, 1, 3, 7, 2, 0, 3, 10, 8, 2, 9, 8, 4, 4, 12, 1, 8, 9, 4, 7, 3, 9, 10, 10, 8, 9, 3, 9, 12, 7, 2, 1, 8, 8, 9, 8, 8, 9, 12, 1, 1, 10, 10, 1, 2, 7, 11, 12, 1, 9, 3, 9, 1, 9, 8, 8, 8, 9, 9, 8, 8, 2, 2, 9, 1, 1, 2, 10, 2, 2, 8, 9, 8, 2, 1, 7, 3, 2, 4, 2, 2, 1, 9, 1, 8, 2, 9, 1, 7, 3, 1, 8, 8, 12, 1, 1, 7, 8, 1, 2, 9, 2, 12, 1, 9, 2, 1, 4, 9, 2, 8, 5, 1, 2, 9, 2, 2, 10, 8, 8, 2, 9, 8, 2, 7, 1, 8, 2, 2, 3, 7, 12, 2, 12, 1, 1, 9, 12, 4, 3, 2, 7, 3, 1, 8, 2, 1, 5, 1, 3, 2, 1, 10, 10, 8, 8, 2, 2, 8, 12, 7, 1, 8, 5, 1, 1, 1, 8, 9, 3, 9, 9, 4, 4, 1, 9, 2, 8, 10, 1, 7, 5, 1, 1, 1, 3, 9, 8, 2, 8, 1, 8, 7, 3, 1, 9, 8, 4, 2, 8, 1, 3, 3, 8, 1, 2, 1, 1, 9, 8, 1, 12, 3, 1, 1, 8, 4, 3, 8, 2, 9, 1, 2, 2, 2, 3, 4, 9, 1, 1, 10, 8, 8, 10, 1, 9, 10, 9, 12, 1, 9, 8, 8, 7, 9, 8, 8, 8, 8, 1, 8, 10, 1, 7, 6, 9, 3, 2, 2, 2, 8, 3, 1, 9, 1, 9, 1, 7, 9, 3, 8, 2, 10, 12, 1, 1, 2, 8, 8, 1, 2, 3, 9, 5, 8, 8, 8, 1, 12, 3, 1, 2, 4, 7, 9, 1, 2, 8, 2, 9, 8, 8, 8, 3, 8, 1, 8, 1, 2, 6, 8, 8, 7, 1, 4, 1, 2, 9, 5, 6, 12, 1, 3, 3, 4, 3, 1, 8, 8, 1, 9, 9, 10, 8, 1, 9, 8, 9, 8, 9, 1, 3, 4, 5, 2, 1, 3, 1, 2, 7, 10, 8, 1, 8, 1, 1, 4, 1, 9, 9, 10, 3, 12, 10, 1, 1, 2, 8, 2, 3, 2, 5, 8, 1, 3, 7, 1, 1, 8, 8, 1, 9, 8, 2, 8, 8, 1, 8, 1, 8, 8, 6, 1, 7, 1, 1, 8, 9, 2, 10, 8, 8, 2, 1, 8, 9, 8, 9, 5, 4, 7, 2, 8, 9, 8, 12, 8, 7, 9, 1, 2, 1, 3, 8, 1, 8, 1, 1, 8, 8, 9, 8, 1, 1, 2, 2, 1, 8, 12, 2, 2, 7, 9, 10, 1, 10, 2, 8, 1, 8, 11, 1, 3, 1, 7, 1, 8, 1, 8, 2, 3, 8, 10, 9, 1, 8, 10, 9, 9, 2, 8, 9, 3, 3, 4, 2, 8, 8, 2, 9, 1, 1, 3, 12, 8, 9, 8, 2, 1, 1, 3, 1, 8, 1, 8, 1, 1, 9, 1, 8, 3, 3, 8, 10, 2, 3, 10, 2, 8, 3, 12, 1, 8, 12, 12, 1, 7, 8, 11, 1, 3, 4, 3, 1, 8, 7, 4, 8, 2, 8, 2, 8, 9, 2, 8, 8, 6, 8, 2, 9, 1, 7, 8, 9, 5, 8, 1, 1, 2, 8, 7, 9, 1, 2, 1, 8, 8, 1, 1, 2, 8, 4, 3, 8, 1, 1, 1, 8, 8, 2, 8, 8, 8, 8, 3, 7, 1, 2, 1, 8, 9, 8, 8, 2, 4, 10, 1, 1, 2, 1, 1, 8, 1, 2, 8, 10, 10, 8, 8, 2, 8, 1, 8, 2, 8, 7, 8, 1, 8, 1, 1, 3, 1, 1, 8, 2, 8, 2, 9, 1, 1, 2, 3, 1, 1, 8, 2, 1, 8, 7, 2, 8, 12, 8, 12, 9, 2, 1, 7, 1, 8, 2, 2, 12, 1, 3, 8, 8, 1, 8, 8, 1, 1, 2, 10, 1, 9, 1, 8, 2, 9, 2, 1, 1, 2, 2, 9, 10, 8, 4, 1, 1, 9, 9, 0, 1, 7, 9, 8, 1, 2, 12, 8, 9, 8, 1, 7, 12, 12, 8, 2, 1]\n",
            "[1, 8, -1, 1, 2, 8, 3, 6, 2, 1, 8, 4, 1, 2, 1, -1, 2, 4, 1, 2, 8, -1, -1, 4, 2, 8, 2, 8, 1, 8, 8, 1, 8, 1, -1, 1, 1, 2, 2, 3, 2, 1, 10, 2, 10, 8, 1, 3, 4, 8, 3, 12, 1, 7, 2, 9, 9, -1, 9, 7, -1, 3, 6, 2, 1, 1, 2, 1, 8, 1, 3, 8, 7, 1, 2, 2, -1, 10, 1, 3, 1, 11, 10, 1, 7, -1, -1, 12, 10, 1, 2, 8, 1, 2, 1, 1, 2, 10, 8, -1, 7, 3, 11, 2, 1, 9, 7, 4, 8, 1, -1, 2, 1, 1, 10, 8, 3, 3, 9, 1, 8, 8, 7, 1, 2, 9, 3, 2, 8, 2, 9, 1, 8, 1, 9, -1, 1, 1, 1, -1, 1, 1, 10, 9, 1, -1, 8, 3, 8, 9, 2, 1, 3, 7, 4, -1, -1, 8, 8, 2, 1, 8, 4, 4, -1, 1, 8, 9, 4, 7, 3, 5, 10, 10, 8, 9, -1, 9, 12, 7, 3, 1, 8, 8, -1, -1, 9, -1, 12, 1, 1, 10, 10, 1, -1, 7, 11, 12, 1, 9, 3, -1, 1, 2, 8, 8, 8, 9, -1, 8, 8, 2, 2, 9, 1, 1, -1, 10, 2, 2, 8, 9, 4, 2, 1, 7, 3, 2, 4, 2, -1, 1, -1, -1, 9, 2, 9, 1, 2, 3, 1, 8, 8, 12, 1, 1, 7, 8, 1, -1, 9, 2, 12, -1, 9, 3, 1, -1, -1, 2, 8, 5, 1, 3, 2, 8, 2, 8, 8, 8, 2, 2, 8, 2, 7, 1, 8, 2, 2, 4, 7, 12, 2, 12, 1, 1, 9, 12, 1, 3, -1, 7, -1, 1, 8, 2, 1, 5, 1, 3, 2, 1, 2, 1, -1, 8, 2, 2, 5, 12, 7, 1, 10, 5, 1, 1, 1, 8, -1, 3, -1, 9, 10, 1, 10, 8, 1, 8, 2, 1, 3, 5, 1, 1, 1, 3, 1, 8, 2, 8, 1, 8, 7, 1, 1, 8, 8, 1, 2, 8, 1, 3, 8, 8, 1, 2, 1, 1, 9, -1, -1, 12, 3, 1, -1, 8, 4, 3, 1, 2, -1, 1, 2, 2, 2, 3, 6, -1, 1, 1, 10, -1, 8, 10, 1, 9, 10, 9, 12, 1, -1, 8, -1, 7, 9, -1, 8, 8, 8, 1, 12, 10, 1, 3, 6, 9, -1, 2, 2, 2, 10, 2, 1, -1, 1, -1, 1, 7, 9, 1, 1, 1, 8, 12, -1, 1, 2, 8, 8, 1, 2, -1, -1, 5, 8, 9, 10, 1, 12, 3, 1, -1, 8, 8, 9, 1, -1, 8, 2, 7, 8, 8, 8, -1, -1, 1, 8, 1, -1, 6, 8, 8, 7, 1, 4, 1, 2, 9, 5, 6, -1, 1, 3, 3, 1, -1, -1, 8, 8, 1, -1, -1, 9, 8, 1, -1, 8, 4, 8, 9, 1, -1, 1, 5, 2, 1, 3, 1, 2, 7, 10, 8, 1, 4, 1, 1, 4, 1, 1, 9, 3, 3, 12, 10, 1, 1, 2, 8, -1, 3, 2, 5, 8, 1, -1, 9, 1, 1, 8, 1, -1, 9, 8, -1, 8, 8, 1, 8, -1, -1, -1, 6, 1, 7, -1, 1, 8, 9, 2, -1, 8, 8, 2, 1, 8, 9, 8, 10, 5, 4, 7, 2, 8, 9, 8, 12, 8, 7, -1, 1, -1, 1, 3, 8, 1, 8, 1, 1, 8, 10, 9, 8, 1, -1, 2, 2, 1, 8, 12, -1, 2, 7, 9, 8, 1, 10, -1, 8, 1, 8, -1, 1, 3, 1, 8, 1, 8, 1, 8, 2, 3, 9, -1, 10, 1, 8, 10, 9, 9, 2, 8, 9, 3, 3, 2, 2, 8, 8, -1, 9, 1, -1, 3, 12, 8, 9, -1, -1, 1, 1, 3, 1, 8, 2, 8, 1, 1, 11, 1, 8, 3, 3, 8, 9, 2, 3, 10, 2, 9, 3, 12, 1, 1, 12, 12, 1, 7, 8, 11, 1, 3, 12, 3, 1, 8, 7, 4, -1, 2, 8, 2, 8, 9, 2, 8, -1, 6, 8, 2, 9, 1, 8, -1, 9, 5, 8, -1, 1, 2, 8, 7, 9, 1, 2, 1, 8, 8, 1, 1, -1, 8, 8, 3, 8, 1, 1, 1, 8, 8, 2, 8, 8, 8, 8, -1, 7, 1, 2, 1, 8, 9, 10, 8, 2, 1, 10, 1, 1, 2, 1, 1, 8, 1, -1, 8, 10, 10, 8, 8, 2, 8, 1, 8, -1, 9, 7, 8, 1, -1, 1, -1, -1, 1, -1, 8, 1, -1, -1, 9, 1, -1, 2, -1, 1, 1, 8, 1, 1, 8, 7, 2, 8, 12, 1, 12, 9, 2, 1, -1, 1, 8, -1, 2, 12, 1, 3, -1, 9, 1, 8, -1, 1, 1, 2, 10, 1, 9, 1, 8, -1, 8, 2, 1, 1, 2, 2, 9, -1, 2, 9, 1, 1, -1, 9, -1, 1, 7, 9, 8, 1, 2, -1, 8, 9, 8, -1, 7, 12, 12, 8, 2, 1]\n",
            "[1, 4, -1, 1, 2, 8, 3, 6, 2, 1, 8, 4, 1, -1, 1, -1, 2, -1, 1, 2, 8, -1, -1, 4, 2, 10, 2, 8, -1, 8, 8, 1, 8, 1, -1, 1, 9, 2, 2, 3, 2, 1, 8, 2, 2, 8, 1, 3, -1, 8, 3, 12, 1, 7, 2, 9, 9, -1, 9, 9, -1, 1, 6, 2, 1, 1, 2, -1, 10, 1, 3, 8, 7, 1, 2, -1, -1, 4, 1, 3, 1, 11, 10, 1, 7, -1, -1, 12, 8, -1, 2, 8, -1, 2, 1, 1, 2, 10, 8, -1, 7, 3, -1, 2, 1, 9, 7, 4, 8, 9, -1, 2, 1, 1, 10, 9, -1, 3, 9, 1, 8, 8, 7, -1, 2, 9, 3, 2, 8, 2, 9, 1, 8, 3, 9, -1, 1, 1, 1, -1, 1, 1, 10, 9, -1, -1, 8, 3, 8, 9, 2, 1, -1, 7, 4, -1, -1, 8, -1, 2, 4, 8, 4, 4, -1, 1, 8, 9, 4, 7, 3, 5, -1, 2, -1, 9, -1, 9, -1, 7, 2, 1, 9, 8, -1, -1, 8, -1, 12, -1, 1, 8, 8, -1, -1, 8, 11, 12, 1, 9, 3, -1, 1, 2, 8, 8, 8, 9, -1, 8, 8, 2, 2, 9, 1, 1, -1, 10, 2, 2, 8, 9, 8, 2, 1, 7, 3, 2, 1, 2, -1, 1, -1, -1, 8, 2, 9, 1, 7, 3, 1, 8, 8, -1, 1, 1, 7, 8, 1, -1, 2, 2, 12, -1, 9, 2, 1, -1, -1, 2, 8, 1, 1, 2, 9, 2, 2, 10, 8, 8, 2, 2, 8, 2, 7, -1, 8, 2, -1, 3, 7, 1, 2, 8, 1, 1, 9, 12, -1, 3, -1, 7, -1, 1, 8, 2, 1, 5, 1, 3, 2, 1, 2, 1, -1, 8, 2, 1, 8, 12, 7, 1, 10, 5, 1, 1, 1, 8, -1, -1, -1, 9, 2, 1, 1, 9, 1, 8, 2, 1, 7, 5, 1, 1, 1, 3, 9, 8, 2, 8, -1, 8, -1, -1, 1, 8, 8, 1, 2, 8, 1, 3, 3, 8, 1, 2, 1, 1, 9, -1, -1, 12, 3, 1, -1, 8, 4, -1, 1, 2, -1, 1, 2, 2, 4, 3, 4, -1, 1, 1, 10, -1, 8, -1, -1, 9, 10, 9, 12, 1, -1, 8, -1, 7, 9, -1, 8, 8, 8, 1, 8, 2, 1, 3, 6, 2, -1, -1, 2, 2, 10, 2, 1, -1, 1, -1, 1, 7, 5, 1, 8, 1, 8, 12, -1, 1, -1, 8, 8, 1, 2, -1, -1, 1, 8, 9, 8, 1, 12, 3, 1, -1, 4, 7, 9, 1, -1, 8, 1, 7, 8, 8, 8, -1, -1, 1, 8, -1, -1, 2, 8, 8, 7, 1, 4, 1, 2, -1, -1, 6, -1, 1, 3, 9, 2, -1, -1, 8, 1, 1, -1, -1, 9, 2, 1, -1, 8, 4, -1, 9, 1, -1, 1, 5, -1, 1, 3, 1, 2, 2, 7, -1, 1, 4, -1, 1, 4, 1, 1, 9, 10, 3, 12, 10, 1, 1, 2, 8, -1, 3, 2, 5, -1, 1, -1, 9, 1, -1, 7, 1, -1, 9, 8, -1, 8, 8, 1, 8, -1, -1, -1, 2, 1, -1, -1, 1, 8, 9, 2, -1, 8, 8, 2, 1, 8, 9, 8, 2, 5, 4, 7, 2, 8, 9, 8, 12, 8, 8, -1, 1, -1, 1, 3, 8, 1, -1, 1, 1, 8, 9, 9, 8, 1, -1, 2, 2, 1, 8, 12, -1, 2, 7, 2, 1, 1, -1, -1, 8, 1, 8, -1, 1, 1, 1, 8, 1, 8, 1, 1, 2, 3, -1, -1, 10, 1, 8, 8, 9, 2, 2, 8, 9, 8, 3, 2, -1, 8, 4, -1, 1, 1, -1, 3, 8, 8, -1, -1, -1, 1, 1, -1, 1, 8, 1, 8, 1, 1, 9, 1, 8, 3, -1, 8, 10, 2, 3, 10, 2, 8, 3, 12, 1, 1, 12, -1, 1, 7, 8, 11, 1, 3, 4, 2, 1, 8, 2, 4, -1, -1, 8, 4, 8, 9, -1, 8, -1, 6, 8, 2, 9, 1, 8, -1, 9, 5, 8, -1, 1, 2, 8, 7, 10, 1, 2, 1, 8, 8, 1, 1, -1, 8, 4, 3, 8, 1, 1, 1, 8, 8, 2, 8, 8, 8, 8, -1, 7, -1, 2, 1, 8, 9, 8, 8, 2, -1, 2, -1, 1, 2, 1, 1, -1, 1, -1, 8, 10, 10, 8, 8, 2, -1, 1, 8, -1, 8, 7, 8, 1, -1, -1, -1, -1, 1, -1, 8, 1, -1, -1, 10, -1, -1, 2, -1, 9, 1, 9, 1, 1, -1, 7, 2, 8, 12, 1, 8, 8, 2, 1, -1, 1, 8, -1, 2, -1, 1, 3, -1, 9, 1, 8, -1, 1, 1, -1, 10, 1, -1, 1, 10, -1, 8, 2, 1, 1, 2, 2, 9, -1, 2, 4, 1, 1, -1, 9, -1, 1, 7, 9, 8, 1, 9, -1, 8, 9, 8, -1, 7, 12, 12, 8, 2, 1]\n",
            "[1, 9, -1, 1, 2, 8, 3, 6, 2, 1, -1, 4, 1, -1, 10, -1, 2, -1, 1, 2, 8, -1, -1, 4, 2, 8, 2, 8, -1, 8, 12, 1, 8, -1, -1, 1, 1, 2, 2, 3, -1, 1, 10, 2, 2, 8, -1, 3, -1, 8, 3, 12, 3, 7, 2, 9, 9, -1, 9, 9, -1, 2, 6, 2, 1, -1, 2, -1, 10, 1, 3, -1, 7, 1, 2, -1, -1, 8, 1, 3, 1, -1, 10, -1, 7, -1, -1, -1, 10, -1, 2, 8, -1, -1, -1, 1, 2, 10, 8, -1, -1, 3, -1, 2, 1, -1, 10, 4, 8, 9, -1, 2, 1, 1, 10, 9, -1, 3, 9, -1, 8, -1, 7, -1, 2, 9, 3, 2, 8, 2, 9, 1, 8, -1, 9, -1, 1, 1, 1, -1, 1, 1, 10, 9, -1, -1, 8, 3, -1, 9, 2, 1, -1, 7, 2, -1, -1, -1, -1, 2, 4, 9, 4, 4, -1, -1, 8, 9, 4, 7, 3, 9, -1, 2, -1, -1, -1, 9, -1, -1, 3, -1, 9, 8, -1, -1, 8, -1, 12, -1, -1, 10, 10, -1, -1, 7, -1, 12, 1, 9, 3, -1, 1, -1, 10, 8, 8, 9, -1, 8, -1, 2, 2, 9, 1, 1, -1, 10, 2, 2, 8, 10, 8, 2, 1, 7, 3, 2, 2, 2, -1, 1, -1, -1, 9, -1, 9, -1, 7, 3, 1, 8, -1, -1, 1, 1, 7, 8, 1, -1, 9, 2, 12, -1, 9, 2, 1, -1, -1, 2, 8, 5, 1, 2, 9, 2, 2, 10, -1, 8, 2, 9, 8, -1, 7, -1, 8, 2, -1, 3, 7, 12, 2, 8, 1, 1, 9, -1, -1, 3, -1, 7, -1, 1, 9, 2, 1, 5, 1, 3, 2, 1, 2, 9, -1, 8, 2, 3, 8, 12, 10, 1, 10, 5, 1, 1, 1, 8, -1, -1, -1, 9, 10, 4, 10, -1, 1, 8, 2, 1, -1, 5, 1, 1, 1, 3, -1, 8, 2, 8, -1, 8, -1, -1, -1, 10, 9, -1, 2, 8, 1, 3, 8, 9, 1, -1, 1, 1, 9, -1, -1, 12, 3, 1, -1, 8, 4, -1, -1, 2, -1, 1, -1, 2, 2, 3, 6, -1, 1, 1, 10, -1, -1, -1, -1, 9, 10, -1, 12, 1, -1, -1, -1, 7, 9, -1, 8, 8, 8, 1, 8, 10, 1, 3, 6, 9, -1, -1, 2, 2, 10, 3, 1, -1, 1, -1, 1, 7, 9, -1, 1, 1, 8, 12, -1, 1, -1, 8, 10, 1, 2, -1, -1, 5, 8, 9, 10, 1, 12, 3, 1, -1, 8, 7, 9, 1, -1, -1, 2, 10, 3, 8, 8, -1, -1, 1, 8, -1, -1, 6, -1, 8, 7, 1, 4, 1, 2, -1, -1, 6, -1, 1, 3, 9, 2, -1, -1, 8, -1, 1, -1, -1, 9, 9, 1, -1, 8, 4, -1, -1, 1, -1, 1, 9, -1, 1, 3, 1, 2, 9, 10, -1, 1, -1, -1, 1, -1, 1, 1, 9, 10, 3, 12, 10, 1, 1, 2, 8, -1, 3, 2, 5, -1, 1, -1, 9, 1, -1, 7, 1, -1, 9, -1, -1, 8, 8, 1, 8, -1, -1, -1, 6, 1, -1, -1, 1, 8, 9, 2, -1, 8, -1, 2, 1, 8, 9, 8, 10, 5, 10, 7, 2, 8, 9, 8, -1, 8, 8, -1, 1, -1, 1, 3, 8, 1, -1, 1, -1, 8, 9, 9, 8, 1, -1, 2, -1, 5, 10, 12, -1, 2, 7, 9, 10, 1, -1, -1, 8, 1, 8, -1, 1, -1, 1, 3, 1, 8, 1, 8, 2, 3, -1, -1, 10, 1, 8, 10, 9, 9, 2, 8, 9, 9, 3, 2, -1, 8, 8, -1, 9, 1, -1, -1, 8, 8, -1, -1, -1, 1, 1, -1, 1, -1, 1, 9, -1, 1, 10, 1, 8, 3, -1, 8, 10, 2, 3, 10, 2, 9, 3, 12, 1, 1, 12, -1, 1, 7, 9, 10, 1, 3, 4, 2, 1, 8, 7, -1, -1, -1, 8, 4, -1, 9, -1, 8, -1, 9, 10, 2, 9, 1, 7, -1, 9, 5, 8, -1, 1, 2, 8, -1, 10, 1, -1, 1, 8, 8, 1, 1, -1, 8, 4, 3, 8, 1, 1, 1, 8, 8, 2, 8, 8, 9, 8, -1, 7, -1, 2, 1, -1, 9, 10, 8, 2, -1, 10, -1, 1, 2, 1, 1, -1, 1, -1, 8, 10, 10, 8, 10, 2, -1, 1, 8, -1, 8, 7, 8, -1, -1, -1, -1, -1, 1, -1, 8, 1, -1, -1, 9, -1, -1, 2, -1, 1, 1, 9, 2, 1, -1, 7, -1, -1, 12, -1, -1, 9, 2, -1, -1, 1, 8, -1, 2, -1, 1, 3, -1, 9, -1, 8, -1, 1, 1, -1, 10, 3, -1, 1, 10, -1, 10, 10, 1, 1, 2, 2, -1, -1, 2, 9, 4, 1, -1, 10, -1, 1, 10, 9, 7, 1, 9, -1, 8, 9, 8, -1, 7, 12, 12, 8, -1, 1]\n",
            "[1, 9, -1, 1, 2, 8, 3, 6, -1, 1, -1, 4, 1, -1, 1, -1, 2, -1, 1, 2, -1, -1, -1, 4, 2, 8, 2, 8, -1, 8, 8, 1, 8, -1, -1, 4, 1, 2, 2, 3, -1, 1, 10, -1, 2, 8, -1, 3, -1, -1, 3, 12, 1, 7, 2, -1, -1, -1, 6, 9, -1, 3, 6, 2, 1, -1, 2, -1, -1, -1, 3, -1, -1, 1, 2, -1, -1, -1, 1, 3, 1, -1, 10, -1, 7, -1, -1, -1, -1, -1, 2, 8, -1, -1, -1, 1, -1, 10, 8, -1, -1, 3, -1, 2, 1, -1, -1, 2, 8, 9, -1, 2, 1, 1, 8, 8, -1, 3, 2, -1, 8, -1, -1, -1, 2, 1, 3, 2, 8, 2, 9, 1, 8, -1, 9, -1, 4, 1, 1, -1, -1, 1, 8, 8, -1, -1, 8, 3, -1, 9, 2, 1, -1, -1, -1, -1, -1, -1, -1, 2, -1, 8, 4, -1, -1, -1, 8, 8, 4, 7, 3, 5, -1, 2, -1, -1, -1, 9, -1, -1, 3, -1, 9, 8, -1, -1, 8, -1, -1, -1, -1, 8, 8, -1, -1, -1, -1, 12, 1, 9, 3, -1, 1, -1, -1, -1, 8, 9, -1, 8, -1, 2, 2, 9, 1, 1, -1, 8, 1, 2, 8, 13, 4, 2, 1, 7, 3, -1, -1, 2, -1, 1, -1, -1, 8, -1, 9, -1, 7, 3, 1, 8, -1, -1, 1, 1, 7, 8, 1, -1, 2, 2, 12, -1, 9, 3, -1, -1, -1, 2, 8, 5, 1, 2, 2, -1, 2, 8, -1, 8, -1, 2, 8, -1, 7, -1, 8, 2, -1, -1, 7, 12, 2, 8, 1, -1, 9, -1, -1, 3, -1, 7, -1, 1, 8, 2, 4, 5, 1, 3, -1, 1, 2, 13, -1, 8, -1, 3, 8, 12, 7, 1, -1, 5, 1, 1, -1, 8, -1, -1, -1, 2, 2, -1, 1, -1, -1, 8, 2, -1, -1, 5, 1, 1, -1, -1, -1, 8, 2, 8, -1, 8, -1, -1, -1, 8, 8, -1, 2, 8, 1, 3, 8, 8, 1, -1, 1, 1, 9, -1, -1, 12, 3, 4, -1, 8, -1, -1, -1, 2, -1, 1, -1, 2, 2, 3, 4, -1, 1, 1, 10, -1, -1, -1, -1, 9, 8, -1, 12, 1, -1, -1, -1, 7, 9, -1, 8, 8, 8, 1, 8, 10, 1, 3, 6, 2, -1, -1, 2, 2, 8, 1, 1, -1, 1, -1, 1, 7, 5, -1, 8, 1, 8, 12, -1, 1, -1, 8, 8, 1, 2, -1, -1, 1, 8, 8, 8, 4, 12, -1, 1, -1, 8, 7, 9, 1, -1, -1, -1, 8, 8, 8, 8, -1, -1, 1, -1, -1, -1, 6, -1, 8, 7, 1, 4, -1, 2, -1, -1, 6, -1, -1, 3, 3, 2, -1, -1, 8, -1, 1, -1, -1, 13, 8, 1, -1, 8, 4, -1, -1, 1, -1, 4, 5, -1, 1, 3, 1, 2, -1, 7, -1, 1, -1, -1, 1, -1, 1, -1, 9, 10, 3, 12, 10, 1, 1, 2, -1, -1, 3, 2, 5, -1, 3, -1, 7, 1, -1, 8, 1, -1, 9, -1, -1, 8, 8, -1, -1, -1, -1, -1, 6, 1, -1, -1, 1, 8, 9, 2, -1, 8, -1, 2, -1, 8, 2, 8, 9, 5, 4, 7, 2, 8, 9, 8, -1, -1, 7, -1, 1, -1, 1, 3, 8, 1, -1, 1, -1, 8, 8, 9, 8, 1, -1, 2, -1, 1, 8, 12, -1, 2, -1, 2, 10, 1, -1, -1, 8, 2, 8, -1, -1, -1, 1, 8, 1, 8, 1, 8, 2, 3, -1, -1, -1, -1, 8, 10, 9, 9, 2, 8, -1, 3, 3, 2, -1, 8, 8, -1, 9, -1, -1, -1, 8, 8, -1, -1, -1, 1, 1, -1, -1, -1, 2, 8, -1, 1, -1, 1, 8, 3, -1, 8, 10, 2, 3, 7, -1, 8, 3, 12, 4, 1, -1, -1, 1, 7, 8, 11, 1, 3, 2, 2, 1, 8, 7, -1, -1, -1, 8, 2, -1, 2, -1, 8, -1, 6, -1, 2, -1, 1, 8, -1, -1, 5, 8, -1, 1, 2, 8, -1, 10, 1, -1, 1, -1, 8, 1, 1, -1, 8, 8, 3, 8, 1, 1, 1, 8, 8, 2, 8, 8, 8, 8, -1, -1, -1, 2, 1, -1, 9, 8, 8, -1, -1, 2, -1, 1, 2, 1, 1, -1, 1, -1, 8, 10, 10, 8, 8, 2, -1, 1, 8, -1, 8, -1, 8, -1, -1, -1, -1, -1, 1, -1, -1, 1, -1, -1, 8, -1, -1, 2, -1, 1, -1, 8, 1, -1, -1, 7, -1, -1, 12, -1, -1, 8, 2, -1, -1, 1, -1, -1, 2, -1, 1, 3, -1, -1, -1, 8, -1, 1, 1, -1, 10, 1, -1, -1, 10, -1, 8, 12, 1, 1, -1, 2, -1, -1, 2, 4, 1, 1, -1, 8, -1, 1, 7, 9, 7, 1, 2, -1, 8, 4, 8, -1, 7, 12, 12, -1, -1, -1]\n",
            "[1, 8, -1, 1, 2, 8, 3, 6, -1, 1, -1, 4, 1, -1, 1, -1, 2, -1, 1, 2, -1, -1, -1, 4, 2, 10, 2, 8, -1, 8, 8, 1, 8, -1, -1, -1, -1, 2, 2, 3, -1, 1, 10, -1, -1, -1, -1, 3, -1, -1, 3, 12, 1, 7, 1, -1, -1, -1, 9, 9, -1, 2, -1, 2, 1, -1, 2, -1, -1, -1, 3, -1, -1, 1, 2, -1, -1, -1, -1, 3, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 2, 8, -1, -1, -1, 1, -1, 10, 8, -1, -1, -1, -1, 2, 1, -1, -1, 4, 8, -1, -1, 2, 1, 1, 10, 9, -1, 3, -1, -1, 8, -1, -1, -1, 2, 1, 3, -1, 8, 2, 9, 1, -1, -1, 9, -1, 4, 1, 1, -1, -1, -1, 10, 8, -1, -1, 8, 3, -1, 9, 2, 1, -1, -1, -1, -1, -1, -1, -1, 2, -1, 4, -1, -1, -1, -1, 8, 9, 4, 7, 3, 9, -1, 10, -1, -1, -1, 9, -1, -1, 3, -1, 9, -1, -1, -1, 8, -1, -1, -1, -1, 10, 10, -1, -1, -1, -1, -1, 1, -1, 3, -1, 1, -1, -1, -1, -1, 9, -1, 8, -1, -1, 2, 9, 1, 1, -1, 10, -1, 2, 8, 9, 4, 2, -1, 7, 3, -1, -1, 2, -1, 1, -1, -1, 8, -1, -1, -1, -1, 3, 1, 8, -1, -1, 1, -1, 7, 8, 1, -1, 2, 2, 3, -1, -1, 3, -1, -1, -1, 2, 8, 5, 1, 3, 6, -1, 2, 10, -1, -1, -1, 9, 8, -1, -1, -1, 8, 2, -1, -1, -1, 1, 2, 10, 1, -1, 9, -1, -1, 3, -1, 7, -1, -1, 8, 2, 1, 5, -1, -1, -1, 1, 2, 10, -1, 8, -1, 3, 5, -1, -1, 1, -1, 5, 1, 1, -1, 8, -1, -1, -1, 9, 2, -1, 1, -1, -1, 8, 2, -1, -1, 5, 1, 1, -1, -1, -1, 8, 2, 8, -1, 8, -1, -1, -1, -1, 6, -1, -1, -1, -1, 3, 6, 8, 1, -1, 1, 1, 9, -1, -1, 12, 3, -1, -1, 8, -1, -1, -1, 2, -1, 1, -1, -1, -1, 3, 6, -1, 1, 1, 10, -1, -1, -1, -1, 9, 10, -1, 12, 1, -1, -1, -1, 7, -1, -1, 8, 8, -1, 1, 9, 10, -1, 3, 6, 9, -1, -1, 2, 2, 10, 3, 1, -1, 1, -1, 1, -1, 5, -1, 1, 1, 10, 12, -1, -1, -1, 8, 10, -1, 2, -1, -1, 1, -1, 9, 10, 1, 12, -1, 1, -1, 8, -1, -1, -1, -1, -1, -1, 9, 3, 8, 8, -1, -1, 1, -1, -1, -1, 6, -1, -1, 7, -1, 4, -1, 2, -1, -1, 6, -1, -1, 3, 3, 4, -1, -1, 8, -1, 1, -1, -1, 9, 8, 1, -1, -1, 4, -1, -1, 1, -1, 4, 9, -1, 1, -1, 1, 2, -1, 10, -1, 1, -1, -1, 1, -1, 1, -1, 9, 10, 3, 12, 10, 1, 1, 2, -1, -1, 3, 2, 5, -1, 1, -1, 7, 1, -1, 7, -1, -1, 8, -1, -1, 8, 8, -1, -1, -1, -1, -1, 6, 1, -1, -1, 1, 9, 9, 2, -1, 8, -1, 4, -1, -1, -1, 8, 10, 5, 10, 7, 2, 8, -1, 8, -1, -1, 7, -1, 1, -1, 1, 3, 8, 1, -1, 1, -1, 8, 9, 9, 8, 1, -1, 2, -1, 1, 10, 12, -1, 2, -1, 9, 10, 1, -1, -1, 8, 1, 8, -1, -1, -1, 1, 3, 1, 8, 1, 8, 2, 3, -1, -1, -1, -1, 8, -1, -1, -1, 2, 8, -1, 3, 3, -1, -1, 8, 10, -1, 1, -1, -1, -1, 8, 8, -1, -1, -1, 1, 1, -1, -1, -1, 1, 8, -1, 1, -1, 1, 8, 3, -1, 8, 10, 2, 3, 10, -1, 8, 3, 12, 1, 1, -1, -1, 1, 7, 8, 11, 1, 3, -1, 3, 1, 8, 7, -1, -1, -1, -1, -1, -1, 1, -1, 8, -1, 6, -1, 2, -1, 1, -1, -1, -1, 5, 8, -1, -1, 2, 8, -1, 10, 1, -1, 1, -1, 8, 1, 1, -1, 8, 4, 3, 8, 1, 1, 1, 3, 8, 2, 8, -1, 9, 8, -1, -1, -1, 2, 1, -1, 9, -1, 8, -1, -1, 10, -1, 1, 2, 1, 1, -1, 1, -1, 8, 10, -1, 8, 10, 2, -1, -1, 8, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, 1, -1, -1, 9, -1, -1, 2, -1, 9, -1, 3, 2, -1, -1, 7, -1, -1, -1, -1, -1, 8, 2, -1, -1, 1, -1, -1, 2, -1, 1, 3, -1, -1, -1, 8, -1, 1, 1, -1, 10, 1, -1, -1, 10, -1, 8, 2, 1, 1, -1, 2, -1, -1, 2, 4, -1, -1, -1, 9, -1, 1, 9, 9, 8, 1, 9, -1, 8, 9, 8, -1, 7, -1, 12, -1, -1, -1]\n",
            "0.7731591448931117\n",
            "[1, 9, 3, 1, 2, 8, 3, 6, 2, 1, 8, 4, 8, 4, 8, 7, 2, 4, 9, 2, 8, 9, 9, 4, 2, 8, 2, 8, 1, 8, 8, 1, 8, 1, 3, 1, 1, 2, 2, 3, 9, 1, 8, 2, 10, 8, 1, 3, 4, 8, 3, 12, 1, 7, 2, 8, 9, 1, 9, 9, 3, 2, 6, 2, 1, 1, 2, 1, 10, 1, 3, 8, 7, 1, 2, 2, 9, 12, 1, 3, 1, 11, 10, 1, 7, 9, 9, 12, 10, 1, 2, 8, 9, 2, 1, 1, 2, 10, 8, 2, 7, 3, 11, 2, 1, 9, 10, 4, 8, 9, 11, 2, 1, 1, 8, 9, 3, 3, 9, 1, 8, 8, 7, 1, 2, 9, 3, 2, 8, 9, 9, 1, 8, 1, 9, 8, 4, 1, 1, 2, 1, 1, 10, 9, 1, 9, 8, 3, 8, 9, 2, 1, 3, 7, 2, 0, 3, 10, 8, 2, 9, 8, 4, 4, 12, 1, 8, 9, 4, 7, 3, 9, 10, 10, 8, 9, 3, 9, 12, 7, 2, 1, 8, 8, 9, 8, 8, 9, 12, 1, 1, 10, 10, 1, 2, 7, 11, 12, 1, 9, 3, 9, 1, 9, 8, 8, 8, 9, 9, 8, 8, 2, 2, 9, 1, 1, 2, 10, 2, 2, 8, 9, 8, 2, 1, 7, 3, 2, 4, 2, 2, 1, 9, 1, 8, 2, 9, 1, 7, 3, 1, 8, 8, 12, 1, 1, 7, 8, 1, 2, 9, 2, 12, 1, 9, 2, 1, 4, 9, 2, 8, 5, 1, 2, 9, 2, 2, 10, 8, 8, 2, 9, 8, 2, 7, 1, 8, 2, 2, 3, 7, 12, 2, 12, 1, 1, 9, 12, 4, 3, 2, 7, 3, 1, 8, 2, 1, 5, 1, 3, 2, 1, 10, 10, 8, 8, 2, 2, 8, 12, 7, 1, 8, 5, 1, 1, 1, 8, 9, 3, 9, 9, 4, 4, 1, 9, 2, 8, 10, 1, 7, 5, 1, 1, 1, 3, 9, 8, 2, 8, 1, 8, 7, 3, 1, 9, 8, 4, 2, 8, 1, 3, 3, 8, 1, 2, 1, 1, 9, 8, 1, 12, 3, 1, 1, 8, 4, 3, 8, 2, 9, 1, 2, 2, 2, 3, 4, 9, 1, 1, 10, 8, 8, 10, 1, 9, 10, 9, 12, 1, 9, 8, 8, 7, 9, 8, 8, 8, 8, 1, 8, 10, 1, 7, 6, 9, 3, 2, 2, 2, 8, 3, 1, 9, 1, 9, 1, 7, 9, 3, 8, 2, 10, 12, 1, 1, 2, 8, 8, 1, 2, 3, 9, 5, 8, 8, 8, 1, 12, 3, 1, 2, 4, 7, 9, 1, 2, 8, 2, 9, 8, 8, 8, 3, 8, 1, 8, 1, 2, 6, 8, 8, 7, 1, 4, 1, 2, 9, 5, 6, 12, 1, 3, 3, 4, 3, 1, 8, 8, 1, 9, 9, 10, 8, 1, 9, 8, 9, 8, 9, 1, 3, 4, 5, 2, 1, 3, 1, 2, 7, 10, 8, 1, 8, 1, 1, 4, 1, 9, 9, 10, 3, 12, 10, 1, 1, 2, 8, 2, 3, 2, 5, 8, 1, 3, 7, 1, 1, 8, 8, 1, 9, 8, 2, 8, 8, 1, 8, 1, 8, 8, 6, 1, 7, 1, 1, 8, 9, 2, 10, 8, 8, 2, 1, 8, 9, 8, 9, 5, 4, 7, 2, 8, 9, 8, 12, 8, 7, 9, 1, 2, 1, 3, 8, 1, 8, 1, 1, 8, 8, 9, 8, 1, 1, 2, 2, 1, 8, 12, 2, 2, 7, 9, 10, 1, 10, 2, 8, 1, 8, 11, 1, 3, 1, 7, 1, 8, 1, 8, 2, 3, 8, 10, 9, 1, 8, 10, 9, 9, 2, 8, 9, 3, 3, 4, 2, 8, 8, 2, 9, 1, 1, 3, 12, 8, 9, 8, 2, 1, 1, 3, 1, 8, 1, 8, 1, 1, 9, 1, 8, 3, 3, 8, 10, 2, 3, 10, 2, 8, 3, 12, 1, 8, 12, 12, 1, 7, 8, 11, 1, 3, 4, 3, 1, 8, 7, 4, 8, 2, 8, 2, 8, 9, 2, 8, 8, 6, 8, 2, 9, 1, 7, 8, 9, 5, 8, 1, 1, 2, 8, 7, 9, 1, 2, 1, 8, 8, 1, 1, 2, 8, 4, 3, 8, 1, 1, 1, 8, 8, 2, 8, 8, 8, 8, 3, 7, 1, 2, 1, 8, 9, 8, 8, 2, 4, 10, 1, 1, 2, 1, 1, 8, 1, 2, 8, 10, 10, 8, 8, 2, 8, 1, 8, 2, 8, 7, 8, 1, 8, 1, 1, 3, 1, 1, 8, 2, 8, 2, 9, 1, 1, 2, 3, 1, 1, 8, 2, 1, 8, 7, 2, 8, 12, 8, 12, 9, 2, 1, 7, 1, 8, 2, 2, 12, 1, 3, 8, 8, 1, 8, 8, 1, 1, 2, 10, 1, 9, 1, 8, 2, 9, 2, 1, 1, 2, 2, 9, 10, 8, 4, 1, 1, 9, 9, 0, 1, 7, 9, 8, 1, 2, 12, 8, 9, 8, 1, 7, 12, 12, 8, 2, 1]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "final_pred_1=[]\n",
        "\n",
        "num_correct=0\n",
        "model_0_1=load_model(loss,accuracy,optimizer,'0-1')\n",
        "mod_labels_0=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_0:\n",
        "    pred_test_0_1=model_0_1.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_0_1 = pred_test_0_1.argmax(axis=1)\n",
        "    mod_labels_0.append(pred_labels_0_1[0])\n",
        "  else:\n",
        "    mod_labels_0.append(-1)\n",
        "\n",
        "model_1_1=load_model(loss,accuracy,optimizer,'1-1')\n",
        "mod_labels_1=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        " \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_1:\n",
        "    pred_test_1_1=model_1_1.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_1_1 = pred_test_1_1.argmax(axis=1)\n",
        "    mod_labels_1.append(pred_labels_1_1[0])\n",
        "  else:\n",
        "    mod_labels_1.append(-1)\n",
        "\n",
        "model_2_1=load_model(loss,accuracy,optimizer,'2-1')\n",
        "mod_labels_2=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_2:\n",
        "    pred_test_2_1=model_2_1.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_2_1 = pred_test_2_1.argmax(axis=1)\n",
        "    mod_labels_2.append(pred_labels_2_1[0])\n",
        "  else:\n",
        "    mod_labels_2.append(-1)\n",
        "\n",
        "model_3_1=load_model(loss,accuracy,optimizer,'3-1')\n",
        "mod_labels_3=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_3:\n",
        "    pred_test_3_1=model_3_1.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_3_1 = pred_test_3_1.argmax(axis=1)\n",
        "    mod_labels_3.append(pred_labels_3_1[0])\n",
        "  else:\n",
        "    mod_labels_3.append(-1)\n",
        "\n",
        "model_4_1=load_model(loss,accuracy,optimizer,'4-1')\n",
        "mod_labels_4=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_4:\n",
        "    pred_test_4_1=model_4_1.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_4_1 = pred_test_4_1.argmax(axis=1)\n",
        "    mod_labels_4.append(pred_labels_4_1[0])\n",
        "  else:\n",
        "    mod_labels_4.append(-1)\n",
        "\n",
        "model_5_1=load_model(loss,accuracy,optimizer,'5-1')\n",
        "mod_labels_5=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_5:\n",
        "    pred_test_5_1=model_5_1.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_5_1 = pred_test_5_1.argmax(axis=1)\n",
        "    mod_labels_5.append(pred_labels_5_1[0])\n",
        "  else:\n",
        "    mod_labels_5.append(-1)\n",
        "\n",
        "print(mod_labels_0)\n",
        "print(mod_labels_1)\n",
        "print(mod_labels_2)\n",
        "print(mod_labels_3)\n",
        "print(mod_labels_4)\n",
        "print(mod_labels_5)\n",
        "\n",
        "\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  fin_labels=[]\n",
        "  if mod_labels_0[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  if mod_labels_1[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  if mod_labels_2[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  if mod_labels_3[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  if mod_labels_4[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  if mod_labels_5[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  res=max(set(fin_labels), key = fin_labels.count)\n",
        "  final_pred_1.append(res)\n",
        "\n",
        "  if res==new_val_label_0[i]:\n",
        "    num_correct=num_correct+1\n",
        "\n",
        "accuracy=num_correct/len(new_val_label_0)\n",
        "print(accuracy)\n",
        "  \n",
        "print(final_pred_1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhhwGEPul5A4",
        "outputId": "dc2bca01-be56-4e2c-b22b-8a89b76b9f9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.7731591448931117\n",
            "Weighted F1: 0.772323496451826\n",
            "Micro F1: 0.7731591448931117\n",
            "Weighted Precision: 0.7761444195979078\n",
            "Micro Precision: 0.7731591448931117\n",
            "Weighted Recall: 0.7731591448931117\n",
            "Micro Recall: 0.7731591448931117\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "\n",
        "accuracy=accuracy_score(new_val_label_0, final_pred_1)\n",
        "print(\"Accuracy: \"+str(accuracy))\n",
        "total_accuracy=total_accuracy+accuracy\n",
        "\n",
        "weighted_f1=f1_score(new_val_label_0,final_pred_1, average='weighted')\n",
        "print(\"Weighted F1: \"+ str(weighted_f1))\n",
        "total_weighted_f1=total_weighted_f1+weighted_f1\n",
        "micro_f1=f1_score(new_val_label_0,final_pred_1, average='micro')\n",
        "print(\"Micro F1: \"+ str(micro_f1))\n",
        "total_micro_f1=total_micro_f1+micro_f1\n",
        "\n",
        "weighted_precision=precision_score(new_val_label_0, final_pred_1, average='weighted')\n",
        "print(\"Weighted Precision: \" + str(weighted_precision))\n",
        "total_weighted_precision=total_weighted_precision+weighted_precision\n",
        "micro_precision=precision_score(new_val_label_0, final_pred_1, average='micro')\n",
        "print(\"Micro Precision: \" + str(micro_precision))\n",
        "total_micro_precision=total_micro_precision+micro_precision\n",
        "\n",
        "weighted_recall=recall_score(new_val_label_0, final_pred_1, average='weighted')\n",
        "print(\"Weighted Recall: \" + str(weighted_recall))\n",
        "total_weighted_recall=total_weighted_recall+weighted_recall\n",
        "micro_recall=recall_score(new_val_label_0, final_pred_1, average='micro')\n",
        "print(\"Micro Recall: \" + str(micro_recall))\n",
        "total_micro_recall=total_micro_recall+micro_recall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Pe7iq0ql_Vy",
        "outputId": "bd6c65dc-c820-4662-e3c5-c893f9ad24ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_14\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_29 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_30 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_29[0][0]',               \n",
            " el)                            thPoolingAndCrossAt               'input_30[0][0]']               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_14 (S  (None, 768)         0           ['tf_roberta_model[14][0]']      \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " dense_28 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_14[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dropout_51 (Dropout)           (None, 512)          0           ['dense_28[0][0]']               \n",
            "                                                                                                  \n",
            " dense_29 (Dense)               (None, 15)           7695        ['dropout_51[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model: \"model_15\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_31 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_32 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_31[0][0]',               \n",
            " el)                            thPoolingAndCrossAt               'input_32[0][0]']               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_15 (S  (None, 768)         0           ['tf_roberta_model[15][0]']      \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " dense_30 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_15[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dropout_52 (Dropout)           (None, 512)          0           ['dense_30[0][0]']               \n",
            "                                                                                                  \n",
            " dense_31 (Dense)               (None, 15)           7695        ['dropout_52[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model: \"model_16\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_33 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_34 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_33[0][0]',               \n",
            " el)                            thPoolingAndCrossAt               'input_34[0][0]']               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_16 (S  (None, 768)         0           ['tf_roberta_model[16][0]']      \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " dense_32 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_16[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dropout_53 (Dropout)           (None, 512)          0           ['dense_32[0][0]']               \n",
            "                                                                                                  \n",
            " dense_33 (Dense)               (None, 15)           7695        ['dropout_53[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model: \"model_17\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_35 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_36 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_35[0][0]',               \n",
            " el)                            thPoolingAndCrossAt               'input_36[0][0]']               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_17 (S  (None, 768)         0           ['tf_roberta_model[17][0]']      \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " dense_34 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_17[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dropout_54 (Dropout)           (None, 512)          0           ['dense_34[0][0]']               \n",
            "                                                                                                  \n",
            " dense_35 (Dense)               (None, 15)           7695        ['dropout_54[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model: \"model_18\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_37 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_38 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_37[0][0]',               \n",
            " el)                            thPoolingAndCrossAt               'input_38[0][0]']               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_18 (S  (None, 768)         0           ['tf_roberta_model[18][0]']      \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " dense_36 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_18[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dropout_55 (Dropout)           (None, 512)          0           ['dense_36[0][0]']               \n",
            "                                                                                                  \n",
            " dense_37 (Dense)               (None, 15)           7695        ['dropout_55[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model: \"model_19\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_39 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_40 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_39[0][0]',               \n",
            " el)                            thPoolingAndCrossAt               'input_40[0][0]']               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_19 (S  (None, 768)         0           ['tf_roberta_model[19][0]']      \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " dense_38 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_19[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dropout_56 (Dropout)           (None, 512)          0           ['dense_38[0][0]']               \n",
            "                                                                                                  \n",
            " dense_39 (Dense)               (None, 15)           7695        ['dropout_56[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "[1, 9, 3, 1, 2, 8, 3, 6, 2, 1, 8, 4, 1, 4, 1, 7, 2, 4, 1, 2, 8, 9, 9, 4, 2, 8, 2, 8, 1, 8, 12, 1, 8, 1, 8, 1, 1, 2, 2, 3, 9, 1, 10, 2, 10, 8, 1, 3, 4, 8, 3, 12, 3, 7, 2, 8, 9, 1, 9, 9, 3, 3, 6, 8, 1, 1, 8, 1, 10, 1, 3, 8, 7, 1, 2, 2, 9, 10, 1, 3, 1, 11, 10, 1, 7, 9, 9, 12, 8, 1, 2, 8, 1, 2, 1, 1, 2, 10, 8, 2, 7, 3, 11, 2, 1, 9, 7, 4, 8, 9, 6, 2, 1, 1, 10, 9, 3, 3, 9, 1, 8, 8, 7, 1, 2, 9, 3, 2, 8, 2, 9, 1, 8, 1, 9, 8, 4, 1, 1, 2, 1, 1, 8, 8, 1, 9, 8, 3, 8, 9, 2, 1, 3, 7, 4, 10, 3, 8, 8, 2, 1, 8, 4, 4, 12, 1, 8, 9, 4, 7, 3, 5, 10, 2, 8, 8, 3, 9, 12, 7, 3, 1, 8, 4, 8, 8, 8, 9, 12, 1, 1, 8, 8, 1, 2, 8, 11, 12, 1, 9, 3, 9, 1, 3, 8, 2, 8, 9, 9, 8, 8, 2, 2, 9, 1, 1, 2, 10, 2, 2, 8, 9, 8, 2, 1, 7, 3, 2, 1, 2, 2, 1, 9, 1, 8, 2, 9, 1, 2, 3, 1, 8, 8, 12, 1, 1, 7, 8, 1, 2, 2, 2, 12, 1, 9, 3, 1, 1, 9, 2, 8, 5, 1, 3, 2, 2, 2, 8, 8, 8, 2, 2, 8, 2, 7, 1, 8, 2, 2, 3, 7, 12, 2, 12, 1, 1, 9, 12, 4, 3, 2, 7, 3, 1, 8, 2, 1, 5, 1, 3, 2, 1, 2, 1, 8, 8, 2, 2, 8, 12, 10, 1, 8, 5, 1, 1, 1, 8, 9, 3, 9, 1, 4, 4, 1, 8, 1, 8, 2, 1, 3, 5, 1, 1, 1, 3, 1, 8, 2, 8, 1, 8, 7, 1, 1, 8, 8, 1, 2, 8, 1, 3, 8, 8, 1, 2, 1, 1, 9, 8, 1, 12, 3, 1, 1, 8, 4, 3, 8, 2, 9, 1, 2, 2, 2, 3, 4, 9, 1, 1, 10, 12, 8, 8, 1, 9, 8, 9, 12, 1, 9, 8, 8, 7, 2, 8, 8, 8, 8, 1, 8, 10, 1, 3, 6, 9, 3, 2, 2, 2, 10, 2, 1, 3, 1, 9, 1, 7, 5, 1, 1, 2, 8, 12, 1, 1, 2, 8, 8, 1, 2, 3, 2, 1, 8, 9, 8, 1, 12, 3, 1, 2, 8, 7, 9, 1, 2, 8, 2, 8, 8, 8, 8, 2, 8, 1, 8, 1, 1, 2, 8, 8, 7, 1, 4, 1, 2, 2, 5, 6, 12, 1, 3, 3, 2, 3, 1, 8, 8, 1, 9, 3, 8, 8, 1, 9, 8, 4, 8, 10, 1, 3, 1, 5, 2, 1, 3, 1, 2, 9, 10, 8, 1, 8, 1, 1, 4, 1, 1, 9, 10, 3, 12, 10, 1, 1, 2, 8, 2, 3, 2, 5, 2, 1, 3, 9, 1, 1, 8, 1, 1, 8, 8, 2, 8, 8, 1, 8, 1, 8, 8, 6, 1, 9, 1, 1, 8, 8, 2, 10, 8, 8, 2, 1, 8, 9, 8, 10, 5, 4, 7, 2, 8, 9, 8, 12, 10, 7, 9, 1, 2, 1, 3, 8, 1, 8, 1, 9, 8, 8, 9, 8, 1, 1, 2, 2, 1, 8, 12, 1, 2, 7, 9, 8, 1, 10, 2, 8, 1, 8, 11, 1, 1, 1, 8, 1, 8, 1, 8, 2, 3, 9, 10, 10, 1, 8, 10, 9, 9, 2, 8, 9, 8, 3, 2, 2, 8, 8, 2, 9, 1, 1, 3, 12, 8, 3, 8, 2, 1, 1, 8, 1, 8, 1, 8, 2, 1, 11, 1, 8, 3, 3, 8, 10, 2, 3, 10, 2, 8, 3, 12, 1, 1, 12, 12, 1, 7, 8, 11, 1, 3, 4, 3, 1, 8, 2, 4, 8, 2, 8, 4, 8, 9, 2, 8, 8, 6, 8, 2, 1, 1, 8, 8, 9, 5, 8, 1, 1, 2, 8, 7, 9, 1, 2, 1, 8, 8, 1, 1, 2, 8, 4, 3, 8, 1, 1, 1, 8, 8, 2, 8, 8, 8, 8, 3, 7, 1, 2, 1, 8, 9, 8, 8, 2, 1, 2, 1, 1, 2, 1, 1, 8, 1, 2, 8, 10, 10, 8, 8, 2, 8, 1, 8, 2, 8, 7, 8, 1, 8, 1, 1, 3, 1, 1, 8, 1, 8, 2, 9, 1, 1, 2, 3, 1, 1, 3, 1, 1, 8, 7, 2, 8, 12, 1, 8, 9, 2, 1, 7, 1, 8, 2, 2, 12, 1, 3, 8, 8, 1, 8, 8, 1, 1, 2, 10, 1, 9, 1, 8, 2, 8, 2, 1, 1, 2, 2, 9, 10, 8, 4, 1, 1, 9, 8, 0, 1, 10, 9, 8, 1, 2, 12, 8, 4, 8, 1, 7, 12, 12, 2, 2, 1]\n",
            "[1, 4, -1, 1, 2, 10, 3, 6, 2, 1, 8, 4, 1, 4, 1, -1, 2, 4, 1, 2, 8, -1, -1, 4, 2, 10, 2, 8, 1, 8, 8, 1, 8, 1, -1, 1, 1, 2, 2, 3, 2, 1, 10, 2, 2, 8, 1, 3, 4, 8, 3, 12, 3, 7, 2, 9, 9, -1, 9, 9, -1, 3, 6, 2, 1, 1, 2, 1, 10, 1, 3, 8, 7, 1, 2, 2, -1, 11, 1, 3, 1, 11, 10, 1, 7, -1, -1, 12, 10, 1, 2, 8, 1, 2, 1, 1, 2, 10, 8, -1, 7, 3, 11, 2, 1, 9, 7, 4, 8, 9, -1, 2, 1, 1, 10, 8, 3, 3, 9, 1, 8, 8, 7, 1, 2, 9, 3, 2, 8, 2, 9, 1, 9, 1, 9, -1, 4, 1, 1, -1, 1, 1, 10, 9, 1, -1, 8, 3, 8, 9, 2, 1, 3, 7, 4, -1, -1, 8, 8, 2, 4, 8, 4, 4, -1, 1, 4, 9, 4, 7, 3, 9, 10, 2, 8, 9, -1, 9, 12, 7, 3, 1, 9, 4, -1, -1, 9, -1, 12, 1, 1, 10, 10, 1, -1, 9, 11, 12, 1, 9, 3, -1, 1, 3, 8, 8, 8, 9, -1, 8, 8, 2, 2, 9, 1, 1, -1, 10, 2, 2, 8, 13, 4, 2, 1, 7, 3, 2, 4, 2, -1, 1, -1, -1, 8, 2, 9, 1, 7, 3, 1, 8, 8, 12, 1, 1, 7, 8, 1, -1, 9, 2, 12, -1, 9, 3, 1, -1, -1, 2, 8, 5, 1, 3, 2, 2, 2, 8, 8, 8, 2, 9, 8, 2, 7, 1, 8, 2, 2, 3, 7, 12, 2, 12, 1, 1, 9, 12, 4, 3, -1, 7, -1, 1, 9, 2, 1, 5, 1, 3, 2, 1, 2, 1, -1, 8, 2, 2, 5, 12, 10, 1, 10, 5, 1, 1, 1, 8, -1, 3, -1, 9, 2, 4, 9, 7, 1, 10, 2, 1, 7, 5, 1, 1, 4, 3, 1, 8, 2, 8, 1, 8, 7, 3, 1, 9, 6, 1, 2, 8, 1, 3, 3, 9, 1, 2, 1, 1, 9, -1, -1, 12, 3, 1, -1, 8, 4, 3, 3, 2, -1, 1, 2, 2, 2, 3, 4, -1, 1, 1, 10, -1, 8, 10, 1, 9, 10, 1, 12, 1, -1, 8, -1, 7, 9, -1, 8, 8, 8, 9, 8, 10, 1, 3, 6, 9, -1, 2, 2, 2, 10, 2, 1, -1, 1, -1, 1, 7, 5, 3, 1, 1, 10, 12, -1, 1, 2, 8, 10, 1, 2, -1, -1, 5, 8, 9, 10, 1, 8, 3, 1, -1, 4, 7, 9, 1, -1, 8, 2, 13, 8, 8, 8, -1, -1, 1, 8, 1, -1, 3, 8, 8, 7, 1, 4, 1, 2, 9, 5, 6, -1, 1, 3, 3, 2, -1, -1, 8, 1, 1, -1, -1, 10, 9, 1, -1, 8, 4, 10, 9, 1, -1, 4, 9, 2, 1, 3, 1, 2, 9, 10, 8, 1, 4, 1, 1, 4, 1, 1, 9, 10, 3, 12, 10, 1, 1, 2, 8, -1, 3, 2, 5, 2, 3, -1, 9, 1, 1, 7, 1, -1, 9, 8, -1, 8, 8, 1, 8, -1, -1, -1, 6, 1, 9, -1, 1, 8, 9, 2, -1, 8, 8, 2, 1, 8, 9, 8, 2, 5, 4, 7, 2, 9, 9, 8, 12, 9, 9, -1, 1, -1, 1, 3, 8, 1, 10, 1, 1, 8, 11, 9, 8, 1, -1, 2, 2, 1, 8, 12, -1, 2, 7, 2, 10, 1, 10, -1, 9, 1, 8, -1, 1, 3, 1, 3, 1, 8, 1, 8, 2, 3, 9, -1, 10, 1, 10, 10, 9, 9, 2, 8, 9, 9, 3, 2, 1, 8, 4, -1, 9, 1, -1, 3, 12, 8, 9, -1, -1, 1, 1, 3, 1, 8, 1, 9, 1, 1, 11, 1, 8, 3, 3, 8, 9, 2, 3, 10, 2, 3, 3, 12, 4, 1, 12, 12, 1, 7, 9, 11, 1, 3, 4, 3, 1, 8, 7, 4, -1, 2, 8, 2, 8, 4, 2, 8, -1, 6, 10, 2, 9, 1, 7, -1, 9, 5, 8, -1, 1, 2, 8, 7, 9, 1, 2, 1, 8, 8, 1, 1, -1, 8, 4, 3, 8, 1, 1, 1, 8, 8, 2, 8, 8, 9, 8, -1, 7, 1, 2, 1, 8, 9, 10, 8, 2, 4, 2, 1, 1, 2, 1, 1, 9, 1, -1, 8, 10, 10, 8, 10, 2, 8, 1, 8, -1, 9, 7, 8, 1, -1, 1, -1, -1, 1, -1, 8, 1, -1, -1, 9, 1, -1, 2, -1, 1, 1, 3, 2, 1, 8, 7, 2, 8, 8, 1, 9, 9, 2, 1, -1, 1, 8, -1, 2, 12, 1, 3, -1, 9, 1, 8, -1, 1, 1, 2, 10, 3, 9, 1, 8, -1, 9, 2, 1, 1, 2, 2, 9, -1, 2, 4, 1, 1, -1, 9, -1, 1, 7, 9, 8, 1, 2, -1, 8, 9, 8, -1, 7, 12, 12, 8, 2, 1]\n",
            "[1, 9, -1, 1, 2, 8, 3, 6, 2, 1, 8, 4, 1, -1, 1, -1, 2, -1, 1, 2, 8, -1, -1, 4, 2, 8, 2, 8, -1, 8, 8, 1, 8, 1, -1, 4, 1, 2, 2, 3, 2, 1, 9, 2, 10, 8, 1, 3, -1, 8, 3, 12, 3, 7, 2, 8, 9, -1, 9, 9, -1, 2, 6, 2, 1, 1, 2, -1, 8, 1, 3, 8, 7, 1, 2, -1, -1, 10, 1, 3, 1, 11, 10, 1, 7, -1, -1, 12, 8, -1, 2, 8, -1, 2, 1, 1, 2, 10, 8, -1, 7, 3, -1, 2, 1, 9, 7, 4, 9, 9, -1, 2, 1, 1, 10, 8, -1, 3, 9, 1, 8, 8, 7, -1, 2, 1, 3, 2, 8, 2, 9, 1, 8, 3, 9, -1, 4, 1, 1, -1, 1, 1, 10, 9, -1, -1, 8, 3, 8, 9, 2, 1, -1, 7, 4, -1, -1, 8, -1, 2, 4, 9, 4, 4, -1, 1, 8, 9, 4, 7, 3, 5, -1, 2, -1, 9, -1, 9, -1, 7, 3, 1, 9, 4, -1, -1, 8, -1, 12, -1, 1, 10, 10, -1, -1, 7, 11, 12, 1, 9, 3, -1, 1, 2, 8, 8, 8, 9, -1, 8, 8, 2, 2, 9, 1, 1, -1, 10, 2, 2, 8, 9, 4, 2, 1, 7, 3, 2, 4, 2, -1, 1, -1, -1, 8, 2, 9, 1, 7, 3, 1, 8, 8, -1, 1, 1, 7, 8, 1, -1, 9, 2, 12, -1, 9, 3, 1, -1, -1, 2, 8, 5, 1, 3, 9, 8, 2, 8, 8, 8, 2, 2, 8, 2, 7, -1, 8, 2, -1, 3, 7, 12, 2, 8, 1, 1, 9, 12, -1, 3, -1, 7, -1, 1, 8, 2, 1, 5, 1, 3, 2, 1, 2, 9, -1, 8, 2, 3, 5, 12, 7, 1, 8, 5, 1, 1, 1, 8, -1, -1, -1, 9, 11, 4, 9, 9, 1, 8, 2, 1, 3, 5, 1, 1, 4, 3, 1, 8, 2, 8, -1, 8, -1, -1, 1, 8, 8, 4, 2, 8, 1, 3, 3, 8, 1, 2, 1, 1, 9, -1, -1, 12, 3, 4, -1, 8, 4, -1, 1, 2, -1, 1, 2, 2, 2, 3, 4, -1, 1, 1, 10, -1, 8, -1, -1, 9, 10, 9, 12, 1, -1, 8, -1, 7, 3, -1, 8, 8, 8, 1, 8, 2, 1, 3, 6, 2, -1, -1, 2, 2, 10, 3, 1, -1, 1, -1, 1, 7, 5, 1, 8, 1, 8, 12, -1, 1, -1, 8, 8, 1, 2, -1, -1, 9, 8, 9, 10, 4, 12, 3, 1, -1, 4, 7, 9, 1, -1, 8, 2, 8, 3, 8, 8, -1, -1, 1, 8, -1, -1, 2, 8, 8, 7, 1, 4, 1, 2, -1, -1, 6, -1, 1, 3, 3, 2, -1, -1, 8, 8, 1, -1, -1, 9, 2, 1, -1, 8, 4, -1, 9, 1, -1, 4, 9, -1, 1, 3, 1, 2, 2, 7, -1, 1, 4, -1, 1, 4, 1, 1, 9, 7, 3, 12, 10, 1, 1, 2, 8, -1, 3, 2, 5, -1, 1, -1, 9, 1, -1, 7, 1, -1, 9, 8, -1, 8, 8, 1, 8, -1, -1, -1, 6, 1, -1, -1, 1, 8, 9, 2, -1, 8, 8, 2, 1, 8, 9, 8, 9, 5, 4, 7, 2, 8, 9, 8, 12, 8, 8, -1, 1, -1, 1, 3, 8, 1, -1, 1, 9, 8, 11, 9, 1, 1, -1, 2, 2, 1, 8, 12, -1, 2, 7, 9, 8, 1, -1, -1, 8, 1, 8, -1, 1, 3, 1, 3, 1, 8, 1, 8, 2, 3, -1, -1, 7, 1, 8, 8, 9, 2, 2, 8, 9, 8, 3, 4, -1, 8, 8, -1, 9, 1, -1, 3, 8, 8, -1, -1, -1, 1, 1, -1, 1, 8, 1, 8, 1, 1, 11, 1, 8, 3, -1, 8, 9, 2, 3, 7, 2, 8, 3, 12, 4, 1, 12, -1, 1, 7, 9, 11, 1, 3, 4, 3, 4, 8, 2, 4, -1, -1, 8, 4, 8, 4, -1, 8, -1, 6, 8, 2, 9, 1, 8, -1, 9, 5, 8, -1, 4, 2, 8, 7, 3, 1, 2, 1, 8, 8, 1, 1, -1, 8, 4, 3, 8, 1, 1, 1, 8, 8, 2, 8, 8, 9, 8, -1, 7, -1, 2, 1, 8, 9, 8, 8, 2, -1, 10, -1, 1, 2, 1, 1, -1, 1, -1, 8, 10, 10, 8, 9, 2, -1, 1, 8, -1, 8, 7, 8, 1, -1, -1, -1, -1, 1, -1, 8, 1, -1, -1, 9, -1, -1, 2, -1, 1, 1, 3, 1, 1, -1, 7, 2, 8, 12, 1, 4, 9, 2, 1, -1, 1, 8, -1, 2, -1, 1, 3, -1, 9, 1, 8, -1, 1, 1, -1, 10, 3, -1, 1, 8, -1, 10, 4, 1, 1, 2, 2, 9, -1, 2, 4, 4, 1, -1, 9, -1, 1, 7, 9, 7, 1, 2, -1, 8, 9, 8, -1, 7, 12, 12, 8, 2, 1]\n",
            "[1, 8, -1, 1, 2, 8, 3, 6, 2, 1, -1, 4, 1, -1, 1, -1, 2, -1, 1, 2, 8, -1, -1, 4, 2, 8, 2, 8, -1, 8, 8, 1, 8, -1, -1, 4, 1, 2, 2, 3, -1, 1, 8, 2, 10, 8, -1, 3, -1, 8, 3, 12, 1, 7, 2, 9, 9, -1, 9, 9, -1, 3, 6, 2, 1, -1, 2, -1, 10, 1, 3, -1, 7, 1, 2, -1, -1, 8, 1, 3, 1, -1, 10, -1, 7, -1, -1, -1, 8, -1, 2, 8, -1, -1, -1, 1, 2, 10, 8, -1, -1, 3, -1, 2, 1, -1, 7, 4, 8, 9, -1, 2, 1, 1, 10, 9, -1, 3, 9, -1, 8, -1, 7, -1, 2, 9, 3, 2, 8, 2, 9, 1, 8, -1, 9, -1, 4, 1, 1, -1, 1, 1, 10, 9, -1, -1, 8, 3, -1, 9, 2, 1, -1, 7, 4, -1, -1, -1, -1, 2, 4, 4, 4, 8, -1, -1, 8, 9, 4, 7, 3, 5, -1, 10, -1, -1, -1, 9, -1, -1, 3, -1, 9, 8, -1, -1, 8, -1, 12, -1, -1, 10, 10, -1, -1, 7, -1, 12, 1, 9, 3, -1, 1, -1, 8, 8, 8, 9, -1, 8, -1, 2, 2, 9, 1, 1, -1, 8, 2, 2, 8, 9, 4, 2, 1, 7, 3, 2, 4, 2, -1, 1, -1, -1, 8, -1, 9, -1, 7, 3, 1, 8, -1, -1, 1, 1, 7, 8, 1, -1, 9, 2, 12, -1, 9, 3, 1, -1, -1, 2, 8, 1, 1, 3, 2, 8, 2, 10, -1, 8, 2, 9, 8, -1, 7, -1, 8, 2, -1, 3, 7, 12, 2, 8, 1, 1, 9, -1, -1, 3, -1, 7, -1, 1, 8, 2, 4, 5, 1, 3, 2, 1, 2, 1, -1, 8, 2, 3, 8, 12, 7, 1, 8, 5, 1, 1, 1, 8, -1, -1, -1, 9, 2, 4, 1, -1, 1, 10, 2, 1, -1, 5, 1, 1, 1, 3, -1, 8, 2, 8, -1, 8, -1, -1, -1, 10, 8, -1, 2, 8, 1, 3, 3, 9, 1, -1, 1, 1, 9, -1, -1, 12, 3, 1, -1, 8, 4, -1, -1, 2, -1, 1, -1, 2, 2, 3, 6, -1, 1, 1, 10, -1, -1, -1, -1, 9, 8, -1, 12, 1, -1, -1, -1, 7, 9, -1, 8, 8, 8, 1, 8, 2, 1, 3, 6, 2, -1, -1, 2, 8, 10, 3, 1, -1, 1, -1, 1, 7, 5, -1, 8, 1, 10, 12, -1, 1, -1, 8, 8, 1, 2, -1, -1, 1, 8, 9, 10, 4, 12, 3, 1, -1, 4, 7, 9, 1, -1, -1, 2, 7, 3, 8, 8, -1, -1, 1, 8, -1, -1, 2, -1, 8, 7, 1, 4, 1, 2, -1, -1, 6, -1, 1, 3, 3, 4, -1, -1, 8, -1, 1, -1, -1, 9, 9, 1, -1, 9, 4, -1, -1, 1, -1, 4, 5, -1, 1, 3, 1, 2, 2, 10, -1, 1, -1, -1, 1, -1, 1, 1, 9, 10, 3, 12, 10, 1, 1, 2, 8, -1, 3, 2, 5, -1, 3, -1, 7, 1, -1, 7, 1, -1, 9, -1, -1, 8, 8, 1, 8, -1, -1, -1, 6, 1, -1, -1, 1, 8, 9, 2, -1, 4, -1, 2, 1, 8, 9, 8, 9, 5, 4, 7, 2, 9, 9, 8, -1, 9, 8, -1, 1, -1, 1, 3, 8, 1, -1, 1, -1, 8, 9, 9, 8, 1, -1, 2, -1, 1, 8, 12, -1, 4, 7, 2, 10, 1, -1, -1, 8, 1, 8, -1, 1, -1, 1, 3, 1, 1, 1, 8, 2, 3, -1, -1, 7, 1, 8, 10, 9, 9, 2, 8, 9, 3, 3, 4, -1, 8, 4, -1, 9, 1, -1, -1, 8, 8, -1, -1, -1, 1, 1, -1, 1, -1, 2, 8, -1, 1, 9, 1, 8, 3, -1, 8, 10, 2, 3, 10, 2, 8, 3, 12, 4, 1, 12, -1, 1, 7, 8, 11, 1, 3, 8, 3, 1, 8, 2, -1, -1, -1, 8, 2, -1, 4, -1, 8, -1, 6, 10, 2, 9, 1, 7, -1, 9, 5, 8, -1, 1, 2, 8, -1, 3, 1, -1, 1, 8, 8, 1, 1, -1, 8, 4, 3, 8, 1, 1, 1, 8, 8, 2, 8, 8, 9, 8, -1, 7, -1, 2, 3, -1, 9, 10, 8, 2, -1, 9, -1, 1, 2, 1, 1, -1, 1, -1, 8, 10, 10, 8, 9, 2, -1, 1, 8, -1, 9, 7, 8, -1, -1, -1, -1, -1, 1, -1, 8, 1, -1, -1, 10, -1, -1, 2, -1, 1, 1, 3, 2, 4, -1, 7, -1, -1, 12, -1, -1, 8, 2, -1, -1, 1, 8, -1, 2, -1, 1, 3, -1, 9, -1, 8, -1, 1, 1, -1, 10, 3, -1, 1, 10, -1, 9, 2, 1, 1, 2, 2, -1, -1, 2, 4, 4, 1, -1, 9, -1, 1, 3, 9, 8, 1, 2, -1, 8, 4, 8, -1, 7, 12, 12, 8, -1, 1]\n",
            "[1, 5, -1, 1, 2, 9, 3, 6, -1, 1, -1, 4, 1, -1, 1, -1, 2, -1, 1, 2, -1, -1, -1, 4, 2, 8, 2, 8, -1, 8, 8, 1, 8, -1, -1, 4, 1, 2, 2, 3, -1, 1, 8, -1, 10, 8, -1, 3, -1, -1, 3, 12, 1, 7, 2, -1, -1, -1, 9, 9, -1, 3, 6, 2, 1, -1, 8, -1, -1, -1, 3, -1, -1, 1, 2, -1, -1, -1, 1, 3, 1, -1, 10, -1, 7, -1, -1, -1, -1, -1, 2, 8, -1, -1, -1, 1, -1, 10, 8, -1, -1, 3, -1, 2, 1, -1, -1, 4, 9, 9, -1, 2, 1, 1, 8, 9, -1, 3, 2, -1, 8, -1, -1, -1, 2, 1, 3, 2, 8, 2, 9, 1, 8, -1, 9, -1, 1, 1, 1, -1, -1, 1, 10, 9, -1, -1, 8, 3, -1, 9, 2, 1, -1, -1, -1, -1, -1, -1, -1, 2, -1, 8, 4, -1, -1, -1, 8, 9, 4, 7, 3, 5, -1, 2, -1, -1, -1, 9, -1, -1, 3, -1, 9, 8, -1, -1, 8, -1, -1, -1, -1, 8, 10, -1, -1, -1, -1, 12, 1, 1, 3, -1, 1, -1, -1, -1, 8, 9, -1, 8, -1, 2, 2, 9, 1, 1, -1, 8, 2, 2, 8, 2, 4, 2, 1, 7, 3, -1, -1, 2, -1, 1, -1, -1, 9, -1, 9, -1, 8, 3, 1, 8, -1, -1, 1, 1, 7, 8, 1, -1, 9, 2, 12, -1, 9, 3, -1, -1, -1, 2, 8, 5, 1, 3, 6, -1, 2, 8, -1, 8, -1, 2, 8, -1, 7, -1, 8, 2, -1, -1, 7, 1, 2, 12, 1, -1, 9, -1, -1, 3, -1, 7, -1, 1, 8, 2, 1, 5, 1, 3, -1, 1, 2, 1, -1, 8, -1, 2, 8, 12, 7, 1, -1, 5, 1, 1, -1, 8, -1, -1, -1, 2, 2, -1, 1, -1, -1, 8, 2, -1, -1, 5, 1, 1, -1, -1, -1, 8, 2, 8, -1, 8, -1, -1, -1, 10, 2, -1, 2, 8, 1, 3, 3, 8, 1, -1, 1, 1, 9, -1, -1, 12, 3, 1, -1, 8, -1, -1, -1, 2, -1, 1, -1, 2, 2, 3, 4, -1, 1, 1, 10, -1, -1, -1, -1, 9, 10, -1, 12, 1, -1, -1, -1, 7, 3, -1, 8, 8, 8, 1, 8, 2, 1, 3, 6, 2, -1, -1, 2, 2, 10, 2, 1, -1, 1, -1, 1, 7, 5, -1, 1, 1, 8, 12, -1, 1, -1, 8, 8, 1, 2, -1, -1, 5, 8, 9, 8, 1, 12, -1, 1, -1, 4, 7, 9, 1, -1, -1, -1, 9, 3, 8, 8, -1, -1, 1, -1, -1, -1, 6, -1, 8, 7, 1, 4, -1, 2, -1, -1, 6, -1, -1, 3, 3, 2, -1, -1, 8, -1, 1, -1, -1, 2, 8, 1, -1, 8, 4, -1, -1, 1, -1, 1, 5, -1, 1, 3, 1, 2, -1, 10, -1, 1, -1, -1, 1, -1, 1, -1, 9, 10, 3, 12, 10, 1, 1, 2, -1, -1, 3, 2, 5, -1, 1, -1, 9, 1, -1, 7, 1, -1, 9, -1, -1, 8, 8, -1, -1, -1, -1, -1, 6, 1, -1, -1, 1, 8, 9, 2, -1, 8, -1, 2, -1, 8, 9, 8, 10, 5, 4, 7, 2, 8, 9, 8, -1, -1, 8, -1, 1, -1, 1, 3, 8, 1, -1, 1, -1, 8, 8, 9, 8, 1, -1, 2, -1, 5, 8, 12, -1, 2, -1, 2, 10, 1, -1, -1, 9, 1, 8, -1, -1, -1, 1, 3, 1, 8, 1, 8, 2, 3, -1, -1, -1, -1, 8, 10, 9, 9, 2, 8, -1, 3, 3, 2, -1, 8, 8, -1, 1, -1, -1, -1, 8, 8, -1, -1, -1, 1, 1, -1, -1, -1, 2, 8, -1, 1, -1, 1, 8, 3, -1, 8, 10, 2, 3, 10, -1, 8, 3, 12, 1, 1, -1, -1, 1, 7, 8, 11, 1, 3, 2, 3, 1, 8, 7, -1, -1, -1, 8, 4, -1, 4, -1, 8, -1, 6, -1, 2, -1, 1, 8, -1, -1, 5, 8, -1, 4, 2, 8, -1, 3, 1, -1, 1, -1, 8, 1, 1, -1, 8, 4, 3, 9, 1, 1, 1, 3, 8, 2, 8, 8, 9, 8, -1, -1, -1, 2, 1, -1, 9, 8, 8, -1, -1, 2, -1, 1, 2, 1, 1, -1, 1, -1, 8, 10, 10, 8, 8, 2, -1, 1, 8, -1, 8, -1, 8, -1, -1, -1, -1, -1, 1, -1, -1, 1, -1, -1, 9, -1, -1, 2, -1, 1, -1, 3, 1, -1, -1, 7, -1, -1, 8, -1, -1, 8, 2, -1, -1, 1, -1, -1, 2, -1, 1, 3, -1, -1, -1, 8, -1, 1, 1, -1, 10, 3, -1, -1, 10, -1, 9, 2, 1, 1, -1, 2, -1, -1, 2, 4, 1, 1, -1, 9, -1, 1, 10, 9, 8, 1, 2, -1, 8, 4, 8, -1, 7, 12, 12, -1, -1, -1]\n",
            "[1, 9, -1, 1, 2, 8, 3, 6, -1, 1, -1, 4, 1, -1, 1, -1, 2, -1, 1, 2, -1, -1, -1, 4, 2, 8, 2, 8, -1, 8, 8, 1, 8, -1, -1, -1, -1, 2, 2, 3, -1, 1, 10, -1, -1, -1, -1, 3, -1, -1, 3, 12, 1, 7, 2, -1, -1, -1, 9, 9, -1, 2, -1, 2, 1, -1, 2, -1, -1, -1, 3, -1, -1, 1, 2, -1, -1, -1, -1, 3, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 2, 8, -1, -1, -1, 1, -1, 10, 8, -1, -1, -1, -1, 2, 1, -1, -1, 4, 9, -1, -1, 2, 1, 1, 10, 9, -1, 3, -1, -1, 8, -1, -1, -1, 2, 9, 3, -1, 8, 2, 9, 1, -1, -1, 9, -1, 1, 1, 1, -1, -1, -1, 10, 9, -1, -1, 8, 3, -1, 9, 2, 1, -1, -1, -1, -1, -1, -1, -1, 2, -1, 8, -1, -1, -1, -1, 8, 9, 4, 7, 3, 5, -1, 2, -1, -1, -1, 9, -1, -1, 3, -1, 9, -1, -1, -1, 8, -1, -1, -1, -1, 10, 8, -1, -1, -1, -1, -1, 1, -1, 3, -1, 1, -1, -1, -1, -1, 9, -1, 8, -1, -1, 2, 9, 1, 1, -1, 8, -1, 2, 8, 9, 8, 2, -1, 7, 3, -1, -1, 2, -1, 1, -1, -1, 8, -1, -1, -1, -1, 3, 1, 8, -1, -1, 1, -1, 7, 8, 1, -1, 9, 2, 12, -1, -1, 3, -1, -1, -1, 2, 8, 5, 1, 3, 9, -1, 2, 8, -1, -1, -1, 2, 8, -1, -1, -1, 8, 2, -1, -1, -1, 1, 2, 8, 1, -1, 9, -1, -1, 3, -1, 7, -1, -1, 9, 2, 1, 5, -1, -1, -1, 1, 1, 9, -1, 8, -1, 2, 8, -1, -1, 1, -1, 5, 1, 1, -1, 8, -1, -1, -1, 9, 2, -1, 1, -1, -1, 8, 2, -1, -1, 5, 1, 1, -1, -1, -1, 8, 2, 8, -1, 8, -1, -1, -1, -1, 9, -1, -1, -1, -1, 3, 9, 8, 1, -1, 1, 1, 9, -1, -1, 12, 3, -1, -1, 8, -1, -1, -1, 2, -1, 1, -1, -1, -1, 3, 4, -1, 1, 1, 10, -1, -1, -1, -1, 9, 10, -1, 12, 1, -1, -1, -1, 7, -1, -1, 8, 8, -1, 1, 8, 2, -1, 3, 6, 2, -1, -1, 2, 8, 10, 3, 1, -1, 1, -1, 1, -1, 5, -1, 1, 1, 10, 12, -1, -1, -1, 8, 8, -1, 2, -1, -1, 5, -1, 9, 10, 1, 12, -1, 1, -1, 8, -1, -1, -1, -1, -1, -1, 9, 9, 8, 8, -1, -1, 1, -1, -1, -1, 2, -1, -1, 7, -1, 4, -1, 2, -1, -1, 6, -1, -1, 3, 3, 2, -1, -1, 8, -1, 1, -1, -1, 9, 9, 1, -1, -1, 4, -1, -1, 1, -1, 1, 9, -1, 1, -1, 1, 2, -1, 10, -1, 1, -1, -1, 1, -1, 1, -1, 9, 10, 3, 12, 10, 1, 1, 2, -1, -1, 3, 2, 5, -1, 1, -1, 9, 1, -1, 8, -1, -1, 9, -1, -1, 8, 8, -1, -1, -1, -1, -1, 6, 1, -1, -1, 1, 8, 9, 2, -1, 8, -1, 2, -1, -1, -1, 8, 2, 5, 4, 7, 2, 8, -1, 8, -1, -1, 8, -1, 1, -1, 2, 3, 8, 1, -1, 1, -1, 8, 9, 9, 8, 1, -1, 2, -1, 1, 10, 12, -1, 2, -1, 2, 10, 1, -1, -1, 8, 1, 8, -1, -1, -1, 1, 8, 1, 9, 1, 8, 2, 3, -1, -1, -1, -1, 8, -1, -1, -1, 2, 8, -1, 9, 3, -1, -1, 8, 4, -1, 9, -1, -1, -1, 10, 8, -1, -1, -1, 1, 1, -1, -1, -1, 1, 9, -1, 1, -1, 1, 8, 3, -1, 8, 10, 2, 3, 10, -1, 8, 3, 12, 1, 1, -1, -1, 1, 7, 8, 9, 1, 3, -1, 3, 1, 8, 7, -1, -1, -1, -1, -1, -1, 1, -1, 8, -1, 6, -1, 2, -1, 1, -1, -1, -1, 5, 8, -1, -1, 2, 8, -1, 9, 1, -1, 1, -1, 9, 1, 1, -1, 8, 4, 3, 9, 1, 1, 1, 8, 8, 2, 8, -1, 9, 8, -1, -1, -1, 2, 1, -1, 9, -1, 8, -1, -1, 2, -1, 1, 1, 1, 1, -1, 1, -1, 8, 10, -1, 8, 9, 2, -1, -1, 8, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, 1, -1, -1, 9, -1, -1, 2, -1, 1, -1, 9, 2, -1, -1, 7, -1, -1, -1, -1, -1, 9, 2, -1, -1, 1, -1, -1, 2, -1, 1, 3, -1, -1, -1, 8, -1, 1, 1, -1, 10, 3, -1, -1, 10, -1, 9, 9, 1, 1, -1, 2, -1, -1, 2, 2, -1, -1, -1, 9, -1, 1, 10, 9, 7, 1, 9, -1, 8, 9, 8, -1, 7, -1, 12, -1, -1, -1]\n",
            "0.7624703087885986\n",
            "[1, 9, 3, 1, 2, 8, 3, 6, 2, 1, 8, 4, 1, 4, 1, 7, 2, 4, 1, 2, 8, 9, 9, 4, 2, 8, 2, 8, 1, 8, 12, 1, 8, 1, 8, 1, 1, 2, 2, 3, 9, 1, 10, 2, 10, 8, 1, 3, 4, 8, 3, 12, 3, 7, 2, 8, 9, 1, 9, 9, 3, 3, 6, 8, 1, 1, 8, 1, 10, 1, 3, 8, 7, 1, 2, 2, 9, 10, 1, 3, 1, 11, 10, 1, 7, 9, 9, 12, 8, 1, 2, 8, 1, 2, 1, 1, 2, 10, 8, 2, 7, 3, 11, 2, 1, 9, 7, 4, 8, 9, 6, 2, 1, 1, 10, 9, 3, 3, 9, 1, 8, 8, 7, 1, 2, 9, 3, 2, 8, 2, 9, 1, 8, 1, 9, 8, 4, 1, 1, 2, 1, 1, 8, 8, 1, 9, 8, 3, 8, 9, 2, 1, 3, 7, 4, 10, 3, 8, 8, 2, 1, 8, 4, 4, 12, 1, 8, 9, 4, 7, 3, 5, 10, 2, 8, 8, 3, 9, 12, 7, 3, 1, 8, 4, 8, 8, 8, 9, 12, 1, 1, 8, 8, 1, 2, 8, 11, 12, 1, 9, 3, 9, 1, 3, 8, 2, 8, 9, 9, 8, 8, 2, 2, 9, 1, 1, 2, 10, 2, 2, 8, 9, 8, 2, 1, 7, 3, 2, 1, 2, 2, 1, 9, 1, 8, 2, 9, 1, 2, 3, 1, 8, 8, 12, 1, 1, 7, 8, 1, 2, 2, 2, 12, 1, 9, 3, 1, 1, 9, 2, 8, 5, 1, 3, 2, 2, 2, 8, 8, 8, 2, 2, 8, 2, 7, 1, 8, 2, 2, 3, 7, 12, 2, 12, 1, 1, 9, 12, 4, 3, 2, 7, 3, 1, 8, 2, 1, 5, 1, 3, 2, 1, 2, 1, 8, 8, 2, 2, 8, 12, 10, 1, 8, 5, 1, 1, 1, 8, 9, 3, 9, 1, 4, 4, 1, 8, 1, 8, 2, 1, 3, 5, 1, 1, 1, 3, 1, 8, 2, 8, 1, 8, 7, 1, 1, 8, 8, 1, 2, 8, 1, 3, 8, 8, 1, 2, 1, 1, 9, 8, 1, 12, 3, 1, 1, 8, 4, 3, 8, 2, 9, 1, 2, 2, 2, 3, 4, 9, 1, 1, 10, 12, 8, 8, 1, 9, 8, 9, 12, 1, 9, 8, 8, 7, 2, 8, 8, 8, 8, 1, 8, 10, 1, 3, 6, 9, 3, 2, 2, 2, 10, 2, 1, 3, 1, 9, 1, 7, 5, 1, 1, 2, 8, 12, 1, 1, 2, 8, 8, 1, 2, 3, 2, 1, 8, 9, 8, 1, 12, 3, 1, 2, 8, 7, 9, 1, 2, 8, 2, 8, 8, 8, 8, 2, 8, 1, 8, 1, 1, 2, 8, 8, 7, 1, 4, 1, 2, 2, 5, 6, 12, 1, 3, 3, 2, 3, 1, 8, 8, 1, 9, 3, 8, 8, 1, 9, 8, 4, 8, 10, 1, 3, 1, 5, 2, 1, 3, 1, 2, 9, 10, 8, 1, 8, 1, 1, 4, 1, 1, 9, 10, 3, 12, 10, 1, 1, 2, 8, 2, 3, 2, 5, 2, 1, 3, 9, 1, 1, 8, 1, 1, 8, 8, 2, 8, 8, 1, 8, 1, 8, 8, 6, 1, 9, 1, 1, 8, 8, 2, 10, 8, 8, 2, 1, 8, 9, 8, 10, 5, 4, 7, 2, 8, 9, 8, 12, 10, 7, 9, 1, 2, 1, 3, 8, 1, 8, 1, 9, 8, 8, 9, 8, 1, 1, 2, 2, 1, 8, 12, 1, 2, 7, 9, 8, 1, 10, 2, 8, 1, 8, 11, 1, 1, 1, 8, 1, 8, 1, 8, 2, 3, 9, 10, 10, 1, 8, 10, 9, 9, 2, 8, 9, 8, 3, 2, 2, 8, 8, 2, 9, 1, 1, 3, 12, 8, 3, 8, 2, 1, 1, 8, 1, 8, 1, 8, 2, 1, 11, 1, 8, 3, 3, 8, 10, 2, 3, 10, 2, 8, 3, 12, 1, 1, 12, 12, 1, 7, 8, 11, 1, 3, 4, 3, 1, 8, 2, 4, 8, 2, 8, 4, 8, 9, 2, 8, 8, 6, 8, 2, 1, 1, 8, 8, 9, 5, 8, 1, 1, 2, 8, 7, 9, 1, 2, 1, 8, 8, 1, 1, 2, 8, 4, 3, 8, 1, 1, 1, 8, 8, 2, 8, 8, 8, 8, 3, 7, 1, 2, 1, 8, 9, 8, 8, 2, 1, 2, 1, 1, 2, 1, 1, 8, 1, 2, 8, 10, 10, 8, 8, 2, 8, 1, 8, 2, 8, 7, 8, 1, 8, 1, 1, 3, 1, 1, 8, 1, 8, 2, 9, 1, 1, 2, 3, 1, 1, 3, 1, 1, 8, 7, 2, 8, 12, 1, 8, 9, 2, 1, 7, 1, 8, 2, 2, 12, 1, 3, 8, 8, 1, 8, 8, 1, 1, 2, 10, 1, 9, 1, 8, 2, 8, 2, 1, 1, 2, 2, 9, 10, 8, 4, 1, 1, 9, 8, 0, 1, 10, 9, 8, 1, 2, 12, 8, 4, 8, 1, 7, 12, 12, 2, 2, 1]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "final_pred_2=[]\n",
        "\n",
        "num_correct=0\n",
        "model_0_2=load_model(loss,accuracy,optimizer,'0-2')\n",
        "mod_labels_0=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_0:\n",
        "    pred_test_0_2=model_0_2.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_0_2 = pred_test_0_2.argmax(axis=1)\n",
        "    mod_labels_0.append(pred_labels_0_2[0])\n",
        "  else:\n",
        "    mod_labels_0.append(-1)\n",
        "\n",
        "model_1_2=load_model(loss,accuracy,optimizer,'1-2')\n",
        "mod_labels_1=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        " \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_1:\n",
        "    pred_test_1_2=model_1_2.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_1_2 = pred_test_1_2.argmax(axis=1)\n",
        "    mod_labels_1.append(pred_labels_1_2[0])\n",
        "  else:\n",
        "    mod_labels_1.append(-1)\n",
        "\n",
        "model_2_2=load_model(loss,accuracy,optimizer,'2-2')\n",
        "mod_labels_2=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_2:\n",
        "    pred_test_2_2=model_2_2.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_2_2 = pred_test_2_2.argmax(axis=1)\n",
        "    mod_labels_2.append(pred_labels_2_2[0])\n",
        "  else:\n",
        "    mod_labels_2.append(-1)\n",
        "\n",
        "model_3_2=load_model(loss,accuracy,optimizer,'3-2')\n",
        "mod_labels_3=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_3:\n",
        "    pred_test_3_2=model_3_2.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_3_2 = pred_test_3_2.argmax(axis=1)\n",
        "    mod_labels_3.append(pred_labels_3_2[0])\n",
        "  else:\n",
        "    mod_labels_3.append(-1)\n",
        "\n",
        "model_4_2=load_model(loss,accuracy,optimizer,'4-2')\n",
        "mod_labels_4=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_4:\n",
        "    pred_test_4_2=model_4_2.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_4_2 = pred_test_4_2.argmax(axis=1)\n",
        "    mod_labels_4.append(pred_labels_4_2[0])\n",
        "  else:\n",
        "    mod_labels_4.append(-1)\n",
        "\n",
        "model_5_2=load_model(loss,accuracy,optimizer,'5-2')\n",
        "mod_labels_5=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_5:\n",
        "    pred_test_5_2=model_5_2.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_5_2 = pred_test_5_2.argmax(axis=1)\n",
        "    mod_labels_5.append(pred_labels_5_2[0])\n",
        "  else:\n",
        "    mod_labels_5.append(-1)\n",
        "\n",
        "print(mod_labels_0)\n",
        "print(mod_labels_1)\n",
        "print(mod_labels_2)\n",
        "print(mod_labels_3)\n",
        "print(mod_labels_4)\n",
        "print(mod_labels_5)\n",
        "\n",
        "\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  fin_labels=[]\n",
        "  if mod_labels_0[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  if mod_labels_1[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  if mod_labels_2[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  if mod_labels_3[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  if mod_labels_4[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  if mod_labels_5[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  res=max(set(fin_labels), key = fin_labels.count)\n",
        "  final_pred_2.append(res)\n",
        "\n",
        "  if res==new_val_label_0[i]:\n",
        "    num_correct=num_correct+1\n",
        "\n",
        "accuracy=num_correct/len(new_val_label_0)\n",
        "print(accuracy)\n",
        "  \n",
        "print(final_pred_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snuZ1OsZn3cC",
        "outputId": "2e194653-f050-40e8-e7b0-decb75ac21a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.7624703087885986\n",
            "Weighted F1: 0.7558265061959636\n",
            "Micro F1: 0.7624703087885987\n",
            "Weighted Precision: 0.7640838452701672\n",
            "Micro Precision: 0.7624703087885986\n",
            "Weighted Recall: 0.7624703087885986\n",
            "Micro Recall: 0.7624703087885986\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "\n",
        "accuracy=accuracy_score(new_val_label_0, final_pred_2)\n",
        "print(\"Accuracy: \"+str(accuracy))\n",
        "total_accuracy=total_accuracy+accuracy\n",
        "\n",
        "weighted_f1=f1_score(new_val_label_0,final_pred_2, average='weighted')\n",
        "print(\"Weighted F1: \"+ str(weighted_f1))\n",
        "total_weighted_f1=total_weighted_f1+weighted_f1\n",
        "micro_f1=f1_score(new_val_label_0,final_pred_2, average='micro')\n",
        "print(\"Micro F1: \"+ str(micro_f1))\n",
        "total_micro_f1=total_micro_f1+micro_f1\n",
        "\n",
        "weighted_precision=precision_score(new_val_label_0, final_pred_2, average='weighted')\n",
        "print(\"Weighted Precision: \" + str(weighted_precision))\n",
        "total_weighted_precision=total_weighted_precision+weighted_precision\n",
        "micro_precision=precision_score(new_val_label_0, final_pred_2, average='micro')\n",
        "print(\"Micro Precision: \" + str(micro_precision))\n",
        "total_micro_precision=total_micro_precision+micro_precision\n",
        "\n",
        "weighted_recall=recall_score(new_val_label_0, final_pred_2, average='weighted')\n",
        "print(\"Weighted Recall: \" + str(weighted_recall))\n",
        "total_weighted_recall=total_weighted_recall+weighted_recall\n",
        "micro_recall=recall_score(new_val_label_0, final_pred_2, average='micro')\n",
        "print(\"Micro Recall: \" + str(micro_recall))\n",
        "total_micro_recall=total_micro_recall+micro_recall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Z-lAlHvoBj6",
        "outputId": "78fc54ef-b018-4d7e-b79e-da3a6d35b627"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_20\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_41 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_42 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_41[0][0]',               \n",
            " el)                            thPoolingAndCrossAt               'input_42[0][0]']               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_20 (S  (None, 768)         0           ['tf_roberta_model[20][0]']      \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " dense_40 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_20[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dropout_57 (Dropout)           (None, 512)          0           ['dense_40[0][0]']               \n",
            "                                                                                                  \n",
            " dense_41 (Dense)               (None, 15)           7695        ['dropout_57[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model: \"model_21\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_43 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_44 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_43[0][0]',               \n",
            " el)                            thPoolingAndCrossAt               'input_44[0][0]']               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_21 (S  (None, 768)         0           ['tf_roberta_model[21][0]']      \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " dense_42 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_21[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dropout_58 (Dropout)           (None, 512)          0           ['dense_42[0][0]']               \n",
            "                                                                                                  \n",
            " dense_43 (Dense)               (None, 15)           7695        ['dropout_58[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model: \"model_22\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_45 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_46 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_45[0][0]',               \n",
            " el)                            thPoolingAndCrossAt               'input_46[0][0]']               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_22 (S  (None, 768)         0           ['tf_roberta_model[22][0]']      \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " dense_44 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_22[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dropout_59 (Dropout)           (None, 512)          0           ['dense_44[0][0]']               \n",
            "                                                                                                  \n",
            " dense_45 (Dense)               (None, 15)           7695        ['dropout_59[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model: \"model_23\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_47 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_48 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_47[0][0]',               \n",
            " el)                            thPoolingAndCrossAt               'input_48[0][0]']               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_23 (S  (None, 768)         0           ['tf_roberta_model[23][0]']      \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " dense_46 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_23[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dropout_60 (Dropout)           (None, 512)          0           ['dense_46[0][0]']               \n",
            "                                                                                                  \n",
            " dense_47 (Dense)               (None, 15)           7695        ['dropout_60[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model: \"model_24\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_49 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_50 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_49[0][0]',               \n",
            " el)                            thPoolingAndCrossAt               'input_50[0][0]']               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_24 (S  (None, 768)         0           ['tf_roberta_model[24][0]']      \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " dense_48 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_24[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dropout_61 (Dropout)           (None, 512)          0           ['dense_48[0][0]']               \n",
            "                                                                                                  \n",
            " dense_49 (Dense)               (None, 15)           7695        ['dropout_61[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model: \"model_25\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_51 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_52 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_51[0][0]',               \n",
            " el)                            thPoolingAndCrossAt               'input_52[0][0]']               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_25 (S  (None, 768)         0           ['tf_roberta_model[25][0]']      \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " dense_50 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_25[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dropout_62 (Dropout)           (None, 512)          0           ['dense_50[0][0]']               \n",
            "                                                                                                  \n",
            " dense_51 (Dense)               (None, 15)           7695        ['dropout_62[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "[1, 4, 3, 1, 2, 9, 3, 6, 2, 1, 8, 4, 1, 4, 1, 7, 2, 4, 9, 2, 8, 9, 9, 4, 2, 10, 2, 8, 1, 8, 8, 1, 8, 1, 9, 4, 1, 2, 2, 3, 2, 1, 10, 2, 10, 8, 1, 3, 4, 8, 3, 12, 1, 7, 2, 8, 9, 1, 9, 9, 3, 2, 6, 2, 1, 1, 8, 1, 10, 1, 3, 8, 7, 1, 2, 2, 9, 8, 1, 3, 1, 11, 10, 1, 7, 9, 9, 12, 8, 1, 2, 8, 9, 2, 1, 1, 2, 10, 8, 2, 7, 3, 11, 2, 1, 9, 10, 4, 8, 9, 6, 2, 1, 1, 9, 9, 3, 3, 9, 1, 8, 8, 7, 1, 2, 9, 3, 2, 8, 2, 9, 1, 8, 1, 9, 8, 4, 1, 1, 2, 1, 1, 10, 9, 1, 9, 8, 3, 8, 9, 2, 1, 3, 7, 4, 0, 3, 10, 8, 2, 9, 8, 4, 4, 12, 1, 8, 9, 4, 7, 3, 5, 10, 10, 8, 8, 3, 9, 12, 7, 3, 1, 9, 8, 10, 8, 8, 9, 12, 1, 1, 10, 10, 1, 2, 9, 11, 12, 1, 9, 3, 9, 1, 3, 8, 8, 8, 9, 9, 8, 8, 2, 2, 9, 1, 1, 2, 10, 2, 2, 8, 9, 4, 2, 1, 7, 3, 2, 4, 2, 2, 1, 9, 1, 9, 2, 9, 1, 7, 3, 1, 8, 8, 12, 1, 1, 7, 8, 1, 2, 9, 2, 3, 1, 9, 3, 1, 1, 9, 2, 8, 5, 1, 3, 9, 8, 2, 8, 8, 8, 2, 9, 8, 2, 7, 1, 8, 2, 2, 3, 7, 12, 2, 12, 1, 1, 9, 12, 4, 3, 2, 7, 3, 1, 8, 2, 1, 5, 1, 3, 9, 1, 2, 10, 8, 8, 2, 3, 5, 12, 10, 1, 8, 5, 1, 1, 1, 8, 9, 3, 9, 9, 2, 4, 1, 7, 1, 8, 10, 1, 7, 5, 1, 1, 1, 3, 1, 8, 2, 8, 1, 8, 7, 1, 1, 9, 8, 1, 2, 8, 1, 3, 8, 9, 1, 2, 1, 1, 9, 8, 1, 12, 3, 4, 1, 8, 4, 3, 3, 2, 9, 1, 2, 2, 10, 3, 6, 9, 1, 1, 10, 8, 8, 10, 1, 9, 10, 9, 12, 1, 9, 8, 8, 7, 9, 8, 8, 8, 8, 1, 8, 10, 1, 3, 6, 9, 3, 2, 2, 2, 10, 3, 1, 9, 1, 9, 1, 7, 5, 4, 1, 2, 8, 12, 1, 1, 2, 8, 8, 1, 2, 2, 9, 5, 8, 8, 10, 1, 8, 3, 1, 2, 8, 7, 9, 1, 2, 8, 3, 9, 8, 8, 8, 2, 8, 1, 8, 1, 1, 6, 8, 8, 7, 1, 4, 1, 2, 9, 5, 6, 12, 1, 3, 3, 2, 3, 1, 8, 1, 1, 9, 3, 10, 9, 1, 9, 8, 9, 8, 10, 1, 3, 4, 5, 2, 1, 3, 1, 2, 7, 10, 8, 1, 8, 1, 1, 4, 1, 9, 9, 10, 3, 12, 10, 1, 1, 2, 8, 2, 3, 2, 5, 8, 1, 3, 9, 1, 1, 10, 1, 1, 9, 8, 2, 8, 8, 1, 8, 1, 8, 8, 6, 1, 9, 1, 1, 8, 9, 2, 10, 8, 8, 2, 1, 8, 9, 8, 9, 5, 4, 7, 2, 8, 9, 8, 12, 8, 10, 9, 1, 3, 1, 3, 8, 1, 8, 1, 9, 8, 10, 9, 8, 1, 1, 2, 2, 1, 8, 12, 2, 2, 7, 9, 10, 1, 10, 9, 9, 1, 8, 11, 1, 3, 1, 8, 1, 1, 1, 8, 2, 3, 9, 10, 9, 1, 8, 10, 9, 9, 2, 8, 9, 3, 3, 2, 1, 8, 8, 2, 9, 1, 1, 3, 12, 8, 9, 8, 2, 1, 1, 3, 1, 8, 1, 8, 2, 1, 9, 1, 8, 3, 3, 8, 9, 2, 3, 10, 3, 8, 3, 12, 4, 1, 12, 12, 1, 7, 8, 11, 1, 3, 4, 3, 1, 8, 7, 4, 8, 2, 8, 4, 8, 9, 2, 8, 8, 6, 10, 2, 9, 1, 8, 8, 9, 5, 8, 1, 1, 2, 8, 7, 9, 1, 2, 1, 8, 8, 1, 1, 1, 8, 4, 3, 8, 1, 1, 1, 8, 8, 2, 8, 8, 8, 8, 3, 7, 1, 2, 1, 8, 9, 8, 8, 2, 4, 9, 1, 1, 2, 1, 1, 8, 1, 2, 8, 10, 10, 8, 8, 2, 9, 1, 8, 9, 8, 7, 8, 1, 8, 1, 1, 3, 1, 1, 8, 1, 8, 2, 9, 1, 1, 2, 3, 9, 1, 3, 2, 1, 8, 7, 2, 8, 8, 1, 9, 9, 2, 1, 7, 1, 8, 2, 2, 12, 1, 3, 8, 8, 1, 8, 8, 1, 1, 2, 10, 1, 9, 1, 8, 2, 9, 2, 1, 1, 2, 2, 9, 10, 2, 9, 1, 1, 9, 9, 2, 1, 7, 9, 8, 1, 2, 12, 8, 9, 8, 1, 7, 12, 12, 8, 2, 1]\n",
            "[1, 5, -1, 1, 2, 8, 3, 6, 2, 1, 8, 4, 1, 4, 1, -1, 2, 4, 1, 2, 8, -1, -1, 4, 2, 10, 2, 8, 1, 8, 8, 1, 8, 1, -1, 4, 1, 8, 2, 3, 2, 1, 10, 2, 10, 8, 1, 3, 4, 8, 3, 12, 1, 7, 2, 9, 9, -1, 9, 9, -1, 3, 6, 2, 1, 1, 2, 1, 10, 1, 3, 8, 7, 1, 2, 2, -1, 10, 1, 3, 1, 11, 10, 1, 7, -1, -1, 12, 10, 1, 2, 8, 9, 2, 1, 1, 2, 10, 8, -1, 7, 3, 11, 9, 1, 9, 7, 4, 9, 9, -1, 2, 1, 1, 10, 9, 3, 3, 9, 1, 8, 8, 7, 1, 2, 9, 3, 2, 8, 9, 9, 1, 8, 1, 9, -1, 1, 1, 1, -1, 1, 1, 10, 9, 1, -1, 8, 3, 8, 9, 2, 1, 3, 7, 4, -1, -1, 8, 8, 2, 4, 9, 4, 4, -1, 1, 4, 8, 4, 7, 3, 9, 10, 10, 8, 9, -1, 9, 9, 7, 3, 1, 9, 8, -1, -1, 9, -1, 12, 1, 1, 10, 10, 1, -1, 9, 10, 12, 1, 9, 3, -1, 1, 2, 8, 8, 8, 9, -1, 8, 8, 2, 2, 9, 1, 1, -1, 10, 2, 2, 8, 13, 4, 2, 1, 7, 3, 2, 4, 2, -1, 1, -1, -1, 8, 9, 9, 1, 7, 3, 1, 9, 8, 12, 1, 1, 7, 8, 1, -1, 9, 2, 3, -1, 9, 3, 1, -1, -1, 2, 8, 5, 1, 3, 9, 2, 2, 8, 8, 8, 2, 9, 8, 2, 7, 1, 8, 2, 2, 3, 7, 1, 2, 8, 1, 1, 9, 12, 1, 3, -1, 7, -1, 1, 8, 2, 1, 5, 1, 3, 9, 1, 2, 13, -1, 8, 2, 2, 8, 12, 10, 1, 8, 5, 1, 1, 1, 8, -1, 3, -1, 9, 10, 4, 9, 9, 1, 8, 2, 1, 9, 5, 1, 1, 1, 3, 1, 8, 2, 8, 1, 8, 7, 3, 1, 9, 6, 1, 2, 8, 1, 3, 3, 9, 1, 2, 1, 1, 9, -1, -1, 12, 3, 1, -1, 8, 4, 3, 1, 2, -1, 1, 2, 2, 2, 3, 4, -1, 1, 1, 10, -1, 8, 10, 1, 9, 10, 1, 12, 1, -1, 8, -1, 7, 9, -1, 8, 8, 8, 1, 9, 10, 1, 3, 6, 9, -1, 2, 2, 2, 10, 2, 1, -1, 1, -1, 1, 7, 5, 4, 1, 1, 10, 12, -1, 1, 2, 8, 8, 1, 2, -1, -1, 1, 8, 9, 8, 1, 8, 3, 1, -1, 4, 7, 9, 1, -1, 8, 3, 7, 8, 8, 8, -1, -1, 1, 8, 1, -1, 3, 8, 8, 7, 1, 4, 1, 2, 9, 5, 6, -1, 1, 3, 3, 2, -1, -1, 8, 1, 1, -1, -1, 13, 9, 1, -1, 9, 4, 8, 9, 1, -1, 1, 9, 2, 1, 3, 1, 2, 2, 10, 8, 1, 4, 1, 1, 4, 1, 1, 9, 10, 3, 12, 10, 1, 1, 2, 8, -1, 3, 2, 5, 8, 1, -1, 9, 1, 1, 8, 1, -1, 9, 8, -1, 8, 8, 1, 8, -1, -1, -1, 6, 1, 9, -1, 1, 8, 9, 2, -1, 8, 8, 2, 1, 8, 9, 8, 10, 5, 4, 7, 2, 8, 9, 8, 12, 8, 7, -1, 1, -1, 1, 3, 8, 1, 8, 1, 1, 8, 11, 9, 8, 1, -1, 2, 2, 1, 8, 12, -1, 2, 7, 9, 10, 1, 10, -1, 9, 1, 8, -1, 1, 3, 1, 3, 1, 8, 5, 8, 10, 3, 9, -1, 9, 1, 8, 10, 9, 9, 2, 8, 9, 8, 3, 2, 1, 8, 9, -1, 9, 1, -1, 3, 12, 8, 9, -1, -1, 1, 1, 3, 1, 8, 2, 9, 2, 1, 9, 1, 9, 3, 3, 8, 9, 2, 3, 10, 2, 8, 3, 12, 4, 1, 12, 12, 1, 7, 8, 11, 1, 3, 4, 3, 1, 8, 7, 4, -1, 2, 8, 2, 8, 1, 2, 8, -1, 6, 10, 2, 9, 1, 7, -1, 9, 5, 8, -1, 1, 2, 8, 7, 9, 1, 2, 1, 8, 8, 1, 1, -1, 8, 4, 3, 8, 1, 1, 1, 9, 8, 2, 8, 8, 9, 8, -1, 7, 1, 2, 1, 8, 9, 10, 8, 2, 4, 10, 1, 1, 2, 1, 1, 8, 1, -1, 8, 10, 10, 8, 9, 2, 9, 1, 8, -1, 8, 7, 8, 1, -1, 1, -1, -1, 1, -1, 8, 1, -1, -1, 9, 1, -1, 2, -1, 9, 1, 8, 1, 1, 4, 7, 9, 8, 12, 1, 4, 9, 2, 1, -1, 1, 8, -1, 2, 12, 1, 3, -1, 9, 1, 8, -1, 1, 1, 2, 10, 1, 9, 1, 10, -1, 10, 9, 1, 1, 2, 2, 9, -1, 3, 4, 4, 1, -1, 8, -1, 1, 10, 9, 8, 1, 2, -1, 8, 9, 8, -1, 7, 12, 12, 8, 2, 10]\n",
            "[1, 9, -1, 1, 2, 10, 3, 6, 2, 1, 8, 4, 1, -1, 1, -1, 2, -1, 9, 2, 8, -1, -1, 4, 2, 8, 2, 8, -1, 8, 8, 1, 8, 1, -1, 1, 9, 2, 2, 3, 2, 1, 10, 2, 10, 8, 1, 3, -1, 8, 3, 12, 3, 7, 2, 8, 9, -1, 9, 9, -1, 2, 6, 2, 1, 1, 2, -1, 10, 1, 3, 8, 7, 1, 2, -1, -1, 10, 1, 3, 1, 4, 10, 1, 7, -1, -1, 12, 8, -1, 2, 8, -1, 2, 1, 1, 2, 10, 8, -1, 7, 3, -1, 9, 1, 9, 7, 4, 9, 9, -1, 2, 1, 1, 10, 9, -1, 3, 9, 1, 8, 8, 7, -1, 2, 9, 3, 2, 8, 2, 9, 1, 8, 3, 9, -1, 4, 1, 1, -1, 1, 1, 10, 8, -1, -1, 8, 3, 8, 9, 2, 1, -1, 7, 4, -1, -1, 8, -1, 2, 4, 8, 4, 4, -1, 1, 8, 9, 4, 7, 3, 9, -1, 2, -1, 9, -1, 9, -1, 7, 3, 1, 8, 4, -1, -1, 8, -1, 12, -1, 1, 10, 10, -1, -1, 7, 10, 12, 1, 9, 3, -1, 1, 2, 8, 8, 8, 9, -1, 8, 8, 2, 2, 9, 1, 1, -1, 10, 2, 2, 9, 9, 4, 2, 1, 7, 3, 2, 4, 2, -1, 1, -1, -1, 8, 2, 9, 1, 8, 3, 1, 8, 8, -1, 1, 1, 7, 8, 1, -1, 9, 2, 12, -1, 9, 3, 1, -1, -1, 2, 8, 5, 1, 3, 9, 8, 2, 10, 8, 8, 2, 9, 8, 2, 7, -1, 8, 2, -1, 3, 7, 12, 2, 8, 1, 1, 9, 12, -1, 3, -1, 7, -1, 1, 8, 2, 1, 5, 1, 3, 2, 1, 2, 1, -1, 9, 2, 3, 5, 12, 7, 1, 10, 5, 1, 1, 1, 8, -1, -1, -1, 9, 4, 4, 9, 8, 1, 10, 2, 1, 7, 5, 1, 1, 1, 3, 1, 8, 2, 8, -1, 8, -1, -1, 1, 9, 8, 1, 2, 8, 1, 3, 3, 8, 1, 2, 1, 1, 9, -1, -1, 12, 3, 1, -1, 8, 4, -1, 8, 2, -1, 1, 2, 2, 4, 3, 4, -1, 1, 1, 10, -1, 8, -1, -1, 9, 10, 9, 12, 1, -1, 8, -1, 7, 9, -1, 8, 8, 8, 1, 8, 10, 1, 3, 6, 2, -1, -1, 2, 2, 10, 3, 1, -1, 1, -1, 1, 7, 5, 1, 8, 1, 10, 12, -1, 1, -1, 8, 8, 1, 2, -1, -1, 5, 8, 9, 10, 1, 12, 3, 1, -1, 4, 7, 9, 1, -1, 8, 2, 9, 3, 8, 8, -1, -1, 1, 8, -1, -1, 6, 8, 8, 7, 1, 4, 1, 2, -1, -1, 6, -1, 1, 3, 3, 4, -1, -1, 8, 8, 1, -1, -1, 9, 2, 1, -1, 9, 4, -1, 9, 1, -1, 4, 5, -1, 1, 3, 1, 2, 7, 10, -1, 1, 4, -1, 1, 4, 1, 1, 9, 10, 3, 12, 10, 1, 1, 2, 8, -1, 3, 2, 5, -1, 3, -1, 9, 1, -1, 7, 1, -1, 9, 8, -1, 8, 8, 1, 8, -1, -1, -1, 6, 1, -1, -1, 1, 9, 9, 2, -1, 10, 8, 2, 1, 8, 9, 8, 10, 5, 4, 7, 2, 8, 9, 8, 12, 10, 8, -1, 1, -1, 1, 3, 8, 1, -1, 1, 1, 8, 10, 9, 8, 1, -1, 2, 2, 1, 10, 12, -1, 2, 7, 9, 10, 1, -1, -1, 8, 2, 8, -1, 1, 3, 1, 3, 1, 8, 1, 8, 2, 3, -1, -1, 7, 1, 8, 10, 9, 9, 2, 8, 9, 3, 3, 4, -1, 8, 10, -1, 9, 1, -1, 3, 12, 8, -1, -1, -1, 1, 1, -1, 1, 8, 2, 9, 2, 1, 11, 1, 8, 3, -1, 8, 9, 2, 3, 10, 2, 9, 3, 12, 4, 1, 12, -1, 1, 7, 9, 11, 1, 3, 4, 3, 4, 8, 2, 4, -1, -1, 8, 4, 8, 9, -1, 8, -1, 6, 8, 2, 9, 1, 8, -1, 9, 5, 8, -1, 1, 2, 8, 7, 3, 1, 2, 1, 9, 8, 1, 1, -1, 8, 8, 3, 8, 1, 1, 1, 8, 8, 2, 8, 8, 9, 8, -1, 7, -1, 2, 1, 8, 9, 10, 8, 2, -1, 9, -1, 1, 2, 1, 1, -1, 1, -1, 8, 10, 10, 8, 10, 2, -1, 1, 8, -1, 8, 7, 8, 1, -1, -1, -1, -1, 1, -1, 8, 1, -1, -1, 9, -1, -1, 2, -1, 9, 1, 8, 2, 1, -1, 7, 2, 8, 8, 1, 4, 9, 2, 1, -1, 1, 8, -1, 2, -1, 1, 3, -1, 9, 1, 8, -1, 1, 1, -1, 10, 1, -1, 1, 10, -1, 10, 12, 1, 1, 2, 2, 9, -1, 8, 9, 4, 1, -1, 9, -1, 1, 10, 9, 8, 1, 6, -1, 8, 9, 8, -1, 7, 12, 12, 8, 2, 1]\n",
            "[1, 5, -1, 1, 2, 8, 3, 6, 2, 1, -1, 4, 1, -1, 1, -1, 2, -1, 9, 2, 8, -1, -1, 4, 2, 10, 2, 8, -1, 8, 8, 1, 8, -1, -1, 4, 1, 2, 2, 3, -1, 1, 10, 3, 10, 8, -1, 3, -1, 8, 3, 12, 3, 7, 2, 9, 9, -1, 9, 9, -1, 3, 6, 2, 1, -1, 2, -1, 10, 1, 3, -1, 7, 1, 2, -1, -1, 8, 1, 3, 1, -1, 10, -1, 7, -1, -1, -1, 10, -1, 2, 8, -1, -1, -1, 1, 2, 10, 8, -1, -1, 3, -1, 9, 1, -1, 10, 4, 8, 3, -1, 2, 1, 1, 10, 9, -1, 3, 9, -1, 8, -1, 7, -1, 2, 9, 3, 2, 8, 2, 9, 1, 8, -1, 9, -1, 4, 1, 1, -1, 1, 1, 10, 9, -1, -1, 8, 3, -1, 9, 2, 1, -1, 7, 4, -1, -1, -1, -1, 2, 9, 9, 4, 4, -1, -1, 8, 9, 4, 7, 3, 5, -1, 10, -1, -1, -1, 9, -1, -1, 3, -1, 9, 8, -1, -1, 9, -1, 12, -1, -1, 10, 10, -1, -1, 7, -1, 12, 1, 9, 3, -1, 1, -1, 8, 8, 8, 9, -1, 8, -1, 2, 2, 9, 1, 1, -1, 10, 2, 2, 8, 3, 4, 2, 1, 7, 3, 2, 1, 2, -1, 1, -1, -1, 8, -1, 9, -1, 7, 3, 1, 8, -1, -1, 1, 1, 7, 8, 1, -1, 9, 2, 12, -1, 9, 3, 1, -1, -1, 2, 8, 5, 1, 3, 9, 8, 2, 10, -1, 8, 2, 2, 8, -1, 7, -1, 8, 2, -1, 3, 7, 1, 2, 12, 1, 1, 9, -1, -1, 3, -1, 7, -1, 1, 9, 2, 4, 5, 1, 3, 2, 1, 2, 10, -1, 9, 2, 3, 8, 12, 7, 1, 10, 5, 1, 1, 1, 8, -1, -1, -1, 9, 4, 4, 9, -1, 1, 8, 2, 1, -1, 5, 1, 1, 1, 3, -1, 8, 2, 8, -1, 8, -1, -1, -1, 10, 6, -1, 2, 8, 1, 3, 3, 9, 1, -1, 1, 1, 9, -1, -1, 12, 3, 1, -1, 8, 4, -1, -1, 2, -1, 1, -1, 2, 8, 3, 6, -1, 1, 1, 10, -1, -1, -1, -1, 9, 10, -1, 12, 1, -1, -1, -1, 7, 3, -1, 8, 8, 8, 1, 8, 10, 1, 3, 6, 2, -1, -1, 2, 2, 10, 3, 1, -1, 1, -1, 1, 7, 5, -1, 1, 1, 10, 12, -1, 1, -1, 8, 8, 1, 2, -1, -1, 1, 8, 9, 10, 1, 12, 3, 1, -1, 4, 7, 9, 1, -1, -1, 3, 9, 8, 8, 8, -1, -1, 1, 8, -1, -1, 2, -1, 8, 7, 1, 4, 1, 2, -1, -1, 6, -1, 1, 3, 3, 2, -1, -1, 8, -1, 1, -1, -1, 9, 9, 1, -1, 8, 4, -1, -1, 1, -1, 4, 5, -1, 1, 3, 1, 2, 9, 10, -1, 1, -1, -1, 1, -1, 1, 1, 9, 10, 3, 12, 10, 1, 1, 2, 8, -1, 3, 2, 5, -1, 3, -1, 9, 1, -1, 7, 1, -1, 9, -1, -1, 8, 8, 1, 8, -1, -1, -1, 6, 1, -1, -1, 1, 8, 9, 2, -1, 8, -1, 2, 1, 8, 9, 8, 9, 5, 4, 7, 2, 7, 9, 8, -1, 9, 10, -1, 1, -1, 1, 3, 8, 1, -1, 1, -1, 8, 9, 9, 8, 1, -1, 2, -1, 1, 8, 12, -1, 2, 7, 2, 10, 1, -1, -1, 8, 1, 8, -1, 1, -1, 1, 3, 1, 8, 1, 8, 2, 3, -1, -1, 7, 1, 8, 10, 9, 9, 2, 8, 9, 3, 3, 2, -1, 8, 8, -1, 9, 1, -1, -1, 8, 8, -1, -1, -1, 1, 1, -1, 1, -1, 2, 9, -1, 1, 11, 1, 9, 3, -1, 8, 10, 2, 3, 10, 2, 8, 3, 12, 4, 1, 12, -1, 1, 7, 9, 11, 1, 3, 4, 3, 1, 8, 2, -1, -1, -1, 8, 4, -1, 4, -1, 8, -1, 6, 10, 2, 9, 1, 8, -1, 9, 5, 8, -1, 1, 2, 8, -1, 9, 1, -1, 1, 8, 8, 1, 1, -1, 8, 4, 3, 8, 1, 1, 1, 3, 8, 2, 8, 8, 9, 8, -1, 7, -1, 2, 1, -1, 9, 10, 8, 2, -1, 9, -1, 1, 2, 1, 1, -1, 1, -1, 8, 10, 10, 8, 10, 2, -1, 1, 8, -1, 8, 7, 8, -1, -1, -1, -1, -1, 1, -1, 8, 1, -1, -1, 9, -1, -1, 2, -1, 9, 1, 3, 3, 1, -1, 7, -1, -1, 12, -1, -1, 9, 2, -1, -1, 1, 8, -1, 2, -1, 1, 3, -1, 9, -1, 8, -1, 1, 1, -1, 10, 3, -1, 1, 10, -1, 9, 4, 1, 1, 2, 2, -1, -1, 3, 4, 1, 1, -1, 9, -1, 1, 10, 9, 9, 1, 9, -1, 8, 9, 8, -1, 7, 12, 12, 8, -1, 10]\n",
            "[1, 9, -1, 1, 2, 9, 3, 6, -1, 1, -1, 4, 1, -1, 1, -1, 2, -1, 1, 2, -1, -1, -1, 4, 2, 8, 2, 8, -1, 8, 8, 1, 8, -1, -1, 1, 1, 2, 2, 3, -1, 1, 9, -1, 2, 8, -1, 3, -1, -1, 3, 12, 1, 7, 1, -1, -1, -1, 9, 9, -1, 2, 6, 2, 1, -1, 2, -1, -1, -1, 3, -1, -1, 1, 2, -1, -1, -1, 1, 3, 1, -1, 10, -1, 7, -1, -1, -1, -1, -1, 2, 8, -1, -1, -1, 1, -1, 9, 8, -1, -1, 3, -1, 2, 1, -1, -1, 4, 8, 9, -1, 1, 1, 1, 10, 9, -1, 3, 2, -1, 8, -1, -1, -1, 2, 1, 3, 2, 8, 2, 9, 1, 8, -1, 9, -1, 1, 1, 1, -1, -1, 1, 10, 9, -1, -1, 8, 3, -1, 9, 1, 1, -1, -1, -1, -1, -1, -1, -1, 2, -1, 8, 4, -1, -1, -1, 8, 9, 4, 7, 3, 5, -1, 2, -1, -1, -1, 9, -1, -1, 3, -1, 9, 4, -1, -1, 8, -1, -1, -1, -1, 8, 9, -1, -1, -1, -1, 12, 1, 9, 3, -1, 1, -1, -1, -1, 8, 9, -1, 8, -1, 2, 2, 9, 1, 1, -1, 10, 1, 2, 8, 9, 4, 2, 1, 7, 3, -1, -1, 2, -1, 1, -1, -1, 9, -1, 9, -1, 7, 3, 1, 8, -1, -1, 1, 1, 7, 8, 1, -1, 9, 2, 3, -1, 9, 3, -1, -1, -1, 2, 8, 1, 1, 3, 9, -1, 4, 8, -1, 8, -1, 2, 8, -1, 7, -1, 8, 2, -1, -1, 7, 1, 2, 8, 1, -1, 9, -1, -1, 3, -1, 7, -1, 1, 6, 2, 1, 5, 1, 3, -1, 1, 2, 1, -1, 8, -1, 2, 8, 12, 7, 1, -1, 5, 1, 1, -1, 8, -1, -1, -1, 1, 2, -1, 1, -1, -1, 8, 2, -1, -1, 5, 1, 1, -1, -1, -1, 8, 2, 8, -1, 8, -1, -1, -1, 10, 6, -1, 2, 8, 1, 3, 3, 8, 1, -1, 1, 1, 9, -1, -1, 12, 3, 1, -1, 8, -1, -1, -1, 2, -1, 1, -1, 2, 4, 3, 6, -1, 1, 1, 10, -1, -1, -1, -1, 9, 10, -1, 12, 1, -1, -1, -1, 7, 3, -1, 8, 8, 8, 1, 8, 2, 1, 3, 6, 2, -1, -1, 2, 1, 10, 2, 1, -1, 1, -1, 1, 7, 5, -1, 1, 1, 10, 12, -1, 1, -1, 8, 12, 1, 2, -1, -1, 1, 8, 9, 8, 1, 12, -1, 1, -1, 4, 7, 9, 1, -1, -1, -1, 9, 3, 8, 8, -1, -1, 1, -1, -1, -1, 6, -1, 6, 7, 1, 4, -1, 2, -1, -1, 6, -1, -1, 3, 3, 1, -1, -1, 8, -1, 1, -1, -1, 9, 2, 1, -1, 8, 4, -1, -1, 1, -1, 1, 5, -1, 1, 3, 1, 2, -1, 10, -1, 1, -1, -1, 1, -1, 1, -1, 9, 3, 3, 12, 10, 1, 1, 9, -1, -1, 3, 2, 5, -1, 1, -1, 9, 1, -1, 7, 1, -1, 9, -1, -1, 8, 8, -1, -1, -1, -1, -1, 6, 1, -1, -1, 1, 8, 9, 2, -1, 8, -1, 2, -1, 8, 9, 8, 9, 5, 8, 7, 2, 8, 9, 8, -1, -1, 9, -1, 1, -1, 1, 3, 8, 1, -1, 1, -1, 8, 9, 9, 1, 1, -1, 2, -1, 1, 10, 12, -1, 4, -1, 2, 8, 1, -1, -1, 8, 1, 8, -1, -1, -1, 1, 3, 1, 1, 1, 8, 2, 3, -1, -1, -1, -1, 8, 10, 9, 9, 2, 8, -1, 3, 3, 4, -1, 8, 8, -1, 1, -1, -1, -1, 8, 8, -1, -1, -1, 1, 1, -1, -1, -1, 1, 8, -1, 1, -1, 1, 8, 3, -1, 8, 9, 2, 3, 10, -1, 8, 3, 12, 1, 1, -1, -1, 1, 7, 8, 11, 1, 3, 4, 3, 1, 8, 2, -1, -1, -1, 8, 4, -1, 1, -1, 8, -1, 6, -1, 2, -1, 1, 8, -1, -1, 5, 8, -1, 1, 2, 8, -1, 3, 1, -1, 1, -1, 8, 1, 1, -1, 8, 4, 3, 8, 1, 1, 1, 3, 8, 2, 8, 8, 9, 8, -1, -1, -1, 2, 1, -1, 9, 8, 8, -1, -1, 9, -1, 1, 1, 1, 1, -1, 1, -1, 8, 10, 10, 8, 8, 2, -1, 1, 8, -1, 9, -1, 8, -1, -1, -1, -1, -1, 1, -1, -1, 1, -1, -1, 9, -1, -1, 2, -1, 1, -1, 3, 1, -1, -1, 7, -1, -1, 12, -1, -1, 8, 2, -1, -1, 1, -1, -1, 2, -1, 1, 3, -1, -1, -1, 8, -1, 1, 1, -1, 10, 1, -1, -1, 10, -1, 9, 9, 1, 1, -1, 2, -1, -1, 8, 4, 1, 1, -1, 9, -1, 1, 9, 9, 7, 1, 1, -1, 8, 9, 8, -1, 7, 12, 12, -1, -1, -1]\n",
            "[1, 9, -1, 1, 2, 10, 3, 6, -1, 1, -1, 4, 1, -1, 8, -1, 2, -1, 1, 2, -1, -1, -1, 4, 2, 8, 2, 8, -1, 8, 8, 1, 8, -1, -1, -1, -1, 2, 2, 2, -1, 1, 10, -1, -1, -1, -1, 3, -1, -1, 3, 12, 3, 7, 2, -1, -1, -1, 9, 7, -1, 2, -1, 2, 1, -1, 2, -1, -1, -1, 3, -1, -1, 1, 2, -1, -1, -1, -1, 3, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 2, 8, -1, -1, -1, 1, -1, 10, 8, -1, -1, -1, -1, 2, 1, -1, -1, 4, 8, -1, -1, 2, 1, 1, 10, 9, -1, 3, -1, -1, 8, -1, -1, -1, 2, 9, 3, -1, 8, 2, 9, 1, -1, -1, 9, -1, 1, 1, 1, -1, -1, -1, 10, 8, -1, -1, 8, 3, -1, 9, 2, 1, -1, -1, -1, -1, -1, -1, -1, 2, -1, 8, -1, -1, -1, -1, 8, 9, 4, 7, 3, 5, -1, 2, -1, -1, -1, 9, -1, -1, 2, -1, 9, -1, -1, -1, 8, -1, -1, -1, -1, 10, 10, -1, -1, -1, -1, -1, 1, -1, 3, -1, 1, -1, -1, -1, -1, 9, -1, 8, -1, -1, 2, 9, 1, 8, -1, 10, -1, 2, 8, 9, 4, 2, -1, 7, 3, -1, -1, 2, -1, 1, -1, -1, 8, -1, -1, -1, -1, 3, 1, 8, -1, -1, 1, -1, 7, 8, 1, -1, 9, 2, 12, -1, -1, 3, -1, -1, -1, 2, 8, 5, 1, 3, 9, -1, 2, 10, -1, -1, -1, 2, 8, -1, -1, -1, 8, 2, -1, -1, -1, 12, 2, 8, 1, -1, 9, -1, -1, 3, -1, 7, -1, -1, 9, 2, 1, 5, -1, -1, -1, 1, 2, 9, -1, 8, -1, 2, 8, -1, -1, 1, -1, 5, 1, 1, -1, 8, -1, -1, -1, 9, 2, -1, 9, -1, -1, 8, 2, -1, -1, 5, 1, 1, -1, -1, -1, 8, 2, 8, -1, 8, -1, -1, -1, -1, 10, -1, -1, -1, -1, 3, 9, 8, 1, -1, 1, 1, 9, -1, -1, 12, 3, -1, -1, 8, -1, -1, -1, 2, -1, 1, -1, -1, -1, 3, 4, -1, 1, 1, 10, -1, -1, -1, -1, 9, 10, -1, 12, 1, -1, -1, -1, 7, -1, -1, 8, 8, -1, 1, 8, 2, -1, 3, 6, 2, -1, -1, 2, 2, 10, 2, 1, -1, 1, -1, 1, -1, 5, -1, 8, 1, 8, 12, -1, -1, -1, 8, 8, -1, 2, -1, -1, 5, -1, 9, 8, 1, 12, -1, 1, -1, 8, -1, -1, -1, -1, -1, -1, 7, 3, 8, 8, -1, -1, 1, -1, -1, -1, 6, -1, -1, 7, -1, 4, -1, 2, -1, -1, 6, -1, -1, 3, 3, 2, -1, -1, 8, -1, 1, -1, -1, 9, 8, 1, -1, -1, 4, -1, -1, 3, -1, 1, 5, -1, 1, -1, 1, 2, -1, 10, -1, 1, -1, -1, 1, -1, 1, -1, 9, 10, 3, 12, 10, 1, 1, 2, -1, -1, 3, 2, 5, -1, 1, -1, 7, 1, -1, 7, -1, -1, 9, -1, -1, 8, 8, -1, -1, -1, -1, -1, 6, 1, -1, -1, 1, 8, 9, 2, -1, 8, -1, 2, -1, -1, -1, 8, 10, 5, 8, 7, 2, 8, -1, 8, -1, -1, 7, -1, 1, -1, 1, 3, 8, 1, -1, 1, -1, 8, 8, 9, 8, 1, -1, 2, -1, 5, 8, 12, -1, 2, -1, 2, 10, 1, -1, -1, 8, 1, 8, -1, -1, -1, 1, 3, 1, 8, 1, 8, 2, 3, -1, -1, -1, -1, 8, -1, -1, -1, 2, 8, -1, 8, 3, -1, -1, 8, 8, -1, 1, -1, -1, -1, 10, 8, -1, -1, -1, 1, 1, -1, -1, -1, 1, 8, -1, 1, -1, 1, 8, 3, -1, 8, 10, 2, 3, 10, -1, 8, 3, 12, 1, 1, -1, -1, 1, 7, 8, 11, 1, 3, -1, 2, 1, 8, 7, -1, -1, -1, -1, -1, -1, 2, -1, 8, -1, 6, -1, 2, -1, 1, -1, -1, -1, 5, 8, -1, -1, 2, 8, -1, 10, 1, -1, 1, -1, 8, 1, 1, -1, 8, 4, 3, 8, 1, 1, 1, 3, 8, 2, 8, -1, 8, 8, -1, -1, -1, 2, 1, -1, 9, -1, 8, -1, -1, 9, -1, 1, 1, 1, 1, -1, 1, -1, 8, 10, -1, 8, 8, 2, -1, -1, 8, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, 1, -1, -1, 9, -1, -1, 2, -1, 9, -1, 7, 2, -1, -1, 7, -1, -1, -1, -1, -1, 8, 2, -1, -1, 1, -1, -1, 2, -1, 1, 3, -1, -1, -1, 8, -1, 1, 1, -1, 10, 3, -1, -1, 10, -1, 10, 2, 1, 1, -1, 2, -1, -1, 2, 2, -1, -1, -1, 8, -1, 1, 10, 9, 7, 1, 9, -1, 8, 4, 8, -1, 7, -1, 12, -1, -1, -1]\n",
            "0.7695961995249406\n",
            "[1, 4, 3, 1, 2, 9, 3, 6, 2, 1, 8, 4, 1, 4, 1, 7, 2, 4, 9, 2, 8, 9, 9, 4, 2, 10, 2, 8, 1, 8, 8, 1, 8, 1, 9, 4, 1, 2, 2, 3, 2, 1, 10, 2, 10, 8, 1, 3, 4, 8, 3, 12, 1, 7, 2, 8, 9, 1, 9, 9, 3, 2, 6, 2, 1, 1, 8, 1, 10, 1, 3, 8, 7, 1, 2, 2, 9, 8, 1, 3, 1, 11, 10, 1, 7, 9, 9, 12, 8, 1, 2, 8, 9, 2, 1, 1, 2, 10, 8, 2, 7, 3, 11, 2, 1, 9, 10, 4, 8, 9, 6, 2, 1, 1, 9, 9, 3, 3, 9, 1, 8, 8, 7, 1, 2, 9, 3, 2, 8, 2, 9, 1, 8, 1, 9, 8, 4, 1, 1, 2, 1, 1, 10, 9, 1, 9, 8, 3, 8, 9, 2, 1, 3, 7, 4, 0, 3, 10, 8, 2, 9, 8, 4, 4, 12, 1, 8, 9, 4, 7, 3, 5, 10, 10, 8, 8, 3, 9, 12, 7, 3, 1, 9, 8, 10, 8, 8, 9, 12, 1, 1, 10, 10, 1, 2, 9, 11, 12, 1, 9, 3, 9, 1, 3, 8, 8, 8, 9, 9, 8, 8, 2, 2, 9, 1, 1, 2, 10, 2, 2, 8, 9, 4, 2, 1, 7, 3, 2, 4, 2, 2, 1, 9, 1, 9, 2, 9, 1, 7, 3, 1, 8, 8, 12, 1, 1, 7, 8, 1, 2, 9, 2, 3, 1, 9, 3, 1, 1, 9, 2, 8, 5, 1, 3, 9, 8, 2, 8, 8, 8, 2, 9, 8, 2, 7, 1, 8, 2, 2, 3, 7, 12, 2, 12, 1, 1, 9, 12, 4, 3, 2, 7, 3, 1, 8, 2, 1, 5, 1, 3, 9, 1, 2, 10, 8, 8, 2, 3, 5, 12, 10, 1, 8, 5, 1, 1, 1, 8, 9, 3, 9, 9, 2, 4, 1, 7, 1, 8, 10, 1, 7, 5, 1, 1, 1, 3, 1, 8, 2, 8, 1, 8, 7, 1, 1, 9, 8, 1, 2, 8, 1, 3, 8, 9, 1, 2, 1, 1, 9, 8, 1, 12, 3, 4, 1, 8, 4, 3, 3, 2, 9, 1, 2, 2, 10, 3, 6, 9, 1, 1, 10, 8, 8, 10, 1, 9, 10, 9, 12, 1, 9, 8, 8, 7, 9, 8, 8, 8, 8, 1, 8, 10, 1, 3, 6, 9, 3, 2, 2, 2, 10, 3, 1, 9, 1, 9, 1, 7, 5, 4, 1, 2, 8, 12, 1, 1, 2, 8, 8, 1, 2, 2, 9, 5, 8, 8, 10, 1, 8, 3, 1, 2, 8, 7, 9, 1, 2, 8, 3, 9, 8, 8, 8, 2, 8, 1, 8, 1, 1, 6, 8, 8, 7, 1, 4, 1, 2, 9, 5, 6, 12, 1, 3, 3, 2, 3, 1, 8, 1, 1, 9, 3, 10, 9, 1, 9, 8, 9, 8, 10, 1, 3, 4, 5, 2, 1, 3, 1, 2, 7, 10, 8, 1, 8, 1, 1, 4, 1, 9, 9, 10, 3, 12, 10, 1, 1, 2, 8, 2, 3, 2, 5, 8, 1, 3, 9, 1, 1, 10, 1, 1, 9, 8, 2, 8, 8, 1, 8, 1, 8, 8, 6, 1, 9, 1, 1, 8, 9, 2, 10, 8, 8, 2, 1, 8, 9, 8, 9, 5, 4, 7, 2, 8, 9, 8, 12, 8, 10, 9, 1, 3, 1, 3, 8, 1, 8, 1, 9, 8, 10, 9, 8, 1, 1, 2, 2, 1, 8, 12, 2, 2, 7, 9, 10, 1, 10, 9, 9, 1, 8, 11, 1, 3, 1, 8, 1, 1, 1, 8, 2, 3, 9, 10, 9, 1, 8, 10, 9, 9, 2, 8, 9, 3, 3, 2, 1, 8, 8, 2, 9, 1, 1, 3, 12, 8, 9, 8, 2, 1, 1, 3, 1, 8, 1, 8, 2, 1, 9, 1, 8, 3, 3, 8, 9, 2, 3, 10, 3, 8, 3, 12, 4, 1, 12, 12, 1, 7, 8, 11, 1, 3, 4, 3, 1, 8, 7, 4, 8, 2, 8, 4, 8, 9, 2, 8, 8, 6, 10, 2, 9, 1, 8, 8, 9, 5, 8, 1, 1, 2, 8, 7, 9, 1, 2, 1, 8, 8, 1, 1, 1, 8, 4, 3, 8, 1, 1, 1, 8, 8, 2, 8, 8, 8, 8, 3, 7, 1, 2, 1, 8, 9, 8, 8, 2, 4, 9, 1, 1, 2, 1, 1, 8, 1, 2, 8, 10, 10, 8, 8, 2, 9, 1, 8, 9, 8, 7, 8, 1, 8, 1, 1, 3, 1, 1, 8, 1, 8, 2, 9, 1, 1, 2, 3, 9, 1, 3, 2, 1, 8, 7, 2, 8, 8, 1, 9, 9, 2, 1, 7, 1, 8, 2, 2, 12, 1, 3, 8, 8, 1, 8, 8, 1, 1, 2, 10, 1, 9, 1, 8, 2, 9, 2, 1, 1, 2, 2, 9, 10, 2, 9, 1, 1, 9, 9, 2, 1, 7, 9, 8, 1, 2, 12, 8, 9, 8, 1, 7, 12, 12, 8, 2, 1]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "final_pred_3=[]\n",
        "\n",
        "num_correct=0\n",
        "model_0_3=load_model(loss,accuracy,optimizer,'0-3')\n",
        "mod_labels_0=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_0:\n",
        "    pred_test_0_3=model_0_3.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_0_3 = pred_test_0_3.argmax(axis=1)\n",
        "    mod_labels_0.append(pred_labels_0_3[0])\n",
        "  else:\n",
        "    mod_labels_0.append(-1)\n",
        "\n",
        "model_1_3=load_model(loss,accuracy,optimizer,'1-3')\n",
        "mod_labels_1=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        " \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_1:\n",
        "    pred_test_1_3=model_1_3.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_1_3 = pred_test_1_3.argmax(axis=1)\n",
        "    mod_labels_1.append(pred_labels_1_3[0])\n",
        "  else:\n",
        "    mod_labels_1.append(-1)\n",
        "\n",
        "model_2_3=load_model(loss,accuracy,optimizer,'2-3')\n",
        "mod_labels_2=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_2:\n",
        "    pred_test_2_3=model_2_3.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_2_3 = pred_test_2_3.argmax(axis=1)\n",
        "    mod_labels_2.append(pred_labels_2_3[0])\n",
        "  else:\n",
        "    mod_labels_2.append(-1)\n",
        "\n",
        "model_3_3=load_model(loss,accuracy,optimizer,'3-3')\n",
        "mod_labels_3=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_3:\n",
        "    pred_test_3_3=model_3_3.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_3_3 = pred_test_3_3.argmax(axis=1)\n",
        "    mod_labels_3.append(pred_labels_3_3[0])\n",
        "  else:\n",
        "    mod_labels_3.append(-1)\n",
        "\n",
        "model_4_3=load_model(loss,accuracy,optimizer,'4-3')\n",
        "mod_labels_4=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_4:\n",
        "    pred_test_4_3=model_4_3.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_4_3 = pred_test_4_3.argmax(axis=1)\n",
        "    mod_labels_4.append(pred_labels_4_3[0])\n",
        "  else:\n",
        "    mod_labels_4.append(-1)\n",
        "\n",
        "model_5_3=load_model(loss,accuracy,optimizer,'5-3')\n",
        "mod_labels_5=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_5:\n",
        "    pred_test_5_3=model_5_3.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_5_3 = pred_test_5_3.argmax(axis=1)\n",
        "    mod_labels_5.append(pred_labels_5_3[0])\n",
        "  else:\n",
        "    mod_labels_5.append(-1)\n",
        "\n",
        "print(mod_labels_0)\n",
        "print(mod_labels_1)\n",
        "print(mod_labels_2)\n",
        "print(mod_labels_3)\n",
        "print(mod_labels_4)\n",
        "print(mod_labels_5)\n",
        "\n",
        "\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  fin_labels=[]\n",
        "  if mod_labels_0[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  if mod_labels_1[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  if mod_labels_2[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  if mod_labels_3[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  if mod_labels_4[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  if mod_labels_5[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  res=max(set(fin_labels), key = fin_labels.count)\n",
        "  final_pred_3.append(res)\n",
        "\n",
        "  if res==new_val_label_0[i]:\n",
        "    num_correct=num_correct+1\n",
        "\n",
        "accuracy=num_correct/len(new_val_label_0)\n",
        "print(accuracy)\n",
        "  \n",
        "print(final_pred_3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGmPqjsrofw9",
        "outputId": "163b1081-c0e8-4665-88f9-f17c5c970814"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.7695961995249406\n",
            "Weighted F1: 0.7699003496423334\n",
            "Micro F1: 0.7695961995249406\n",
            "Weighted Precision: 0.772262805060756\n",
            "Micro Precision: 0.7695961995249406\n",
            "Weighted Recall: 0.7695961995249406\n",
            "Micro Recall: 0.7695961995249406\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "\n",
        "accuracy=accuracy_score(new_val_label_0, final_pred_3)\n",
        "print(\"Accuracy: \"+str(accuracy))\n",
        "total_accuracy=total_accuracy+accuracy\n",
        "\n",
        "weighted_f1=f1_score(new_val_label_0,final_pred_3, average='weighted')\n",
        "print(\"Weighted F1: \"+ str(weighted_f1))\n",
        "total_weighted_f1=total_weighted_f1+weighted_f1\n",
        "micro_f1=f1_score(new_val_label_0,final_pred_3, average='micro')\n",
        "print(\"Micro F1: \"+ str(micro_f1))\n",
        "total_micro_f1=total_micro_f1+micro_f1\n",
        "\n",
        "weighted_precision=precision_score(new_val_label_0, final_pred_3, average='weighted')\n",
        "print(\"Weighted Precision: \" + str(weighted_precision))\n",
        "total_weighted_precision=total_weighted_precision+weighted_precision\n",
        "micro_precision=precision_score(new_val_label_0, final_pred_3, average='micro')\n",
        "print(\"Micro Precision: \" + str(micro_precision))\n",
        "total_micro_precision=total_micro_precision+micro_precision\n",
        "\n",
        "weighted_recall=recall_score(new_val_label_0, final_pred_3, average='weighted')\n",
        "print(\"Weighted Recall: \" + str(weighted_recall))\n",
        "total_weighted_recall=total_weighted_recall+weighted_recall\n",
        "micro_recall=recall_score(new_val_label_0, final_pred_3, average='micro')\n",
        "print(\"Micro Recall: \" + str(micro_recall))\n",
        "total_micro_recall=total_micro_recall+micro_recall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcnIY-AfoqM9",
        "outputId": "e118800e-c710-4bd6-d52f-c60446d22fe1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_26\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_53 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_54 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_53[0][0]',               \n",
            " el)                            thPoolingAndCrossAt               'input_54[0][0]']               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_26 (S  (None, 768)         0           ['tf_roberta_model[26][0]']      \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " dense_52 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_26[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dropout_63 (Dropout)           (None, 512)          0           ['dense_52[0][0]']               \n",
            "                                                                                                  \n",
            " dense_53 (Dense)               (None, 15)           7695        ['dropout_63[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model: \"model_27\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_55 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_56 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_55[0][0]',               \n",
            " el)                            thPoolingAndCrossAt               'input_56[0][0]']               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_27 (S  (None, 768)         0           ['tf_roberta_model[27][0]']      \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " dense_54 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_27[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dropout_64 (Dropout)           (None, 512)          0           ['dense_54[0][0]']               \n",
            "                                                                                                  \n",
            " dense_55 (Dense)               (None, 15)           7695        ['dropout_64[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model: \"model_28\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_57 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_58 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_57[0][0]',               \n",
            " el)                            thPoolingAndCrossAt               'input_58[0][0]']               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_28 (S  (None, 768)         0           ['tf_roberta_model[28][0]']      \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " dense_56 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_28[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dropout_65 (Dropout)           (None, 512)          0           ['dense_56[0][0]']               \n",
            "                                                                                                  \n",
            " dense_57 (Dense)               (None, 15)           7695        ['dropout_65[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model: \"model_29\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_59 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_60 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_59[0][0]',               \n",
            " el)                            thPoolingAndCrossAt               'input_60[0][0]']               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_29 (S  (None, 768)         0           ['tf_roberta_model[29][0]']      \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " dense_58 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_29[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dropout_66 (Dropout)           (None, 512)          0           ['dense_58[0][0]']               \n",
            "                                                                                                  \n",
            " dense_59 (Dense)               (None, 15)           7695        ['dropout_66[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model: \"model_30\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_61 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_62 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_61[0][0]',               \n",
            " el)                            thPoolingAndCrossAt               'input_62[0][0]']               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_30 (S  (None, 768)         0           ['tf_roberta_model[30][0]']      \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " dense_60 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_30[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dropout_67 (Dropout)           (None, 512)          0           ['dense_60[0][0]']               \n",
            "                                                                                                  \n",
            " dense_61 (Dense)               (None, 15)           7695        ['dropout_67[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model: \"model_31\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_63 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_64 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_63[0][0]',               \n",
            " el)                            thPoolingAndCrossAt               'input_64[0][0]']               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_31 (S  (None, 768)         0           ['tf_roberta_model[31][0]']      \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " dense_62 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_31[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dropout_68 (Dropout)           (None, 512)          0           ['dense_62[0][0]']               \n",
            "                                                                                                  \n",
            " dense_63 (Dense)               (None, 15)           7695        ['dropout_68[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,047,055\n",
            "Trainable params: 125,047,055\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "[1, 8, 3, 1, 2, 10, 3, 6, 2, 1, 8, 4, 1, 4, 1, 7, 2, 9, 1, 2, 8, 9, 9, 4, 2, 8, 2, 8, 1, 8, 8, 1, 8, 1, 9, 1, 1, 2, 2, 3, 2, 1, 10, 2, 10, 8, 1, 3, 4, 8, 3, 12, 1, 7, 2, 1, 9, 2, 9, 9, 3, 2, 6, 2, 1, 1, 2, 1, 10, 1, 3, 8, 7, 1, 2, 2, 9, 2, 1, 3, 1, 11, 10, 1, 7, 9, 9, 12, 10, 1, 2, 8, 1, 2, 1, 1, 2, 10, 8, 2, 7, 3, 11, 2, 1, 9, 10, 4, 8, 9, 6, 2, 1, 1, 10, 9, 3, 3, 10, 1, 8, 8, 7, 1, 2, 1, 3, 2, 8, 2, 9, 1, 8, 1, 9, 8, 1, 1, 1, 2, 1, 1, 10, 9, 1, 9, 8, 3, 8, 9, 2, 1, 3, 8, 2, 0, 3, 8, 8, 2, 9, 9, 4, 8, 12, 1, 8, 8, 4, 7, 3, 5, 10, 2, 8, 9, 3, 9, 12, 9, 3, 1, 9, 8, 10, 8, 8, 9, 12, 1, 1, 10, 10, 1, 2, 9, 10, 12, 1, 1, 2, 9, 1, 2, 8, 2, 8, 9, 9, 8, 8, 2, 2, 9, 1, 1, 2, 10, 2, 2, 8, 9, 8, 2, 1, 7, 3, 2, 4, 2, 2, 1, 9, 1, 8, 2, 9, 1, 8, 3, 1, 8, 8, 12, 1, 1, 7, 8, 1, 2, 9, 2, 12, 1, 9, 3, 1, 1, 9, 2, 8, 5, 1, 3, 2, 2, 2, 10, 8, 8, 2, 2, 8, 2, 3, 1, 8, 2, 2, 3, 7, 12, 2, 12, 1, 1, 9, 12, 4, 3, 2, 7, 3, 1, 6, 2, 4, 5, 1, 3, 2, 1, 2, 1, 10, 8, 2, 2, 7, 12, 10, 1, 10, 5, 1, 1, 1, 8, 9, 3, 9, 2, 4, 4, 10, 9, 1, 10, 2, 1, 3, 5, 1, 1, 1, 3, 1, 8, 2, 8, 1, 7, 7, 3, 1, 10, 8, 1, 2, 8, 1, 3, 3, 8, 1, 2, 1, 1, 9, 8, 1, 12, 3, 1, 1, 8, 4, 3, 3, 2, 9, 1, 2, 2, 2, 3, 6, 9, 1, 1, 10, 8, 8, 10, 1, 9, 8, 1, 12, 1, 9, 8, 8, 7, 9, 8, 8, 8, 8, 1, 8, 10, 1, 9, 6, 2, 3, 2, 2, 2, 10, 2, 1, 9, 1, 9, 1, 7, 5, 4, 8, 2, 10, 12, 1, 1, 2, 8, 8, 1, 2, 2, 9, 5, 8, 9, 10, 1, 12, 3, 1, 2, 8, 7, 9, 1, 2, 8, 2, 7, 8, 8, 8, 2, 0, 1, 8, 1, 1, 6, 8, 8, 7, 1, 4, 1, 2, 2, 5, 6, 12, 1, 3, 3, 2, 3, 1, 8, 8, 1, 9, 3, 10, 2, 1, 9, 8, 4, 8, 10, 1, 3, 1, 5, 2, 1, 3, 1, 2, 9, 10, 8, 1, 8, 1, 1, 4, 1, 1, 9, 10, 3, 12, 10, 1, 1, 2, 8, 2, 3, 2, 5, 2, 1, 3, 9, 1, 1, 7, 1, 1, 8, 8, 2, 8, 8, 1, 8, 1, 8, 10, 6, 1, 9, 1, 1, 8, 9, 2, 10, 8, 8, 2, 1, 8, 9, 8, 10, 5, 4, 7, 2, 8, 9, 8, 12, 10, 7, 9, 1, 2, 2, 3, 8, 1, 10, 1, 9, 8, 10, 9, 8, 1, 1, 2, 2, 1, 8, 12, 2, 2, 7, 9, 10, 1, 10, 2, 8, 1, 8, 11, 1, 3, 1, 8, 1, 1, 1, 8, 2, 3, 9, 10, 10, 1, 8, 10, 9, 9, 2, 8, 9, 3, 3, 2, 2, 8, 10, 2, 9, 1, 2, 3, 8, 8, 9, 8, 2, 1, 1, 3, 1, 8, 1, 8, 1, 1, 10, 1, 8, 3, 3, 8, 10, 2, 3, 10, 2, 8, 3, 12, 1, 1, 12, 12, 1, 7, 8, 11, 1, 3, 4, 3, 1, 8, 2, 4, 9, 2, 8, 2, 8, 1, 2, 8, 8, 6, 8, 2, 1, 1, 8, 10, 9, 5, 8, 1, 1, 2, 8, 7, 10, 1, 2, 1, 8, 8, 1, 1, 2, 8, 4, 3, 8, 1, 1, 1, 3, 8, 2, 8, 8, 9, 8, 3, 7, 1, 2, 1, 8, 9, 10, 8, 2, 2, 10, 1, 1, 2, 1, 1, 8, 1, 2, 8, 10, 10, 8, 10, 2, 8, 1, 8, 9, 8, 7, 8, 1, 8, 1, 1, 3, 1, 1, 8, 1, 8, 9, 10, 1, 1, 2, 3, 1, 1, 3, 1, 1, 8, 7, 2, 8, 8, 1, 4, 9, 2, 1, 7, 1, 8, 2, 2, 12, 1, 3, 8, 9, 1, 8, 8, 1, 1, 2, 10, 1, 9, 1, 10, 10, 10, 4, 1, 1, 2, 2, 9, 10, 2, 9, 1, 1, 9, 9, 0, 1, 10, 9, 8, 1, 2, 12, 8, 9, 8, 1, 7, 12, 12, 8, 2, 1]\n",
            "[1, 5, -1, 1, 2, 9, 3, 6, 2, 1, 8, 4, 1, 4, 1, -1, 2, 4, 1, 2, 8, -1, -1, 4, 2, 10, 2, 8, 1, 8, 8, 1, 8, 1, -1, 1, 1, 2, 2, 3, 2, 1, 1, 2, 10, 8, 1, 3, 4, 8, 3, 12, 1, 7, 2, 9, 9, -1, 9, 2, -1, 3, 6, 2, 1, 1, 2, 1, 8, 1, 3, 8, 7, 1, 2, 2, -1, 10, 1, 3, 1, 11, 10, 1, 7, -1, -1, 12, 8, 1, 2, 8, 9, 2, 1, 1, 2, 10, 8, -1, 7, 3, 11, 2, 1, 9, 7, 4, 8, 9, -1, 2, 1, 1, 10, 9, 3, 3, 9, 1, 8, 8, 7, 1, 2, 9, 3, 2, 8, 9, 9, 1, 8, 1, 9, -1, 4, 1, 1, -1, 1, 1, 10, 8, 1, -1, 8, 3, 8, 9, 2, 1, 3, 7, 4, -1, -1, 8, 8, 2, 4, 8, 4, 4, -1, 1, 4, 9, 4, 7, 3, 9, 10, 10, 8, 9, -1, 9, 12, 7, 3, 1, 9, 4, -1, -1, 9, -1, 12, 1, 1, 10, 10, 1, -1, 9, 9, 12, 1, 9, 3, -1, 1, 2, 8, 8, 8, 9, -1, 8, 8, 2, 2, 9, 1, 1, -1, 8, 2, 2, 8, 13, 4, 2, 1, 7, 3, 2, 4, 2, -1, 1, -1, -1, 8, 2, 9, 1, 7, 3, 1, 9, 8, 8, 1, 1, 7, 8, 1, -1, 2, 2, 12, -1, 9, 2, 1, -1, -1, 2, 8, 5, 1, 2, 2, 8, 2, 10, 8, 8, 2, 2, 8, 2, 7, 1, 8, 2, 2, 3, 7, 1, 2, 12, 1, 1, 9, 12, 1, 3, -1, 7, -1, 1, 8, 2, 1, 5, 1, 3, 2, 1, 2, 1, -1, 8, 2, 2, 5, 12, 10, 1, 10, 5, 1, 1, 1, 8, -1, 3, -1, 9, 2, 4, 9, 8, 1, 9, 2, 1, 3, 5, 1, 1, 1, 3, 1, 8, 2, 8, 1, 8, 7, 3, 1, 9, 1, 1, 2, 8, 1, 3, 3, 9, 1, 2, 1, 1, 9, -1, -1, 12, 3, 1, -1, 8, 4, 3, 1, 2, -1, 1, 2, 2, 2, 3, 4, -1, 1, 1, 10, -1, 8, 10, 1, 9, 10, 1, 12, 1, -1, 8, -1, 7, 9, -1, 8, 8, 8, 1, 9, 10, 1, 3, 6, 9, -1, 2, 2, 2, 10, 3, 1, -1, 1, -1, 1, 7, 5, 1, 1, 1, 10, 12, -1, 1, 2, 8, 10, 1, 2, -1, -1, 1, 8, 9, 10, 1, 12, 3, 1, -1, 4, 7, 9, 1, -1, 8, 1, 13, 3, 8, 8, -1, -1, 1, 8, 1, -1, 4, 8, 8, 7, 1, 4, 1, 2, 9, 5, 6, -1, 1, 3, 3, 4, -1, -1, 8, 1, 1, -1, -1, 13, 9, 1, -1, 9, 4, 8, 9, 1, -1, 4, 9, 2, 1, 3, 1, 2, 2, 10, 8, 1, 4, 1, 1, 4, 1, 1, 13, 3, 3, 12, 10, 1, 1, 2, 8, -1, 3, 2, 5, 1, 3, -1, 9, 1, 1, 7, 1, -1, 9, 8, -1, 8, 8, 1, 8, -1, -1, -1, 6, 1, 9, -1, 1, 9, 9, 2, -1, 10, 8, 4, 1, 9, 9, 8, 9, 5, 4, 7, 2, 8, 9, 8, 12, 8, 7, -1, 1, -1, 1, 3, 8, 1, 10, 1, 1, 8, 9, 9, 8, 1, -1, 2, 2, 1, 8, 12, -1, 2, 7, 9, 10, 1, 10, -1, 9, 1, 8, -1, 1, 1, 1, 3, 1, 1, 1, 8, 2, 3, 9, -1, 9, 1, 8, 10, 9, 9, 2, 8, 9, 3, 3, 2, 1, 8, 8, -1, 9, 1, -1, 3, 12, 8, 9, -1, -1, 1, 1, 3, 1, 8, 1, 9, 2, 1, 9, 1, 9, 3, 3, 8, 9, 2, 3, 10, 1, 3, 3, 12, 4, 1, 12, 12, 1, 7, 8, 11, 1, 3, 4, 3, 1, 8, 7, 4, -1, 2, 8, 4, 9, 9, 2, 8, -1, 6, 10, 2, 9, 1, 8, -1, 9, 5, 8, -1, 1, 2, 8, 7, 9, 1, 2, 1, 9, 8, 1, 1, -1, 8, 4, 3, 8, 1, 1, 1, 8, 8, 2, 8, 8, 9, 8, -1, 7, 1, 2, 1, 8, 9, 10, 8, 2, 4, 2, 1, 1, 1, 1, 1, 8, 1, -1, 8, 10, 10, 8, 9, 2, 8, 1, 8, -1, 9, 7, 8, 1, -1, 1, -1, -1, 1, -1, 8, 1, -1, -1, 9, 1, -1, 2, -1, 1, 1, 8, 1, 1, 4, 7, 2, 8, 8, 1, 4, 9, 2, 1, -1, 1, 8, -1, 2, 12, 1, 3, -1, 9, 1, 8, -1, 1, 1, 2, 10, 1, 9, 1, 10, -1, 9, 11, 1, 1, 2, 2, 9, -1, 2, 4, 1, 1, -1, 9, -1, 1, 10, 9, 8, 1, 2, -1, 8, 9, 9, -1, 7, 12, 12, 8, 2, 1]\n",
            "[1, 5, -1, 1, 2, 8, 3, 6, 2, 1, 8, 4, 1, -1, 1, -1, 2, -1, 1, 2, 8, -1, -1, 4, 2, 10, 2, 8, -1, 8, 8, 1, 8, 1, -1, 4, 1, 2, 2, 3, 2, 1, 8, 2, 10, 8, 1, 3, -1, 8, 3, 12, 3, 7, 1, 8, 9, -1, 9, 9, -1, 2, 6, 2, 1, 1, 8, -1, 10, 1, 3, 8, 7, 1, 2, -1, -1, 10, 1, 3, 1, 11, 10, 1, 7, -1, -1, 12, 10, -1, 2, 8, -1, 2, 1, 1, 2, 10, 8, -1, 7, 3, -1, 9, 1, 9, 7, 4, 8, 9, -1, 2, 1, 1, 10, 8, -1, 3, 9, 1, 8, 8, 7, -1, 2, 1, 3, 2, 8, 2, 9, 1, 8, 3, 9, -1, 4, 1, 1, -1, 1, 1, 10, 8, -1, -1, 8, 3, 8, 9, 2, 1, -1, 8, 2, -1, -1, 10, -1, 2, 2, 8, 4, 4, -1, 1, 8, 9, 4, 7, 3, 5, -1, 10, -1, 8, -1, 9, -1, 7, 3, 1, 8, 4, -1, -1, 8, -1, 12, -1, 1, 10, 10, -1, -1, 9, 10, 12, 1, 9, 3, -1, 1, 2, 8, 8, 8, 9, -1, 8, 8, 2, 2, 5, 1, 1, -1, 10, 1, 2, 8, 9, 4, 2, 1, 9, 3, 2, 4, 2, -1, 1, -1, -1, 8, 2, 9, 1, 7, 3, 1, 8, 8, -1, 1, 1, 7, 8, 1, -1, 9, 2, 3, -1, 9, 2, 1, -1, -1, 2, 8, 5, 1, 3, 8, 2, 2, 10, 8, 8, 2, 2, 8, 2, 7, -1, 8, 2, -1, 3, 7, 1, 2, 8, 1, 1, 9, 12, -1, 3, -1, 7, -1, 1, 8, 2, 1, 5, 1, 3, 2, 1, 2, 1, -1, 8, 2, 2, 5, 12, 7, 1, 8, 5, 1, 1, 1, 8, -1, -1, -1, 2, 4, 4, 9, 8, 2, 8, 2, 1, 3, 5, 1, 1, 1, 3, 1, 8, 2, 8, -1, 8, -1, -1, 1, 8, 8, 5, 2, 8, 1, 3, 3, 8, 1, 2, 1, 1, 9, -1, -1, 12, 3, 1, -1, 8, 4, -1, 8, 2, -1, 1, 2, 2, 2, 3, 6, -1, 1, 1, 10, -1, 8, -1, -1, 9, 10, 1, 12, 1, -1, 8, -1, 7, 9, -1, 8, 8, 8, 1, 12, 2, 1, 3, 6, 2, -1, -1, 2, 2, 10, 3, 1, -1, 1, -1, 1, 7, 5, 4, 1, 2, 10, 12, -1, 1, -1, 8, 8, 1, 2, -1, -1, 5, 8, 9, 8, 1, 12, 3, 1, -1, 4, 7, 8, 1, -1, 8, 2, 10, 8, 8, 8, -1, -1, 1, 8, -1, -1, 2, 8, 8, 7, 1, 4, 1, 2, -1, -1, 6, -1, 1, 3, 3, 4, -1, -1, 8, 8, 1, -1, -1, 8, 8, 1, -1, 9, 4, -1, 10, 1, -1, 1, 5, -1, 1, 3, 1, 2, 7, 10, -1, 2, 8, -1, 1, 4, 1, 1, 9, 10, 3, 12, 10, 1, 1, 2, 8, -1, 3, 2, 5, -1, 1, -1, 7, 1, -1, 10, 1, -1, 8, 8, -1, 8, 8, 1, 8, -1, -1, -1, 6, 1, -1, -1, 1, 8, 9, 2, -1, 10, 8, 2, 1, 8, 9, 8, 10, 5, 4, 7, 2, 8, 9, 8, 12, 8, 10, -1, 1, -1, 1, 3, 8, 1, -1, 1, 9, 8, 10, 9, 8, 1, -1, 2, 2, 5, 8, 12, -1, 2, 7, 9, 1, 1, -1, -1, 8, 1, 8, -1, 1, 3, 1, 3, 1, 8, 1, 8, 2, 3, -1, -1, 9, 1, 8, 10, 9, 9, 2, 8, 9, 3, 3, 2, -1, 8, 4, -1, 9, 1, -1, 3, 12, 8, -1, -1, -1, 1, 1, -1, 1, 8, 1, 8, 1, 1, 11, 1, 8, 3, -1, 8, 10, 2, 3, 10, 2, 8, 3, 12, 1, 1, 12, -1, 1, 7, 8, 11, 1, 3, 8, 3, 1, 8, 7, 4, -1, -1, 8, 2, 8, 4, -1, 8, -1, 6, 10, 2, 1, 1, 8, -1, 9, 5, 8, -1, 1, 2, 8, 7, 3, 1, 2, 1, 8, 8, 1, 1, -1, 8, 4, 3, 8, 1, 1, 1, 8, 8, 2, 8, 8, 9, 8, -1, 7, -1, 2, 1, 8, 9, 8, 8, 2, -1, 10, -1, 1, 2, 1, 1, -1, 1, -1, 8, 10, 10, 8, 8, 2, -1, 1, 8, -1, 2, 7, 8, 1, -1, -1, -1, -1, 1, -1, 8, 1, -1, -1, 9, -1, -1, 2, -1, 1, 1, 3, 2, 1, -1, 7, 2, 8, 12, 1, 4, 8, 2, 1, -1, 1, 8, -1, 2, -1, 1, 3, -1, 9, 1, 8, -1, 1, 1, -1, 10, 3, -1, 1, 10, -1, 10, 2, 1, 1, 2, 2, 9, -1, 8, 9, 4, 1, -1, 8, -1, 1, 10, 9, 8, 1, 2, -1, 8, 4, 8, -1, 7, 12, 12, 8, 2, 1]\n",
            "[1, 9, -1, 1, 2, 8, 3, 6, 2, 1, -1, 4, 1, -1, 1, -1, 2, -1, 9, 2, 8, -1, -1, 4, 2, 8, 2, 8, -1, 8, 8, 1, 8, -1, -1, 4, 1, 2, 2, 3, -1, 1, 8, 2, 10, 8, -1, 3, -1, 8, 3, 12, 1, 7, 2, 9, 9, -1, 9, 9, -1, 2, 6, 2, 1, -1, 2, -1, 10, 1, 3, -1, 7, 1, 2, -1, -1, 4, 1, 3, 1, -1, 10, -1, 7, -1, -1, -1, 8, -1, 2, 8, -1, -1, -1, 1, 2, 10, 8, -1, -1, 3, -1, 2, 1, -1, 7, 4, 8, 1, -1, 2, 1, 1, 8, 9, -1, 3, 10, -1, 8, -1, 7, -1, 2, 1, 3, 2, 8, 2, 9, 1, 8, -1, 9, -1, 4, 1, 1, -1, 1, 1, 10, 9, -1, -1, 8, 3, -1, 9, 2, 1, -1, 7, 2, -1, -1, -1, -1, 2, 9, 9, 4, 4, -1, -1, 8, 9, 4, 7, 3, 9, -1, 2, -1, -1, -1, 9, -1, -1, 3, -1, 9, 8, -1, -1, 8, -1, 12, -1, -1, 8, 9, -1, -1, 9, -1, 12, 1, 9, 3, -1, 1, -1, 8, 8, 8, 9, -1, 8, -1, 2, 2, 9, 1, 1, -1, 8, 2, 2, 8, 13, 4, 2, 1, 7, 3, 2, 4, 2, -1, 1, -1, -1, 8, -1, 9, -1, 7, 3, 1, 8, -1, -1, 1, 1, 7, 8, 1, -1, 9, 2, 12, -1, 8, 3, 1, -1, -1, 2, 8, 5, 1, 3, 9, 2, 2, 10, -1, 8, 2, 2, 8, -1, 7, -1, 8, 2, -1, 4, 7, 1, 2, 12, 1, 1, 9, -1, -1, 3, -1, 7, -1, 1, 8, 2, 4, 5, 1, 3, 2, 1, 2, 13, -1, 8, 2, 2, 8, 12, 7, 1, 8, 5, 1, 1, 1, 8, -1, -1, -1, 9, 2, 4, 9, -1, 1, 8, 2, 1, -1, 5, 1, 1, 1, 3, -1, 8, 2, 8, -1, 8, -1, -1, -1, 8, 9, -1, 2, 8, 1, 3, 3, 9, 1, -1, 1, 1, 9, -1, -1, 12, 3, 1, -1, 8, 4, -1, -1, 2, -1, 1, -1, 2, 4, 3, 4, -1, 1, 1, 8, -1, -1, -1, -1, 9, 10, -1, 12, 1, -1, -1, -1, 7, 9, -1, 8, 8, 8, 1, 9, 10, 1, 3, 6, 2, -1, -1, 2, 2, 8, 2, 1, -1, 1, -1, 1, 7, 5, -1, 1, 1, 10, 12, -1, 1, -1, 8, 8, 1, 2, -1, -1, 1, 8, 9, 8, 1, 12, 3, 1, -1, 4, 7, 9, 1, -1, -1, 2, 7, 3, 8, 8, -1, -1, 1, 8, -1, -1, 6, -1, 8, 7, 1, 4, 1, 2, -1, -1, 6, -1, 1, 3, 3, 2, -1, -1, 8, -1, 1, -1, -1, 13, 8, 1, -1, 8, 4, -1, -1, 1, -1, 4, 5, -1, 1, 3, 1, 2, 7, 7, -1, 1, -1, -1, 1, -1, 1, 1, 9, 7, 3, 12, 8, 1, 1, 2, 8, -1, 3, 2, 5, -1, 1, -1, 9, 1, -1, 7, 1, -1, 9, -1, -1, 8, 8, 1, 8, -1, -1, -1, 6, 1, -1, -1, 1, 8, 9, 2, -1, 8, -1, 2, 1, 8, 9, 8, 9, 5, 4, 7, 2, 9, 9, 8, -1, 8, 7, -1, 1, -1, 1, 3, 8, 1, -1, 1, -1, 8, 9, 9, 8, 1, -1, 2, -1, 1, 8, 12, -1, 2, 7, 9, 10, 1, -1, -1, 8, 1, 8, -1, 1, -1, 1, 3, 1, 4, 1, 8, 2, 3, -1, -1, 7, 1, 8, 8, 9, 9, 2, 8, 9, 8, 3, 2, -1, 8, 4, -1, 9, 1, -1, -1, 8, 8, -1, -1, -1, 1, 1, -1, 1, -1, 1, 9, -1, 1, 11, 1, 9, 3, -1, 8, 9, 2, 3, 7, 2, 8, 3, 12, 1, 1, 12, -1, 1, 7, 9, 11, 1, 3, 4, 3, 1, 8, 2, -1, -1, -1, 8, 2, -1, 9, -1, 8, -1, 6, 8, 2, 9, 1, 8, -1, 9, 5, 8, -1, 1, 2, 8, -1, 3, 1, -1, 1, 8, 8, 1, 1, -1, 8, 8, 3, 8, 1, 1, 1, 8, 8, 2, 8, 8, 9, 8, -1, 7, -1, 2, 1, -1, 9, 10, 8, 2, -1, 9, -1, 1, 2, 1, 1, -1, 1, -1, 8, 10, 10, 8, 10, 2, -1, 1, 8, -1, 8, 7, 8, -1, -1, -1, -1, -1, 1, -1, 8, 1, -1, -1, 9, -1, -1, 2, -1, 9, 1, 8, 2, 1, -1, 7, -1, -1, 8, -1, -1, 8, 2, -1, -1, 1, 8, -1, 2, -1, 1, 3, -1, 8, -1, 8, -1, 1, 1, -1, 10, 1, -1, 1, 8, -1, 9, 4, 1, 1, 2, 2, -1, -1, 2, 4, 1, 1, -1, 8, -1, 1, 7, 9, 8, 1, 9, -1, 8, 9, 8, -1, 7, 12, 12, 8, -1, 1]\n",
            "[1, 9, -1, 1, 2, 8, 3, 6, -1, 1, -1, 4, 1, -1, 1, -1, 2, -1, 1, 2, -1, -1, -1, 4, 2, 8, 2, 8, -1, 8, 8, 1, 8, -1, -1, 4, 1, 2, 2, 3, -1, 1, 1, -1, 10, 8, -1, 3, -1, -1, 3, 12, 1, 7, 2, -1, -1, -1, 2, 4, -1, 4, 6, 2, 1, -1, 8, -1, -1, -1, 3, -1, -1, 1, 2, -1, -1, -1, 1, 3, 1, -1, 10, -1, 7, -1, -1, -1, -1, -1, 2, 8, -1, -1, -1, 1, -1, 10, 8, -1, -1, 3, -1, 2, 1, -1, -1, 4, 9, 9, -1, 2, 1, 1, 10, 9, -1, 3, 2, -1, 8, -1, -1, -1, 2, 9, 3, 2, 8, 2, 9, 1, 8, -1, 9, -1, 1, 1, 1, -1, -1, 1, 10, 9, -1, -1, 8, 3, -1, 9, 2, 1, -1, -1, -1, -1, -1, -1, -1, 2, -1, 8, 4, -1, -1, -1, 8, 9, 4, 7, 3, 5, -1, 2, -1, -1, -1, 9, -1, -1, 3, -1, 9, 3, -1, -1, 8, -1, -1, -1, -1, 10, 8, -1, -1, -1, -1, 12, 1, 9, 3, -1, 1, -1, -1, -1, 8, 9, -1, 8, -1, 2, 2, 9, 1, 1, -1, 10, 2, 2, 8, 9, 4, 2, 1, 7, 3, -1, -1, 2, -1, 1, -1, -1, 9, -1, 9, -1, 7, 3, 1, 8, -1, -1, 1, 1, 7, 8, 1, -1, 2, 2, 3, -1, 9, 3, -1, -1, -1, 2, 8, 5, 1, 3, 6, -1, 2, 8, -1, 8, -1, 9, 8, -1, 7, -1, 8, 2, -1, -1, 7, 1, 2, 8, 1, -1, 2, -1, -1, 3, -1, 7, -1, 1, 8, 2, 1, 5, 1, 3, -1, 1, 2, 10, -1, 8, -1, 3, 8, 12, 7, 1, -1, 5, 1, 1, -1, 8, -1, -1, -1, 2, 2, -1, 1, -1, -1, 8, 2, -1, -1, 5, 1, 1, -1, -1, -1, 8, 2, 8, -1, 8, -1, -1, -1, 8, 6, -1, 2, 8, 1, 3, 3, 8, 1, -1, 1, 1, 9, -1, -1, 12, 3, 1, -1, 8, -1, -1, -1, 2, -1, 1, -1, 2, 2, 3, 6, -1, 1, 1, 10, -1, -1, -1, -1, 9, 10, -1, 12, 1, -1, -1, -1, 7, 9, -1, 8, 8, 8, 1, 9, 10, 1, 3, 6, 2, -1, -1, 2, 2, 10, 2, 1, -1, 1, -1, 1, 7, 5, -1, 1, 1, 8, 12, -1, 1, -1, 8, 8, 1, 2, -1, -1, 5, 8, 9, 8, 1, 12, -1, 2, -1, 8, 7, 9, 1, -1, -1, -1, 10, 8, 8, 8, -1, -1, 1, -1, -1, -1, 6, -1, 8, 7, 1, 4, -1, 2, -1, -1, 6, -1, -1, 3, 3, 2, -1, -1, 8, -1, 1, -1, -1, 9, 2, 1, -1, 8, 2, -1, -1, 1, -1, 1, 5, -1, 1, 3, 1, 2, -1, 10, -1, 1, -1, -1, 1, -1, 1, -1, 9, 10, 3, 12, 10, 1, 1, 2, -1, -1, 3, 2, 5, -1, 3, -1, 9, 1, -1, 8, 1, -1, 9, -1, -1, 8, 8, -1, -1, -1, -1, -1, 6, 1, -1, -1, 1, 8, 9, 2, -1, 8, -1, 2, -1, 8, 2, 8, 9, 5, 8, 7, 2, 8, 9, 8, -1, -1, 8, -1, 1, -1, 1, 3, 8, 1, -1, 1, -1, 8, 10, 9, 1, 1, -1, 2, -1, 1, 8, 12, -1, 2, -1, 2, 10, 1, -1, -1, 8, 1, 8, -1, -1, -1, 1, 3, 1, 8, 1, 8, 2, 3, -1, -1, -1, -1, 8, 8, 9, 9, 2, 8, -1, 3, 3, 2, -1, 8, 8, -1, 9, -1, -1, -1, 8, 8, -1, -1, -1, 1, 1, -1, -1, -1, 1, 8, -1, 1, -1, 1, 8, 3, -1, 8, 10, 2, 3, 10, -1, 8, 3, 12, 1, 1, -1, -1, 1, 7, 8, 11, 1, 3, 8, 3, 1, 8, 2, -1, -1, -1, 8, 2, -1, 4, -1, 8, -1, 6, -1, 2, -1, 1, 8, -1, -1, 5, 8, -1, 1, 2, 8, -1, 3, 1, -1, 1, -1, 8, 1, 1, -1, 8, 4, 3, 8, 1, 1, 1, 8, 8, 2, 8, 8, 9, 8, -1, -1, -1, 2, 1, -1, 9, 8, 8, -1, -1, 10, -1, 1, 2, 1, 1, -1, 1, -1, 8, 10, 10, 8, 10, 2, -1, 1, 8, -1, 8, -1, 8, -1, -1, -1, -1, -1, 1, -1, -1, 1, -1, -1, 9, -1, -1, 2, -1, 1, -1, 3, 1, -1, -1, 7, -1, -1, 8, -1, -1, 9, 2, -1, -1, 1, -1, -1, 2, -1, 1, 3, -1, -1, -1, 8, -1, 1, 1, -1, 10, 1, -1, -1, 8, -1, 8, 2, 1, 1, -1, 2, -1, -1, 2, 4, 1, 1, -1, 9, -1, 1, 2, 9, 8, 1, 2, -1, 8, 9, 8, -1, 7, 12, 12, -1, -1, -1]\n",
            "[1, 8, -1, 1, 2, 8, 3, 6, -1, 1, -1, 4, 1, -1, 8, -1, 2, -1, 1, 2, -1, -1, -1, 4, 2, 8, 2, 8, -1, 8, 8, 1, 8, -1, -1, -1, -1, 2, 2, 3, -1, 1, 10, -1, -1, -1, -1, 3, -1, -1, 3, 12, 3, 7, 1, -1, -1, -1, 9, 9, -1, 3, -1, 2, 1, -1, 2, -1, -1, -1, 3, -1, -1, 1, 2, -1, -1, -1, -1, 3, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 2, 8, -1, -1, -1, 1, -1, 10, 8, -1, -1, -1, -1, 9, 1, -1, -1, 4, 8, -1, -1, 2, 1, 1, 8, 9, -1, 3, -1, -1, 8, -1, -1, -1, 2, 9, 3, -1, 8, 9, 9, 1, -1, -1, 9, -1, 1, 1, 1, -1, -1, -1, 10, 9, -1, -1, 8, 3, -1, 9, 2, 1, -1, -1, -1, -1, -1, -1, -1, 2, -1, 8, -1, -1, -1, -1, 8, 9, 4, 7, 3, 9, -1, 2, -1, -1, -1, 9, -1, -1, 3, -1, 9, -1, -1, -1, 8, -1, -1, -1, -1, 8, 8, -1, -1, -1, -1, -1, 1, -1, 3, -1, 1, -1, -1, -1, -1, 9, -1, 8, -1, -1, 2, 9, 1, 1, -1, 8, -1, 2, 8, 9, 4, 2, -1, 7, 3, -1, -1, 2, -1, 1, -1, -1, 8, -1, -1, -1, -1, 3, 1, 8, -1, -1, 1, -1, 7, 8, 1, -1, 9, 6, 3, -1, -1, 3, -1, -1, -1, 2, 8, 5, 1, 3, 9, -1, 2, 8, -1, -1, -1, 9, 8, -1, -1, -1, 8, 2, -1, -1, -1, 12, 2, 8, 1, -1, 2, -1, -1, 3, -1, 7, -1, -1, 8, 2, 1, 5, -1, -1, -1, 1, 2, 1, -1, 8, -1, 3, 8, -1, -1, 1, -1, 5, 1, 1, -1, 8, -1, -1, -1, 9, 2, -1, 9, -1, -1, 8, 2, -1, -1, 5, 1, 1, -1, -1, -1, 8, 2, 8, -1, 8, -1, -1, -1, -1, 8, -1, -1, -1, -1, 3, 3, 8, 1, -1, 1, 1, 9, -1, -1, 12, 3, -1, -1, 8, -1, -1, -1, 2, -1, 1, -1, -1, -1, 3, 6, -1, 1, 1, 10, -1, -1, -1, -1, 9, 10, -1, 12, 1, -1, -1, -1, 7, -1, -1, 8, 8, -1, 1, 8, 10, -1, 3, 6, 9, -1, -1, 2, 2, 8, 3, 1, -1, 1, -1, 1, -1, 5, -1, 8, 1, 8, 12, -1, -1, -1, 8, 8, -1, 2, -1, -1, 1, -1, 9, 10, 1, 12, -1, 1, -1, 8, -1, -1, -1, -1, -1, -1, 7, 8, 8, 8, -1, -1, 1, -1, -1, -1, 6, -1, -1, 7, -1, 4, -1, 2, -1, -1, 6, -1, -1, 3, 3, 2, -1, -1, 8, -1, 1, -1, -1, 9, 8, 1, -1, -1, 4, -1, -1, 1, -1, 4, 9, -1, 1, -1, 1, 2, -1, 10, -1, 2, -1, -1, 1, -1, 1, -1, 9, 10, 3, 12, 10, 1, 1, 2, -1, -1, 3, 2, 5, -1, 1, -1, 7, 1, -1, 7, -1, -1, 9, -1, -1, 8, 8, -1, -1, -1, -1, -1, 6, 1, -1, -1, 1, 8, 8, 2, -1, 8, -1, 2, -1, -1, -1, 8, 10, 5, 8, 7, 2, 8, -1, 8, -1, -1, 7, -1, 1, -1, 2, 3, 8, 1, -1, 1, -1, 8, 8, 9, 8, 1, -1, 2, -1, 1, 8, 12, -1, 2, -1, 9, 10, 1, -1, -1, 8, 1, 8, -1, -1, -1, 1, 3, 1, 8, 5, 8, 2, 3, -1, -1, -1, -1, 8, -1, -1, -1, 2, 8, -1, 8, 3, -1, -1, 8, 4, -1, 9, -1, -1, -1, 8, 8, -1, -1, -1, 1, 1, -1, -1, -1, 2, 8, -1, 1, -1, 1, 8, 3, -1, 8, 10, 2, 3, 10, -1, 8, 3, 12, 1, 8, -1, -1, 1, 7, 8, 11, 1, 3, -1, 3, 1, 8, 7, -1, -1, -1, -1, -1, -1, 2, -1, 8, -1, 6, -1, 2, -1, 1, -1, -1, -1, 5, 8, -1, -1, 2, 8, -1, 10, 1, -1, 1, -1, 8, 1, 1, -1, 8, 8, 3, 8, 1, 1, 1, 3, 8, 2, 8, -1, 8, 8, -1, -1, -1, 2, 1, -1, 9, -1, 8, -1, -1, 9, -1, 1, 2, 1, 1, -1, 1, -1, 8, 8, -1, 8, 8, 2, -1, -1, 8, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, 1, -1, -1, 9, -1, -1, 2, -1, 1, -1, 8, 2, -1, -1, 7, -1, -1, -1, -1, -1, 8, 2, -1, -1, 1, -1, -1, 2, -1, 1, 3, -1, -1, -1, 8, -1, 1, 1, -1, 10, 3, -1, -1, 10, -1, 9, 4, 1, 1, -1, 2, -1, -1, 2, 4, -1, -1, -1, 8, -1, 1, 10, 9, 7, 1, 9, -1, 8, 9, 8, -1, 7, -1, 12, -1, -1, -1]\n",
            "0.7648456057007126\n",
            "[1, 8, 3, 1, 2, 10, 3, 6, 2, 1, 8, 4, 1, 4, 1, 7, 2, 9, 1, 2, 8, 9, 9, 4, 2, 8, 2, 8, 1, 8, 8, 1, 8, 1, 9, 1, 1, 2, 2, 3, 2, 1, 10, 2, 10, 8, 1, 3, 4, 8, 3, 12, 1, 7, 2, 1, 9, 2, 9, 9, 3, 2, 6, 2, 1, 1, 2, 1, 10, 1, 3, 8, 7, 1, 2, 2, 9, 2, 1, 3, 1, 11, 10, 1, 7, 9, 9, 12, 10, 1, 2, 8, 1, 2, 1, 1, 2, 10, 8, 2, 7, 3, 11, 2, 1, 9, 10, 4, 8, 9, 6, 2, 1, 1, 10, 9, 3, 3, 10, 1, 8, 8, 7, 1, 2, 1, 3, 2, 8, 2, 9, 1, 8, 1, 9, 8, 1, 1, 1, 2, 1, 1, 10, 9, 1, 9, 8, 3, 8, 9, 2, 1, 3, 8, 2, 0, 3, 8, 8, 2, 9, 9, 4, 8, 12, 1, 8, 8, 4, 7, 3, 5, 10, 2, 8, 9, 3, 9, 12, 9, 3, 1, 9, 8, 10, 8, 8, 9, 12, 1, 1, 10, 10, 1, 2, 9, 10, 12, 1, 1, 2, 9, 1, 2, 8, 2, 8, 9, 9, 8, 8, 2, 2, 9, 1, 1, 2, 10, 2, 2, 8, 9, 8, 2, 1, 7, 3, 2, 4, 2, 2, 1, 9, 1, 8, 2, 9, 1, 8, 3, 1, 8, 8, 12, 1, 1, 7, 8, 1, 2, 9, 2, 12, 1, 9, 3, 1, 1, 9, 2, 8, 5, 1, 3, 2, 2, 2, 10, 8, 8, 2, 2, 8, 2, 3, 1, 8, 2, 2, 3, 7, 12, 2, 12, 1, 1, 9, 12, 4, 3, 2, 7, 3, 1, 6, 2, 4, 5, 1, 3, 2, 1, 2, 1, 10, 8, 2, 2, 7, 12, 10, 1, 10, 5, 1, 1, 1, 8, 9, 3, 9, 2, 4, 4, 10, 9, 1, 10, 2, 1, 3, 5, 1, 1, 1, 3, 1, 8, 2, 8, 1, 7, 7, 3, 1, 10, 8, 1, 2, 8, 1, 3, 3, 8, 1, 2, 1, 1, 9, 8, 1, 12, 3, 1, 1, 8, 4, 3, 3, 2, 9, 1, 2, 2, 2, 3, 6, 9, 1, 1, 10, 8, 8, 10, 1, 9, 8, 1, 12, 1, 9, 8, 8, 7, 9, 8, 8, 8, 8, 1, 8, 10, 1, 9, 6, 2, 3, 2, 2, 2, 10, 2, 1, 9, 1, 9, 1, 7, 5, 4, 8, 2, 10, 12, 1, 1, 2, 8, 8, 1, 2, 2, 9, 5, 8, 9, 10, 1, 12, 3, 1, 2, 8, 7, 9, 1, 2, 8, 2, 7, 8, 8, 8, 2, 0, 1, 8, 1, 1, 6, 8, 8, 7, 1, 4, 1, 2, 2, 5, 6, 12, 1, 3, 3, 2, 3, 1, 8, 8, 1, 9, 3, 10, 2, 1, 9, 8, 4, 8, 10, 1, 3, 1, 5, 2, 1, 3, 1, 2, 9, 10, 8, 1, 8, 1, 1, 4, 1, 1, 9, 10, 3, 12, 10, 1, 1, 2, 8, 2, 3, 2, 5, 2, 1, 3, 9, 1, 1, 7, 1, 1, 8, 8, 2, 8, 8, 1, 8, 1, 8, 10, 6, 1, 9, 1, 1, 8, 9, 2, 10, 8, 8, 2, 1, 8, 9, 8, 10, 5, 4, 7, 2, 8, 9, 8, 12, 10, 7, 9, 1, 2, 2, 3, 8, 1, 10, 1, 9, 8, 10, 9, 8, 1, 1, 2, 2, 1, 8, 12, 2, 2, 7, 9, 10, 1, 10, 2, 8, 1, 8, 11, 1, 3, 1, 8, 1, 1, 1, 8, 2, 3, 9, 10, 10, 1, 8, 10, 9, 9, 2, 8, 9, 3, 3, 2, 2, 8, 10, 2, 9, 1, 2, 3, 8, 8, 9, 8, 2, 1, 1, 3, 1, 8, 1, 8, 1, 1, 10, 1, 8, 3, 3, 8, 10, 2, 3, 10, 2, 8, 3, 12, 1, 1, 12, 12, 1, 7, 8, 11, 1, 3, 4, 3, 1, 8, 2, 4, 9, 2, 8, 2, 8, 1, 2, 8, 8, 6, 8, 2, 1, 1, 8, 10, 9, 5, 8, 1, 1, 2, 8, 7, 10, 1, 2, 1, 8, 8, 1, 1, 2, 8, 4, 3, 8, 1, 1, 1, 3, 8, 2, 8, 8, 9, 8, 3, 7, 1, 2, 1, 8, 9, 10, 8, 2, 2, 10, 1, 1, 2, 1, 1, 8, 1, 2, 8, 10, 10, 8, 10, 2, 8, 1, 8, 9, 8, 7, 8, 1, 8, 1, 1, 3, 1, 1, 8, 1, 8, 9, 10, 1, 1, 2, 3, 1, 1, 3, 1, 1, 8, 7, 2, 8, 8, 1, 4, 9, 2, 1, 7, 1, 8, 2, 2, 12, 1, 3, 8, 9, 1, 8, 8, 1, 1, 2, 10, 1, 9, 1, 10, 10, 10, 4, 1, 1, 2, 2, 9, 10, 2, 9, 1, 1, 9, 9, 0, 1, 10, 9, 8, 1, 2, 12, 8, 9, 8, 1, 7, 12, 12, 8, 2, 1]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "final_pred_4=[]\n",
        "\n",
        "num_correct=0\n",
        "model_0_4=load_model(loss,accuracy,optimizer,'0-4')\n",
        "mod_labels_0=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_0:\n",
        "    pred_test_0_4=model_0_4.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_0_4 = pred_test_0_4.argmax(axis=1)\n",
        "    mod_labels_0.append(pred_labels_0_4[0])\n",
        "  else:\n",
        "    mod_labels_0.append(-1)\n",
        "\n",
        "model_1_4=load_model(loss,accuracy,optimizer,'1-4')\n",
        "mod_labels_1=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        " \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_1:\n",
        "    pred_test_1_4=model_1_4.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_1_4 = pred_test_1_4.argmax(axis=1)\n",
        "    mod_labels_1.append(pred_labels_1_4[0])\n",
        "  else:\n",
        "    mod_labels_1.append(-1)\n",
        "\n",
        "model_2_4=load_model(loss,accuracy,optimizer,'2-4')\n",
        "mod_labels_2=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_2:\n",
        "    pred_test_2_4=model_2_4.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_2_4 = pred_test_2_4.argmax(axis=1)\n",
        "    mod_labels_2.append(pred_labels_2_4[0])\n",
        "  else:\n",
        "    mod_labels_2.append(-1)\n",
        "\n",
        "model_3_4=load_model(loss,accuracy,optimizer,'3-4')\n",
        "mod_labels_3=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_3:\n",
        "    pred_test_3_4=model_3_4.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_3_4 = pred_test_3_4.argmax(axis=1)\n",
        "    mod_labels_3.append(pred_labels_3_4[0])\n",
        "  else:\n",
        "    mod_labels_3.append(-1)\n",
        "\n",
        "model_4_4=load_model(loss,accuracy,optimizer,'4-4')\n",
        "mod_labels_4=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_4:\n",
        "    pred_test_4_4=model_4_4.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_4_4 = pred_test_4_4.argmax(axis=1)\n",
        "    mod_labels_4.append(pred_labels_4_4[0])\n",
        "  else:\n",
        "    mod_labels_4.append(-1)\n",
        "\n",
        "model_5_4=load_model(loss,accuracy,optimizer,'5-4')\n",
        "mod_labels_5=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_5:\n",
        "    pred_test_5_4=model_5_4.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_5_4 = pred_test_5_4.argmax(axis=1)\n",
        "    mod_labels_5.append(pred_labels_5_4[0])\n",
        "  else:\n",
        "    mod_labels_5.append(-1)\n",
        "\n",
        "print(mod_labels_0)\n",
        "print(mod_labels_1)\n",
        "print(mod_labels_2)\n",
        "print(mod_labels_3)\n",
        "print(mod_labels_4)\n",
        "print(mod_labels_5)\n",
        "\n",
        "\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  fin_labels=[]\n",
        "  if mod_labels_0[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  if mod_labels_1[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  if mod_labels_2[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  if mod_labels_3[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  if mod_labels_4[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  if mod_labels_5[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  res=max(set(fin_labels), key = fin_labels.count)\n",
        "  final_pred_4.append(res)\n",
        "\n",
        "  if res==new_val_label_0[i]:\n",
        "    num_correct=num_correct+1\n",
        "\n",
        "accuracy=num_correct/len(new_val_label_0)\n",
        "print(accuracy)\n",
        "  \n",
        "print(final_pred_4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bhJg0uGdpPP6",
        "outputId": "95e73a84-0c14-402d-f63f-9d47d82fa0ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.7648456057007126\n",
            "Weighted F1: 0.7646855594630132\n",
            "Micro F1: 0.7648456057007126\n",
            "Weighted Precision: 0.7762568156095959\n",
            "Micro Precision: 0.7648456057007126\n",
            "Weighted Recall: 0.7648456057007126\n",
            "Micro Recall: 0.7648456057007126\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "\n",
        "accuracy=accuracy_score(new_val_label_0, final_pred_4)\n",
        "print(\"Accuracy: \"+str(accuracy))\n",
        "total_accuracy=total_accuracy+accuracy\n",
        "\n",
        "weighted_f1=f1_score(new_val_label_0,final_pred_4, average='weighted')\n",
        "print(\"Weighted F1: \"+ str(weighted_f1))\n",
        "total_weighted_f1=total_weighted_f1+weighted_f1\n",
        "micro_f1=f1_score(new_val_label_0,final_pred_4, average='micro')\n",
        "print(\"Micro F1: \"+ str(micro_f1))\n",
        "total_micro_f1=total_micro_f1+micro_f1\n",
        "\n",
        "weighted_precision=precision_score(new_val_label_0, final_pred_4, average='weighted')\n",
        "print(\"Weighted Precision: \" + str(weighted_precision))\n",
        "total_weighted_precision=total_weighted_precision+weighted_precision\n",
        "micro_precision=precision_score(new_val_label_0, final_pred_4, average='micro')\n",
        "print(\"Micro Precision: \" + str(micro_precision))\n",
        "total_micro_precision=total_micro_precision+micro_precision\n",
        "\n",
        "weighted_recall=recall_score(new_val_label_0, final_pred_4, average='weighted')\n",
        "print(\"Weighted Recall: \" + str(weighted_recall))\n",
        "total_weighted_recall=total_weighted_recall+weighted_recall\n",
        "micro_recall=recall_score(new_val_label_0, final_pred_4, average='micro')\n",
        "print(\"Micro Recall: \" + str(micro_recall))\n",
        "total_micro_recall=total_micro_recall+micro_recall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "UnV81NZ-paGM",
        "outputId": "030f4dc7-cb66-461f-c88b-090fa881eaf4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Accuracy: 0.7665083135391924\n",
            "Average Weighted F1: 0.7633636437410567\n",
            "Average Micro F1: 0.7665083135391924\n",
            "Average Weighted Precision: 0.7706085770363593\n",
            "Average Micro Precision: 0.7665083135391924\n",
            "Average Weighted Recall: 0.7665083135391924\n",
            "Average Micro Recall: 0.7665083135391924\n"
          ]
        }
      ],
      "source": [
        "print(\"Average Accuracy: \"+str(total_accuracy/5))\n",
        "print(\"Average Weighted F1: \"+str(total_weighted_f1/5))\n",
        "print(\"Average Micro F1: \"+str(total_micro_f1/5))\n",
        "print(\"Average Weighted Precision: \"+str(total_weighted_precision/5))\n",
        "print(\"Average Micro Precision: \"+str(total_micro_precision/5))\n",
        "print(\"Average Weighted Recall: \"+str(total_weighted_recall/5))\n",
        "print(\"Average Micro Recall: \"+str(total_micro_recall/5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wl_ZrlhOsMJz"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "RoBERTa_Ensemble_15labels.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0187e8e709324358b8e8adf86b4fef51": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0549cad61c314736991691edce9896cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "086075ff60364174a005c02a20d16bc2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0bf30ea7782d4456b3857f41fb26f993": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b00c933bf144e0e9de04e641a3c7bb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1ea34926896c4352a5b3237e0fefd353": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9fc93fdb90f4427d910c43a23cd66e40",
            "placeholder": "​",
            "style": "IPY_MODEL_bbb1c0accaa44221b2b8ccee0a5acdd3",
            "value": " 878k/878k [00:00&lt;00:00, 2.03MB/s]"
          }
        },
        "27584f7e1c8c4f7b95d457d4a8bd9171": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "414c1977a7244908bd111159ad9f8436": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4589b993751d46c5a22c0294b239e95b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4aeb02a1804e43deacab431ccb445e2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "543b38434a8540a0bda6af97aa636c5a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57d2978550fb4faa839010f3327ac3bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5b15c45d641f40a4808ee408a09bea9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_414c1977a7244908bd111159ad9f8436",
            "placeholder": "​",
            "style": "IPY_MODEL_1b00c933bf144e0e9de04e641a3c7bb1",
            "value": "Downloading: 100%"
          }
        },
        "5c9b80ec960d4601bebc9609d5571f49": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c0a03d1b96eb44548a745e1f8c7dad70",
              "IPY_MODEL_bc0adc16bea742d08428c61db660367a",
              "IPY_MODEL_6d9592282d064475b821b3ecb467bdd8"
            ],
            "layout": "IPY_MODEL_64781d94d65d4647b7b50a08bf0680fc"
          }
        },
        "625ee541d52a42349ca3dcd7782c191e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_67f175fae04e442fb3a5ff137bbee7e5",
              "IPY_MODEL_bde15d420aa3447bbad113eb5054c855",
              "IPY_MODEL_f537c8a0a0a9441394fcab519c3019b4"
            ],
            "layout": "IPY_MODEL_9128b5729d9c416696046d2e7bb31cc4"
          }
        },
        "64781d94d65d4647b7b50a08bf0680fc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67f175fae04e442fb3a5ff137bbee7e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8125a3ce13be4916a43f62837b99e1fa",
            "placeholder": "​",
            "style": "IPY_MODEL_0549cad61c314736991691edce9896cf",
            "value": "Downloading: 100%"
          }
        },
        "6d9592282d064475b821b3ecb467bdd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f2161407bc9f4587b439721cf10252db",
            "placeholder": "​",
            "style": "IPY_MODEL_d037e93131ab4c1a9dacaad5e6c320a6",
            "value": " 481/481 [00:00&lt;00:00, 13.7kB/s]"
          }
        },
        "73854b2f8bf04b3286555e91121ff6aa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "755d35cb5a58400cb2eec8804ab652f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7a18f8605f624849b227f87398449795": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7e1d6d93f06d4a63b696f37c9cf93277": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0bf30ea7782d4456b3857f41fb26f993",
            "placeholder": "​",
            "style": "IPY_MODEL_4589b993751d46c5a22c0294b239e95b",
            "value": " 446k/446k [00:00&lt;00:00, 608kB/s]"
          }
        },
        "8125a3ce13be4916a43f62837b99e1fa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8278ca504261441eadefc4d2f72f693b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8758d872f8fd4e3195bd0d6e68923ff8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_99fbe21a71b04d0d834792309af01c97",
              "IPY_MODEL_a716e417a0684d0cbc9f63bf69b84ee8",
              "IPY_MODEL_1ea34926896c4352a5b3237e0fefd353"
            ],
            "layout": "IPY_MODEL_b135e3191d644c03821749babe145797"
          }
        },
        "9128b5729d9c416696046d2e7bb31cc4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9202f64e927348d29ba6d42a9c0b05fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "99fbe21a71b04d0d834792309af01c97": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f9ff88f5e7ae40a8aebe70ebfad969ef",
            "placeholder": "​",
            "style": "IPY_MODEL_27584f7e1c8c4f7b95d457d4a8bd9171",
            "value": "Downloading: 100%"
          }
        },
        "9fc93fdb90f4427d910c43a23cd66e40": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1c6a5379c4d4f148e0492bb7f600d13": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a716e417a0684d0cbc9f63bf69b84ee8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8278ca504261441eadefc4d2f72f693b",
            "max": 898823,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0187e8e709324358b8e8adf86b4fef51",
            "value": 898823
          }
        },
        "b135e3191d644c03821749babe145797": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bbb1c0accaa44221b2b8ccee0a5acdd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bc0adc16bea742d08428c61db660367a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_edd7e7645ddb4d15862ca653ea4ad995",
            "max": 481,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7a18f8605f624849b227f87398449795",
            "value": 481
          }
        },
        "bde15d420aa3447bbad113eb5054c855": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_086075ff60364174a005c02a20d16bc2",
            "max": 657434796,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9202f64e927348d29ba6d42a9c0b05fe",
            "value": 657434796
          }
        },
        "c0a03d1b96eb44548a745e1f8c7dad70": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1c6a5379c4d4f148e0492bb7f600d13",
            "placeholder": "​",
            "style": "IPY_MODEL_57d2978550fb4faa839010f3327ac3bd",
            "value": "Downloading: 100%"
          }
        },
        "cd26a756126f4df2b70f85679c04c093": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d037e93131ab4c1a9dacaad5e6c320a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "de60f30acd29404f813e8b78da5e35a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd26a756126f4df2b70f85679c04c093",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4aeb02a1804e43deacab431ccb445e2a",
            "value": 456318
          }
        },
        "e2034e0c6fad45e8a5713c0da190a335": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5b15c45d641f40a4808ee408a09bea9a",
              "IPY_MODEL_de60f30acd29404f813e8b78da5e35a8",
              "IPY_MODEL_7e1d6d93f06d4a63b696f37c9cf93277"
            ],
            "layout": "IPY_MODEL_543b38434a8540a0bda6af97aa636c5a"
          }
        },
        "edd7e7645ddb4d15862ca653ea4ad995": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2161407bc9f4587b439721cf10252db": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f537c8a0a0a9441394fcab519c3019b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_73854b2f8bf04b3286555e91121ff6aa",
            "placeholder": "​",
            "style": "IPY_MODEL_755d35cb5a58400cb2eec8804ab652f2",
            "value": " 627M/627M [00:10&lt;00:00, 67.6MB/s]"
          }
        },
        "f9ff88f5e7ae40a8aebe70ebfad969ef": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}