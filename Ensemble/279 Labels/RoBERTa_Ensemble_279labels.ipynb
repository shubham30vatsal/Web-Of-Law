{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mj3gUkvvEZW2"
      },
      "source": [
        "### <font color='blue'>Import all packages</font> ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVqcG_Q_FX4T",
        "outputId": "5a7bf4ed-16d2-4883-c32d-89d68964c8d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "#drive.flush_and_unmount()\n",
        "drive.mount('/content/drive')\n",
        "#drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "AwA6C92yJyaP",
        "outputId": "9be8aa87-b9c7-4792-9005-e38c8c324bfb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 14.2 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 47.7 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 82.1 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.5.18.1)\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.7.0 pyyaml-6.0 tokenizers-0.12.1 transformers-4.19.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 13.7 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.96\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==2.7.0\n",
            "  Downloading https://us-python.pkg.dev/colab-wheels/public/tensorflow/tensorflow-2.7.0%2Bzzzcolab20220506150900-cp37-cp37m-linux_x86_64.whl\n",
            "\u001b[K     \\ 665.5 MB 126.4 MB/s\n",
            "\u001b[?25hRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (1.6.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (0.2.0)\n",
            "Collecting tensorflow-estimator<2.8,~=2.7.0rc0\n",
            "  Downloading tensorflow_estimator-2.7.0-py2.py3-none-any.whl (463 kB)\n",
            "\u001b[K     |████████████████████████████████| 463 kB 13.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (1.14.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (3.3.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (1.15.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (3.1.0)\n",
            "Collecting gast<0.5.0,>=0.2.1\n",
            "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (3.17.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (1.46.3)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (2.8.0)\n",
            "Collecting keras<2.8,>=2.7.0rc0\n",
            "  Downloading keras-2.7.0-py2.py3-none-any.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 67.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (0.26.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (1.1.0)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (2.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (0.37.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (1.1.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (4.2.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (14.0.1)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (1.21.6)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (1.0.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow==2.7.0) (1.5.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (0.4.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (57.4.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (1.35.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (2.23.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (3.3.7)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (1.8.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.7.0) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow==2.7.0) (4.11.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow==2.7.0) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0) (2022.5.18.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.7.0) (3.2.0)\n",
            "Installing collected packages: tensorflow-estimator, keras, gast, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.8.0\n",
            "    Uninstalling tensorflow-estimator-2.8.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.8.0\n",
            "    Uninstalling keras-2.8.0:\n",
            "      Successfully uninstalled keras-2.8.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.3\n",
            "    Uninstalling gast-0.5.3:\n",
            "      Successfully uninstalled gast-0.5.3\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.8.2+zzzcolab20220527125636\n",
            "    Uninstalling tensorflow-2.8.2+zzzcolab20220527125636:\n",
            "      Successfully uninstalled tensorflow-2.8.2+zzzcolab20220527125636\n",
            "Successfully installed gast-0.4.0 keras-2.7.0 tensorflow-2.7.0+zzzcolab20220506150900 tensorflow-estimator-2.7.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting stanza\n",
            "  Downloading stanza-1.4.0-py3-none-any.whl (574 kB)\n",
            "\u001b[K     |████████████████████████████████| 574 kB 14.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from stanza) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from stanza) (1.21.6)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from stanza) (3.17.3)\n",
            "Collecting emoji\n",
            "  Downloading emoji-1.7.0.tar.gz (175 kB)\n",
            "\u001b[K     |████████████████████████████████| 175 kB 77.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from stanza) (1.11.0+cu113)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (from stanza) (4.19.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from stanza) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from stanza) (4.64.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3.0->stanza) (4.2.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->stanza) (2022.5.18.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->stanza) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->stanza) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->stanza) (2.10)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers->stanza) (0.12.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers->stanza) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers->stanza) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers->stanza) (3.7.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers->stanza) (4.11.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers->stanza) (6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers->stanza) (0.7.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers->stanza) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers->stanza) (3.8.0)\n",
            "Building wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171046 sha256=f58f81494cbfd56bf59f0cf175284a0e0f7dd8e0e4eff1ff1d073fc5051f0041\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/4e/b6/57b01db010d17ef6ea9b40300af725ef3e210cb1acfb7ac8b6\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji, stanza\n",
            "Successfully installed emoji-1.7.0 stanza-1.4.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.19.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.5.18.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.17.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 9.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow-addons) (3.0.9)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.17.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting textacy\n",
            "  Downloading textacy-0.11.0-py3-none-any.whl (200 kB)\n",
            "\u001b[K     |████████████████████████████████| 200 kB 14.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.13.0 in /usr/local/lib/python3.7/dist-packages (from textacy) (1.1.0)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from textacy) (2.6.3)\n",
            "Collecting cytoolz>=0.10.1\n",
            "  Downloading cytoolz-0.11.2.tar.gz (481 kB)\n",
            "\u001b[K     |████████████████████████████████| 481 kB 82.8 MB/s \n",
            "\u001b[?25hCollecting spacy>=3.0.0\n",
            "  Downloading spacy-3.3.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.2 MB 65.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from textacy) (1.21.6)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from textacy) (2.23.0)\n",
            "Requirement already satisfied: cachetools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from textacy) (4.2.4)\n",
            "Collecting jellyfish>=0.8.0\n",
            "  Downloading jellyfish-0.9.0.tar.gz (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 74.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from textacy) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.19.0 in /usr/local/lib/python3.7/dist-packages (from textacy) (1.0.2)\n",
            "Requirement already satisfied: tqdm>=4.19.6 in /usr/local/lib/python3.7/dist-packages (from textacy) (4.64.0)\n",
            "Collecting pyphen>=0.10.0\n",
            "  Downloading pyphen-0.12.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0 MB 86.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from cytoolz>=0.10.1->textacy) (0.11.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->textacy) (2022.5.18.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->textacy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->textacy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->textacy) (1.24.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.0->textacy) (3.1.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (0.4.1)\n",
            "Collecting pathy>=0.3.5\n",
            "  Downloading pathy-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (1.0.7)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (2.0.6)\n",
            "Collecting catalogue<2.1.0,>=2.0.6\n",
            "  Downloading catalogue-2.0.7-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (3.0.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (2.11.3)\n",
            "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
            "  Downloading spacy_loggers-1.0.2-py3-none-any.whl (7.2 kB)\n",
            "Collecting langcodes<4.0.0,>=3.2.0\n",
            "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 75.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (21.3)\n",
            "Collecting thinc<8.1.0,>=8.0.14\n",
            "  Downloading thinc-8.0.17-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (660 kB)\n",
            "\u001b[K     |████████████████████████████████| 660 kB 80.7 MB/s \n",
            "\u001b[?25hCollecting typer<0.5.0,>=0.3.0\n",
            "  Downloading typer-0.4.1-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (0.9.1)\n",
            "Collecting srsly<3.0.0,>=2.4.3\n",
            "  Downloading srsly-2.4.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (457 kB)\n",
            "\u001b[K     |████████████████████████████████| 457 kB 79.0 MB/s \n",
            "\u001b[?25hCollecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
            "  Downloading pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 73.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (57.4.0)\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.9\n",
            "  Downloading spacy_legacy-3.0.9-py2.py3-none-any.whl (20 kB)\n",
            "Collecting typing-extensions<4.0.0.0,>=3.7.4\n",
            "  Downloading typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy>=3.0.0->textacy) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy>=3.0.0->textacy) (3.0.9)\n",
            "Collecting smart-open<6.0.0,>=5.0.0\n",
            "  Downloading smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 5.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy>=3.0.0->textacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy>=3.0.0->textacy) (2.0.1)\n",
            "Building wheels for collected packages: cytoolz, jellyfish\n",
            "  Building wheel for cytoolz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cytoolz: filename=cytoolz-0.11.2-cp37-cp37m-linux_x86_64.whl size=1236746 sha256=506c328b744eb1499af632ccc7b1fd4f5af3e2200b041bbd505114d2577eee0c\n",
            "  Stored in directory: /root/.cache/pip/wheels/38/70/71/ca13ea3d36ccd0b3d0ec7d7a4ca67522048d695b556bba4f59\n",
            "  Building wheel for jellyfish (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jellyfish: filename=jellyfish-0.9.0-cp37-cp37m-linux_x86_64.whl size=73995 sha256=b2fb845dfdede1265104d922ff1b0fcdb71b037c9eb7cc86731745d3de6a4178\n",
            "  Stored in directory: /root/.cache/pip/wheels/fe/99/4e/646ce766df0d070b0ef04db27aa11543e2767fda3075aec31b\n",
            "Successfully built cytoolz jellyfish\n",
            "Installing collected packages: typing-extensions, catalogue, typer, srsly, smart-open, pydantic, thinc, spacy-loggers, spacy-legacy, pathy, langcodes, spacy, pyphen, jellyfish, cytoolz, textacy\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing-extensions 4.2.0\n",
            "    Uninstalling typing-extensions-4.2.0:\n",
            "      Successfully uninstalled typing-extensions-4.2.0\n",
            "  Attempting uninstall: catalogue\n",
            "    Found existing installation: catalogue 1.0.0\n",
            "    Uninstalling catalogue-1.0.0:\n",
            "      Successfully uninstalled catalogue-1.0.0\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 1.0.5\n",
            "    Uninstalling srsly-1.0.5:\n",
            "      Successfully uninstalled srsly-1.0.5\n",
            "  Attempting uninstall: smart-open\n",
            "    Found existing installation: smart-open 6.0.0\n",
            "    Uninstalling smart-open-6.0.0:\n",
            "      Successfully uninstalled smart-open-6.0.0\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "Successfully installed catalogue-2.0.7 cytoolz-0.11.2 jellyfish-0.9.0 langcodes-3.3.0 pathy-0.6.1 pydantic-1.8.2 pyphen-0.12.0 smart-open-5.2.1 spacy-3.3.0 spacy-legacy-3.0.9 spacy-loggers-1.0.2 srsly-2.4.3 textacy-0.11.0 thinc-8.0.17 typer-0.4.1 typing-extensions-3.10.0.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "typing_extensions"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "!pip install tensorflow==2.7.0\n",
        "!pip install stanza\n",
        "!pip install transformers\n",
        "!pip install tensorflow-addons\n",
        "!pip install nltk\n",
        "!pip install textacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jV9qKCQ3qpl",
        "outputId": "4d51ca84-137e-4fe5-944f-a5c0dd74f1b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.7.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "DC59AD4KEZW7"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from textacy.datasets.supreme_court import SupremeCourt\n",
        "import numpy as np\n",
        "import re\n",
        "import unicodedata\n",
        "import nltk\n",
        "#from transformers import pipeline\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Dense,Dropout, Input, BatchNormalization\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "from sklearn.metrics import confusion_matrix,f1_score,classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "from sklearn.utils import shuffle\n",
        "from tensorflow.keras import regularizers\n",
        "#from transformers import *\n",
        "from transformers import BertTokenizer, TFBertModel, BertConfig,TFDistilBertModel,DistilBertTokenizer,DistilBertConfig\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, TFAutoModel\n",
        "import numpy as np\n",
        "import gc\n",
        "import math\n",
        "import json\n",
        "import stanza\n",
        "from tensorflow.keras import *\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import *\n",
        "import tensorflow.keras.backend as K\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import classification_report\n",
        "from transformers import TFRobertaModel,RobertaTokenizer\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.initializers import RandomUniform\n",
        "\n",
        "from numpy.random import seed\n",
        "import random as python_random\n",
        "import os\n",
        "import sys\n",
        "\n",
        "np.random.seed(1)\n",
        "python_random.seed(1)\n",
        "tf.random.set_seed(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "OzfozpOUBkOX"
      },
      "outputs": [],
      "source": [
        "!cp \"/content/drive/My Drive/summarized_sc.txt\" \"./summarized_sc.txt\"\n",
        "!cp \"/content/drive/My Drive/labels_sc.txt\" \"./labels_sc.txt\"\n",
        "!cp \"/content/drive/My Drive/labels_sc_279.txt\" \"./labels_sc_279.txt\"\n",
        "!cp \"/content/drive/My Drive/sc_model_0.txt\" \"./sc_model_0.txt\"\n",
        "!cp \"/content/drive/My Drive/sc_model_1.txt\" \"./sc_model_1.txt\"\n",
        "!cp \"/content/drive/My Drive/sc_model_2.txt\" \"./sc_model_2.txt\"\n",
        "!cp \"/content/drive/My Drive/sc_model_3.txt\" \"./sc_model_3.txt\"\n",
        "!cp \"/content/drive/My Drive/sc_model_4.txt\" \"./sc_model_4.txt\"\n",
        "!cp \"/content/drive/My Drive/sc_model_5.txt\" \"./sc_model_5.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cVKqFgEEWBz",
        "outputId": "a192a1a7-3881-45c8-e9ee-7e222302b866"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8419\n",
            "8419\n",
            "Average Length 489.8335906877301\n",
            "['209', '63', '216', '108', '196', '200', '203', '246', '188', '193', '25', '106', '56', '53', '223', '161', '200', '208', '78', '147', '147', '179', '147', '80', '56', '209', '209', '200', '214', '202', '203', '194', '26', '237', '165', '258', '238', '238', '76', '193', '196', '213', '161', '213', '209', '194', '196', '25', '151', '260', '135', '173', '30', '108', '272', '259', '176', '213', '213', '21', '5', '171', '200', '195', '240', '194', '176', '240', '240', '80', '173', '98', '214', '21', '76', '76', '73', '173', '173', '78', '61', '258', '191', '3', '78', '173', '193', '118', '200', '264', '260', '49', '211', '258', '18', '173', '78', '3', '3', '1', '213', '135', '182', '239', '208', '278', '50', '194', '200', '124', '189', '117', '264', '244', '76', '193', '221', '78', '78', '173', '197', '78', '161', '231', '135', '22', '78', '25', '25', '197', '200', '147', '203', '191', '196', '193', '263', '32', '197', '191', '4', '56', '196', '104', '191', '196', '173', '147', '194', '96', '161', '161', '238', '200', '3', '78', '78', '53', '41', '237', '53', '18', '10', '194', '97', '96', '208', '25', '10', '117', '104', '54', '96', '194', '21', '245', '238', '202', '209', '161', '97', '258', '240', '213', '136', '155', '152', '21', '191', '191', '118', '68', '258', '189', '197', '208', '61', '78', '135', '202', '96', '155', '108', '272', '80', '25', '155', '35', '213', '191', '194', '96', '96', '191', '264', '191', '191', '191', '2', '258', '191', '173', '259', '15', '47', '258', '156', '118', '160', '115', '105', '26', '173', '192', '76', '76', '135', '240', '118', '161', '200', '230', '18', '200', '25', '10', '161', '244', '24', '24', '200', '205', '104', '15', '193', '156', '95', '209', '240', '191', '194', '128', '208', '208', '218', '173', '10', '25', '18', '107', '46', '32', '197', '172', '209', '213', '79', '121', '78', '78', '193', '2', '78', '194', '149', '136', '80', '3', '21', '200', '78', '21', '197', '194', '118', '202', '161', '180', '209', '173', '2', '193', '6', '260', '197', '78', '78', '42', '191', '213', '135', '193', '196', '202', '222', '217', '42', '259', '213', '76', '46', '30', '15', '260', '202', '18', '259', '104', '135', '76', '196', '240', '240', '240', '108', '24', '200', '197', '262', '176', '15', '156', '264', '156', '200', '191', '161', '196', '78', '238', '161', '221', '118', '14', '14', '14', '200', '278', '238', '200', '196', '78', '173', '25', '197', '197', '196', '161', '18', '10', '10', '10', '18', '66', '203', '19', '246', '196', '244', '240', '161', '200', '78', '240', '209', '208', '118', '215', '194', '193', '168', '229', '191', '135', '172', '196', '197', '161', '78', '196', '197', '258', '78', '78', '238', '221', '121', '264', '104', '146', '50', '78', '217', '11', '238', '238', '238', '3', '3', '154', '215', '79', '18', '158', '104', '18', '13', '197', '117', '197', '161', '213', '242', '32', '238', '230', '2', '2', '32', '161', '251', '96', '156', '242', '22', '22', '140', '3', '149', '135', '191', '173', '135', '200', '238', '238', '78', '156', '209', '78', '97', '97', '160', '25', '258', '240', '161', '15', '161', '96', '209', '80', '132', '140', '4', '1', '240', '196', '274', '272', '62', '80', '196', '196', '191', '79', '156', '15', '196', '209', '22', '251', '202', '202', '121', '73', '191', '238', '191', '150', '150', '135', '46', '203', '202', '238', '22', '3', '258', '239', '238', '238', '165', '238', '238', '194', '200', '200', '197', '200', '196', '191', '194', '76', '78', '208', '200', '71', '208', '240', '76', '117', '120', '108', '96', '144', '30', '133', '133', '202', '117', '161', '251', '29', '104', '242', '197', '118', '216', '262', '236', '141', '11', '235', '235', '133', '191', '238', '202', '80', '80', '22', '140', '191', '197', '202', '133', '184', '184', '184', '184', '143', '274', '50', '237', '196', '274', '265', '10', '18', '10', '121', '238', '245', '228', '76', '25', '191', '197', '268', '10', '240', '195', '196', '195', '15', '48', '200', '183', '70', '59', '197', '196', '235', '161', '161', '215', '104', '97', '191', '194', '191', '200', '200', '197', '76', '200', '93', '256', '160', '238', '146', '238', '239', '15', '140', '258', '140', '21', '79', '202', '258', '79', '62', '205', '161', '104', '10', '161', '196', '137', '152', '191', '117', '213', '242', '176', '218', '195', '191', '162', '191', '135', '241', '213', '213', '84', '79', '68', '18', '98', '197', '155', '41', '246', '269', '78', '209', '238', '80', '238', '62', '237', '273', '273', '230', '150', '121', '260', '244', '161', '202', '78', '109', '155', '264', '145', '50', '274', '173', '210', '258', '238', '211', '156', '215', '195', '177', '244', '3', '141', '196', '191', '194', '29', '30', '10', '16', '200', '200', '156', '53', '3', '62', '165', '173', '173', '22', '205', '274', '150', '183', '116', '188', '188', '98', '161', '204', '80', '172', '156', '104', '229', '259', '78', '169', '3', '200', '118', '161', '80', '260', '240', '150', '197', '196', '62', '93', '6', '272', '272', '197', '160', '78', '96', '70', '195', '191', '191', '80', '3', '3', '240', '147', '196', '191', '238', '96', '203', '3', '15', '10', '58', '55', '96', '41', '15', '26', '50', '238', '194', '79', '240', '161', '191', '195', '263', '50', '240', '147', '235', '195', '203', '59', '178', '176', '2', '15', '203', '191', '18', '278', '196', '14', '153', '58', '175', '18', '217', '217', '80', '191', '200', '18', '200', '173', '24', '171', '171', '210', '124', '34', '200', '218', '104', '4', '223', '193', '194', '200', '200', '13', '62', '194', '197', '80', '197', '146', '146', '96', '97', '97', '2', '196', '191', '191', '104', '238', '10', '200', '239', '135', '104', '213', '2', '221', '169', '25', '21', '194', '161', '21', '278', '3', '195', '46', '176', '25', '117', '191', '62', '62', '62', '78', '50', '264', '79', '197', '80', '80', '80', '191', '191', '78', '161', '194', '195', '197', '203', '259', '278', '49', '147', '147', '147', '147', '22', '78', '78', '258', '2', '202', '221', '55', '258', '104', '104', '84', '240', '221', '274', '104', '22', '241', '63', '197', '197', '196', '159', '200', '22', '22', '22', '78', '78', '173', '104', '97', '104', '191', '146', '223', '268', '96', '14', '173', '204', '241', '97', '15', '118', '78', '78', '196', '194', '258', '96', '96', '259', '75', '10', '197', '30', '2', '238', '121', '50', '161', '173', '173', '186', '185', '186', '78', '194', '221', '218', '35', '33', '13', '34', '78', '21', '215', '97', '60', '24', '78', '193', '173', '235', '78', '200', '258', '140', '259', '194', '197', '195', '200', '145', '76', '108', '130', '238', '215', '213', '2', '217', '121', '178', '140', '62', '156', '176', '196', '196', '41', '223', '147', '172', '79', '200', '2', '244', '244', '191', '213', '104', '191', '237', '237', '33', '117', '195', '195', '264', '195', '140', '210', '41', '197', '14', '257', '196', '238', '238', '79', '161', '156', '196', '238', '238', '213', '104', '3', '10', '118', '204', '235', '33', '259', '274', '259', '176', '78', '78', '196', '34', '52', '25', '30', '194', '153', '21', '70', '191', '18', '191', '191', '172', '194', '194', '194', '194', '148', '104', '125', '2', '2', '2', '185', '194', '29', '104', '78', '189', '272', '204', '183', '213', '78', '104', '197', '240', '97', '168', '168', '182', '238', '197', '18', '194', '229', '221', '62', '78', '53', '104', '214', '189', '189', '189', '213', '117', '117', '191', '29', '104', '104', '276', '276', '22', '195', '195', '155', '139', '139', '59', '168', '135', '140', '79', '146', '200', '245', '70', '160', '153', '50', '202', '154', '263', '130', '194', '117', '194', '194', '191', '33', '29', '29', '238', '239', '194', '21', '274', '215', '18', '18', '140', '2', '215', '25', '46', '137', '238', '30', '160', '156', '46', '196', '263', '242', '104', '0', '0', '73', '213', '135', '22', '191', '191', '241', '202', '221', '15', '15', '191', '238', '213', '194', '197', '179', '238', '200', '200', '200', '5', '238', '194', '55', '117', '166', '49', '244', '239', '191', '78', '78', '78', '194', '200', '117', '210', '22', '121', '121', '121', '21', '237', '194', '121', '78', '104', '194', '79', '191', '251', '195', '197', '274', '176', '21', '43', '43', '10', '61', '215', '215', '30', '214', '194', '10', '30', '139', '96', '226', '226', '265', '2', '2', '121', '121', '41', '237', '104', '238', '238', '161', '78', '78', '78', '11', '184', '140', '140', '161', '104', '104', '237', '130', '195', '160', '118', '161', '18', '11', '241', '184', '196', '178', '22', '30', '146', '146', '10', '10', '141', '141', '135', '18', '18', '10', '26', '208', '97', '141', '18', '18', '97', '194', '194', '79', '213', '205', '205', '205', '75', '241', '50', '226', '238', '97', '222', '215', '215', '139', '238', '51', '238', '96', '173', '109', '200', '117', '192', '2', '195', '10', '3', '18', '49', '238', '238', '191', '195', '52', '197', '200', '200', '79', '191', '200', '200', '30', '195', '197', '195', '25', '260', '73', '30', '193', '21', '200', '242', '203', '120', '78', '260', '241', '0', '30', '202', '195', '30', '191', '78', '194', '194', '2', '205', '191', '196', '173', '197', '30', '196', '172', '139', '2', '18', '191', '230', '56', '194', '215', '195', '2', '71', '13', '146', '264', '50', '191', '235', '93', '191', '139', '139', '227', '235', '186', '235', '24', '18', '261', '128', '33', '71', '25', '176', '39', '10', '195', '202', '29', '29', '221', '29', '197', '139', '78', '227', '146', '137', '195', '205', '205', '195', '135', '62', '153', '146', '29', '75', '0', '194', '194', '180', '213', '78', '18', '194', '239', '196', '239', '197', '194', '153', '192', '221', '10', '46', '50', '241', '117', '117', '117', '195', '173', '78', '195', '194', '239', '2', '12', '173', '197', '30', '269', '197', '176', '135', '30', '146', '191', '93', '93', '135', '260', '218', '156', '155', '200', '241', '125', '18', '18', '183', '203', '239', '259', '173', '213', '180', '180', '210', '120', '104', '194', '156', '258', '172', '9', '0', '241', '192', '143', '189', '204', '50', '63', '197', '243', '241', '93', '125', '2', '21', '260', '4', '4', '3', '191', '150', '128', '78', '238', '245', '8', '80', '238', '79', '79', '240', '58', '104', '155', '80', '80', '188', '191', '235', '170', '170', '170', '146', '258', '195', '276', '25', '78', '215', '240', '161', '53', '147', '78', '78', '78', '78', '221', '221', '96', '18', '19', '130', '18', '18', '18', '80', '80', '194', '79', '93', '121', '78', '139', '139', '78', '197', '121', '194', '200', '80', '101', '22', '12', '135', '97', '205', '181', '60', '193', '15', '140', '172', '191', '241', '222', '135', '29', '25', '179', '191', '191', '259', '194', '133', '240', '121', '121', '241', '195', '29', '191', '209', '18', '139', '139', '50', '18', '12', '18', '133', '200', '25', '10', '70', '25', '242', '18', '161', '49', '172', '172', '172', '172', '128', '96', '195', '200', '78', '10', '239', '22', '173', '142', '142', '22', '194', '192', '3', '200', '241', '96', '241', '62', '139', '213', '202', '135', '79', '120', '192', '78', '118', '241', '46', '151', '150', '151', '150', '118', '221', '183', '237', '196', '140', '140', '39', '179', '213', '140', '41', '73', '78', '217', '258', '30', '118', '140', '219', '50', '10', '139', '150', '251', '24', '10', '18', '78', '18', '172', '135', '146', '71', '196', '202', '197', '25', '106', '160', '215', '15', '6', '101', '165', '140', '145', '80', '3', '238', '239', '78', '268', '0', '109', '191', '215', '238', '238', '238', '203', '50', '146', '15', '25', '156', '191', '16', '2', '195', '189', '96', '193', '109', '204', '161', '185', '210', '50', '235', '30', '242', '193', '102', '194', '121', '177', '241', '102', '132', '240', '71', '192', '79', '25', '197', '39', '161', '22', '258', '237', '15', '7', '78', '156', '197', '70', '176', '139', '240', '132', '78', '177', '191', '194', '117', '10', '79', '238', '195', '24', '101', '118', '196', '2', '102', '21', '180', '185', '170', '79', '139', '135', '191', '151', '200', '191', '154', '262', '200', '237', '56', '27', '191', '180', '258', '139', '139', '192', '30', '0', '242', '239', '242', '25', '191', '242', '240', '3', '56', '215', '171', '192', '193', '222', '238', '237', '215', '272', '2', '156', '237', '151', '189', '80', '15', '260', '260', '217', '235', '22', '228', '135', '18', '191', '157', '78', '2', '240', '213', '179', '2', '191', '79', '79', '153', '3', '194', '78', '121', '71', '121', '149', '257', '194', '191', '188', '15', '25', '125', '102', '15', '130', '130', '177', '10', '135', '265', '240', '149', '196', '200', '204', '204', '213', '177', '197', '194', '13', '202', '30', '213', '125', '193', '125', '125', '125', '125', '125', '15', '180', '78', '1', '25', '96', '200', '29', '156', '172', '202', '22', '78', '197', '188', '241', '101', '101', '101', '221', '50', '191', '15', '15', '0', '101', '101', '168', '56', '197', '130', '62', '204', '29', '10', '96', '97', '139', '97', '97', '2', '2', '262', '39', '176', '172', '194', '194', '18', '78', '13', '80', '139', '196', '207', '191', '151', '192', '150', '13', '104', '104', '101', '15', '125', '101', '15', '0', '50', '194', '264', '50', '25', '25', '125', '125', '125', '125', '125', '25', '29', '25', '25', '78', '240', '261', '19', '2', '74', '78', '238', '50', '50', '203', '197', '71', '104', '80', '194', '149', '170', '130', '2', '237', '170', '202', '48', '97', '93', '179', '235', '78', '102', '102', '70', '208', '238', '104', '146', '118', '118', '96', '237', '274', '218', '241', '102', '146', '209', '209', '240', '170', '137', '7', '197', '191', '264', '29', '46', '96', '193', '19', '12', '213', '215', '275', '179', '18', '183', '78', '221', '18', '170', '119', '21', '140', '240', '96', '192', '192', '21', '96', '149', '239', '135', '191', '240', '184', '183', '24', '2', '96', '197', '194', '215', '18', '192', '121', '202', '196', '25', '97', '214', '3', '97', '16', '0', '192', '135', '215', '5', '12', '80', '143', '202', '242', '151', '13', '209', '203', '200', '30', '209', '194', '102', '102', '102', '102', '102', '102', '22', '24', '6', '18', '5', '191', '101', '50', '101', '101', '192', '153', '153', '101', '101', '10', '192', '25', '140', '50', '241', '102', '102', '32', '150', '101', '102', '153', '102', '101', '153', '102', '25', '10', '125', '18', '125', '177', '209', '102', '10', '78', '78', '137', '18', '237', '241', '175', '195', '175', '96', '176', '237', '96', '96', '101', '193', '238', '170', '238', '102', '222', '268', '238', '102', '244', '34', '18', '244', '202', '149', '149', '193', '102', '170', '194', '75', '101', '222', '238', '50', '195', '135', '22', '93', '18', '102', '102', '93', '93', '191', '147', '244', '96', '2', '241', '241', '93', '189', '189', '177', '35', '137', '197', '191', '40', '40', '195', '176', '35', '200', '200', '237', '135', '140', '140', '93', '156', '200', '78', '191', '22', '81', '238', '101', '213', '101', '78', '109', '19', '140', '196', '78', '78', '76', '218', '14', '265', '101', '241', '155', '4', '50', '78', '78', '238', '135', '191', '265', '81', '238', '238', '102', '102', '102', '140', '162', '32', '17', '17', '171', '171', '191', '101', '102', '192', '101', '17', '171', '135', '102', '242', '195', '237', '21', '18', '238', '191', '278', '140', '149', '97', '260', '155', '276', '192', '196', '21', '191', '2', '97', '0', '150', '221', '213', '209', '238', '80', '260', '194', '104', '96', '193', '14', '240', '193', '117', '117', '155', '17', '264', '260', '209', '209', '2', '137', '71', '72', '101', '205', '173', '191', '118', '24', '41', '197', '78', '94', '237', '156', '193', '153', '154', '153', '209', '110', '78', '78', '213', '238', '78', '191', '93', '79', '177', '267', '133', '133', '46', '139', '40', '143', '74', '241', '202', '50', '3', '102', '78', '149', '191', '156', '18', '93', '195', '192', '137', '240', '135', '204', '189', '22', '56', '154', '25', '192', '130', '277', '191', '80', '39', '21', '21', '238', '238', '23', '192', '191', '192', '94', '94', '80', '139', '17', '10', '22', '227', '227', '140', '102', '21', '17', '227', '227', '23', '243', '264', '18', '274', '149', '238', '30', '221', '22', '238', '193', '135', '205', '195', '238', '238', '197', '130', '195', '18', '104', '102', '14', '59', '104', '18', '18', '258', '34', '241', '138', '188', '176', '102', '1', '140', '140', '22', '22', '278', '10', '96', '35', '145', '120', '33', '33', '22', '33', '22', '35', '22', '19', '33', '102', '192', '192', '175', '22', '72', '191', '222', '125', '22', '79', '18', '194', '18', '250', '192', '192', '96', '48', '274', '273', '132', '133', '192', '40', '171', '171', '246', '191', '10', '166', '197', '195', '125', '125', '200', '153', '192', '149', '110', '139', '194', '48', '102', '102', '104', '203', '238', '238', '238', '78', '18', '238', '121', '18', '238', '96', '238', '238', '13', '191', '18', '18', '240', '96', '40', '188', '18', '137', '175', '28', '28', '28', '149', '191', '191', '207', '239', '239', '153', '153', '153', '153', '153', '153', '154', '153', '154', '153', '153', '153', '18', '0', '27', '18', '25', '96', '96', '137', '33', '10', '130', '153', '153', '154', '180', '137', '44', '241', '153', '268', '25', '161', '25', '39', '242', '4', '238', '195', '102', '135', '13', '229', '140', '1', '79', '192', '229', '191', '118', '21', '260', '203', '18', '177', '191', '10', '238', '10', '133', '14', '175', '175', '192', '18', '153', '153', '254', '140', '149', '22', '22', '22', '237', '40', '135', '191', '192', '192', '62', '205', '19', '238', '176', '214', '200', '96', '15', '153', '191', '108', '29', '166', '28', '193', '197', '96', '195', '108', '102', '241', '195', '10', '33', '166', '197', '168', '2', '96', '26', '33', '204', '258', '149', '153', '153', '40', '137', '241', '166', '221', '23', '221', '153', '15', '113', '113', '78', '171', '40', '71', '21', '71', '80', '15', '191', '191', '135', '110', '10', '259', '102', '149', '149', '109', '109', '175', '97', '97', '97', '153', '15', '238', '193', '175', '38', '38', '137', '18', '235', '155', '137', '265', '110', '18', '18', '256', '191', '224', '173', '104', '10', '152', '24', '24', '24', '17', '40', '40', '40', '40', '40', '40', '112', '6', '18', '18', '210', '96', '191', '27', '213', '17', '153', '26', '133', '18', '18', '153', '38', '96', '40', '38', '40', '40', '10', '17', '17', '95', '71', '192', '17', '259', '213', '200', '15', '11', '207', '194', '18', '135', '238', '191', '241', '191', '116', '35', '147', '156', '67', '78', '17', '227', '191', '117', '188', '130', '72', '96', '235', '97', '18', '150', '203', '25', '126', '149', '200', '94', '4', '4', '1', '244', '228', '252', '149', '191', '149', '15', '17', '17', '29', '18', '18', '116', '23', '237', '94', '184', '268', '197', '175', '243', '28', '16', '10', '50', '191', '102', '102', '153', '149', '111', '78', '191', '135', '18', '35', '78', '238', '208', '93', '95', '258', '17', '111', '111', '262', '22', '22', '104', '161', '209', '21', '195', '12', '116', '241', '242', '239', '17', '97', '16', '40', '117', '94', '96', '78', '15', '103', '195', '135', '235', '247', '135', '130', '260', '239', '176', '93', '191', '209', '239', '93', '264', '30', '18', '30', '18', '18', '103', '191', '97', '17', '95', '115', '209', '140', '22', '22', '241', '10', '153', '3', '188', '238', '226', '195', '97', '96', '238', '97', '130', '97', '116', '96', '96', '242', '45', '149', '166', '155', '96', '116', '264', '10', '237', '192', '71', '22', '71', '175', '149', '116', '237', '274', '18', '235', '176', '116', '202', '250', '250', '78', '2', '196', '2', '97', '143', '18', '111', '111', '80', '78', '153', '80', '40', '110', '30', '112', '30', '111', '215', '112', '149', '78', '78', '209', '110', '110', '238', '109', '239', '112', '151', '154', '16', '16', '16', '78', '137', '179', '72', '149', '10', '111', '268', '96', '170', '111', '38', '192', '153', '183', '195', '30', '30', '147', '195', '147', '93', '264', '153', '25', '18', '19', '22', '74', '40', '93', '166', '15', '127', '147', '192', '260', '238', '103', '269', '127', '19', '19', '153', '18', '78', '238', '30', '30', '241', '16', '235', '250', '204', '238', '218', '40', '93', '184', '18', '0', '241', '195', '94', '154', '109', '156', '10', '21', '30', '197', '39', '78', '24', '107', '142', '142', '228', '229', '229', '229', '232', '179', '239', '142', '232', '23', '175', '147', '137', '137', '137', '171', '191', '117', '128', '127', '204', '98', '147', '235', '239', '153', '15', '204', '255', '80', '204', '204', '149', '113', '19', '167', '22', '250', '17', '17', '17', '18', '17', '67', '121', '125', '26', '97', '97', '97', '97', '247', '124', '163', '147', '112', '111', '208', '108', '38', '209', '154', '154', '194', '135', '22', '147', '241', '120', '218', '260', '156', '155', '241', '192', '215', '180', '179', '149', '40', '193', '192', '150', '102', '93', '1', '149', '137', '133', '102', '102', '102', '78', '145', '21', '96', '175', '175', '78', '123', '30', '95', '19', '18', '110', '18', '152', '152', '147', '26', '135', '260', '158', '203', '195', '158', '112', '156', '125', '213', '230', '30', '15', '125', '106', '111', '195', '181', '130', '195', '130', '239', '15', '194', '16', '15', '111', '61', '72', '57', '128', '239', '239', '203', '25', '218', '10', '155', '191', '46', '98', '175', '260', '102', '21', '191', '260', '191', '239', '15', '30', '93', '147', '103', '78', '177', '30', '95', '155', '155', '155', '103', '103', '112', '241', '191', '191', '17', '78', '153', '150', '113', '115', '147', '78', '40', '163', '33', '15', '149', '103', '192', '191', '96', '106', '115', '143', '235', '200', '204', '238', '204', '204', '74', '196', '113', '246', '102', '150', '16', '176', '20', '195', '128', '197', '15', '71', '71', '193', '24', '24', '24', '170', '204', '209', '111', '144', '112', '22', '222', '224', '28', '209', '155', '213', '275', '196', '239', '195', '25', '103', '149', '128', '19', '96', '102', '135', '17', '232', '155', '191', '20', '18', '241', '30', '25', '205', '97', '97', '96', '72', '182', '135', '135', '20', '149', '149', '78', '135', '40', '18', '153', '10', '26', '158', '205', '157', '157', '135', '135', '140', '155', '192', '26', '268', '263', '245', '155', '15', '177', '159', '212', '161', '239', '200', '40', '35', '153', '197', '259', '15', '254', '175', '10', '30', '239', '67', '195', '200', '191', '22', '66', '191', '112', '133', '128', '235', '161', '155', '17', '93', '36', '113', '239', '18', '18', '191', '165', '163', '163', '238', '79', '158', '40', '102', '40', '200', '191', '60', '241', '245', '96', '78', '30', '15', '173', '192', '192', '158', '64', '113', '200', '74', '128', '135', '115', '115', '10', '74', '102', '102', '115', '111', '238', '244', '109', '109', '238', '152', '10', '237', '18', '50', '16', '173', '204', '15', '208', '276', '43', '109', '258', '222', '15', '94', '235', '203', '79', '128', '113', '3', '204', '106', '133', '214', '218', '25', '98', '166', '71', '30', '175', '175', '97', '135', '108', '19', '18', '196', '35', '12', '196', '78', '166', '155', '72', '155', '22', '109', '258', '196', '263', '161', '204', '210', '14', '208', '208', '208', '208', '204', '102', '102', '213', '155', '251', '153', '153', '153', '154', '154', '237', '97', '19', '28', '94', '136', '112', '19', '97', '152', '153', '153', '111', '111', '135', '135', '105', '17', '105', '152', '152', '152', '153', '79', '266', '135', '120', '125', '149', '50', '162', '72', '17', '147', '109', '115', '24', '105', '244', '149', '238', '203', '35', '202', '177', '173', '239', '19', '19', '179', '237', '14', '18', '185', '58', '158', '144', '213', '237', '155', '250', '239', '93', '237', '97', '195', '157', '107', '109', '246', '165', '98', '49', '2', '266', '149', '20', '18', '71', '108', '22', '115', '239', '40', '178', '232', '238', '218', '118', '118', '210', '235', '46', '228', '192', '112', '149', '96', '111', '21', '95', '95', '95', '18', '57', '206', '18', '239', '157', '80', '196', '112', '176', '99', '106', '71', '235', '135', '18', '209', '78', '20', '20', '103', '33', '3', '103', '166', '79', '79', '156', '229', '18', '238', '2', '16', '128', '200', '117', '22', '197', '166', '78', '237', '241', '107', '53', '189', '204', '243', '163', '221', '15', '200', '221', '152', '23', '193', '107', '195', '99', '97', '19', '125', '276', '113', '193', '64', '78', '197', '117', '175', '135', '135', '78', '93', '154', '153', '256', '256', '135', '137', '135', '137', '149', '110', '15', '159', '21', '158', '192', '192', '117', '29', '97', '258', '78', '46', '5', '80', '15', '105', '260', '161', '191', '2', '138', '117', '215', '176', '156', '194', '265', '115', '181', '21', '14', '106', '106', '108', '200', '156', '103', '102', '29', '17', '204', '175', '227', '118', '18', '239', '204', '258', '78', '155', '213', '109', '192', '186', '186', '15', '241', '133', '30', '30', '239', '30', '218', '109', '138', '241', '241', '45', '135', '112', '232', '104', '106', '200', '53', '23', '112', '17', '117', '59', '260', '195', '106', '117', '159', '204', '200', '228', '165', '165', '261', '200', '227', '166', '93', '21', '80', '112', '152', '196', '203', '228', '98', '102', '15', '139', '110', '13', '238', '6', '171', '94', '208', '155', '112', '203', '239', '191', '39', '136', '203', '195', '46', '203', '148', '192', '210', '23', '4', '153', '29', '165', '192', '204', '232', '94', '112', '239', '98', '71', '248', '17', '155', '10', '78', '191', '191', '111', '25', '25', '19', '19', '17', '235', '102', '265', '195', '235', '163', '238', '71', '197', '107', '155', '30', '72', '19', '259', '80', '155', '23', '64', '239', '197', '156', '57', '203', '258', '200', '205', '246', '238', '258', '227', '238', '238', '19', '148', '161', '111', '105', '202', '109', '262', '133', '137', '221', '135', '52', '170', '111', '36', '30', '156', '102', '268', '115', '22', '153', '133', '259', '98', '235', '135', '191', '25', '235', '25', '29', '94', '195', '203', '212', '112', '135', '245', '260', '200', '96', '158', '23', '165', '22', '18', '249', '240', '109', '43', '37', '15', '241', '241', '23', '135', '240', '108', '98', '268', '275', '191', '136', '263', '259', '196', '98', '98', '204', '248', '123', '105', '204', '240', '9', '9', '98', '218', '189', '238', '246', '155', '130', '157', '74', '109', '29', '189', '203', '213', '195', '241', '159', '238', '204', '218', '105', '23', '16', '263', '206', '150', '152', '204', '202', '173', '267', '18', '153', '155', '196', '2', '97', '158', '158', '204', '98', '202', '98', '204', '258', '98', '22', '113', '113', '135', '71', '237', '155', '163', '250', '233', '26', '26', '26', '26', '26', '19', '170', '18', '15', '19', '191', '30', '30', '48', '35', '30', '237', '46', '119', '16', '78', '239', '150', '245', '235', '158', '27', '241', '30', '38', '107', '75', '241', '135', '106', '106', '98', '159', '96', '135', '111', '176', '175', '200', '18', '14', '242', '163', '187', '20', '12', '191', '23', '106', '171', '25', '78', '162', '191', '157', '94', '237', '246', '203', '108', '238', '204', '204', '1', '102', '154', '106', '170', '102', '200', '2', '135', '106', '258', '258', '227', '26', '15', '25', '71', '203', '96', '3', '241', '200', '30', '108', '98', '108', '27', '106', '26', '150', '259', '78', '113', '113', '158', '202', '16', '136', '239', '156', '239', '35', '276', '252', '115', '22', '22', '258', '172', '3', '154', '3', '98', '237', '102', '235', '112', '206', '98', '57', '118', '153', '18', '26', '30', '196', '163', '191', '153', '72', '24', '156', '117', '122', '30', '135', '192', '98', '28', '30', '30', '95', '241', '155', '17', '195', '31', '103', '202', '98', '237', '94', '112', '163', '163', '94', '163', '112', '19', '195', '191', '15', '135', '78', '109', '160', '152', '97', '98', '107', '167', '97', '138', '138', '26', '191', '97', '97', '30', '30', '206', '202', '13', '30', '240', '111', '261', '79', '0', '111', '19', '152', '107', '107', '18', '98', '28', '258', '244', '106', '237', '204', '191', '15', '183', '202', '16', '111', '166', '268', '202', '12', '30', '78', '110', '133', '71', '197', '111', '52', '79', '171', '156', '94', '13', '118', '111', '71', '133', '18', '105', '30', '22', '133', '60', '237', '191', '6', '126', '3', '238', '276', '78', '162', '150', '115', '191', '107', '200', '148', '135', '72', '155', '241', '4', '108', '160', '203', '20', '133', '2', '80', '80', '154', '18', '15', '115', '239', '167', '167', '35', '268', '18', '221', '18', '195', '204', '195', '216', '133', '112', '104', '223', '191', '115', '30', '30', '30', '30', '30', '202', '204', '165', '200', '200', '29', '191', '237', '267', '18', '175', '81', '264', '264', '178', '115', '102', '178', '212', '273', '202', '109', '259', '166', '135', '155', '219', '161', '18', '94', '241', '191', '196', '191', '98', '26', '26', '204', '204', '135', '27', '238', '260', '182', '26', '98', '94', '93', '238', '156', '19', '111', '79', '16', '192', '113', '19', '241', '18', '202', '260', '272', '106', '163', '156', '106', '17', '109', '221', '78', '203', '7', '136', '197', '193', '191', '98', '112', '200', '95', '221', '98', '161', '191', '3', '106', '189', '111', '25', '196', '196', '241', '163', '200', '24', '78', '176', '3', '98', '102', '98', '19', '161', '224', '80', '18', '191', '112', '105', '254', '71', '200', '137', '29', '20', '165', '8', '106', '23', '106', '155', '200', '265', '135', '126', '203', '188', '161', '158', '258', '255', '98', '203', '50', '35', '102', '158', '175', '40', '239', '26', '66', '57', '193', '155', '15', '135', '10', '107', '106', '229', '18', '191', '204', '107', '99', '235', '238', '18', '205', '205', '35', '238', '208', '203', '110', '110', '109', '112', '23', '18', '19', '182', '156', '18', '18', '156', '111', '135', '137', '133', '137', '240', '99', '197', '113', '15', '165', '39', '97', '97', '15', '150', '163', '109', '268', '15', '15', '238', '203', '67', '161', '197', '18', '196', '97', '161', '197', '161', '185', '238', '191', '30', '121', '133', '160', '160', '74', '135', '75', '54', '135', '196', '23', '241', '166', '194', '239', '135', '240', '208', '238', '98', '135', '152', '176', '22', '174', '113', '241', '237', '57', '195', '191', '24', '165', '165', '204', '204', '56', '109', '27', '153', '252', '228', '165', '252', '200', '28', '158', '135', '196', '22', '18', '133', '30', '204', '237', '196', '93', '106', '94', '25', '93', '159', '4', '195', '23', '237', '25', '199', '57', '26', '276', '274', '204', '75', '18', '204', '108', '22', '133', '191', '111', '204', '203', '133', '237', '166', '4', '62', '202', '166', '161', '165', '74', '109', '118', '200', '22', '206', '25', '209', '17', '155', '204', '64', '23', '30', '202', '149', '171', '135', '136', '184', '26', '18', '159', '195', '18', '166', '156', '98', '133', '38', '40', '18', '18', '166', '109', '109', '209', '22', '195', '163', '163', '108', '18', '15', '99', '135', '174', '26', '19', '166', '133', '259', '151', '235', '204', '228', '133', '30', '161', '111', '242', '84', '238', '98', '195', '78', '160', '28', '25', '264', '165', '41', '19', '55', '46', '202', '121', '15', '32', '98', '238', '0', '78', '31', '30', '203', '264', '203', '135', '104', '111', '78', '212', '111', '98', '126', '22', '2', '30', '237', '212', '265', '163', '106', '106', '235', '109', '223', '161', '202', '150', '170', '255', '112', '114', '170', '98', '96', '197', '36', '18', '228', '14', '255', '204', '102', '239', '0', '185', '30', '22', '23', '246', '199', '133', '191', '78', '195', '153', '112', '191', '200', '170', '171', '200', '78', '128', '25', '153', '237', '99', '94', '208', '107', '208', '208', '78', '204', '204', '27', '191', '272', '134', '158', '158', '174', '118', '64', '18', '258', '150', '189', '18', '153', '241', '204', '112', '106', '135', '71', '148', '7', '133', '140', '187', '23', '223', '19', '19', '2', '136', '2', '200', '161', '15', '98', '158', '148', '3', '250', '153', '235', '165', '220', '176', '110', '240', '151', '148', '133', '74', '193', '46', '27', '173', '94', '176', '105', '256', '161', '226', '18', '78', '191', '171', '243', '26', '0', '108', '167', '159', '239', '150', '112', '155', '79', '241', '202', '165', '254', '107', '202', '156', '166', '193', '239', '135', '15', '203', '187', '148', '25', '15', '22', '194', '239', '56', '191', '176', '262', '200', '110', '72', '99', '102', '135', '98', '113', '15', '15', '159', '262', '171', '151', '98', '204', '213', '255', '26', '243', '175', '156', '98', '107', '191', '165', '241', '26', '165', '171', '30', '160', '166', '266', '11', '19', '211', '93', '254', '30', '239', '115', '273', '205', '112', '237', '263', '74', '98', '79', '122', '94', '265', '155', '191', '16', '235', '229', '98', '191', '227', '17', '111', '267', '135', '202', '195', '10', '180', '196', '196', '133', '135', '133', '95', '111', '122', '258', '191', '3', '114', '107', '19', '264', '74', '56', '200', '200', '133', '161', '97', '15', '97', '197', '196', '253', '93', '202', '258', '106', '153', '26', '109', '40', '149', '8', '196', '15', '119', '155', '23', '237', '193', '264', '148', '151', '238', '200', '94', '75', '148', '175', '24', '195', '171', '30', '203', '200', '202', '16', '158', '223', '191', '22', '264', '135', '35', '57', '94', '191', '196', '202', '98', '18', '196', '3', '133', '78', '191', '208', '258', '18', '200', '29', '135', '109', '110', '2', '185', '98', '133', '19', '22', '204', '95', '238', '25', '133', '250', '135', '135', '3', '47', '158', '187', '2', '78', '115', '155', '3', '218', '166', '112', '266', '22', '195', '135', '72', '97', '197', '209', '127', '80', '2', '98', '14', '239', '15', '113', '165', '204', '78', '204', '191', '175', '200', '193', '18', '196', '133', '109', '237', '56', '196', '3', '163', '163', '163', '195', '15', '263', '18', '84', '18', '133', '107', '18', '102', '156', '238', '102', '26', '244', '82', '75', '23', '258', '238', '136', '3', '204', '184', '200', '196', '158', '106', '27', '238', '155', '152', '50', '50', '2', '235', '98', '203', '166', '109', '25', '18', '151', '133', '200', '26', '26', '26', '19', '107', '156', '23', '26', '3', '70', '225', '41', '26', '239', '26', '186', '26', '34', '272', '272', '104', '78', '260', '18', '204', '26', '78', '18', '210', '135', '203', '71', '207', '26', '133', '13', '25', '26', '115', '94', '264', '135', '244', '79', '30', '263', '22', '109', '205', '176', '106', '22', '17', '151', '106', '191', '160', '160', '165', '78', '177', '196', '260', '166', '35', '191', '260', '98', '18', '237', '18', '161', '18', '197', '133', '30', '35', '200', '170', '155', '245', '19', '200', '160', '96', '172', '165', '55', '137', '241', '133', '191', '112', '25', '126', '238', '18', '18', '108', '135', '161', '135', '73', '112', '107', '8', '112', '109', '195', '25', '30', '105', '161', '241', '110', '75', '238', '215', '238', '104', '25', '3', '155', '30', '16', '3', '175', '99', '3', '200', '23', '3', '199', '203', '191', '196', '196', '204', '98', '105', '133', '197', '135', '161', '39', '15', '30', '133', '55', '191', '203', '133', '203', '140', '200', '149', '30', '96', '135', '23', '26', '3', '18', '155', '158', '135', '135', '18', '250', '18', '22', '114', '18', '18', '166', '18', '30', '18', '239', '18', '161', '50', '50', '49', '51', '23', '55', '98', '78', '210', '211', '18', '239', '78', '13', '193', '114', '78', '18', '25', '133', '19', '176', '4', '173', '50', '108', '17', '4', '204', '94', '112', '207', '109', '115', '23', '235', '197', '194', '197', '213', '148', '40', '157', '107', '200', '147', '14', '14', '18', '238', '18', '238', '18', '238', '57', '200', '135', '30', '18', '191', '191', '200', '166', '151', '161', '73', '19', '73', '239', '239', '165', '108', '2', '93', '166', '133', '133', '173', '35', '193', '114', '74', '19', '40', '58', '104', '16', '260', '160', '158', '112', '210', '158', '167', '203', '203', '3', '3', '109', '30', '133', '258', '70', '204', '200', '152', '155', '192', '204', '108', '191', '204', '26', '98', '26', '98', '264', '158', '18', '150', '137', '153', '196', '199', '191', '115', '168', '157', '149', '151', '78', '137', '160', '259', '166', '196', '171', '175', '199', '166', '161', '70', '13', '166', '152', '152', '206', '58', '18', '207', '58', '74', '207', '29', '46', '109', '135', '74', '174', '109', '40', '10', '30', '258', '16', '111', '30', '15', '239', '204', '48', '25', '196', '0', '156', '72', '72', '96', '22', '72', '133', '133', '238', '26', '3', '50', '152', '193', '235', '136', '153', '50', '1', '19', '239', '30', '25', '179', '161', '30', '191', '3', '27', '133', '3', '40', '23', '133', '150', '151', '265', '32', '191', '3', '25', '170', '238', '40', '259', '166', '137', '241', '133', '209', '159', '78', '78', '153', '26', '52', '241', '36', '250', '96', '30', '30', '38', '18', '18', '238', '99', '72', '111', '3', '244', '191', '226', '3', '191', '114', '135', '109', '40', '79', '202', '78', '114', '238', '10', '150', '108', '163', '108', '113', '197', '109', '115', '161', '3', '238', '3', '237', '161', '107', '155', '78', '202', '112', '38', '195', '137', '250', '133', '237', '161', '25', '26', '25', '15', '15', '239', '166', '114', '229', '111', '40', '135', '94', '102', '162', '204', '237', '17', '166', '136', '22', '98', '99', '99', '166', '35', '196', '3', '203', '135', '135', '84', '98', '266', '238', '13', '166', '78', '193', '98', '241', '104', '153', '192', '200', '10', '95', '95', '148', '107', '14', '17', '191', '239', '19', '203', '255', '197', '213', '94', '3', '107', '23', '35', '10', '237', '235', '78', '29', '18', '160', '200', '150', '99', '109', '155', '161', '133', '114', '18', '18', '18', '264', '133', '104', '161', '70', '17', '204', '195', '204', '241', '99', '82', '241', '241', '161', '18', '235', '175', '2', '2', '202', '50', '230', '26', '40', '40', '200', '78', '156', '26', '112', '26', '30', '71', '184', '200', '135', '153', '23', '106', '125', '175', '133', '133', '7', '38', '196', '161', '0', '49', '241', '158', '15', '21', '104', '2', '3', '176', '259', '135', '78', '75', '112', '200', '1', '207', '238', '161', '150', '158', '2', '45', '31', '237', '149', '3', '26', '165', '135', '152', '115', '133', '18', '40', '166', '30', '173', '40', '26', '78', '50', '48', '50', '13', '45', '200', '200', '18', '151', '205', '135', '38', '117', '196', '135', '155', '111', '196', '196', '166', '23', '25', '161', '18', '124', '259', '165', '58', '170', '204', '250', '103', '160', '238', '194', '163', '241', '255', '235', '17', '133', '26', '135', '196', '237', '114', '258', '193', '248', '40', '195', '84', '238', '146', '199', '40', '161', '22', '137', '43', '166', '156', '4', '104', '133', '177', '211', '193', '78', '203', '246', '237', '264', '3', '149', '78', '78', '111', '239', '13', '238', '176', '112', '150', '156', '3', '6', '118', '178', '178', '21', '186', '150', '241', '50', '243', '191', '121', '278', '29', '200', '18', '3', '197', '191', '98', '173', '195', '25', '195', '264', '221', '15', '156', '25', '200', '211', '195', '26', '35', '2', '135', '126', '113', '167', '191', '264', '196', '133', '18', '26', '146', '203', '219', '23', '160', '160', '135', '259', '3', '159', '120', '160', '106', '240', '133', '115', '258', '38', '22', '3', '26', '22', '107', '49', '48', '25', '237', '72', '219', '196', '196', '111', '135', '196', '18', '166', '172', '135', '26', '258', '70', '98', '40', '166', '23', '96', '3', '30', '155', '126', '146', '114', '242', '237', '157', '238', '241', '40', '133', '200', '161', '175', '200', '175', '84', '266', '18', '99', '30', '242', '151', '153', '166', '199', '228', '3', '237', '132', '110', '135', '80', '15', '15', '15', '84', '133', '15', '177', '3', '202', '255', '71', '115', '235', '19', '18', '18', '102', '58', '78', '165', '166', '264', '12', '150', '15', '2', '18', '135', '109', '200', '258', '128', '3', '199', '197', '239', '109', '222', '239', '107', '125', '132', '204', '204', '133', '135', '30', '158', '203', '15', '264', '237', '133', '196', '3', '252', '26', '98', '79', '199', '210', '0', '99', '30', '26', '259', '6', '49', '166', '98', '108', '9', '133', '102', '155', '98', '114', '78', '35', '166', '188', '202', '235', '30', '149', '165', '176', '138', '29', '153', '25', '25', '137', '133', '166', '135', '125', '71', '193', '154', '165', '98', '23', '56', '198', '26', '26', '0', '109', '136', '163', '151', '25', '15', '241', '195', '204', '191', '195', '161', '175', '133', '237', '78', '165', '258', '135', '78', '135', '258', '18', '235', '30', '187', '150', '275', '149', '2', '36', '112', '22', '161', '238', '203', '112', '265', '75', '133', '205', '258', '158', '18', '26', '18', '25', '26', '15', '26', '239', '15', '240', '18', '71', '71', '78', '135', '3', '148', '166', '26', '176', '2', '197', '150', '176', '19', '18', '97', '166', '18', '153', '250', '133', '210', '20', '192', '196', '26', '191', '2', '81', '70', '5', '79', '3', '30', '193', '193', '69', '160', '176', '246', '108', '54', '250', '135', '200', '80', '3', '167', '18', '166', '200', '152', '23', '149', '19', '14', '13', '237', '196', '19', '112', '254', '204', '191', '23', '264', '199', '209', '137', '31', '135', '238', '78', '18', '191', '15', '164', '195', '74', '163', '163', '41', '99', '26', '135', '26', '40', '40', '250', '200', '200', '26', '237', '4', '195', '158', '35', '71', '191', '3', '218', '274', '58', '3', '23', '177', '132', '49', '62', '238', '166', '97', '264', '193', '29', '26', '49', '241', '197', '200', '64', '227', '258', '44', '15', '133', '175', '238', '109', '254', '237', '198', '175', '166', '213', '174', '196', '107', '259', '98', '10', '196', '210', '191', '96', '166', '135', '15', '197', '78', '78', '195', '176', '18', '98', '129', '170', '18', '247', '203', '26', '158', '40', '64', '200', '163', '196', '19', '205', '197', '193', '191', '69', '96', '74', '35', '36', '64', '133', '172', '19', '266', '96', '94', '103', '258', '193', '264', '98', '254', '50', '193', '25', '170', '75', '84', '27', '252', '203', '94', '94', '19', '98', '137', '12', '153', '3', '155', '135', '238', '15', '196', '15', '26', '84', '49', '27', '135', '203', '48', '133', '129', '133', '258', '155', '35', '195', '71', '241', '135', '166', '193', '165', '104', '196', '117', '196', '26', '237', '109', '95', '196', '60', '40', '158', '193', '250', '104', '94', '178', '27', '193', '78', '106', '79', '204', '133', '237', '235', '135', '202', '70', '15', '264', '241', '254', '110', '199', '49', '255', '30', '193', '238', '204', '102', '97', '161', '43', '228', '1', '6', '196', '193', '26', '15', '128', '46', '155', '32', '133', '69', '47', '135', '78', '205', '200', '200', '202', '3', '95', '191', '78', '26', '248', '258', '191', '240', '49', '110', '38', '193', '200', '200', '96', '200', '204', '135', '9', '200', '258', '15', '46', '15', '149', '155', '197', '2', '166', '151', '72', '135', '97', '211', '238', '135', '163', '161', '26', '163', '129', '241', '239', '241', '16', '26', '18', '263', '260', '166', '3', '264', '195', '79', '132', '251', '50', '133', '15', '25', '15', '191', '15', '78', '50', '78', '71', '227', '69', '200', '69', '109', '64', '133', '197', '196', '3', '45', '238', '26', '132', '104', '196', '193', '136', '80', '26', '75', '117', '14', '78', '75', '98', '15', '2', '15', '50', '255', '136', '3', '112', '173', '69', '191', '19', '239', '235', '109', '70', '135', '78', '165', '238', '262', '199', '35', '203', '224', '193', '15', '18', '150', '238', '197', '238', '193', '135', '3', '150', '187', '199', '239', '109', '152', '27', '257', '6', '235', '104', '191', '133', '166', '155', '26', '16', '136', '198', '98', '154', '235', '27', '102', '30', '191', '15', '132', '132', '114', '107', '241', '241', '156', '199', '241', '49', '62', '117', '258', '30', '163', '133', '238', '200', '3', '15', '109', '2', '196', '165', '166', '159', '210', '35', '15', '17', '200', '241', '106', '203', '197', '197', '79', '98', '98', '23', '204', '48', '129', '57', '258', '202', '215', '238', '70', '64', '193', '189', '57', '200', '0', '98', '135', '204', '25', '30', '204', '27', '26', '78', '135', '109', '3', '197', '3', '167', '35', '196', '225', '2', '194', '200', '15', '161', '198', '23', '133', '238', '195', '35', '50', '135', '152', '163', '21', '15', '94', '26', '94', '53', '260', '196', '154', '277', '200', '118', '6', '238', '50', '209', '50', '3', '238', '3', '15', '107', '135', '199', '15', '135', '26', '258', '251', '203', '19', '259', '195', '199', '238', '15', '254', '112', '211', '200', '129', '84', '196', '3', '208', '259', '135', '104', '29', '136', '31', '78', '13', '3', '64', '75', '55', '114', '160', '96', '14', '18', '207', '75', '48', '15', '49', '97', '117', '196', '99', '258', '195', '264', '264', '78', '197', '30', '238', '109', '158', '32', '195', '149', '227', '65', '167', '162', '204', '250', '151', '151', '102', '26', '29', '26', '193', '74', '50', '193', '186', '15', '74', '259', '57', '188', '277', '277', '195', '195', '212', '78', '197', '64', '129', '259', '200', '199', '129', '4', '235', '197', '196', '161', '3', '255', '199', '102', '3', '13', '94', '53', '98', '15', '129', '15', '155', '71', '179', '50', '29', '136', '80', '254', '198', '162', '237', '3', '18', '260', '238', '117', '179', '156', '19', '45', '197', '78', '199', '102', '102', '237', '155', '69', '69', '163', '239', '15', '196', '80', '191', '30', '71', '158', '259', '2', '106', '148', '15', '135', '135', '135', '196', '19', '123', '15', '104', '19', '128', '230', '69', '98', '197', '241', '161', '195', '200', '3', '98', '78', '165', '163', '19', '196', '31', '173', '203', '55', '69', '209', '239', '79', '158', '254', '135', '51', '94', '162', '255', '95', '18', '133', '109', '50', '94', '199', '15', '197', '200', '133', '116', '50', '196', '161', '69', '133', '3', '258', '3', '197', '29', '3', '157', '197', '193', '4', '94', '195', '17', '48', '152', '8', '15', '155', '133', '195', '136', '150', '102', '237', '203', '164', '164', '250', '154', '13', '191', '58', '94', '70', '93', '264', '15', '30', '133', '237', '267', '199', '193', '76', '241', '157', '26', '200', '109', '179', '55', '98', '200', '129', '238', '235', '208', '109', '239', '258', '196', '193', '18', '107', '210', '70', '40', '129', '259', '193', '30', '158', '252', '40', '210', '78', '15', '78', '96', '208', '113', '258', '64', '80', '15', '133', '241', '15', '15', '258', '135', '109', '251', '251', '109', '74', '196', '170', '240', '254', '258', '204', '199', '26', '109', '197', '57', '45', '161', '57', '114', '3', '15', '264', '106', '57', '27', '18', '258', '29', '84', '161', '135', '114', '24', '30', '107', '107', '172', '209', '170', '18', '19', '133', '191', '195', '135', '156', '196', '94', '3', '84', '225', '199', '238', '106', '104', '70', '50', '12', '155', '114', '176', '237', '200', '108', '70', '240', '196', '19', '22', '276', '199', '238', '205', '104', '193', '108', '115', '259', '102', '19', '258', '18', '106', '161', '191', '114', '64', '15', '204', '35', '155', '40', '238', '136', '264', '133', '186', '15', '258', '166', '26', '6', '114', '114', '133', '114', '114', '13', '13', '13', '237', '250', '245', '136', '78', '13', '48', '18', '13', '25', '26', '93', '125', '50', '94', '148', '78', '194', '200', '25', '93', '274', '266', '70', '22', '3', '208', '240', '211', '135', '31', '18', '153', '18', '2', '15', '15', '15', '156', '15', '70', '73', '173', '13', '112', '69', '204', '70', '248', '135', '70', '3', '193', '193', '22', '162', '238', '57', '98', '15', '199', '199', '52', '151', '73', '5', '108', '23', '69', '95', '220', '106', '163', '152', '163', '108', '15', '58', '250', '19', '177', '245', '264', '93', '204', '25', '78', '155', '69', '155', '239', '241', '155', '200', '18', '3', '13', '196', '197', '204', '272', '95', '135', '165', '22', '210', '35', '69', '18', '170', '3', '25', '156', '78', '135', '102', '107', '255', '19', '70', '70', '109', '198', '156', '64', '170', '138', '78', '203', '166', '108', '114', '179', '228', '74', '49', '14', '264', '26', '195', '78', '107', '75', '18', '106', '151', '70', '58', '15', '196', '238', '109', '104', '104', '109', '136', '148', '210', '3', '161', '15', '104', '238', '208', '133', '108', '241', '18', '209', '74', '156', '114', '199', '174', '35', '18', '170', '135', '238', '244', '155', '162', '197', '3', '98', '227', '267', '238', '193', '50', '238', '98', '112', '124', '25', '114', '241', '159', '154', '78', '161', '136', '114', '153', '225', '154', '13', '50', '11', '25', '15', '209', '11', '166', '203', '259', '237', '22', '114', '276', '98', '278', '150', '198', '18', '15', '78', '15', '162', '26', '3', '196', '3', '102', '189', '71', '69', '16', '18', '152', '133', '135', '162', '241', '15', '104', '15', '259', '202', '2', '238', '207', '15', '204', '30', '215', '112', '210', '42', '96', '193', '15', '110', '163', '211', '78', '108', '15', '31', '27', '15', '31', '196', '195', '96', '15', '161', '94', '193', '195', '198', '98', '261', '258', '76', '15', '104', '258', '135', '18', '15', '202', '258', '108', '156', '13', '22', '251', '199', '155', '258', '211', '3', '207', '202', '259', '107', '202', '259', '135', '135', '148', '158', '154', '99', '99', '3', '207', '94', '25', '162', '31', '241', '25', '15', '114', '18', '114', '148', '19', '15', '203', '191', '19', '13', '193', '204', '25', '166', '155', '196', '98', '162', '197', '29', '152', '191', '133', '199', '230', '40', '25', '204', '79', '3', '19', '165', '204', '108', '208', '3', '251', '274', '15', '166', '264', '35', '13', '193', '114', '258', '13', '19', '133', '23', '17', '199', '204', '153', '250', '204', '16', '12', '107', '191', '18', '3', '15', '165', '15', '69', '17', '165', '17', '15', '15', '156', '23', '23', '15', '197', '104', '197', '26', '208', '135', '211', '250', '18', '204', '25', '196', '53', '69', '104', '104', '19', '78', '241', '230', '30', '195', '96', '26', '109', '196', '69', '238', '15', '18', '133', '26', '106', '109', '98', '15', '258', '274', '193', '203', '58', '57', '274', '2', '202', '161', '136', '135', '155', '241', '35', '151', '137', '5', '114', '96', '16', '209', '158', '15', '266', '161', '15', '25', '274', '202', '3', '161', '237', '267', '204', '125', '15', '151', '155', '259', '151', '210', '238', '15', '26', '132', '50', '173', '197', '114', '241', '15', '258', '109', '166', '274', '13', '191', '15', '26', '164', '258', '163', '15', '264', '11', '260', '148', '150', '207', '196', '98', '272', '133', '196', '107', '26', '191', '163', '191', '135', '3', '18', '18', '119', '69', '196', '15', '156', '135', '112', '258', '155', '256', '199', '204', '212', '18', '135', '254', '72', '258', '254', '15', '133', '18', '259', '193', '258', '204', '40', '18', '29', '155', '104', '107', '274', '241', '25', '26', '69', '148', '166', '46', '94', '135', '15', '155', '104', '241', '32', '274', '249', '15', '197', '104', '44', '259', '69', '191', '15', '198', '193', '133', '17', '258', '248', '166', '197', '200', '250', '204', '3', '225', '238', '163', '69', '15', '15', '26', '202', '19', '209', '78', '212', '15', '78', '243', '110', '191', '15', '18', '107', '38', '208', '166', '239', '237', '199', '15', '204', '258', '173', '172', '200', '274', '243', '19', '191', '135', '203', '69', '149', '148', '196', '256', '204', '97', '191', '26', '114', '274', '200', '69', '69', '64', '69', '239', '25', '274', '203', '78', '135', '196', '202', '17', '2', '2', '3', '98', '98', '62', '2', '95', '96', '2', '207', '78', '200', '26', '64', '18', '93', '49', '57', '154', '200', '69', '94', '98', '98', '56', '56', '166', '98', '212', '226', '226', '15', '69', '15', '273', '272', '104', '193', '3', '98', '199', '98', '25', '125', '259', '249', '108', '40', '26', '198', '218', '73', '148', '69', '204', '241', '35', '3', '274', '70', '18', '71', '35', '172', '133', '106', '69', '107', '199', '219', '19', '133', '69', '135', '109', '57', '191', '135', '252', '104', '3', '94', '3', '72', '126', '16', '36', '84', '241', '3', '218', '98', '108', '46', '19', '196', '118', '104', '29', '238', '69', '10', '204', '264', '259', '124', '242', '107', '133', '25', '64', '30', '194', '259', '159', '264', '274', '52', '200', '104', '29', '30', '193', '98', '94', '114', '204', '40', '18', '198', '97', '3', '99', '274', '239', '177', '15', '18', '126', '126', '208', '148', '15', '104', '29', '36', '218', '70', '15', '23', '278', '23', '36', '191', '210', '51', '245', '167', '72', '237', '25', '203', '36', '196', '193', '196', '15', '151', '207', '203', '103', '166', '135', '110', '27', '13', '27', '191', '98', '57', '166', '50', '21', '15', '197', '23', '2', '70', '69', '193', '274', '104', '166', '258', '126', '238', '161', '18', '193', '69', '140', '170', '197', '204', '165', '203', '170', '15', '58', '35', '83', '212', '150', '73', '25', '15', '64', '133', '210', '108', '79', '193', '15', '126', '165', '15', '208', '98', '3', '200', '126', '3', '40', '165', '98', '69', '15', '165', '133', '96', '173', '203', '133', '238', '248', '15', '7', '38', '207', '108', '165', '199', '18', '162', '27', '3', '72', '65', '250', '209', '166', '64', '209', '237', '212', '64', '225', '120', '108', '203', '135', '248', '69', '251', '135', '204', '160', '160', '135', '148', '15', '15', '23', '207', '274', '29', '28', '194', '150', '40', '104', '208', '125', '70', '18', '3', '19', '102', '104', '23', '29', '207', '133', '204', '111', '2', '125', '126', '7', '194', '209', '203', '83', '126', '126', '258', '119', '162', '241', '133', '18', '208', '197', '209', '15', '78', '80', '110', '237', '104', '30', '208', '103', '15', '200', '15', '238', '254', '108', '173', '40', '69', '175', '17', '155', '136', '26', '3', '135', '13', '241', '161', '166', '170', '162', '133', '53', '211', '204', '15', '161', '238', '110', '18', '191', '18', '50', '71', '30', '17', '208', '248', '203', '203', '196', '210', '258', '204', '3', '18', '196', '125', '173', '199', '249', '18', '104', '165', '241', '2', '193', '238', '78', '15', '166', '15', '126', '3', '237', '161', '52', '207', '3', '3', '50', '56', '22', '56', '69', '93', '135', '207', '69', '99', '3', '133', '68', '133', '161', '108', '126', '250', '60', '155', '98', '18', '16', '78', '78', '240', '235', '176', '22', '199', '160', '235', '274', '209', '64', '173', '137', '128', '25', '18', '160', '203', '70', '165', '193', '110', '207', '64', '85', '252', '78', '51', '2', '99', '19', '22', '198', '204', '166', '166', '151', '56', '133', '210', '26', '149', '109', '30', '209', '209', '51', '3', '193', '119', '208', '93', '165', '248', '55', '203', '204', '58', '18', '199', '211', '83', '32', '135', '212', '156', '19', '235', '173', '191', '209', '203', '146', '208', '58', '158', '102', '199', '200', '200', '173', '70', '52', '107', '211', '213', '237', '196', '205', '104', '135', '258', '255', '96', '19', '124', '128', '57', '199', '98', '166', '102', '40', '15', '238', '3', '148', '225', '126', '135', '114', '193', '204', '15', '209', '209', '193', '259', '64', '161', '26', '274', '126', '18', '196', '0', '60', '207', '17', '71', '218', '239', '248', '38', '154', '61', '128', '199', '196', '173', '25', '46', '98', '258', '108', '3', '108', '196', '93', '93', '99', '172', '69', '241', '196', '30', '135', '278', '109', '18', '5', '209', '57', '159', '70', '193', '163', '249', '196', '196', '72', '103', '133', '17', '204', '199', '166', '202', '29', '57', '193', '150', '98', '119', '25', '205', '18', '173', '166', '209', '57', '25', '58', '258', '26', '15', '15', '101', '26', '74', '1', '4', '74', '263', '']\n",
            "['209', '63', '216', '108', '196', '200', '203', '246', '188', '193', '25', '106', '56', '53', '223', '161', '200', '208', '78', '147', '147', '179', '147', '80', '56', '209', '209', '200', '214', '202', '203', '194', '26', '237', '165', '258', '238', '238', '76', '193', '196', '213', '161', '213', '209', '194', '196', '25', '151', '260', '135', '173', '30', '108', '272', '259', '176', '213', '213', '21', '5', '171', '200', '195', '240', '194', '176', '240', '240', '80', '173', '98', '214', '21', '76', '76', '73', '173', '173', '78', '61', '258', '191', '3', '78', '173', '193', '118', '200', '264', '260', '49', '211', '258', '18', '173', '78', '3', '3', '1', '213', '135', '182', '239', '208', '278', '50', '194', '200', '124', '189', '117', '264', '244', '76', '193', '221', '78', '78', '173', '197', '78', '161', '231', '135', '22', '78', '25', '25', '197', '200', '147', '203', '191', '196', '193', '263', '32', '197', '191', '4', '56', '196', '104', '191', '196', '173', '147', '194', '96', '161', '161', '238', '200', '3', '78', '78', '53', '41', '237', '53', '18', '10', '194', '97', '96', '208', '25', '10', '117', '104', '54', '96', '194', '21', '245', '238', '202', '209', '161', '97', '258', '240', '213', '136', '155', '152', '21', '191', '191', '118', '68', '258', '189', '197', '208', '61', '78', '135', '202', '96', '155', '108', '272', '80', '25', '155', '35', '213', '191', '194', '96', '96', '191', '264', '191', '191', '191', '2', '258', '191', '173', '259', '15', '47', '258', '156', '118', '160', '115', '105', '26', '173', '192', '76', '76', '135', '240', '118', '161', '200', '230', '18', '200', '25', '10', '161', '244', '24', '24', '200', '205', '104', '15', '193', '156', '95', '209', '240', '191', '194', '128', '208', '208', '218', '173', '10', '25', '18', '107', '46', '32', '197', '172', '209', '213', '79', '121', '78', '78', '193', '2', '78', '194', '149', '136', '80', '3', '21', '200', '78', '21', '197', '194', '118', '202', '161', '180', '209', '173', '2', '193', '6', '260', '197', '78', '78', '42', '191', '213', '135', '193', '196', '202', '222', '217', '42', '259', '213', '76', '46', '30', '15', '260', '202', '18', '259', '104', '135', '76', '196', '240', '240', '240', '108', '24', '200', '197', '262', '176', '15', '156', '264', '156', '200', '191', '161', '196', '78', '238', '161', '221', '118', '14', '14', '14', '200', '278', '238', '200', '196', '78', '173', '25', '197', '197', '196', '161', '18', '10', '10', '10', '18', '66', '203', '19', '246', '196', '244', '240', '161', '200', '78', '240', '209', '208', '118', '215', '194', '193', '168', '229', '191', '135', '172', '196', '197', '161', '78', '196', '197', '258', '78', '78', '238', '221', '121', '264', '104', '146', '50', '78', '217', '11', '238', '238', '238', '3', '3', '154', '215', '79', '18', '158', '104', '18', '13', '197', '117', '197', '161', '213', '242', '32', '238', '230', '2', '2', '32', '161', '251', '96', '156', '242', '22', '22', '140', '3', '149', '135', '191', '173', '135', '200', '238', '238', '78', '156', '209', '78', '97', '97', '160', '25', '258', '240', '161', '15', '161', '96', '209', '80', '132', '140', '4', '1', '240', '196', '274', '272', '62', '80', '196', '196', '191', '79', '156', '15', '196', '209', '22', '251', '202', '202', '121', '73', '191', '238', '191', '150', '150', '135', '46', '203', '202', '238', '22', '3', '258', '239', '238', '238', '165', '238', '238', '194', '200', '200', '197', '200', '196', '191', '194', '76', '78', '208', '200', '71', '208', '240', '76', '117', '120', '108', '96', '144', '30', '133', '133', '202', '117', '161', '251', '29', '104', '242', '197', '118', '216', '262', '236', '141', '11', '235', '235', '133', '191', '238', '202', '80', '80', '22', '140', '191', '197', '202', '133', '184', '184', '184', '184', '143', '274', '50', '237', '196', '274', '265', '10', '18', '10', '121', '238', '245', '228', '76', '25', '191', '197', '268', '10', '240', '195', '196', '195', '15', '48', '200', '183', '70', '59', '197', '196', '235', '161', '161', '215', '104', '97', '191', '194', '191', '200', '200', '197', '76', '200', '93', '256', '160', '238', '146', '238', '239', '15', '140', '258', '140', '21', '79', '202', '258', '79', '62', '205', '161', '104', '10', '161', '196', '137', '152', '191', '117', '213', '242', '176', '218', '195', '191', '162', '191', '135', '241', '213', '213', '84', '79', '68', '18', '98', '197', '155', '41', '246', '269', '78', '209', '238', '80', '238', '62', '237', '273', '273', '230', '150', '121', '260', '244', '161', '202', '78', '109', '155', '264', '145', '50', '274', '173', '210', '258', '238', '211', '156', '215', '195', '177', '244', '3', '141', '196', '191', '194', '29', '30', '10', '16', '200', '200', '156', '53', '3', '62', '165', '173', '173', '22', '205', '274', '150', '183', '116', '188', '188', '98', '161', '204', '80', '172', '156', '104', '229', '259', '78', '169', '3', '200', '118', '161', '80', '260', '240', '150', '197', '196', '62', '93', '6', '272', '272', '197', '160', '78', '96', '70', '195', '191', '191', '80', '3', '3', '240', '147', '196', '191', '238', '96', '203', '3', '15', '10', '58', '55', '96', '41', '15', '26', '50', '238', '194', '79', '240', '161', '191', '195', '263', '50', '240', '147', '235', '195', '203', '59', '178', '176', '2', '15', '203', '191', '18', '278', '196', '14', '153', '58', '175', '18', '217', '217', '80', '191', '200', '18', '200', '173', '24', '171', '171', '210', '124', '34', '200', '218', '104', '4', '223', '193', '194', '200', '200', '13', '62', '194', '197', '80', '197', '146', '146', '96', '97', '97', '2', '196', '191', '191', '104', '238', '10', '200', '239', '135', '104', '213', '2', '221', '169', '25', '21', '194', '161', '21', '278', '3', '195', '46', '176', '25', '117', '191', '62', '62', '62', '78', '50', '264', '79', '197', '80', '80', '80', '191', '191', '78', '161', '194', '195', '197', '203', '259', '278', '49', '147', '147', '147', '147', '22', '78', '78', '258', '2', '202', '221', '55', '258', '104', '104', '84', '240', '221', '274', '104', '22', '241', '63', '197', '197', '196', '159', '200', '22', '22', '22', '78', '78', '173', '104', '97', '104', '191', '146', '223', '268', '96', '14', '173', '204', '241', '97', '15', '118', '78', '78', '196', '194', '258', '96', '96', '259', '75', '10', '197', '30', '2', '238', '121', '50', '161', '173', '173', '186', '185', '186', '78', '194', '221', '218', '35', '33', '13', '34', '78', '21', '215', '97', '60', '24', '78', '193', '173', '235', '78', '200', '258', '140', '259', '194', '197', '195', '200', '145', '76', '108', '130', '238', '215', '213', '2', '217', '121', '178', '140', '62', '156', '176', '196', '196', '41', '223', '147', '172', '79', '200', '2', '244', '244', '191', '213', '104', '191', '237', '237', '33', '117', '195', '195', '264', '195', '140', '210', '41', '197', '14', '257', '196', '238', '238', '79', '161', '156', '196', '238', '238', '213', '104', '3', '10', '118', '204', '235', '33', '259', '274', '259', '176', '78', '78', '196', '34', '52', '25', '30', '194', '153', '21', '70', '191', '18', '191', '191', '172', '194', '194', '194', '194', '148', '104', '125', '2', '2', '2', '185', '194', '29', '104', '78', '189', '272', '204', '183', '213', '78', '104', '197', '240', '97', '168', '168', '182', '238', '197', '18', '194', '229', '221', '62', '78', '53', '104', '214', '189', '189', '189', '213', '117', '117', '191', '29', '104', '104', '276', '276', '22', '195', '195', '155', '139', '139', '59', '168', '135', '140', '79', '146', '200', '245', '70', '160', '153', '50', '202', '154', '263', '130', '194', '117', '194', '194', '191', '33', '29', '29', '238', '239', '194', '21', '274', '215', '18', '18', '140', '2', '215', '25', '46', '137', '238', '30', '160', '156', '46', '196', '263', '242', '104', '0', '0', '73', '213', '135', '22', '191', '191', '241', '202', '221', '15', '15', '191', '238', '213', '194', '197', '179', '238', '200', '200', '200', '5', '238', '194', '55', '117', '166', '49', '244', '239', '191', '78', '78', '78', '194', '200', '117', '210', '22', '121', '121', '121', '21', '237', '194', '121', '78', '104', '194', '79', '191', '251', '195', '197', '274', '176', '21', '43', '43', '10', '61', '215', '215', '30', '214', '194', '10', '30', '139', '96', '226', '226', '265', '2', '2', '121', '121', '41', '237', '104', '238', '238', '161', '78', '78', '78', '11', '184', '140', '140', '161', '104', '104', '237', '130', '195', '160', '118', '161', '18', '11', '241', '184', '196', '178', '22', '30', '146', '146', '10', '10', '141', '141', '135', '18', '18', '10', '26', '208', '97', '141', '18', '18', '97', '194', '194', '79', '213', '205', '205', '205', '75', '241', '50', '226', '238', '97', '222', '215', '215', '139', '238', '51', '238', '96', '173', '109', '200', '117', '192', '2', '195', '10', '3', '18', '49', '238', '238', '191', '195', '52', '197', '200', '200', '79', '191', '200', '200', '30', '195', '197', '195', '25', '260', '73', '30', '193', '21', '200', '242', '203', '120', '78', '260', '241', '0', '30', '202', '195', '30', '191', '78', '194', '194', '2', '205', '191', '196', '173', '197', '30', '196', '172', '139', '2', '18', '191', '230', '56', '194', '215', '195', '2', '71', '13', '146', '264', '50', '191', '235', '93', '191', '139', '139', '227', '235', '186', '235', '24', '18', '261', '128', '33', '71', '25', '176', '39', '10', '195', '202', '29', '29', '221', '29', '197', '139', '78', '227', '146', '137', '195', '205', '205', '195', '135', '62', '153', '146', '29', '75', '0', '194', '194', '180', '213', '78', '18', '194', '239', '196', '239', '197', '194', '153', '192', '221', '10', '46', '50', '241', '117', '117', '117', '195', '173', '78', '195', '194', '239', '2', '12', '173', '197', '30', '269', '197', '176', '135', '30', '146', '191', '93', '93', '135', '260', '218', '156', '155', '200', '241', '125', '18', '18', '183', '203', '239', '259', '173', '213', '180', '180', '210', '120', '104', '194', '156', '258', '172', '9', '0', '241', '192', '143', '189', '204', '50', '63', '197', '243', '241', '93', '125', '2', '21', '260', '4', '4', '3', '191', '150', '128', '78', '238', '245', '8', '80', '238', '79', '79', '240', '58', '104', '155', '80', '80', '188', '191', '235', '170', '170', '170', '146', '258', '195', '276', '25', '78', '215', '240', '161', '53', '147', '78', '78', '78', '78', '221', '221', '96', '18', '19', '130', '18', '18', '18', '80', '80', '194', '79', '93', '121', '78', '139', '139', '78', '197', '121', '194', '200', '80', '101', '22', '12', '135', '97', '205', '181', '60', '193', '15', '140', '172', '191', '241', '222', '135', '29', '25', '179', '191', '191', '259', '194', '133', '240', '121', '121', '241', '195', '29', '191', '209', '18', '139', '139', '50', '18', '12', '18', '133', '200', '25', '10', '70', '25', '242', '18', '161', '49', '172', '172', '172', '172', '128', '96', '195', '200', '78', '10', '239', '22', '173', '142', '142', '22', '194', '192', '3', '200', '241', '96', '241', '62', '139', '213', '202', '135', '79', '120', '192', '78', '118', '241', '46', '151', '150', '151', '150', '118', '221', '183', '237', '196', '140', '140', '39', '179', '213', '140', '41', '73', '78', '217', '258', '30', '118', '140', '219', '50', '10', '139', '150', '251', '24', '10', '18', '78', '18', '172', '135', '146', '71', '196', '202', '197', '25', '106', '160', '215', '15', '6', '101', '165', '140', '145', '80', '3', '238', '239', '78', '268', '0', '109', '191', '215', '238', '238', '238', '203', '50', '146', '15', '25', '156', '191', '16', '2', '195', '189', '96', '193', '109', '204', '161', '185', '210', '50', '235', '30', '242', '193', '102', '194', '121', '177', '241', '102', '132', '240', '71', '192', '79', '25', '197', '39', '161', '22', '258', '237', '15', '7', '78', '156', '197', '70', '176', '139', '240', '132', '78', '177', '191', '194', '117', '10', '79', '238', '195', '24', '101', '118', '196', '2', '102', '21', '180', '185', '170', '79', '139', '135', '191', '151', '200', '191', '154', '262', '200', '237', '56', '27', '191', '180', '258', '139', '139', '192', '30', '0', '242', '239', '242', '25', '191', '242', '240', '3', '56', '215', '171', '192', '193', '222', '238', '237', '215', '272', '2', '156', '237', '151', '189', '80', '15', '260', '260', '217', '235', '22', '228', '135', '18', '191', '157', '78', '2', '240', '213', '179', '2', '191', '79', '79', '153', '3', '194', '78', '121', '71', '121', '149', '257', '194', '191', '188', '15', '25', '125', '102', '15', '130', '130', '177', '10', '135', '265', '240', '149', '196', '200', '204', '204', '213', '177', '197', '194', '13', '202', '30', '213', '125', '193', '125', '125', '125', '125', '125', '15', '180', '78', '1', '25', '96', '200', '29', '156', '172', '202', '22', '78', '197', '188', '241', '101', '101', '101', '221', '50', '191', '15', '15', '0', '101', '101', '168', '56', '197', '130', '62', '204', '29', '10', '96', '97', '139', '97', '97', '2', '2', '262', '39', '176', '172', '194', '194', '18', '78', '13', '80', '139', '196', '207', '191', '151', '192', '150', '13', '104', '104', '101', '15', '125', '101', '15', '0', '50', '194', '264', '50', '25', '25', '125', '125', '125', '125', '125', '25', '29', '25', '25', '78', '240', '261', '19', '2', '74', '78', '238', '50', '50', '203', '197', '71', '104', '80', '194', '149', '170', '130', '2', '237', '170', '202', '48', '97', '93', '179', '235', '78', '102', '102', '70', '208', '238', '104', '146', '118', '118', '96', '237', '274', '218', '241', '102', '146', '209', '209', '240', '170', '137', '7', '197', '191', '264', '29', '46', '96', '193', '19', '12', '213', '215', '275', '179', '18', '183', '78', '221', '18', '170', '119', '21', '140', '240', '96', '192', '192', '21', '96', '149', '239', '135', '191', '240', '184', '183', '24', '2', '96', '197', '194', '215', '18', '192', '121', '202', '196', '25', '97', '214', '3', '97', '16', '0', '192', '135', '215', '5', '12', '80', '143', '202', '242', '151', '13', '209', '203', '200', '30', '209', '194', '102', '102', '102', '102', '102', '102', '22', '24', '6', '18', '5', '191', '101', '50', '101', '101', '192', '153', '153', '101', '101', '10', '192', '25', '140', '50', '241', '102', '102', '32', '150', '101', '102', '153', '102', '101', '153', '102', '25', '10', '125', '18', '125', '177', '209', '102', '10', '78', '78', '137', '18', '237', '241', '175', '195', '175', '96', '176', '237', '96', '96', '101', '193', '238', '170', '238', '102', '222', '268', '238', '102', '244', '34', '18', '244', '202', '149', '149', '193', '102', '170', '194', '75', '101', '222', '238', '50', '195', '135', '22', '93', '18', '102', '102', '93', '93', '191', '147', '244', '96', '2', '241', '241', '93', '189', '189', '177', '35', '137', '197', '191', '40', '40', '195', '176', '35', '200', '200', '237', '135', '140', '140', '93', '156', '200', '78', '191', '22', '81', '238', '101', '213', '101', '78', '109', '19', '140', '196', '78', '78', '76', '218', '14', '265', '101', '241', '155', '4', '50', '78', '78', '238', '135', '191', '265', '81', '238', '238', '102', '102', '102', '140', '162', '32', '17', '17', '171', '171', '191', '101', '102', '192', '101', '17', '171', '135', '102', '242', '195', '237', '21', '18', '238', '191', '278', '140', '149', '97', '260', '155', '276', '192', '196', '21', '191', '2', '97', '0', '150', '221', '213', '209', '238', '80', '260', '194', '104', '96', '193', '14', '240', '193', '117', '117', '155', '17', '264', '260', '209', '209', '2', '137', '71', '72', '101', '205', '173', '191', '118', '24', '41', '197', '78', '94', '237', '156', '193', '153', '154', '153', '209', '110', '78', '78', '213', '238', '78', '191', '93', '79', '177', '267', '133', '133', '46', '139', '40', '143', '74', '241', '202', '50', '3', '102', '78', '149', '191', '156', '18', '93', '195', '192', '137', '240', '135', '204', '189', '22', '56', '154', '25', '192', '130', '277', '191', '80', '39', '21', '21', '238', '238', '23', '192', '191', '192', '94', '94', '80', '139', '17', '10', '22', '227', '227', '140', '102', '21', '17', '227', '227', '23', '243', '264', '18', '274', '149', '238', '30', '221', '22', '238', '193', '135', '205', '195', '238', '238', '197', '130', '195', '18', '104', '102', '14', '59', '104', '18', '18', '258', '34', '241', '138', '188', '176', '102', '1', '140', '140', '22', '22', '278', '10', '96', '35', '145', '120', '33', '33', '22', '33', '22', '35', '22', '19', '33', '102', '192', '192', '175', '22', '72', '191', '222', '125', '22', '79', '18', '194', '18', '250', '192', '192', '96', '48', '274', '273', '132', '133', '192', '40', '171', '171', '246', '191', '10', '166', '197', '195', '125', '125', '200', '153', '192', '149', '110', '139', '194', '48', '102', '102', '104', '203', '238', '238', '238', '78', '18', '238', '121', '18', '238', '96', '238', '238', '13', '191', '18', '18', '240', '96', '40', '188', '18', '137', '175', '28', '28', '28', '149', '191', '191', '207', '239', '239', '153', '153', '153', '153', '153', '153', '154', '153', '154', '153', '153', '153', '18', '0', '27', '18', '25', '96', '96', '137', '33', '10', '130', '153', '153', '154', '180', '137', '44', '241', '153', '268', '25', '161', '25', '39', '242', '4', '238', '195', '102', '135', '13', '229', '140', '1', '79', '192', '229', '191', '118', '21', '260', '203', '18', '177', '191', '10', '238', '10', '133', '14', '175', '175', '192', '18', '153', '153', '254', '140', '149', '22', '22', '22', '237', '40', '135', '191', '192', '192', '62', '205', '19', '238', '176', '214', '200', '96', '15', '153', '191', '108', '29', '166', '28', '193', '197', '96', '195', '108', '102', '241', '195', '10', '33', '166', '197', '168', '2', '96', '26', '33', '204', '258', '149', '153', '153', '40', '137', '241', '166', '221', '23', '221', '153', '15', '113', '113', '78', '171', '40', '71', '21', '71', '80', '15', '191', '191', '135', '110', '10', '259', '102', '149', '149', '109', '109', '175', '97', '97', '97', '153', '15', '238', '193', '175', '38', '38', '137', '18', '235', '155', '137', '265', '110', '18', '18', '256', '191', '224', '173', '104', '10', '152', '24', '24', '24', '17', '40', '40', '40', '40', '40', '40', '112', '6', '18', '18', '210', '96', '191', '27', '213', '17', '153', '26', '133', '18', '18', '153', '38', '96', '40', '38', '40', '40', '10', '17', '17', '95', '71', '192', '17', '259', '213', '200', '15', '11', '207', '194', '18', '135', '238', '191', '241', '191', '116', '35', '147', '156', '67', '78', '17', '227', '191', '117', '188', '130', '72', '96', '235', '97', '18', '150', '203', '25', '126', '149', '200', '94', '4', '4', '1', '244', '228', '252', '149', '191', '149', '15', '17', '17', '29', '18', '18', '116', '23', '237', '94', '184', '268', '197', '175', '243', '28', '16', '10', '50', '191', '102', '102', '153', '149', '111', '78', '191', '135', '18', '35', '78', '238', '208', '93', '95', '258', '17', '111', '111', '262', '22', '22', '104', '161', '209', '21', '195', '12', '116', '241', '242', '239', '17', '97', '16', '40', '117', '94', '96', '78', '15', '103', '195', '135', '235', '247', '135', '130', '260', '239', '176', '93', '191', '209', '239', '93', '264', '30', '18', '30', '18', '18', '103', '191', '97', '17', '95', '115', '209', '140', '22', '22', '241', '10', '153', '3', '188', '238', '226', '195', '97', '96', '238', '97', '130', '97', '116', '96', '96', '242', '45', '149', '166', '155', '96', '116', '264', '10', '237', '192', '71', '22', '71', '175', '149', '116', '237', '274', '18', '235', '176', '116', '202', '250', '250', '78', '2', '196', '2', '97', '143', '18', '111', '111', '80', '78', '153', '80', '40', '110', '30', '112', '30', '111', '215', '112', '149', '78', '78', '209', '110', '110', '238', '109', '239', '112', '151', '154', '16', '16', '16', '78', '137', '179', '72', '149', '10', '111', '268', '96', '170', '111', '38', '192', '153', '183', '195', '30', '30', '147', '195', '147', '93', '264', '153', '25', '18', '19', '22', '74', '40', '93', '166', '15', '127', '147', '192', '260', '238', '103', '269', '127', '19', '19', '153', '18', '78', '238', '30', '30', '241', '16', '235', '250', '204', '238', '218', '40', '93', '184', '18', '0', '241', '195', '94', '154', '109', '156', '10', '21', '30', '197', '39', '78', '24', '107', '142', '142', '228', '229', '229', '229', '232', '179', '239', '142', '232', '23', '175', '147', '137', '137', '137', '171', '191', '117', '128', '127', '204', '98', '147', '235', '239', '153', '15', '204', '255', '80', '204', '204', '149', '113', '19', '167', '22', '250', '17', '17', '17', '18', '17', '67', '121', '125', '26', '97', '97', '97', '97', '247', '124', '163', '147', '112', '111', '208', '108', '38', '209', '154', '154', '194', '135', '22', '147', '241', '120', '218', '260', '156', '155', '241', '192', '215', '180', '179', '149', '40', '193', '192', '150', '102', '93', '1', '149', '137', '133', '102', '102', '102', '78', '145', '21', '96', '175', '175', '78', '123', '30', '95', '19', '18', '110', '18', '152', '152', '147', '26', '135', '260', '158', '203', '195', '158', '112', '156', '125', '213', '230', '30', '15', '125', '106', '111', '195', '181', '130', '195', '130', '239', '15', '194', '16', '15', '111', '61', '72', '57', '128', '239', '239', '203', '25', '218', '10', '155', '191', '46', '98', '175', '260', '102', '21', '191', '260', '191', '239', '15', '30', '93', '147', '103', '78', '177', '30', '95', '155', '155', '155', '103', '103', '112', '241', '191', '191', '17', '78', '153', '150', '113', '115', '147', '78', '40', '163', '33', '15', '149', '103', '192', '191', '96', '106', '115', '143', '235', '200', '204', '238', '204', '204', '74', '196', '113', '246', '102', '150', '16', '176', '20', '195', '128', '197', '15', '71', '71', '193', '24', '24', '24', '170', '204', '209', '111', '144', '112', '22', '222', '224', '28', '209', '155', '213', '275', '196', '239', '195', '25', '103', '149', '128', '19', '96', '102', '135', '17', '232', '155', '191', '20', '18', '241', '30', '25', '205', '97', '97', '96', '72', '182', '135', '135', '20', '149', '149', '78', '135', '40', '18', '153', '10', '26', '158', '205', '157', '157', '135', '135', '140', '155', '192', '26', '268', '263', '245', '155', '15', '177', '159', '212', '161', '239', '200', '40', '35', '153', '197', '259', '15', '254', '175', '10', '30', '239', '67', '195', '200', '191', '22', '66', '191', '112', '133', '128', '235', '161', '155', '17', '93', '36', '113', '239', '18', '18', '191', '165', '163', '163', '238', '79', '158', '40', '102', '40', '200', '191', '60', '241', '245', '96', '78', '30', '15', '173', '192', '192', '158', '64', '113', '200', '74', '128', '135', '115', '115', '10', '74', '102', '102', '115', '111', '238', '244', '109', '109', '238', '152', '10', '237', '18', '50', '16', '173', '204', '15', '208', '276', '43', '109', '258', '222', '15', '94', '235', '203', '79', '128', '113', '3', '204', '106', '133', '214', '218', '25', '98', '166', '71', '30', '175', '175', '97', '135', '108', '19', '18', '196', '35', '12', '196', '78', '166', '155', '72', '155', '22', '109', '258', '196', '263', '161', '204', '210', '14', '208', '208', '208', '208', '204', '102', '102', '213', '155', '251', '153', '153', '153', '154', '154', '237', '97', '19', '28', '94', '136', '112', '19', '97', '152', '153', '153', '111', '111', '135', '135', '105', '17', '105', '152', '152', '152', '153', '79', '266', '135', '120', '125', '149', '50', '162', '72', '17', '147', '109', '115', '24', '105', '244', '149', '238', '203', '35', '202', '177', '173', '239', '19', '19', '179', '237', '14', '18', '185', '58', '158', '144', '213', '237', '155', '250', '239', '93', '237', '97', '195', '157', '107', '109', '246', '165', '98', '49', '2', '266', '149', '20', '18', '71', '108', '22', '115', '239', '40', '178', '232', '238', '218', '118', '118', '210', '235', '46', '228', '192', '112', '149', '96', '111', '21', '95', '95', '95', '18', '57', '206', '18', '239', '157', '80', '196', '112', '176', '99', '106', '71', '235', '135', '18', '209', '78', '20', '20', '103', '33', '3', '103', '166', '79', '79', '156', '229', '18', '238', '2', '16', '128', '200', '117', '22', '197', '166', '78', '237', '241', '107', '53', '189', '204', '243', '163', '221', '15', '200', '221', '152', '23', '193', '107', '195', '99', '97', '19', '125', '276', '113', '193', '64', '78', '197', '117', '175', '135', '135', '78', '93', '154', '153', '256', '256', '135', '137', '135', '137', '149', '110', '15', '159', '21', '158', '192', '192', '117', '29', '97', '258', '78', '46', '5', '80', '15', '105', '260', '161', '191', '2', '138', '117', '215', '176', '156', '194', '265', '115', '181', '21', '14', '106', '106', '108', '200', '156', '103', '102', '29', '17', '204', '175', '227', '118', '18', '239', '204', '258', '78', '155', '213', '109', '192', '186', '186', '15', '241', '133', '30', '30', '239', '30', '218', '109', '138', '241', '241', '45', '135', '112', '232', '104', '106', '200', '53', '23', '112', '17', '117', '59', '260', '195', '106', '117', '159', '204', '200', '228', '165', '165', '261', '200', '227', '166', '93', '21', '80', '112', '152', '196', '203', '228', '98', '102', '15', '139', '110', '13', '238', '6', '171', '94', '208', '155', '112', '203', '239', '191', '39', '136', '203', '195', '46', '203', '148', '192', '210', '23', '4', '153', '29', '165', '192', '204', '232', '94', '112', '239', '98', '71', '248', '17', '155', '10', '78', '191', '191', '111', '25', '25', '19', '19', '17', '235', '102', '265', '195', '235', '163', '238', '71', '197', '107', '155', '30', '72', '19', '259', '80', '155', '23', '64', '239', '197', '156', '57', '203', '258', '200', '205', '246', '238', '258', '227', '238', '238', '19', '148', '161', '111', '105', '202', '109', '262', '133', '137', '221', '135', '52', '170', '111', '36', '30', '156', '102', '268', '115', '22', '153', '133', '259', '98', '235', '135', '191', '25', '235', '25', '29', '94', '195', '203', '212', '112', '135', '245', '260', '200', '96', '158', '23', '165', '22', '18', '249', '240', '109', '43', '37', '15', '241', '241', '23', '135', '240', '108', '98', '268', '275', '191', '136', '263', '259', '196', '98', '98', '204', '248', '123', '105', '204', '240', '9', '9', '98', '218', '189', '238', '246', '155', '130', '157', '74', '109', '29', '189', '203', '213', '195', '241', '159', '238', '204', '218', '105', '23', '16', '263', '206', '150', '152', '204', '202', '173', '267', '18', '153', '155', '196', '2', '97', '158', '158', '204', '98', '202', '98', '204', '258', '98', '22', '113', '113', '135', '71', '237', '155', '163', '250', '233', '26', '26', '26', '26', '26', '19', '170', '18', '15', '19', '191', '30', '30', '48', '35', '30', '237', '46', '119', '16', '78', '239', '150', '245', '235', '158', '27', '241', '30', '38', '107', '75', '241', '135', '106', '106', '98', '159', '96', '135', '111', '176', '175', '200', '18', '14', '242', '163', '187', '20', '12', '191', '23', '106', '171', '25', '78', '162', '191', '157', '94', '237', '246', '203', '108', '238', '204', '204', '1', '102', '154', '106', '170', '102', '200', '2', '135', '106', '258', '258', '227', '26', '15', '25', '71', '203', '96', '3', '241', '200', '30', '108', '98', '108', '27', '106', '26', '150', '259', '78', '113', '113', '158', '202', '16', '136', '239', '156', '239', '35', '276', '252', '115', '22', '22', '258', '172', '3', '154', '3', '98', '237', '102', '235', '112', '206', '98', '57', '118', '153', '18', '26', '30', '196', '163', '191', '153', '72', '24', '156', '117', '122', '30', '135', '192', '98', '28', '30', '30', '95', '241', '155', '17', '195', '31', '103', '202', '98', '237', '94', '112', '163', '163', '94', '163', '112', '19', '195', '191', '15', '135', '78', '109', '160', '152', '97', '98', '107', '167', '97', '138', '138', '26', '191', '97', '97', '30', '30', '206', '202', '13', '30', '240', '111', '261', '79', '0', '111', '19', '152', '107', '107', '18', '98', '28', '258', '244', '106', '237', '204', '191', '15', '183', '202', '16', '111', '166', '268', '202', '12', '30', '78', '110', '133', '71', '197', '111', '52', '79', '171', '156', '94', '13', '118', '111', '71', '133', '18', '105', '30', '22', '133', '60', '237', '191', '6', '126', '3', '238', '276', '78', '162', '150', '115', '191', '107', '200', '148', '135', '72', '155', '241', '4', '108', '160', '203', '20', '133', '2', '80', '80', '154', '18', '15', '115', '239', '167', '167', '35', '268', '18', '221', '18', '195', '204', '195', '216', '133', '112', '104', '223', '191', '115', '30', '30', '30', '30', '30', '202', '204', '165', '200', '200', '29', '191', '237', '267', '18', '175', '81', '264', '264', '178', '115', '102', '178', '212', '273', '202', '109', '259', '166', '135', '155', '219', '161', '18', '94', '241', '191', '196', '191', '98', '26', '26', '204', '204', '135', '27', '238', '260', '182', '26', '98', '94', '93', '238', '156', '19', '111', '79', '16', '192', '113', '19', '241', '18', '202', '260', '272', '106', '163', '156', '106', '17', '109', '221', '78', '203', '7', '136', '197', '193', '191', '98', '112', '200', '95', '221', '98', '161', '191', '3', '106', '189', '111', '25', '196', '196', '241', '163', '200', '24', '78', '176', '3', '98', '102', '98', '19', '161', '224', '80', '18', '191', '112', '105', '254', '71', '200', '137', '29', '20', '165', '8', '106', '23', '106', '155', '200', '265', '135', '126', '203', '188', '161', '158', '258', '255', '98', '203', '50', '35', '102', '158', '175', '40', '239', '26', '66', '57', '193', '155', '15', '135', '10', '107', '106', '229', '18', '191', '204', '107', '99', '235', '238', '18', '205', '205', '35', '238', '208', '203', '110', '110', '109', '112', '23', '18', '19', '182', '156', '18', '18', '156', '111', '135', '137', '133', '137', '240', '99', '197', '113', '15', '165', '39', '97', '97', '15', '150', '163', '109', '268', '15', '15', '238', '203', '67', '161', '197', '18', '196', '97', '161', '197', '161', '185', '238', '191', '30', '121', '133', '160', '160', '74', '135', '75', '54', '135', '196', '23', '241', '166', '194', '239', '135', '240', '208', '238', '98', '135', '152', '176', '22', '174', '113', '241', '237', '57', '195', '191', '24', '165', '165', '204', '204', '56', '109', '27', '153', '252', '228', '165', '252', '200', '28', '158', '135', '196', '22', '18', '133', '30', '204', '237', '196', '93', '106', '94', '25', '93', '159', '4', '195', '23', '237', '25', '199', '57', '26', '276', '274', '204', '75', '18', '204', '108', '22', '133', '191', '111', '204', '203', '133', '237', '166', '4', '62', '202', '166', '161', '165', '74', '109', '118', '200', '22', '206', '25', '209', '17', '155', '204', '64', '23', '30', '202', '149', '171', '135', '136', '184', '26', '18', '159', '195', '18', '166', '156', '98', '133', '38', '40', '18', '18', '166', '109', '109', '209', '22', '195', '163', '163', '108', '18', '15', '99', '135', '174', '26', '19', '166', '133', '259', '151', '235', '204', '228', '133', '30', '161', '111', '242', '84', '238', '98', '195', '78', '160', '28', '25', '264', '165', '41', '19', '55', '46', '202', '121', '15', '32', '98', '238', '0', '78', '31', '30', '203', '264', '203', '135', '104', '111', '78', '212', '111', '98', '126', '22', '2', '30', '237', '212', '265', '163', '106', '106', '235', '109', '223', '161', '202', '150', '170', '255', '112', '114', '170', '98', '96', '197', '36', '18', '228', '14', '255', '204', '102', '239', '0', '185', '30', '22', '23', '246', '199', '133', '191', '78', '195', '153', '112', '191', '200', '170', '171', '200', '78', '128', '25', '153', '237', '99', '94', '208', '107', '208', '208', '78', '204', '204', '27', '191', '272', '134', '158', '158', '174', '118', '64', '18', '258', '150', '189', '18', '153', '241', '204', '112', '106', '135', '71', '148', '7', '133', '140', '187', '23', '223', '19', '19', '2', '136', '2', '200', '161', '15', '98', '158', '148', '3', '250', '153', '235', '165', '220', '176', '110', '240', '151', '148', '133', '74', '193', '46', '27', '173', '94', '176', '105', '256', '161', '226', '18', '78', '191', '171', '243', '26', '0', '108', '167', '159', '239', '150', '112', '155', '79', '241', '202', '165', '254', '107', '202', '156', '166', '193', '239', '135', '15', '203', '187', '148', '25', '15', '22', '194', '239', '56', '191', '176', '262', '200', '110', '72', '99', '102', '135', '98', '113', '15', '15', '159', '262', '171', '151', '98', '204', '213', '255', '26', '243', '175', '156', '98', '107', '191', '165', '241', '26', '165', '171', '30', '160', '166', '266', '11', '19', '211', '93', '254', '30', '239', '115', '273', '205', '112', '237', '263', '74', '98', '79', '122', '94', '265', '155', '191', '16', '235', '229', '98', '191', '227', '17', '111', '267', '135', '202', '195', '10', '180', '196', '196', '133', '135', '133', '95', '111', '122', '258', '191', '3', '114', '107', '19', '264', '74', '56', '200', '200', '133', '161', '97', '15', '97', '197', '196', '253', '93', '202', '258', '106', '153', '26', '109', '40', '149', '8', '196', '15', '119', '155', '23', '237', '193', '264', '148', '151', '238', '200', '94', '75', '148', '175', '24', '195', '171', '30', '203', '200', '202', '16', '158', '223', '191', '22', '264', '135', '35', '57', '94', '191', '196', '202', '98', '18', '196', '3', '133', '78', '191', '208', '258', '18', '200', '29', '135', '109', '110', '2', '185', '98', '133', '19', '22', '204', '95', '238', '25', '133', '250', '135', '135', '3', '47', '158', '187', '2', '78', '115', '155', '3', '218', '166', '112', '266', '22', '195', '135', '72', '97', '197', '209', '127', '80', '2', '98', '14', '239', '15', '113', '165', '204', '78', '204', '191', '175', '200', '193', '18', '196', '133', '109', '237', '56', '196', '3', '163', '163', '163', '195', '15', '263', '18', '84', '18', '133', '107', '18', '102', '156', '238', '102', '26', '244', '82', '75', '23', '258', '238', '136', '3', '204', '184', '200', '196', '158', '106', '27', '238', '155', '152', '50', '50', '2', '235', '98', '203', '166', '109', '25', '18', '151', '133', '200', '26', '26', '26', '19', '107', '156', '23', '26', '3', '70', '225', '41', '26', '239', '26', '186', '26', '34', '272', '272', '104', '78', '260', '18', '204', '26', '78', '18', '210', '135', '203', '71', '207', '26', '133', '13', '25', '26', '115', '94', '264', '135', '244', '79', '30', '263', '22', '109', '205', '176', '106', '22', '17', '151', '106', '191', '160', '160', '165', '78', '177', '196', '260', '166', '35', '191', '260', '98', '18', '237', '18', '161', '18', '197', '133', '30', '35', '200', '170', '155', '245', '19', '200', '160', '96', '172', '165', '55', '137', '241', '133', '191', '112', '25', '126', '238', '18', '18', '108', '135', '161', '135', '73', '112', '107', '8', '112', '109', '195', '25', '30', '105', '161', '241', '110', '75', '238', '215', '238', '104', '25', '3', '155', '30', '16', '3', '175', '99', '3', '200', '23', '3', '199', '203', '191', '196', '196', '204', '98', '105', '133', '197', '135', '161', '39', '15', '30', '133', '55', '191', '203', '133', '203', '140', '200', '149', '30', '96', '135', '23', '26', '3', '18', '155', '158', '135', '135', '18', '250', '18', '22', '114', '18', '18', '166', '18', '30', '18', '239', '18', '161', '50', '50', '49', '51', '23', '55', '98', '78', '210', '211', '18', '239', '78', '13', '193', '114', '78', '18', '25', '133', '19', '176', '4', '173', '50', '108', '17', '4', '204', '94', '112', '207', '109', '115', '23', '235', '197', '194', '197', '213', '148', '40', '157', '107', '200', '147', '14', '14', '18', '238', '18', '238', '18', '238', '57', '200', '135', '30', '18', '191', '191', '200', '166', '151', '161', '73', '19', '73', '239', '239', '165', '108', '2', '93', '166', '133', '133', '173', '35', '193', '114', '74', '19', '40', '58', '104', '16', '260', '160', '158', '112', '210', '158', '167', '203', '203', '3', '3', '109', '30', '133', '258', '70', '204', '200', '152', '155', '192', '204', '108', '191', '204', '26', '98', '26', '98', '264', '158', '18', '150', '137', '153', '196', '199', '191', '115', '168', '157', '149', '151', '78', '137', '160', '259', '166', '196', '171', '175', '199', '166', '161', '70', '13', '166', '152', '152', '206', '58', '18', '207', '58', '74', '207', '29', '46', '109', '135', '74', '174', '109', '40', '10', '30', '258', '16', '111', '30', '15', '239', '204', '48', '25', '196', '0', '156', '72', '72', '96', '22', '72', '133', '133', '238', '26', '3', '50', '152', '193', '235', '136', '153', '50', '1', '19', '239', '30', '25', '179', '161', '30', '191', '3', '27', '133', '3', '40', '23', '133', '150', '151', '265', '32', '191', '3', '25', '170', '238', '40', '259', '166', '137', '241', '133', '209', '159', '78', '78', '153', '26', '52', '241', '36', '250', '96', '30', '30', '38', '18', '18', '238', '99', '72', '111', '3', '244', '191', '226', '3', '191', '114', '135', '109', '40', '79', '202', '78', '114', '238', '10', '150', '108', '163', '108', '113', '197', '109', '115', '161', '3', '238', '3', '237', '161', '107', '155', '78', '202', '112', '38', '195', '137', '250', '133', '237', '161', '25', '26', '25', '15', '15', '239', '166', '114', '229', '111', '40', '135', '94', '102', '162', '204', '237', '17', '166', '136', '22', '98', '99', '99', '166', '35', '196', '3', '203', '135', '135', '84', '98', '266', '238', '13', '166', '78', '193', '98', '241', '104', '153', '192', '200', '10', '95', '95', '148', '107', '14', '17', '191', '239', '19', '203', '255', '197', '213', '94', '3', '107', '23', '35', '10', '237', '235', '78', '29', '18', '160', '200', '150', '99', '109', '155', '161', '133', '114', '18', '18', '18', '264', '133', '104', '161', '70', '17', '204', '195', '204', '241', '99', '82', '241', '241', '161', '18', '235', '175', '2', '2', '202', '50', '230', '26', '40', '40', '200', '78', '156', '26', '112', '26', '30', '71', '184', '200', '135', '153', '23', '106', '125', '175', '133', '133', '7', '38', '196', '161', '0', '49', '241', '158', '15', '21', '104', '2', '3', '176', '259', '135', '78', '75', '112', '200', '1', '207', '238', '161', '150', '158', '2', '45', '31', '237', '149', '3', '26', '165', '135', '152', '115', '133', '18', '40', '166', '30', '173', '40', '26', '78', '50', '48', '50', '13', '45', '200', '200', '18', '151', '205', '135', '38', '117', '196', '135', '155', '111', '196', '196', '166', '23', '25', '161', '18', '124', '259', '165', '58', '170', '204', '250', '103', '160', '238', '194', '163', '241', '255', '235', '17', '133', '26', '135', '196', '237', '114', '258', '193', '248', '40', '195', '84', '238', '146', '199', '40', '161', '22', '137', '43', '166', '156', '4', '104', '133', '177', '211', '193', '78', '203', '246', '237', '264', '3', '149', '78', '78', '111', '239', '13', '238', '176', '112', '150', '156', '3', '6', '118', '178', '178', '21', '186', '150', '241', '50', '243', '191', '121', '278', '29', '200', '18', '3', '197', '191', '98', '173', '195', '25', '195', '264', '221', '15', '156', '25', '200', '211', '195', '26', '35', '2', '135', '126', '113', '167', '191', '264', '196', '133', '18', '26', '146', '203', '219', '23', '160', '160', '135', '259', '3', '159', '120', '160', '106', '240', '133', '115', '258', '38', '22', '3', '26', '22', '107', '49', '48', '25', '237', '72', '219', '196', '196', '111', '135', '196', '18', '166', '172', '135', '26', '258', '70', '98', '40', '166', '23', '96', '3', '30', '155', '126', '146', '114', '242', '237', '157', '238', '241', '40', '133', '200', '161', '175', '200', '175', '84', '266', '18', '99', '30', '242', '151', '153', '166', '199', '228', '3', '237', '132', '110', '135', '80', '15', '15', '15', '84', '133', '15', '177', '3', '202', '255', '71', '115', '235', '19', '18', '18', '102', '58', '78', '165', '166', '264', '12', '150', '15', '2', '18', '135', '109', '200', '258', '128', '3', '199', '197', '239', '109', '222', '239', '107', '125', '132', '204', '204', '133', '135', '30', '158', '203', '15', '264', '237', '133', '196', '3', '252', '26', '98', '79', '199', '210', '0', '99', '30', '26', '259', '6', '49', '166', '98', '108', '9', '133', '102', '155', '98', '114', '78', '35', '166', '188', '202', '235', '30', '149', '165', '176', '138', '29', '153', '25', '25', '137', '133', '166', '135', '125', '71', '193', '154', '165', '98', '23', '56', '198', '26', '26', '0', '109', '136', '163', '151', '25', '15', '241', '195', '204', '191', '195', '161', '175', '133', '237', '78', '165', '258', '135', '78', '135', '258', '18', '235', '30', '187', '150', '275', '149', '2', '36', '112', '22', '161', '238', '203', '112', '265', '75', '133', '205', '258', '158', '18', '26', '18', '25', '26', '15', '26', '239', '15', '240', '18', '71', '71', '78', '135', '3', '148', '166', '26', '176', '2', '197', '150', '176', '19', '18', '97', '166', '18', '153', '250', '133', '210', '20', '192', '196', '26', '191', '2', '81', '70', '5', '79', '3', '30', '193', '193', '69', '160', '176', '246', '108', '54', '250', '135', '200', '80', '3', '167', '18', '166', '200', '152', '23', '149', '19', '14', '13', '237', '196', '19', '112', '254', '204', '191', '23', '264', '199', '209', '137', '31', '135', '238', '78', '18', '191', '15', '164', '195', '74', '163', '163', '41', '99', '26', '135', '26', '40', '40', '250', '200', '200', '26', '237', '4', '195', '158', '35', '71', '191', '3', '218', '274', '58', '3', '23', '177', '132', '49', '62', '238', '166', '97', '264', '193', '29', '26', '49', '241', '197', '200', '64', '227', '258', '44', '15', '133', '175', '238', '109', '254', '237', '198', '175', '166', '213', '174', '196', '107', '259', '98', '10', '196', '210', '191', '96', '166', '135', '15', '197', '78', '78', '195', '176', '18', '98', '129', '170', '18', '247', '203', '26', '158', '40', '64', '200', '163', '196', '19', '205', '197', '193', '191', '69', '96', '74', '35', '36', '64', '133', '172', '19', '266', '96', '94', '103', '258', '193', '264', '98', '254', '50', '193', '25', '170', '75', '84', '27', '252', '203', '94', '94', '19', '98', '137', '12', '153', '3', '155', '135', '238', '15', '196', '15', '26', '84', '49', '27', '135', '203', '48', '133', '129', '133', '258', '155', '35', '195', '71', '241', '135', '166', '193', '165', '104', '196', '117', '196', '26', '237', '109', '95', '196', '60', '40', '158', '193', '250', '104', '94', '178', '27', '193', '78', '106', '79', '204', '133', '237', '235', '135', '202', '70', '15', '264', '241', '254', '110', '199', '49', '255', '30', '193', '238', '204', '102', '97', '161', '43', '228', '1', '6', '196', '193', '26', '15', '128', '46', '155', '32', '133', '69', '47', '135', '78', '205', '200', '200', '202', '3', '95', '191', '78', '26', '248', '258', '191', '240', '49', '110', '38', '193', '200', '200', '96', '200', '204', '135', '9', '200', '258', '15', '46', '15', '149', '155', '197', '2', '166', '151', '72', '135', '97', '211', '238', '135', '163', '161', '26', '163', '129', '241', '239', '241', '16', '26', '18', '263', '260', '166', '3', '264', '195', '79', '132', '251', '50', '133', '15', '25', '15', '191', '15', '78', '50', '78', '71', '227', '69', '200', '69', '109', '64', '133', '197', '196', '3', '45', '238', '26', '132', '104', '196', '193', '136', '80', '26', '75', '117', '14', '78', '75', '98', '15', '2', '15', '50', '255', '136', '3', '112', '173', '69', '191', '19', '239', '235', '109', '70', '135', '78', '165', '238', '262', '199', '35', '203', '224', '193', '15', '18', '150', '238', '197', '238', '193', '135', '3', '150', '187', '199', '239', '109', '152', '27', '257', '6', '235', '104', '191', '133', '166', '155', '26', '16', '136', '198', '98', '154', '235', '27', '102', '30', '191', '15', '132', '132', '114', '107', '241', '241', '156', '199', '241', '49', '62', '117', '258', '30', '163', '133', '238', '200', '3', '15', '109', '2', '196', '165', '166', '159', '210', '35', '15', '17', '200', '241', '106', '203', '197', '197', '79', '98', '98', '23', '204', '48', '129', '57', '258', '202', '215', '238', '70', '64', '193', '189', '57', '200', '0', '98', '135', '204', '25', '30', '204', '27', '26', '78', '135', '109', '3', '197', '3', '167', '35', '196', '225', '2', '194', '200', '15', '161', '198', '23', '133', '238', '195', '35', '50', '135', '152', '163', '21', '15', '94', '26', '94', '53', '260', '196', '154', '277', '200', '118', '6', '238', '50', '209', '50', '3', '238', '3', '15', '107', '135', '199', '15', '135', '26', '258', '251', '203', '19', '259', '195', '199', '238', '15', '254', '112', '211', '200', '129', '84', '196', '3', '208', '259', '135', '104', '29', '136', '31', '78', '13', '3', '64', '75', '55', '114', '160', '96', '14', '18', '207', '75', '48', '15', '49', '97', '117', '196', '99', '258', '195', '264', '264', '78', '197', '30', '238', '109', '158', '32', '195', '149', '227', '65', '167', '162', '204', '250', '151', '151', '102', '26', '29', '26', '193', '74', '50', '193', '186', '15', '74', '259', '57', '188', '277', '277', '195', '195', '212', '78', '197', '64', '129', '259', '200', '199', '129', '4', '235', '197', '196', '161', '3', '255', '199', '102', '3', '13', '94', '53', '98', '15', '129', '15', '155', '71', '179', '50', '29', '136', '80', '254', '198', '162', '237', '3', '18', '260', '238', '117', '179', '156', '19', '45', '197', '78', '199', '102', '102', '237', '155', '69', '69', '163', '239', '15', '196', '80', '191', '30', '71', '158', '259', '2', '106', '148', '15', '135', '135', '135', '196', '19', '123', '15', '104', '19', '128', '230', '69', '98', '197', '241', '161', '195', '200', '3', '98', '78', '165', '163', '19', '196', '31', '173', '203', '55', '69', '209', '239', '79', '158', '254', '135', '51', '94', '162', '255', '95', '18', '133', '109', '50', '94', '199', '15', '197', '200', '133', '116', '50', '196', '161', '69', '133', '3', '258', '3', '197', '29', '3', '157', '197', '193', '4', '94', '195', '17', '48', '152', '8', '15', '155', '133', '195', '136', '150', '102', '237', '203', '164', '164', '250', '154', '13', '191', '58', '94', '70', '93', '264', '15', '30', '133', '237', '267', '199', '193', '76', '241', '157', '26', '200', '109', '179', '55', '98', '200', '129', '238', '235', '208', '109', '239', '258', '196', '193', '18', '107', '210', '70', '40', '129', '259', '193', '30', '158', '252', '40', '210', '78', '15', '78', '96', '208', '113', '258', '64', '80', '15', '133', '241', '15', '15', '258', '135', '109', '251', '251', '109', '74', '196', '170', '240', '254', '258', '204', '199', '26', '109', '197', '57', '45', '161', '57', '114', '3', '15', '264', '106', '57', '27', '18', '258', '29', '84', '161', '135', '114', '24', '30', '107', '107', '172', '209', '170', '18', '19', '133', '191', '195', '135', '156', '196', '94', '3', '84', '225', '199', '238', '106', '104', '70', '50', '12', '155', '114', '176', '237', '200', '108', '70', '240', '196', '19', '22', '276', '199', '238', '205', '104', '193', '108', '115', '259', '102', '19', '258', '18', '106', '161', '191', '114', '64', '15', '204', '35', '155', '40', '238', '136', '264', '133', '186', '15', '258', '166', '26', '6', '114', '114', '133', '114', '114', '13', '13', '13', '237', '250', '245', '136', '78', '13', '48', '18', '13', '25', '26', '93', '125', '50', '94', '148', '78', '194', '200', '25', '93', '274', '266', '70', '22', '3', '208', '240', '211', '135', '31', '18', '153', '18', '2', '15', '15', '15', '156', '15', '70', '73', '173', '13', '112', '69', '204', '70', '248', '135', '70', '3', '193', '193', '22', '162', '238', '57', '98', '15', '199', '199', '52', '151', '73', '5', '108', '23', '69', '95', '220', '106', '163', '152', '163', '108', '15', '58', '250', '19', '177', '245', '264', '93', '204', '25', '78', '155', '69', '155', '239', '241', '155', '200', '18', '3', '13', '196', '197', '204', '272', '95', '135', '165', '22', '210', '35', '69', '18', '170', '3', '25', '156', '78', '135', '102', '107', '255', '19', '70', '70', '109', '198', '156', '64', '170', '138', '78', '203', '166', '108', '114', '179', '228', '74', '49', '14', '264', '26', '195', '78', '107', '75', '18', '106', '151', '70', '58', '15', '196', '238', '109', '104', '104', '109', '136', '148', '210', '3', '161', '15', '104', '238', '208', '133', '108', '241', '18', '209', '74', '156', '114', '199', '174', '35', '18', '170', '135', '238', '244', '155', '162', '197', '3', '98', '227', '267', '238', '193', '50', '238', '98', '112', '124', '25', '114', '241', '159', '154', '78', '161', '136', '114', '153', '225', '154', '13', '50', '11', '25', '15', '209', '11', '166', '203', '259', '237', '22', '114', '276', '98', '278', '150', '198', '18', '15', '78', '15', '162', '26', '3', '196', '3', '102', '189', '71', '69', '16', '18', '152', '133', '135', '162', '241', '15', '104', '15', '259', '202', '2', '238', '207', '15', '204', '30', '215', '112', '210', '42', '96', '193', '15', '110', '163', '211', '78', '108', '15', '31', '27', '15', '31', '196', '195', '96', '15', '161', '94', '193', '195', '198', '98', '261', '258', '76', '15', '104', '258', '135', '18', '15', '202', '258', '108', '156', '13', '22', '251', '199', '155', '258', '211', '3', '207', '202', '259', '107', '202', '259', '135', '135', '148', '158', '154', '99', '99', '3', '207', '94', '25', '162', '31', '241', '25', '15', '114', '18', '114', '148', '19', '15', '203', '191', '19', '13', '193', '204', '25', '166', '155', '196', '98', '162', '197', '29', '152', '191', '133', '199', '230', '40', '25', '204', '79', '3', '19', '165', '204', '108', '208', '3', '251', '274', '15', '166', '264', '35', '13', '193', '114', '258', '13', '19', '133', '23', '17', '199', '204', '153', '250', '204', '16', '12', '107', '191', '18', '3', '15', '165', '15', '69', '17', '165', '17', '15', '15', '156', '23', '23', '15', '197', '104', '197', '26', '208', '135', '211', '250', '18', '204', '25', '196', '53', '69', '104', '104', '19', '78', '241', '230', '30', '195', '96', '26', '109', '196', '69', '238', '15', '18', '133', '26', '106', '109', '98', '15', '258', '274', '193', '203', '58', '57', '274', '2', '202', '161', '136', '135', '155', '241', '35', '151', '137', '5', '114', '96', '16', '209', '158', '15', '266', '161', '15', '25', '274', '202', '3', '161', '237', '267', '204', '125', '15', '151', '155', '259', '151', '210', '238', '15', '26', '132', '50', '173', '197', '114', '241', '15', '258', '109', '166', '274', '13', '191', '15', '26', '164', '258', '163', '15', '264', '11', '260', '148', '150', '207', '196', '98', '272', '133', '196', '107', '26', '191', '163', '191', '135', '3', '18', '18', '119', '69', '196', '15', '156', '135', '112', '258', '155', '256', '199', '204', '212', '18', '135', '254', '72', '258', '254', '15', '133', '18', '259', '193', '258', '204', '40', '18', '29', '155', '104', '107', '274', '241', '25', '26', '69', '148', '166', '46', '94', '135', '15', '155', '104', '241', '32', '274', '249', '15', '197', '104', '44', '259', '69', '191', '15', '198', '193', '133', '17', '258', '248', '166', '197', '200', '250', '204', '3', '225', '238', '163', '69', '15', '15', '26', '202', '19', '209', '78', '212', '15', '78', '243', '110', '191', '15', '18', '107', '38', '208', '166', '239', '237', '199', '15', '204', '258', '173', '172', '200', '274', '243', '19', '191', '135', '203', '69', '149', '148', '196', '256', '204', '97', '191', '26', '114', '274', '200', '69', '69', '64', '69', '239', '25', '274', '203', '78', '135', '196', '202', '17', '2', '2', '3', '98', '98', '62', '2', '95', '96', '2', '207', '78', '200', '26', '64', '18', '93', '49', '57', '154', '200', '69', '94', '98', '98', '56', '56', '166', '98', '212', '226', '226', '15', '69', '15', '273', '272', '104', '193', '3', '98', '199', '98', '25', '125', '259', '249', '108', '40', '26', '198', '218', '73', '148', '69', '204', '241', '35', '3', '274', '70', '18', '71', '35', '172', '133', '106', '69', '107', '199', '219', '19', '133', '69', '135', '109', '57', '191', '135', '252', '104', '3', '94', '3', '72', '126', '16', '36', '84', '241', '3', '218', '98', '108', '46', '19', '196', '118', '104', '29', '238', '69', '10', '204', '264', '259', '124', '242', '107', '133', '25', '64', '30', '194', '259', '159', '264', '274', '52', '200', '104', '29', '30', '193', '98', '94', '114', '204', '40', '18', '198', '97', '3', '99', '274', '239', '177', '15', '18', '126', '126', '208', '148', '15', '104', '29', '36', '218', '70', '15', '23', '278', '23', '36', '191', '210', '51', '245', '167', '72', '237', '25', '203', '36', '196', '193', '196', '15', '151', '207', '203', '103', '166', '135', '110', '27', '13', '27', '191', '98', '57', '166', '50', '21', '15', '197', '23', '2', '70', '69', '193', '274', '104', '166', '258', '126', '238', '161', '18', '193', '69', '140', '170', '197', '204', '165', '203', '170', '15', '58', '35', '83', '212', '150', '73', '25', '15', '64', '133', '210', '108', '79', '193', '15', '126', '165', '15', '208', '98', '3', '200', '126', '3', '40', '165', '98', '69', '15', '165', '133', '96', '173', '203', '133', '238', '248', '15', '7', '38', '207', '108', '165', '199', '18', '162', '27', '3', '72', '65', '250', '209', '166', '64', '209', '237', '212', '64', '225', '120', '108', '203', '135', '248', '69', '251', '135', '204', '160', '160', '135', '148', '15', '15', '23', '207', '274', '29', '28', '194', '150', '40', '104', '208', '125', '70', '18', '3', '19', '102', '104', '23', '29', '207', '133', '204', '111', '2', '125', '126', '7', '194', '209', '203', '83', '126', '126', '258', '119', '162', '241', '133', '18', '208', '197', '209', '15', '78', '80', '110', '237', '104', '30', '208', '103', '15', '200', '15', '238', '254', '108', '173', '40', '69', '175', '17', '155', '136', '26', '3', '135', '13', '241', '161', '166', '170', '162', '133', '53', '211', '204', '15', '161', '238', '110', '18', '191', '18', '50', '71', '30', '17', '208', '248', '203', '203', '196', '210', '258', '204', '3', '18', '196', '125', '173', '199', '249', '18', '104', '165', '241', '2', '193', '238', '78', '15', '166', '15', '126', '3', '237', '161', '52', '207', '3', '3', '50', '56', '22', '56', '69', '93', '135', '207', '69', '99', '3', '133', '68', '133', '161', '108', '126', '250', '60', '155', '98', '18', '16', '78', '78', '240', '235', '176', '22', '199', '160', '235', '274', '209', '64', '173', '137', '128', '25', '18', '160', '203', '70', '165', '193', '110', '207', '64', '85', '252', '78', '51', '2', '99', '19', '22', '198', '204', '166', '166', '151', '56', '133', '210', '26', '149', '109', '30', '209', '209', '51', '3', '193', '119', '208', '93', '165', '248', '55', '203', '204', '58', '18', '199', '211', '83', '32', '135', '212', '156', '19', '235', '173', '191', '209', '203', '146', '208', '58', '158', '102', '199', '200', '200', '173', '70', '52', '107', '211', '213', '237', '196', '205', '104', '135', '258', '255', '96', '19', '124', '128', '57', '199', '98', '166', '102', '40', '15', '238', '3', '148', '225', '126', '135', '114', '193', '204', '15', '209', '209', '193', '259', '64', '161', '26', '274', '126', '18', '196', '0', '60', '207', '17', '71', '218', '239', '248', '38', '154', '61', '128', '199', '196', '173', '25', '46', '98', '258', '108', '3', '108', '196', '93', '93', '99', '172', '69', '241', '196', '30', '135', '278', '109', '18', '5', '209', '57', '159', '70', '193', '163', '249', '196', '196', '72', '103', '133', '17', '204', '199', '166', '202', '29', '57', '193', '150', '98', '119', '25', '205', '18', '173', '166', '209', '57', '25', '58', '258', '26', '15', '15', '101', '26', '74', '1', '4', '74', '263']\n",
            "     filenum                                               text  label\n",
            "0          1  [ Halliburton Oil Well Cementing Co. v. Walker...    209\n",
            "1          2  Rehearing Denied Dec. See . Mr.Claude T. Barne...     63\n",
            "2          3  Rehearing Denied Dec. See . Appeal from the Di...    216\n",
            "3          4  Mr. Walter J. Cummings, Jr., of Washington, D....    108\n",
            "4          5  Mr.A. Devitt Vaneck, of Washington, D.C., for ...    196\n",
            "...      ...                                                ...    ...\n",
            "8414    8415  Opinion reported: Ante, p. DECREE It is ordere...     74\n",
            "8415    8416  In this dispute between Utah and the United St...      1\n",
            "8416    8417  The United States, to the exclusion of defenda...      4\n",
            "8417    8418  Louisiana's exception to the portion of the Sp...     74\n",
            "8418    8419  To resolve a dispute over the ownership of cer...    263\n",
            "\n",
            "[8419 rows x 3 columns]\n"
          ]
        }
      ],
      "source": [
        "fh = open('sc_model_0.txt')\n",
        "filenums=[]\n",
        "records=[]\n",
        "for record in fh:\n",
        "    record=record.split(\"---\")\n",
        "    filenum=record[0]\n",
        "    modelnum=record[1]\n",
        "    text=record[2]\n",
        "    filenums.append(filenum)\n",
        "    records.append(text)\n",
        "    \n",
        "print(len(filenums))\n",
        "print(len(records))\n",
        "\n",
        "summarized_data = pd.DataFrame(list(zip(filenums, records)),columns =['filenum','text'])\n",
        "len_list = [len(ele.split()) for ele in records]\n",
        "res = 0 if len(len_list) == 0 else (float(sum(len_list)) / len(len_list))\n",
        "print(\"Average Length %s\" % res)\n",
        "\n",
        "#temp_file = open(\"labels_sc.txt\", \"r\")\n",
        "temp_file = open(\"labels_sc_279.txt\", \"r\")\n",
        "\n",
        "data = temp_file.read()\n",
        "\n",
        "label_list = data.split(\"\\n\")\n",
        "print(label_list)\n",
        "label_list = label_list[0:-1]\n",
        "print(label_list)\n",
        "label_list = [int(i) for i in label_list]\n",
        "temp_file.close()\n",
        "\n",
        "\n",
        "new_label_list=[]\n",
        "for num in filenums:\n",
        "  new_label_list.append(label_list[int(num)-1])\n",
        "summarized_data['label'] = new_label_list\n",
        "\n",
        "print(summarized_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "kEIh_vBxF52U"
      },
      "outputs": [],
      "source": [
        "def create_model():\n",
        "    inps = Input(shape = (max_len,), dtype='int64')\n",
        "    masks= Input(shape = (max_len,), dtype='int64')\n",
        "    dbert_layer = dbert_model(inps, attention_mask=masks)[0][:,0,:]\n",
        "    dense_0 = Dense(512,activation='relu',kernel_regularizer=regularizers.l2(0.01))(dbert_layer)\n",
        "    dropout_0= Dropout(0.5)(dense_0)\n",
        "    pred = Dense(279, activation='softmax',kernel_regularizer=regularizers.l2(0.01))(dropout_0)\n",
        "    model = tf.keras.Model(inputs=[inps,masks], outputs=pred)\n",
        "    print(model.summary())\n",
        "    return model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILY_HaQbtgIj",
        "outputId": "94bb49f0-cbad-4b79-81eb-38f66ec10d6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_1[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_roberta_model[0][0]']       \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 279)          143127      ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[209  63 216 ...   4  74 263]\n",
            "[   1    2    3 ... 8417 8418 8419]\n"
          ]
        }
      ],
      "source": [
        "gc.collect()\n",
        "tf.keras.backend.clear_session()\n",
        "dbert_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "dbert_model = TFRobertaModel.from_pretrained('roberta-base')\n",
        "max_len=512\n",
        "sentences=summarized_data['text']\n",
        "labels=summarized_data['label']\n",
        "filenumbers=summarized_data['filenum']\n",
        "len(sentences),len(labels),len(filenumbers)\n",
        "\n",
        "model_0=create_model()\n",
        "input_ids=[]\n",
        "attention_masks=[]\n",
        "\n",
        "for sent in sentences:\n",
        "  dbert_inps=dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n",
        "  input_ids.append(dbert_inps['input_ids'])\n",
        "  attention_masks.append(dbert_inps['attention_mask'])\n",
        "input_ids=np.asarray(input_ids)\n",
        "\n",
        "attention_masks=np.array(attention_masks)\n",
        "labels=np.array(labels)\n",
        "filenumbers=[int(i) for i in filenumbers]\n",
        "filenumbers=np.array(filenumbers)\n",
        "print(labels)\n",
        "print(filenumbers)\n",
        "\n",
        "train_inp,val_inp,train_label,val_label,train_mask,val_mask,train_fnum,val_fnum=train_test_split(input_ids,labels,attention_masks,filenumbers,test_size=0.1,random_state=42)\n",
        "\n",
        "new_train_inp_0=[]\n",
        "new_val_inp_0=[]\n",
        "new_train_label_0=[]\n",
        "new_val_label_0=[]\n",
        "new_train_mask_0=[]\n",
        "new_train_fnum_0=[]\n",
        "new_val_fnum_0=[]\n",
        "new_val_mask_0=[]\n",
        "for i in range(len(train_fnum)):\n",
        "  if train_fnum[i] in filenumbers:\n",
        "    new_train_inp_0.append(train_inp[i])\n",
        "    #new_val_inp.append(val_inp[i])\n",
        "    new_train_label_0.append(train_label[i])\n",
        "    #new_val_label.append(val_label[i])\n",
        "    new_train_mask_0.append(train_mask[i])\n",
        "    #new_val_mask.append(val_mask[i])\n",
        "    new_train_fnum_0.append(train_fnum[i])\n",
        "\n",
        "for i in range(len(val_fnum)):\n",
        "  if val_fnum[i] in filenumbers:\n",
        "    #new_train_inp.append(train_inp[i])\n",
        "    new_val_inp_0.append(val_inp[i])\n",
        "    #new_train_label.append(train_label[i])\n",
        "    new_val_label_0.append(val_label[i])\n",
        "    #new_train_mask.append(train_mask[i])\n",
        "    new_val_mask_0.append(val_mask[i])\n",
        "    new_val_fnum_0.append(val_fnum[i])\n",
        "\n",
        "new_train_inp_0=np.array(new_train_inp_0)\n",
        "new_val_inp_0=np.array(new_val_inp_0)\n",
        "new_train_label_0=np.array(new_train_label_0)\n",
        "new_val_label_0=np.array(new_val_label_0)\n",
        "new_train_mask_0=np.array(new_train_mask_0)\n",
        "new_train_fnum_0=np.array(new_train_fnum_0)\n",
        "new_val_fnum_0=np.array(new_val_fnum_0)\n",
        "new_val_mask_0=np.array(new_val_mask_0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "a2RiZXaKGABO",
        "outputId": "5ff1e1ec-46db-4759-964d-40f75078afa4"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    376\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Python 2.7, use buffering of HTTP responses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m                 \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Python 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: getresponse() got an unexpected keyword argument 'buffering'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-817216a1bbcd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m   \u001b[0mdbert_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRobertaTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'roberta-base'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m   \u001b[0mdbert_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTFRobertaModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'roberta-base'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0mmax_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1789\u001b[0m             \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1790\u001b[0m             \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1791\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1792\u001b[0m         )\n\u001b[1;32m   1793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, use_auth_token, cache_dir, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1843\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1844\u001b[0m                     \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1845\u001b[0;31m                     \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1846\u001b[0m                 )\n\u001b[1;32m   1847\u001b[0m                 \u001b[0mconfig_tokenizer_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer_class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/auto/configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    678\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"name_or_path\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m         \u001b[0mtrust_remote_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"trust_remote_code\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"auto_map\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"AutoConfig\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"auto_map\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    551\u001b[0m         \u001b[0moriginal_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m         \u001b[0;31m# Get config dict associated with the base config file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m         \u001b[0;31m# That config file may point us toward another config file to use.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36m_get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    607\u001b[0m                 \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m                 \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 609\u001b[0;31m                 \u001b[0muser_agent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muser_agent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    610\u001b[0m             )\n\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m    288\u001b[0m             \u001b[0muser_agent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muser_agent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m             \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m         )\n\u001b[1;32m    292\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_or_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m    483\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 485\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0metag_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    486\u001b[0m             \u001b[0m_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0metag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"X-Linked-Etag\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ETag\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mhead\u001b[0;34m(url, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'head'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    528\u001b[0m         }\n\u001b[1;32m    529\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    447\u001b[0m                     \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m                 )\n\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    598\u001b[0m                                                   \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_obj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m                                                   chunked=chunked)\n\u001b[0m\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0;31m# If we're going to release the connection in ``finally:``, then\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Python 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m                     \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m                     \u001b[0;31m# Remove the TypeError from the exception chain in Python 3;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1371\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1373\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1374\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1375\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1069\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1071\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    927\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 929\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    930\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "total_accuracy=0\n",
        "total_weighted_f1=0\n",
        "total_micro_f1=0\n",
        "total_weighted_precision=0\n",
        "total_micro_precision=0\n",
        "total_weighted_recall=0\n",
        "total_micro_recall=0\n",
        "\n",
        "for f in range(5):\n",
        "  gc.collect()\n",
        "  tf.keras.backend.clear_session()\n",
        "  dbert_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "  dbert_model = TFRobertaModel.from_pretrained('roberta-base')\n",
        "  max_len=512\n",
        "  sentences=summarized_data['text']\n",
        "  labels=summarized_data['label']\n",
        "  filenumbers=summarized_data['filenum']\n",
        "  len(sentences),len(labels),len(filenumbers)\n",
        "\n",
        "  model_0=create_model()\n",
        "  input_ids=[]\n",
        "  attention_masks=[]\n",
        "\n",
        "  for sent in sentences:\n",
        "    dbert_inps=dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n",
        "    input_ids.append(dbert_inps['input_ids'])\n",
        "    attention_masks.append(dbert_inps['attention_mask'])\n",
        "  input_ids=np.asarray(input_ids)\n",
        "\n",
        "  attention_masks=np.array(attention_masks)\n",
        "  labels=np.array(labels)\n",
        "  filenumbers=[int(i) for i in filenumbers]\n",
        "  filenumbers=np.array(filenumbers)\n",
        "  print(labels)\n",
        "  print(filenumbers)\n",
        "\n",
        "  train_inp,val_inp,train_label,val_label,train_mask,val_mask,train_fnum,val_fnum=train_test_split(input_ids,labels,attention_masks,filenumbers,test_size=0.1,random_state=42)\n",
        "\n",
        "  new_train_inp_0=[]\n",
        "  new_val_inp_0=[]\n",
        "  new_train_label_0=[]\n",
        "  new_val_label_0=[]\n",
        "  new_train_mask_0=[]\n",
        "  new_train_fnum_0=[]\n",
        "  new_val_fnum_0=[]\n",
        "  new_val_mask_0=[]\n",
        "  for i in range(len(train_fnum)):\n",
        "    if train_fnum[i] in filenumbers:\n",
        "      new_train_inp_0.append(train_inp[i])\n",
        "      #new_val_inp.append(val_inp[i])\n",
        "      new_train_label_0.append(train_label[i])\n",
        "      #new_val_label.append(val_label[i])\n",
        "      new_train_mask_0.append(train_mask[i])\n",
        "      #new_val_mask.append(val_mask[i])\n",
        "      new_train_fnum_0.append(train_fnum[i])\n",
        "\n",
        "  for i in range(len(val_fnum)):\n",
        "    if val_fnum[i] in filenumbers:\n",
        "      #new_train_inp.append(train_inp[i])\n",
        "      new_val_inp_0.append(val_inp[i])\n",
        "      #new_train_label.append(train_label[i])\n",
        "      new_val_label_0.append(val_label[i])\n",
        "      #new_train_mask.append(train_mask[i])\n",
        "      new_val_mask_0.append(val_mask[i])\n",
        "      new_val_fnum_0.append(val_fnum[i])\n",
        "\n",
        "  # print(new_train_label)\n",
        "  # print(len(new_train_label))\n",
        "  # print(new_val_label)\n",
        "  # print(len(new_val_label))\n",
        "  new_train_inp_0=np.array(new_train_inp_0)\n",
        "  new_val_inp_0=np.array(new_val_inp_0)\n",
        "  new_train_label_0=np.array(new_train_label_0)\n",
        "  new_val_label_0=np.array(new_val_label_0)\n",
        "  new_train_mask_0=np.array(new_train_mask_0)\n",
        "  new_train_fnum_0=np.array(new_train_fnum_0)\n",
        "  new_val_fnum_0=np.array(new_val_fnum_0)\n",
        "  new_val_mask_0=np.array(new_val_mask_0)\n",
        "  \n",
        "  log_dir='dbert_model'\n",
        "\n",
        "  model_save_path='./drive/MyDrive/Ensemble/roberta-ensemble-512-model_0-'+str(f)+'-279labels.h5'\n",
        "\n",
        "  loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "  accuracy = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
        "  callbacks= [tf.keras.callbacks.ModelCheckpoint(filepath=model_save_path,monitor='val_accuracy',mode='max',save_best_only=True,save_weights_only=True),keras.callbacks.TensorBoard(log_dir=log_dir)]\n",
        "  model_0.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])\n",
        "  gpu_info = !nvidia-smi\n",
        "  gpu_info = '\\n'.join(gpu_info)\n",
        "  if gpu_info.find('failed') >= 0:\n",
        "    print('Not connected to a GPU')\n",
        "  else:\n",
        "    print(gpu_info)\n",
        "  \n",
        "  history=model_0.fit([new_train_inp_0,new_train_mask_0],new_train_label_0,batch_size=8,epochs=5,validation_data=([new_val_inp_0,new_val_mask_0],new_val_label_0),callbacks=callbacks)\n",
        "\n",
        "  pred_labels=[]\n",
        "\n",
        "  model_saved_0= create_model()\n",
        "  model_saved_0.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])\n",
        "  model_saved_0.load_weights('./drive/MyDrive/Ensemble/roberta-ensemble-512-model_0-'+str(f)+'-279labels.h5')\n",
        "\n",
        "  for i in range(0,len(val_inp)):\n",
        "    pred=model_saved_0.predict([val_inp[i].reshape(1,512),val_mask[i].reshape(1,512)])\n",
        "    pred_label = pred.argmax(axis=1)\n",
        "    pred_labels.append(pred_label)\n",
        "  accuracy=accuracy_score(val_label, pred_labels)\n",
        "  print(\"Accuracy: \"+str(accuracy))\n",
        "  total_accuracy=total_accuracy+accuracy\n",
        "  \n",
        "  weighted_f1=f1_score(val_label,pred_labels, average='weighted')\n",
        "  print(\"Weighted F1: \"+ str(weighted_f1))\n",
        "  total_weighted_f1=total_weighted_f1+weighted_f1\n",
        "  micro_f1=f1_score(val_label,pred_labels, average='micro')\n",
        "  print(\"Micro F1: \"+ str(micro_f1))\n",
        "  total_micro_f1=total_micro_f1+micro_f1\n",
        "\n",
        "  weighted_precision=precision_score(val_label, pred_labels, average='weighted')\n",
        "  print(\"Weighted Precision: \" + str(weighted_precision))\n",
        "  total_weighted_precision=total_weighted_precision+weighted_precision\n",
        "  micro_precision=precision_score(val_label, pred_labels, average='micro')\n",
        "  print(\"Micro Precision: \" + str(micro_precision))\n",
        "  total_micro_precision=total_micro_precision+micro_precision\n",
        "\n",
        "  weighted_recall=recall_score(val_label, pred_labels, average='weighted')\n",
        "  print(\"Weighted Recall: \" + str(weighted_recall))\n",
        "  total_weighted_recall=total_weighted_recall+weighted_recall\n",
        "  micro_recall=recall_score(val_label, pred_labels, average='micro')\n",
        "  print(\"Micro Recall: \" + str(micro_recall))\n",
        "  total_micro_recall=total_micro_recall+micro_recall\n",
        "\n",
        "\n",
        "print(\"Average Accuracy: \"+str(total_accuracy/5))\n",
        "print(\"Average Weighted F1: \"+str(total_weighted_f1/5))\n",
        "print(\"Average Micro F1: \"+str(total_micro_f1/5))\n",
        "print(\"Average Weighted Precision: \"+str(total_weighted_precision/5))\n",
        "print(\"Average Micro Precision: \"+str(total_micro_precision/5))\n",
        "print(\"Average Weighted Recall: \"+str(total_weighted_recall/5))\n",
        "print(\"Average Micro Recall: \"+str(total_micro_recall/5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NuZhHmEL6We",
        "outputId": "2e7dbb01-bafe-4c0f-f2c4-89d19b9d3c6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7332\n",
            "7332\n",
            "Average Length 510.89102564102564\n",
            "['209', '63', '216', '108', '196', '200', '203', '246', '188', '193', '25', '106', '56', '53', '223', '161', '200', '208', '78', '147', '147', '179', '147', '80', '56', '209', '209', '200', '214', '202', '203', '194', '26', '237', '165', '258', '238', '238', '76', '193', '196', '213', '161', '213', '209', '194', '196', '25', '151', '260', '135', '173', '30', '108', '272', '259', '176', '213', '213', '21', '5', '171', '200', '195', '240', '194', '176', '240', '240', '80', '173', '98', '214', '21', '76', '76', '73', '173', '173', '78', '61', '258', '191', '3', '78', '173', '193', '118', '200', '264', '260', '49', '211', '258', '18', '173', '78', '3', '3', '1', '213', '135', '182', '239', '208', '278', '50', '194', '200', '124', '189', '117', '264', '244', '76', '193', '221', '78', '78', '173', '197', '78', '161', '231', '135', '22', '78', '25', '25', '197', '200', '147', '203', '191', '196', '193', '263', '32', '197', '191', '4', '56', '196', '104', '191', '196', '173', '147', '194', '96', '161', '161', '238', '200', '3', '78', '78', '53', '41', '237', '53', '18', '10', '194', '97', '96', '208', '25', '10', '117', '104', '54', '96', '194', '21', '245', '238', '202', '209', '161', '97', '258', '240', '213', '136', '155', '152', '21', '191', '191', '118', '68', '258', '189', '197', '208', '61', '78', '135', '202', '96', '155', '108', '272', '80', '25', '155', '35', '213', '191', '194', '96', '96', '191', '264', '191', '191', '191', '2', '258', '191', '173', '259', '15', '47', '258', '156', '118', '160', '115', '105', '26', '173', '192', '76', '76', '135', '240', '118', '161', '200', '230', '18', '200', '25', '10', '161', '244', '24', '24', '200', '205', '104', '15', '193', '156', '95', '209', '240', '191', '194', '128', '208', '208', '218', '173', '10', '25', '18', '107', '46', '32', '197', '172', '209', '213', '79', '121', '78', '78', '193', '2', '78', '194', '149', '136', '80', '3', '21', '200', '78', '21', '197', '194', '118', '202', '161', '180', '209', '173', '2', '193', '6', '260', '197', '78', '78', '42', '191', '213', '135', '193', '196', '202', '222', '217', '42', '259', '213', '76', '46', '30', '15', '260', '202', '18', '259', '104', '135', '76', '196', '240', '240', '240', '108', '24', '200', '197', '262', '176', '15', '156', '264', '156', '200', '191', '161', '196', '78', '238', '161', '221', '118', '14', '14', '14', '200', '278', '238', '200', '196', '78', '173', '25', '197', '197', '196', '161', '18', '10', '10', '10', '18', '66', '203', '19', '246', '196', '244', '240', '161', '200', '78', '240', '209', '208', '118', '215', '194', '193', '168', '229', '191', '135', '172', '196', '197', '161', '78', '196', '197', '258', '78', '78', '238', '221', '121', '264', '104', '146', '50', '78', '217', '11', '238', '238', '238', '3', '3', '154', '215', '79', '18', '158', '104', '18', '13', '197', '117', '197', '161', '213', '242', '32', '238', '230', '2', '2', '32', '161', '251', '96', '156', '242', '22', '22', '140', '3', '149', '135', '191', '173', '135', '200', '238', '238', '78', '156', '209', '78', '97', '97', '160', '25', '258', '240', '161', '15', '161', '96', '209', '80', '132', '140', '4', '1', '240', '196', '274', '272', '62', '80', '196', '196', '191', '79', '156', '15', '196', '209', '22', '251', '202', '202', '121', '73', '191', '238', '191', '150', '150', '135', '46', '203', '202', '238', '22', '3', '258', '239', '238', '238', '165', '238', '238', '194', '200', '200', '197', '200', '196', '191', '194', '76', '78', '208', '200', '71', '208', '240', '76', '117', '120', '108', '96', '144', '30', '133', '133', '202', '117', '161', '251', '29', '104', '242', '197', '118', '216', '262', '236', '141', '11', '235', '235', '133', '191', '238', '202', '80', '80', '22', '140', '191', '197', '202', '133', '184', '184', '184', '184', '143', '274', '50', '237', '196', '274', '265', '10', '18', '10', '121', '238', '245', '228', '76', '25', '191', '197', '268', '10', '240', '195', '196', '195', '15', '48', '200', '183', '70', '59', '197', '196', '235', '161', '161', '215', '104', '97', '191', '194', '191', '200', '200', '197', '76', '200', '93', '256', '160', '238', '146', '238', '239', '15', '140', '258', '140', '21', '79', '202', '258', '79', '62', '205', '161', '104', '10', '161', '196', '137', '152', '191', '117', '213', '242', '176', '218', '195', '191', '162', '191', '135', '241', '213', '213', '84', '79', '68', '18', '98', '197', '155', '41', '246', '269', '78', '209', '238', '80', '238', '62', '237', '273', '273', '230', '150', '121', '260', '244', '161', '202', '78', '109', '155', '264', '145', '50', '274', '173', '210', '258', '238', '211', '156', '215', '195', '177', '244', '3', '141', '196', '191', '194', '29', '30', '10', '16', '200', '200', '156', '53', '3', '62', '165', '173', '173', '22', '205', '274', '150', '183', '116', '188', '188', '98', '161', '204', '80', '172', '156', '104', '229', '259', '78', '169', '3', '200', '118', '161', '80', '260', '240', '150', '197', '196', '62', '93', '6', '272', '272', '197', '160', '78', '96', '70', '195', '191', '191', '80', '3', '3', '240', '147', '196', '191', '238', '96', '203', '3', '15', '10', '58', '55', '96', '41', '15', '26', '50', '238', '194', '79', '240', '161', '191', '195', '263', '50', '240', '147', '235', '195', '203', '59', '178', '176', '2', '15', '203', '191', '18', '278', '196', '14', '153', '58', '175', '18', '217', '217', '80', '191', '200', '18', '200', '173', '24', '171', '171', '210', '124', '34', '200', '218', '104', '4', '223', '193', '194', '200', '200', '13', '62', '194', '197', '80', '197', '146', '146', '96', '97', '97', '2', '196', '191', '191', '104', '238', '10', '200', '239', '135', '104', '213', '2', '221', '169', '25', '21', '194', '161', '21', '278', '3', '195', '46', '176', '25', '117', '191', '62', '62', '62', '78', '50', '264', '79', '197', '80', '80', '80', '191', '191', '78', '161', '194', '195', '197', '203', '259', '278', '49', '147', '147', '147', '147', '22', '78', '78', '258', '2', '202', '221', '55', '258', '104', '104', '84', '240', '221', '274', '104', '22', '241', '63', '197', '197', '196', '159', '200', '22', '22', '22', '78', '78', '173', '104', '97', '104', '191', '146', '223', '268', '96', '14', '173', '204', '241', '97', '15', '118', '78', '78', '196', '194', '258', '96', '96', '259', '75', '10', '197', '30', '2', '238', '121', '50', '161', '173', '173', '186', '185', '186', '78', '194', '221', '218', '35', '33', '13', '34', '78', '21', '215', '97', '60', '24', '78', '193', '173', '235', '78', '200', '258', '140', '259', '194', '197', '195', '200', '145', '76', '108', '130', '238', '215', '213', '2', '217', '121', '178', '140', '62', '156', '176', '196', '196', '41', '223', '147', '172', '79', '200', '2', '244', '244', '191', '213', '104', '191', '237', '237', '33', '117', '195', '195', '264', '195', '140', '210', '41', '197', '14', '257', '196', '238', '238', '79', '161', '156', '196', '238', '238', '213', '104', '3', '10', '118', '204', '235', '33', '259', '274', '259', '176', '78', '78', '196', '34', '52', '25', '30', '194', '153', '21', '70', '191', '18', '191', '191', '172', '194', '194', '194', '194', '148', '104', '125', '2', '2', '2', '185', '194', '29', '104', '78', '189', '272', '204', '183', '213', '78', '104', '197', '240', '97', '168', '168', '182', '238', '197', '18', '194', '229', '221', '62', '78', '53', '104', '214', '189', '189', '189', '213', '117', '117', '191', '29', '104', '104', '276', '276', '22', '195', '195', '155', '139', '139', '59', '168', '135', '140', '79', '146', '200', '245', '70', '160', '153', '50', '202', '154', '263', '130', '194', '117', '194', '194', '191', '33', '29', '29', '238', '239', '194', '21', '274', '215', '18', '18', '140', '2', '215', '25', '46', '137', '238', '30', '160', '156', '46', '196', '263', '242', '104', '0', '0', '73', '213', '135', '22', '191', '191', '241', '202', '221', '15', '15', '191', '238', '213', '194', '197', '179', '238', '200', '200', '200', '5', '238', '194', '55', '117', '166', '49', '244', '239', '191', '78', '78', '78', '194', '200', '117', '210', '22', '121', '121', '121', '21', '237', '194', '121', '78', '104', '194', '79', '191', '251', '195', '197', '274', '176', '21', '43', '43', '10', '61', '215', '215', '30', '214', '194', '10', '30', '139', '96', '226', '226', '265', '2', '2', '121', '121', '41', '237', '104', '238', '238', '161', '78', '78', '78', '11', '184', '140', '140', '161', '104', '104', '237', '130', '195', '160', '118', '161', '18', '11', '241', '184', '196', '178', '22', '30', '146', '146', '10', '10', '141', '141', '135', '18', '18', '10', '26', '208', '97', '141', '18', '18', '97', '194', '194', '79', '213', '205', '205', '205', '75', '241', '50', '226', '238', '97', '222', '215', '215', '139', '238', '51', '238', '96', '173', '109', '200', '117', '192', '2', '195', '10', '3', '18', '49', '238', '238', '191', '195', '52', '197', '200', '200', '79', '191', '200', '200', '30', '195', '197', '195', '25', '260', '73', '30', '193', '21', '200', '242', '203', '120', '78', '260', '241', '0', '30', '202', '195', '30', '191', '78', '194', '194', '2', '205', '191', '196', '173', '197', '30', '196', '172', '139', '2', '18', '191', '230', '56', '194', '215', '195', '2', '71', '13', '146', '264', '50', '191', '235', '93', '191', '139', '139', '227', '235', '186', '235', '24', '18', '261', '128', '33', '71', '25', '176', '39', '10', '195', '202', '29', '29', '221', '29', '197', '139', '78', '227', '146', '137', '195', '205', '205', '195', '135', '62', '153', '146', '29', '75', '0', '194', '194', '180', '213', '78', '18', '194', '239', '196', '239', '197', '194', '153', '192', '221', '10', '46', '50', '241', '117', '117', '117', '195', '173', '78', '195', '194', '239', '2', '12', '173', '197', '30', '269', '197', '176', '135', '30', '146', '191', '93', '93', '135', '260', '218', '156', '155', '200', '241', '125', '18', '18', '183', '203', '239', '259', '173', '213', '180', '180', '210', '120', '104', '194', '156', '258', '172', '9', '0', '241', '192', '143', '189', '204', '50', '63', '197', '243', '241', '93', '125', '2', '21', '260', '4', '4', '3', '191', '150', '128', '78', '238', '245', '8', '80', '238', '79', '79', '240', '58', '104', '155', '80', '80', '188', '191', '235', '170', '170', '170', '146', '258', '195', '276', '25', '78', '215', '240', '161', '53', '147', '78', '78', '78', '78', '221', '221', '96', '18', '19', '130', '18', '18', '18', '80', '80', '194', '79', '93', '121', '78', '139', '139', '78', '197', '121', '194', '200', '80', '101', '22', '12', '135', '97', '205', '181', '60', '193', '15', '140', '172', '191', '241', '222', '135', '29', '25', '179', '191', '191', '259', '194', '133', '240', '121', '121', '241', '195', '29', '191', '209', '18', '139', '139', '50', '18', '12', '18', '133', '200', '25', '10', '70', '25', '242', '18', '161', '49', '172', '172', '172', '172', '128', '96', '195', '200', '78', '10', '239', '22', '173', '142', '142', '22', '194', '192', '3', '200', '241', '96', '241', '62', '139', '213', '202', '135', '79', '120', '192', '78', '118', '241', '46', '151', '150', '151', '150', '118', '221', '183', '237', '196', '140', '140', '39', '179', '213', '140', '41', '73', '78', '217', '258', '30', '118', '140', '219', '50', '10', '139', '150', '251', '24', '10', '18', '78', '18', '172', '135', '146', '71', '196', '202', '197', '25', '106', '160', '215', '15', '6', '101', '165', '140', '145', '80', '3', '238', '239', '78', '268', '0', '109', '191', '215', '238', '238', '238', '203', '50', '146', '15', '25', '156', '191', '16', '2', '195', '189', '96', '193', '109', '204', '161', '185', '210', '50', '235', '30', '242', '193', '102', '194', '121', '177', '241', '102', '132', '240', '71', '192', '79', '25', '197', '39', '161', '22', '258', '237', '15', '7', '78', '156', '197', '70', '176', '139', '240', '132', '78', '177', '191', '194', '117', '10', '79', '238', '195', '24', '101', '118', '196', '2', '102', '21', '180', '185', '170', '79', '139', '135', '191', '151', '200', '191', '154', '262', '200', '237', '56', '27', '191', '180', '258', '139', '139', '192', '30', '0', '242', '239', '242', '25', '191', '242', '240', '3', '56', '215', '171', '192', '193', '222', '238', '237', '215', '272', '2', '156', '237', '151', '189', '80', '15', '260', '260', '217', '235', '22', '228', '135', '18', '191', '157', '78', '2', '240', '213', '179', '2', '191', '79', '79', '153', '3', '194', '78', '121', '71', '121', '149', '257', '194', '191', '188', '15', '25', '125', '102', '15', '130', '130', '177', '10', '135', '265', '240', '149', '196', '200', '204', '204', '213', '177', '197', '194', '13', '202', '30', '213', '125', '193', '125', '125', '125', '125', '125', '15', '180', '78', '1', '25', '96', '200', '29', '156', '172', '202', '22', '78', '197', '188', '241', '101', '101', '101', '221', '50', '191', '15', '15', '0', '101', '101', '168', '56', '197', '130', '62', '204', '29', '10', '96', '97', '139', '97', '97', '2', '2', '262', '39', '176', '172', '194', '194', '18', '78', '13', '80', '139', '196', '207', '191', '151', '192', '150', '13', '104', '104', '101', '15', '125', '101', '15', '0', '50', '194', '264', '50', '25', '25', '125', '125', '125', '125', '125', '25', '29', '25', '25', '78', '240', '261', '19', '2', '74', '78', '238', '50', '50', '203', '197', '71', '104', '80', '194', '149', '170', '130', '2', '237', '170', '202', '48', '97', '93', '179', '235', '78', '102', '102', '70', '208', '238', '104', '146', '118', '118', '96', '237', '274', '218', '241', '102', '146', '209', '209', '240', '170', '137', '7', '197', '191', '264', '29', '46', '96', '193', '19', '12', '213', '215', '275', '179', '18', '183', '78', '221', '18', '170', '119', '21', '140', '240', '96', '192', '192', '21', '96', '149', '239', '135', '191', '240', '184', '183', '24', '2', '96', '197', '194', '215', '18', '192', '121', '202', '196', '25', '97', '214', '3', '97', '16', '0', '192', '135', '215', '5', '12', '80', '143', '202', '242', '151', '13', '209', '203', '200', '30', '209', '194', '102', '102', '102', '102', '102', '102', '22', '24', '6', '18', '5', '191', '101', '50', '101', '101', '192', '153', '153', '101', '101', '10', '192', '25', '140', '50', '241', '102', '102', '32', '150', '101', '102', '153', '102', '101', '153', '102', '25', '10', '125', '18', '125', '177', '209', '102', '10', '78', '78', '137', '18', '237', '241', '175', '195', '175', '96', '176', '237', '96', '96', '101', '193', '238', '170', '238', '102', '222', '268', '238', '102', '244', '34', '18', '244', '202', '149', '149', '193', '102', '170', '194', '75', '101', '222', '238', '50', '195', '135', '22', '93', '18', '102', '102', '93', '93', '191', '147', '244', '96', '2', '241', '241', '93', '189', '189', '177', '35', '137', '197', '191', '40', '40', '195', '176', '35', '200', '200', '237', '135', '140', '140', '93', '156', '200', '78', '191', '22', '81', '238', '101', '213', '101', '78', '109', '19', '140', '196', '78', '78', '76', '218', '14', '265', '101', '241', '155', '4', '50', '78', '78', '238', '135', '191', '265', '81', '238', '238', '102', '102', '102', '140', '162', '32', '17', '17', '171', '171', '191', '101', '102', '192', '101', '17', '171', '135', '102', '242', '195', '237', '21', '18', '238', '191', '278', '140', '149', '97', '260', '155', '276', '192', '196', '21', '191', '2', '97', '0', '150', '221', '213', '209', '238', '80', '260', '194', '104', '96', '193', '14', '240', '193', '117', '117', '155', '17', '264', '260', '209', '209', '2', '137', '71', '72', '101', '205', '173', '191', '118', '24', '41', '197', '78', '94', '237', '156', '193', '153', '154', '153', '209', '110', '78', '78', '213', '238', '78', '191', '93', '79', '177', '267', '133', '133', '46', '139', '40', '143', '74', '241', '202', '50', '3', '102', '78', '149', '191', '156', '18', '93', '195', '192', '137', '240', '135', '204', '189', '22', '56', '154', '25', '192', '130', '277', '191', '80', '39', '21', '21', '238', '238', '23', '192', '191', '192', '94', '94', '80', '139', '17', '10', '22', '227', '227', '140', '102', '21', '17', '227', '227', '23', '243', '264', '18', '274', '149', '238', '30', '221', '22', '238', '193', '135', '205', '195', '238', '238', '197', '130', '195', '18', '104', '102', '14', '59', '104', '18', '18', '258', '34', '241', '138', '188', '176', '102', '1', '140', '140', '22', '22', '278', '10', '96', '35', '145', '120', '33', '33', '22', '33', '22', '35', '22', '19', '33', '102', '192', '192', '175', '22', '72', '191', '222', '125', '22', '79', '18', '194', '18', '250', '192', '192', '96', '48', '274', '273', '132', '133', '192', '40', '171', '171', '246', '191', '10', '166', '197', '195', '125', '125', '200', '153', '192', '149', '110', '139', '194', '48', '102', '102', '104', '203', '238', '238', '238', '78', '18', '238', '121', '18', '238', '96', '238', '238', '13', '191', '18', '18', '240', '96', '40', '188', '18', '137', '175', '28', '28', '28', '149', '191', '191', '207', '239', '239', '153', '153', '153', '153', '153', '153', '154', '153', '154', '153', '153', '153', '18', '0', '27', '18', '25', '96', '96', '137', '33', '10', '130', '153', '153', '154', '180', '137', '44', '241', '153', '268', '25', '161', '25', '39', '242', '4', '238', '195', '102', '135', '13', '229', '140', '1', '79', '192', '229', '191', '118', '21', '260', '203', '18', '177', '191', '10', '238', '10', '133', '14', '175', '175', '192', '18', '153', '153', '254', '140', '149', '22', '22', '22', '237', '40', '135', '191', '192', '192', '62', '205', '19', '238', '176', '214', '200', '96', '15', '153', '191', '108', '29', '166', '28', '193', '197', '96', '195', '108', '102', '241', '195', '10', '33', '166', '197', '168', '2', '96', '26', '33', '204', '258', '149', '153', '153', '40', '137', '241', '166', '221', '23', '221', '153', '15', '113', '113', '78', '171', '40', '71', '21', '71', '80', '15', '191', '191', '135', '110', '10', '259', '102', '149', '149', '109', '109', '175', '97', '97', '97', '153', '15', '238', '193', '175', '38', '38', '137', '18', '235', '155', '137', '265', '110', '18', '18', '256', '191', '224', '173', '104', '10', '152', '24', '24', '24', '17', '40', '40', '40', '40', '40', '40', '112', '6', '18', '18', '210', '96', '191', '27', '213', '17', '153', '26', '133', '18', '18', '153', '38', '96', '40', '38', '40', '40', '10', '17', '17', '95', '71', '192', '17', '259', '213', '200', '15', '11', '207', '194', '18', '135', '238', '191', '241', '191', '116', '35', '147', '156', '67', '78', '17', '227', '191', '117', '188', '130', '72', '96', '235', '97', '18', '150', '203', '25', '126', '149', '200', '94', '4', '4', '1', '244', '228', '252', '149', '191', '149', '15', '17', '17', '29', '18', '18', '116', '23', '237', '94', '184', '268', '197', '175', '243', '28', '16', '10', '50', '191', '102', '102', '153', '149', '111', '78', '191', '135', '18', '35', '78', '238', '208', '93', '95', '258', '17', '111', '111', '262', '22', '22', '104', '161', '209', '21', '195', '12', '116', '241', '242', '239', '17', '97', '16', '40', '117', '94', '96', '78', '15', '103', '195', '135', '235', '247', '135', '130', '260', '239', '176', '93', '191', '209', '239', '93', '264', '30', '18', '30', '18', '18', '103', '191', '97', '17', '95', '115', '209', '140', '22', '22', '241', '10', '153', '3', '188', '238', '226', '195', '97', '96', '238', '97', '130', '97', '116', '96', '96', '242', '45', '149', '166', '155', '96', '116', '264', '10', '237', '192', '71', '22', '71', '175', '149', '116', '237', '274', '18', '235', '176', '116', '202', '250', '250', '78', '2', '196', '2', '97', '143', '18', '111', '111', '80', '78', '153', '80', '40', '110', '30', '112', '30', '111', '215', '112', '149', '78', '78', '209', '110', '110', '238', '109', '239', '112', '151', '154', '16', '16', '16', '78', '137', '179', '72', '149', '10', '111', '268', '96', '170', '111', '38', '192', '153', '183', '195', '30', '30', '147', '195', '147', '93', '264', '153', '25', '18', '19', '22', '74', '40', '93', '166', '15', '127', '147', '192', '260', '238', '103', '269', '127', '19', '19', '153', '18', '78', '238', '30', '30', '241', '16', '235', '250', '204', '238', '218', '40', '93', '184', '18', '0', '241', '195', '94', '154', '109', '156', '10', '21', '30', '197', '39', '78', '24', '107', '142', '142', '228', '229', '229', '229', '232', '179', '239', '142', '232', '23', '175', '147', '137', '137', '137', '171', '191', '117', '128', '127', '204', '98', '147', '235', '239', '153', '15', '204', '255', '80', '204', '204', '149', '113', '19', '167', '22', '250', '17', '17', '17', '18', '17', '67', '121', '125', '26', '97', '97', '97', '97', '247', '124', '163', '147', '112', '111', '208', '108', '38', '209', '154', '154', '194', '135', '22', '147', '241', '120', '218', '260', '156', '155', '241', '192', '215', '180', '179', '149', '40', '193', '192', '150', '102', '93', '1', '149', '137', '133', '102', '102', '102', '78', '145', '21', '96', '175', '175', '78', '123', '30', '95', '19', '18', '110', '18', '152', '152', '147', '26', '135', '260', '158', '203', '195', '158', '112', '156', '125', '213', '230', '30', '15', '125', '106', '111', '195', '181', '130', '195', '130', '239', '15', '194', '16', '15', '111', '61', '72', '57', '128', '239', '239', '203', '25', '218', '10', '155', '191', '46', '98', '175', '260', '102', '21', '191', '260', '191', '239', '15', '30', '93', '147', '103', '78', '177', '30', '95', '155', '155', '155', '103', '103', '112', '241', '191', '191', '17', '78', '153', '150', '113', '115', '147', '78', '40', '163', '33', '15', '149', '103', '192', '191', '96', '106', '115', '143', '235', '200', '204', '238', '204', '204', '74', '196', '113', '246', '102', '150', '16', '176', '20', '195', '128', '197', '15', '71', '71', '193', '24', '24', '24', '170', '204', '209', '111', '144', '112', '22', '222', '224', '28', '209', '155', '213', '275', '196', '239', '195', '25', '103', '149', '128', '19', '96', '102', '135', '17', '232', '155', '191', '20', '18', '241', '30', '25', '205', '97', '97', '96', '72', '182', '135', '135', '20', '149', '149', '78', '135', '40', '18', '153', '10', '26', '158', '205', '157', '157', '135', '135', '140', '155', '192', '26', '268', '263', '245', '155', '15', '177', '159', '212', '161', '239', '200', '40', '35', '153', '197', '259', '15', '254', '175', '10', '30', '239', '67', '195', '200', '191', '22', '66', '191', '112', '133', '128', '235', '161', '155', '17', '93', '36', '113', '239', '18', '18', '191', '165', '163', '163', '238', '79', '158', '40', '102', '40', '200', '191', '60', '241', '245', '96', '78', '30', '15', '173', '192', '192', '158', '64', '113', '200', '74', '128', '135', '115', '115', '10', '74', '102', '102', '115', '111', '238', '244', '109', '109', '238', '152', '10', '237', '18', '50', '16', '173', '204', '15', '208', '276', '43', '109', '258', '222', '15', '94', '235', '203', '79', '128', '113', '3', '204', '106', '133', '214', '218', '25', '98', '166', '71', '30', '175', '175', '97', '135', '108', '19', '18', '196', '35', '12', '196', '78', '166', '155', '72', '155', '22', '109', '258', '196', '263', '161', '204', '210', '14', '208', '208', '208', '208', '204', '102', '102', '213', '155', '251', '153', '153', '153', '154', '154', '237', '97', '19', '28', '94', '136', '112', '19', '97', '152', '153', '153', '111', '111', '135', '135', '105', '17', '105', '152', '152', '152', '153', '79', '266', '135', '120', '125', '149', '50', '162', '72', '17', '147', '109', '115', '24', '105', '244', '149', '238', '203', '35', '202', '177', '173', '239', '19', '19', '179', '237', '14', '18', '185', '58', '158', '144', '213', '237', '155', '250', '239', '93', '237', '97', '195', '157', '107', '109', '246', '165', '98', '49', '2', '266', '149', '20', '18', '71', '108', '22', '115', '239', '40', '178', '232', '238', '218', '118', '118', '210', '235', '46', '228', '192', '112', '149', '96', '111', '21', '95', '95', '95', '18', '57', '206', '18', '239', '157', '80', '196', '112', '176', '99', '106', '71', '235', '135', '18', '209', '78', '20', '20', '103', '33', '3', '103', '166', '79', '79', '156', '229', '18', '238', '2', '16', '128', '200', '117', '22', '197', '166', '78', '237', '241', '107', '53', '189', '204', '243', '163', '221', '15', '200', '221', '152', '23', '193', '107', '195', '99', '97', '19', '125', '276', '113', '193', '64', '78', '197', '117', '175', '135', '135', '78', '93', '154', '153', '256', '256', '135', '137', '135', '137', '149', '110', '15', '159', '21', '158', '192', '192', '117', '29', '97', '258', '78', '46', '5', '80', '15', '105', '260', '161', '191', '2', '138', '117', '215', '176', '156', '194', '265', '115', '181', '21', '14', '106', '106', '108', '200', '156', '103', '102', '29', '17', '204', '175', '227', '118', '18', '239', '204', '258', '78', '155', '213', '109', '192', '186', '186', '15', '241', '133', '30', '30', '239', '30', '218', '109', '138', '241', '241', '45', '135', '112', '232', '104', '106', '200', '53', '23', '112', '17', '117', '59', '260', '195', '106', '117', '159', '204', '200', '228', '165', '165', '261', '200', '227', '166', '93', '21', '80', '112', '152', '196', '203', '228', '98', '102', '15', '139', '110', '13', '238', '6', '171', '94', '208', '155', '112', '203', '239', '191', '39', '136', '203', '195', '46', '203', '148', '192', '210', '23', '4', '153', '29', '165', '192', '204', '232', '94', '112', '239', '98', '71', '248', '17', '155', '10', '78', '191', '191', '111', '25', '25', '19', '19', '17', '235', '102', '265', '195', '235', '163', '238', '71', '197', '107', '155', '30', '72', '19', '259', '80', '155', '23', '64', '239', '197', '156', '57', '203', '258', '200', '205', '246', '238', '258', '227', '238', '238', '19', '148', '161', '111', '105', '202', '109', '262', '133', '137', '221', '135', '52', '170', '111', '36', '30', '156', '102', '268', '115', '22', '153', '133', '259', '98', '235', '135', '191', '25', '235', '25', '29', '94', '195', '203', '212', '112', '135', '245', '260', '200', '96', '158', '23', '165', '22', '18', '249', '240', '109', '43', '37', '15', '241', '241', '23', '135', '240', '108', '98', '268', '275', '191', '136', '263', '259', '196', '98', '98', '204', '248', '123', '105', '204', '240', '9', '9', '98', '218', '189', '238', '246', '155', '130', '157', '74', '109', '29', '189', '203', '213', '195', '241', '159', '238', '204', '218', '105', '23', '16', '263', '206', '150', '152', '204', '202', '173', '267', '18', '153', '155', '196', '2', '97', '158', '158', '204', '98', '202', '98', '204', '258', '98', '22', '113', '113', '135', '71', '237', '155', '163', '250', '233', '26', '26', '26', '26', '26', '19', '170', '18', '15', '19', '191', '30', '30', '48', '35', '30', '237', '46', '119', '16', '78', '239', '150', '245', '235', '158', '27', '241', '30', '38', '107', '75', '241', '135', '106', '106', '98', '159', '96', '135', '111', '176', '175', '200', '18', '14', '242', '163', '187', '20', '12', '191', '23', '106', '171', '25', '78', '162', '191', '157', '94', '237', '246', '203', '108', '238', '204', '204', '1', '102', '154', '106', '170', '102', '200', '2', '135', '106', '258', '258', '227', '26', '15', '25', '71', '203', '96', '3', '241', '200', '30', '108', '98', '108', '27', '106', '26', '150', '259', '78', '113', '113', '158', '202', '16', '136', '239', '156', '239', '35', '276', '252', '115', '22', '22', '258', '172', '3', '154', '3', '98', '237', '102', '235', '112', '206', '98', '57', '118', '153', '18', '26', '30', '196', '163', '191', '153', '72', '24', '156', '117', '122', '30', '135', '192', '98', '28', '30', '30', '95', '241', '155', '17', '195', '31', '103', '202', '98', '237', '94', '112', '163', '163', '94', '163', '112', '19', '195', '191', '15', '135', '78', '109', '160', '152', '97', '98', '107', '167', '97', '138', '138', '26', '191', '97', '97', '30', '30', '206', '202', '13', '30', '240', '111', '261', '79', '0', '111', '19', '152', '107', '107', '18', '98', '28', '258', '244', '106', '237', '204', '191', '15', '183', '202', '16', '111', '166', '268', '202', '12', '30', '78', '110', '133', '71', '197', '111', '52', '79', '171', '156', '94', '13', '118', '111', '71', '133', '18', '105', '30', '22', '133', '60', '237', '191', '6', '126', '3', '238', '276', '78', '162', '150', '115', '191', '107', '200', '148', '135', '72', '155', '241', '4', '108', '160', '203', '20', '133', '2', '80', '80', '154', '18', '15', '115', '239', '167', '167', '35', '268', '18', '221', '18', '195', '204', '195', '216', '133', '112', '104', '223', '191', '115', '30', '30', '30', '30', '30', '202', '204', '165', '200', '200', '29', '191', '237', '267', '18', '175', '81', '264', '264', '178', '115', '102', '178', '212', '273', '202', '109', '259', '166', '135', '155', '219', '161', '18', '94', '241', '191', '196', '191', '98', '26', '26', '204', '204', '135', '27', '238', '260', '182', '26', '98', '94', '93', '238', '156', '19', '111', '79', '16', '192', '113', '19', '241', '18', '202', '260', '272', '106', '163', '156', '106', '17', '109', '221', '78', '203', '7', '136', '197', '193', '191', '98', '112', '200', '95', '221', '98', '161', '191', '3', '106', '189', '111', '25', '196', '196', '241', '163', '200', '24', '78', '176', '3', '98', '102', '98', '19', '161', '224', '80', '18', '191', '112', '105', '254', '71', '200', '137', '29', '20', '165', '8', '106', '23', '106', '155', '200', '265', '135', '126', '203', '188', '161', '158', '258', '255', '98', '203', '50', '35', '102', '158', '175', '40', '239', '26', '66', '57', '193', '155', '15', '135', '10', '107', '106', '229', '18', '191', '204', '107', '99', '235', '238', '18', '205', '205', '35', '238', '208', '203', '110', '110', '109', '112', '23', '18', '19', '182', '156', '18', '18', '156', '111', '135', '137', '133', '137', '240', '99', '197', '113', '15', '165', '39', '97', '97', '15', '150', '163', '109', '268', '15', '15', '238', '203', '67', '161', '197', '18', '196', '97', '161', '197', '161', '185', '238', '191', '30', '121', '133', '160', '160', '74', '135', '75', '54', '135', '196', '23', '241', '166', '194', '239', '135', '240', '208', '238', '98', '135', '152', '176', '22', '174', '113', '241', '237', '57', '195', '191', '24', '165', '165', '204', '204', '56', '109', '27', '153', '252', '228', '165', '252', '200', '28', '158', '135', '196', '22', '18', '133', '30', '204', '237', '196', '93', '106', '94', '25', '93', '159', '4', '195', '23', '237', '25', '199', '57', '26', '276', '274', '204', '75', '18', '204', '108', '22', '133', '191', '111', '204', '203', '133', '237', '166', '4', '62', '202', '166', '161', '165', '74', '109', '118', '200', '22', '206', '25', '209', '17', '155', '204', '64', '23', '30', '202', '149', '171', '135', '136', '184', '26', '18', '159', '195', '18', '166', '156', '98', '133', '38', '40', '18', '18', '166', '109', '109', '209', '22', '195', '163', '163', '108', '18', '15', '99', '135', '174', '26', '19', '166', '133', '259', '151', '235', '204', '228', '133', '30', '161', '111', '242', '84', '238', '98', '195', '78', '160', '28', '25', '264', '165', '41', '19', '55', '46', '202', '121', '15', '32', '98', '238', '0', '78', '31', '30', '203', '264', '203', '135', '104', '111', '78', '212', '111', '98', '126', '22', '2', '30', '237', '212', '265', '163', '106', '106', '235', '109', '223', '161', '202', '150', '170', '255', '112', '114', '170', '98', '96', '197', '36', '18', '228', '14', '255', '204', '102', '239', '0', '185', '30', '22', '23', '246', '199', '133', '191', '78', '195', '153', '112', '191', '200', '170', '171', '200', '78', '128', '25', '153', '237', '99', '94', '208', '107', '208', '208', '78', '204', '204', '27', '191', '272', '134', '158', '158', '174', '118', '64', '18', '258', '150', '189', '18', '153', '241', '204', '112', '106', '135', '71', '148', '7', '133', '140', '187', '23', '223', '19', '19', '2', '136', '2', '200', '161', '15', '98', '158', '148', '3', '250', '153', '235', '165', '220', '176', '110', '240', '151', '148', '133', '74', '193', '46', '27', '173', '94', '176', '105', '256', '161', '226', '18', '78', '191', '171', '243', '26', '0', '108', '167', '159', '239', '150', '112', '155', '79', '241', '202', '165', '254', '107', '202', '156', '166', '193', '239', '135', '15', '203', '187', '148', '25', '15', '22', '194', '239', '56', '191', '176', '262', '200', '110', '72', '99', '102', '135', '98', '113', '15', '15', '159', '262', '171', '151', '98', '204', '213', '255', '26', '243', '175', '156', '98', '107', '191', '165', '241', '26', '165', '171', '30', '160', '166', '266', '11', '19', '211', '93', '254', '30', '239', '115', '273', '205', '112', '237', '263', '74', '98', '79', '122', '94', '265', '155', '191', '16', '235', '229', '98', '191', '227', '17', '111', '267', '135', '202', '195', '10', '180', '196', '196', '133', '135', '133', '95', '111', '122', '258', '191', '3', '114', '107', '19', '264', '74', '56', '200', '200', '133', '161', '97', '15', '97', '197', '196', '253', '93', '202', '258', '106', '153', '26', '109', '40', '149', '8', '196', '15', '119', '155', '23', '237', '193', '264', '148', '151', '238', '200', '94', '75', '148', '175', '24', '195', '171', '30', '203', '200', '202', '16', '158', '223', '191', '22', '264', '135', '35', '57', '94', '191', '196', '202', '98', '18', '196', '3', '133', '78', '191', '208', '258', '18', '200', '29', '135', '109', '110', '2', '185', '98', '133', '19', '22', '204', '95', '238', '25', '133', '250', '135', '135', '3', '47', '158', '187', '2', '78', '115', '155', '3', '218', '166', '112', '266', '22', '195', '135', '72', '97', '197', '209', '127', '80', '2', '98', '14', '239', '15', '113', '165', '204', '78', '204', '191', '175', '200', '193', '18', '196', '133', '109', '237', '56', '196', '3', '163', '163', '163', '195', '15', '263', '18', '84', '18', '133', '107', '18', '102', '156', '238', '102', '26', '244', '82', '75', '23', '258', '238', '136', '3', '204', '184', '200', '196', '158', '106', '27', '238', '155', '152', '50', '50', '2', '235', '98', '203', '166', '109', '25', '18', '151', '133', '200', '26', '26', '26', '19', '107', '156', '23', '26', '3', '70', '225', '41', '26', '239', '26', '186', '26', '34', '272', '272', '104', '78', '260', '18', '204', '26', '78', '18', '210', '135', '203', '71', '207', '26', '133', '13', '25', '26', '115', '94', '264', '135', '244', '79', '30', '263', '22', '109', '205', '176', '106', '22', '17', '151', '106', '191', '160', '160', '165', '78', '177', '196', '260', '166', '35', '191', '260', '98', '18', '237', '18', '161', '18', '197', '133', '30', '35', '200', '170', '155', '245', '19', '200', '160', '96', '172', '165', '55', '137', '241', '133', '191', '112', '25', '126', '238', '18', '18', '108', '135', '161', '135', '73', '112', '107', '8', '112', '109', '195', '25', '30', '105', '161', '241', '110', '75', '238', '215', '238', '104', '25', '3', '155', '30', '16', '3', '175', '99', '3', '200', '23', '3', '199', '203', '191', '196', '196', '204', '98', '105', '133', '197', '135', '161', '39', '15', '30', '133', '55', '191', '203', '133', '203', '140', '200', '149', '30', '96', '135', '23', '26', '3', '18', '155', '158', '135', '135', '18', '250', '18', '22', '114', '18', '18', '166', '18', '30', '18', '239', '18', '161', '50', '50', '49', '51', '23', '55', '98', '78', '210', '211', '18', '239', '78', '13', '193', '114', '78', '18', '25', '133', '19', '176', '4', '173', '50', '108', '17', '4', '204', '94', '112', '207', '109', '115', '23', '235', '197', '194', '197', '213', '148', '40', '157', '107', '200', '147', '14', '14', '18', '238', '18', '238', '18', '238', '57', '200', '135', '30', '18', '191', '191', '200', '166', '151', '161', '73', '19', '73', '239', '239', '165', '108', '2', '93', '166', '133', '133', '173', '35', '193', '114', '74', '19', '40', '58', '104', '16', '260', '160', '158', '112', '210', '158', '167', '203', '203', '3', '3', '109', '30', '133', '258', '70', '204', '200', '152', '155', '192', '204', '108', '191', '204', '26', '98', '26', '98', '264', '158', '18', '150', '137', '153', '196', '199', '191', '115', '168', '157', '149', '151', '78', '137', '160', '259', '166', '196', '171', '175', '199', '166', '161', '70', '13', '166', '152', '152', '206', '58', '18', '207', '58', '74', '207', '29', '46', '109', '135', '74', '174', '109', '40', '10', '30', '258', '16', '111', '30', '15', '239', '204', '48', '25', '196', '0', '156', '72', '72', '96', '22', '72', '133', '133', '238', '26', '3', '50', '152', '193', '235', '136', '153', '50', '1', '19', '239', '30', '25', '179', '161', '30', '191', '3', '27', '133', '3', '40', '23', '133', '150', '151', '265', '32', '191', '3', '25', '170', '238', '40', '259', '166', '137', '241', '133', '209', '159', '78', '78', '153', '26', '52', '241', '36', '250', '96', '30', '30', '38', '18', '18', '238', '99', '72', '111', '3', '244', '191', '226', '3', '191', '114', '135', '109', '40', '79', '202', '78', '114', '238', '10', '150', '108', '163', '108', '113', '197', '109', '115', '161', '3', '238', '3', '237', '161', '107', '155', '78', '202', '112', '38', '195', '137', '250', '133', '237', '161', '25', '26', '25', '15', '15', '239', '166', '114', '229', '111', '40', '135', '94', '102', '162', '204', '237', '17', '166', '136', '22', '98', '99', '99', '166', '35', '196', '3', '203', '135', '135', '84', '98', '266', '238', '13', '166', '78', '193', '98', '241', '104', '153', '192', '200', '10', '95', '95', '148', '107', '14', '17', '191', '239', '19', '203', '255', '197', '213', '94', '3', '107', '23', '35', '10', '237', '235', '78', '29', '18', '160', '200', '150', '99', '109', '155', '161', '133', '114', '18', '18', '18', '264', '133', '104', '161', '70', '17', '204', '195', '204', '241', '99', '82', '241', '241', '161', '18', '235', '175', '2', '2', '202', '50', '230', '26', '40', '40', '200', '78', '156', '26', '112', '26', '30', '71', '184', '200', '135', '153', '23', '106', '125', '175', '133', '133', '7', '38', '196', '161', '0', '49', '241', '158', '15', '21', '104', '2', '3', '176', '259', '135', '78', '75', '112', '200', '1', '207', '238', '161', '150', '158', '2', '45', '31', '237', '149', '3', '26', '165', '135', '152', '115', '133', '18', '40', '166', '30', '173', '40', '26', '78', '50', '48', '50', '13', '45', '200', '200', '18', '151', '205', '135', '38', '117', '196', '135', '155', '111', '196', '196', '166', '23', '25', '161', '18', '124', '259', '165', '58', '170', '204', '250', '103', '160', '238', '194', '163', '241', '255', '235', '17', '133', '26', '135', '196', '237', '114', '258', '193', '248', '40', '195', '84', '238', '146', '199', '40', '161', '22', '137', '43', '166', '156', '4', '104', '133', '177', '211', '193', '78', '203', '246', '237', '264', '3', '149', '78', '78', '111', '239', '13', '238', '176', '112', '150', '156', '3', '6', '118', '178', '178', '21', '186', '150', '241', '50', '243', '191', '121', '278', '29', '200', '18', '3', '197', '191', '98', '173', '195', '25', '195', '264', '221', '15', '156', '25', '200', '211', '195', '26', '35', '2', '135', '126', '113', '167', '191', '264', '196', '133', '18', '26', '146', '203', '219', '23', '160', '160', '135', '259', '3', '159', '120', '160', '106', '240', '133', '115', '258', '38', '22', '3', '26', '22', '107', '49', '48', '25', '237', '72', '219', '196', '196', '111', '135', '196', '18', '166', '172', '135', '26', '258', '70', '98', '40', '166', '23', '96', '3', '30', '155', '126', '146', '114', '242', '237', '157', '238', '241', '40', '133', '200', '161', '175', '200', '175', '84', '266', '18', '99', '30', '242', '151', '153', '166', '199', '228', '3', '237', '132', '110', '135', '80', '15', '15', '15', '84', '133', '15', '177', '3', '202', '255', '71', '115', '235', '19', '18', '18', '102', '58', '78', '165', '166', '264', '12', '150', '15', '2', '18', '135', '109', '200', '258', '128', '3', '199', '197', '239', '109', '222', '239', '107', '125', '132', '204', '204', '133', '135', '30', '158', '203', '15', '264', '237', '133', '196', '3', '252', '26', '98', '79', '199', '210', '0', '99', '30', '26', '259', '6', '49', '166', '98', '108', '9', '133', '102', '155', '98', '114', '78', '35', '166', '188', '202', '235', '30', '149', '165', '176', '138', '29', '153', '25', '25', '137', '133', '166', '135', '125', '71', '193', '154', '165', '98', '23', '56', '198', '26', '26', '0', '109', '136', '163', '151', '25', '15', '241', '195', '204', '191', '195', '161', '175', '133', '237', '78', '165', '258', '135', '78', '135', '258', '18', '235', '30', '187', '150', '275', '149', '2', '36', '112', '22', '161', '238', '203', '112', '265', '75', '133', '205', '258', '158', '18', '26', '18', '25', '26', '15', '26', '239', '15', '240', '18', '71', '71', '78', '135', '3', '148', '166', '26', '176', '2', '197', '150', '176', '19', '18', '97', '166', '18', '153', '250', '133', '210', '20', '192', '196', '26', '191', '2', '81', '70', '5', '79', '3', '30', '193', '193', '69', '160', '176', '246', '108', '54', '250', '135', '200', '80', '3', '167', '18', '166', '200', '152', '23', '149', '19', '14', '13', '237', '196', '19', '112', '254', '204', '191', '23', '264', '199', '209', '137', '31', '135', '238', '78', '18', '191', '15', '164', '195', '74', '163', '163', '41', '99', '26', '135', '26', '40', '40', '250', '200', '200', '26', '237', '4', '195', '158', '35', '71', '191', '3', '218', '274', '58', '3', '23', '177', '132', '49', '62', '238', '166', '97', '264', '193', '29', '26', '49', '241', '197', '200', '64', '227', '258', '44', '15', '133', '175', '238', '109', '254', '237', '198', '175', '166', '213', '174', '196', '107', '259', '98', '10', '196', '210', '191', '96', '166', '135', '15', '197', '78', '78', '195', '176', '18', '98', '129', '170', '18', '247', '203', '26', '158', '40', '64', '200', '163', '196', '19', '205', '197', '193', '191', '69', '96', '74', '35', '36', '64', '133', '172', '19', '266', '96', '94', '103', '258', '193', '264', '98', '254', '50', '193', '25', '170', '75', '84', '27', '252', '203', '94', '94', '19', '98', '137', '12', '153', '3', '155', '135', '238', '15', '196', '15', '26', '84', '49', '27', '135', '203', '48', '133', '129', '133', '258', '155', '35', '195', '71', '241', '135', '166', '193', '165', '104', '196', '117', '196', '26', '237', '109', '95', '196', '60', '40', '158', '193', '250', '104', '94', '178', '27', '193', '78', '106', '79', '204', '133', '237', '235', '135', '202', '70', '15', '264', '241', '254', '110', '199', '49', '255', '30', '193', '238', '204', '102', '97', '161', '43', '228', '1', '6', '196', '193', '26', '15', '128', '46', '155', '32', '133', '69', '47', '135', '78', '205', '200', '200', '202', '3', '95', '191', '78', '26', '248', '258', '191', '240', '49', '110', '38', '193', '200', '200', '96', '200', '204', '135', '9', '200', '258', '15', '46', '15', '149', '155', '197', '2', '166', '151', '72', '135', '97', '211', '238', '135', '163', '161', '26', '163', '129', '241', '239', '241', '16', '26', '18', '263', '260', '166', '3', '264', '195', '79', '132', '251', '50', '133', '15', '25', '15', '191', '15', '78', '50', '78', '71', '227', '69', '200', '69', '109', '64', '133', '197', '196', '3', '45', '238', '26', '132', '104', '196', '193', '136', '80', '26', '75', '117', '14', '78', '75', '98', '15', '2', '15', '50', '255', '136', '3', '112', '173', '69', '191', '19', '239', '235', '109', '70', '135', '78', '165', '238', '262', '199', '35', '203', '224', '193', '15', '18', '150', '238', '197', '238', '193', '135', '3', '150', '187', '199', '239', '109', '152', '27', '257', '6', '235', '104', '191', '133', '166', '155', '26', '16', '136', '198', '98', '154', '235', '27', '102', '30', '191', '15', '132', '132', '114', '107', '241', '241', '156', '199', '241', '49', '62', '117', '258', '30', '163', '133', '238', '200', '3', '15', '109', '2', '196', '165', '166', '159', '210', '35', '15', '17', '200', '241', '106', '203', '197', '197', '79', '98', '98', '23', '204', '48', '129', '57', '258', '202', '215', '238', '70', '64', '193', '189', '57', '200', '0', '98', '135', '204', '25', '30', '204', '27', '26', '78', '135', '109', '3', '197', '3', '167', '35', '196', '225', '2', '194', '200', '15', '161', '198', '23', '133', '238', '195', '35', '50', '135', '152', '163', '21', '15', '94', '26', '94', '53', '260', '196', '154', '277', '200', '118', '6', '238', '50', '209', '50', '3', '238', '3', '15', '107', '135', '199', '15', '135', '26', '258', '251', '203', '19', '259', '195', '199', '238', '15', '254', '112', '211', '200', '129', '84', '196', '3', '208', '259', '135', '104', '29', '136', '31', '78', '13', '3', '64', '75', '55', '114', '160', '96', '14', '18', '207', '75', '48', '15', '49', '97', '117', '196', '99', '258', '195', '264', '264', '78', '197', '30', '238', '109', '158', '32', '195', '149', '227', '65', '167', '162', '204', '250', '151', '151', '102', '26', '29', '26', '193', '74', '50', '193', '186', '15', '74', '259', '57', '188', '277', '277', '195', '195', '212', '78', '197', '64', '129', '259', '200', '199', '129', '4', '235', '197', '196', '161', '3', '255', '199', '102', '3', '13', '94', '53', '98', '15', '129', '15', '155', '71', '179', '50', '29', '136', '80', '254', '198', '162', '237', '3', '18', '260', '238', '117', '179', '156', '19', '45', '197', '78', '199', '102', '102', '237', '155', '69', '69', '163', '239', '15', '196', '80', '191', '30', '71', '158', '259', '2', '106', '148', '15', '135', '135', '135', '196', '19', '123', '15', '104', '19', '128', '230', '69', '98', '197', '241', '161', '195', '200', '3', '98', '78', '165', '163', '19', '196', '31', '173', '203', '55', '69', '209', '239', '79', '158', '254', '135', '51', '94', '162', '255', '95', '18', '133', '109', '50', '94', '199', '15', '197', '200', '133', '116', '50', '196', '161', '69', '133', '3', '258', '3', '197', '29', '3', '157', '197', '193', '4', '94', '195', '17', '48', '152', '8', '15', '155', '133', '195', '136', '150', '102', '237', '203', '164', '164', '250', '154', '13', '191', '58', '94', '70', '93', '264', '15', '30', '133', '237', '267', '199', '193', '76', '241', '157', '26', '200', '109', '179', '55', '98', '200', '129', '238', '235', '208', '109', '239', '258', '196', '193', '18', '107', '210', '70', '40', '129', '259', '193', '30', '158', '252', '40', '210', '78', '15', '78', '96', '208', '113', '258', '64', '80', '15', '133', '241', '15', '15', '258', '135', '109', '251', '251', '109', '74', '196', '170', '240', '254', '258', '204', '199', '26', '109', '197', '57', '45', '161', '57', '114', '3', '15', '264', '106', '57', '27', '18', '258', '29', '84', '161', '135', '114', '24', '30', '107', '107', '172', '209', '170', '18', '19', '133', '191', '195', '135', '156', '196', '94', '3', '84', '225', '199', '238', '106', '104', '70', '50', '12', '155', '114', '176', '237', '200', '108', '70', '240', '196', '19', '22', '276', '199', '238', '205', '104', '193', '108', '115', '259', '102', '19', '258', '18', '106', '161', '191', '114', '64', '15', '204', '35', '155', '40', '238', '136', '264', '133', '186', '15', '258', '166', '26', '6', '114', '114', '133', '114', '114', '13', '13', '13', '237', '250', '245', '136', '78', '13', '48', '18', '13', '25', '26', '93', '125', '50', '94', '148', '78', '194', '200', '25', '93', '274', '266', '70', '22', '3', '208', '240', '211', '135', '31', '18', '153', '18', '2', '15', '15', '15', '156', '15', '70', '73', '173', '13', '112', '69', '204', '70', '248', '135', '70', '3', '193', '193', '22', '162', '238', '57', '98', '15', '199', '199', '52', '151', '73', '5', '108', '23', '69', '95', '220', '106', '163', '152', '163', '108', '15', '58', '250', '19', '177', '245', '264', '93', '204', '25', '78', '155', '69', '155', '239', '241', '155', '200', '18', '3', '13', '196', '197', '204', '272', '95', '135', '165', '22', '210', '35', '69', '18', '170', '3', '25', '156', '78', '135', '102', '107', '255', '19', '70', '70', '109', '198', '156', '64', '170', '138', '78', '203', '166', '108', '114', '179', '228', '74', '49', '14', '264', '26', '195', '78', '107', '75', '18', '106', '151', '70', '58', '15', '196', '238', '109', '104', '104', '109', '136', '148', '210', '3', '161', '15', '104', '238', '208', '133', '108', '241', '18', '209', '74', '156', '114', '199', '174', '35', '18', '170', '135', '238', '244', '155', '162', '197', '3', '98', '227', '267', '238', '193', '50', '238', '98', '112', '124', '25', '114', '241', '159', '154', '78', '161', '136', '114', '153', '225', '154', '13', '50', '11', '25', '15', '209', '11', '166', '203', '259', '237', '22', '114', '276', '98', '278', '150', '198', '18', '15', '78', '15', '162', '26', '3', '196', '3', '102', '189', '71', '69', '16', '18', '152', '133', '135', '162', '241', '15', '104', '15', '259', '202', '2', '238', '207', '15', '204', '30', '215', '112', '210', '42', '96', '193', '15', '110', '163', '211', '78', '108', '15', '31', '27', '15', '31', '196', '195', '96', '15', '161', '94', '193', '195', '198', '98', '261', '258', '76', '15', '104', '258', '135', '18', '15', '202', '258', '108', '156', '13', '22', '251', '199', '155', '258', '211', '3', '207', '202', '259', '107', '202', '259', '135', '135', '148', '158', '154', '99', '99', '3', '207', '94', '25', '162', '31', '241', '25', '15', '114', '18', '114', '148', '19', '15', '203', '191', '19', '13', '193', '204', '25', '166', '155', '196', '98', '162', '197', '29', '152', '191', '133', '199', '230', '40', '25', '204', '79', '3', '19', '165', '204', '108', '208', '3', '251', '274', '15', '166', '264', '35', '13', '193', '114', '258', '13', '19', '133', '23', '17', '199', '204', '153', '250', '204', '16', '12', '107', '191', '18', '3', '15', '165', '15', '69', '17', '165', '17', '15', '15', '156', '23', '23', '15', '197', '104', '197', '26', '208', '135', '211', '250', '18', '204', '25', '196', '53', '69', '104', '104', '19', '78', '241', '230', '30', '195', '96', '26', '109', '196', '69', '238', '15', '18', '133', '26', '106', '109', '98', '15', '258', '274', '193', '203', '58', '57', '274', '2', '202', '161', '136', '135', '155', '241', '35', '151', '137', '5', '114', '96', '16', '209', '158', '15', '266', '161', '15', '25', '274', '202', '3', '161', '237', '267', '204', '125', '15', '151', '155', '259', '151', '210', '238', '15', '26', '132', '50', '173', '197', '114', '241', '15', '258', '109', '166', '274', '13', '191', '15', '26', '164', '258', '163', '15', '264', '11', '260', '148', '150', '207', '196', '98', '272', '133', '196', '107', '26', '191', '163', '191', '135', '3', '18', '18', '119', '69', '196', '15', '156', '135', '112', '258', '155', '256', '199', '204', '212', '18', '135', '254', '72', '258', '254', '15', '133', '18', '259', '193', '258', '204', '40', '18', '29', '155', '104', '107', '274', '241', '25', '26', '69', '148', '166', '46', '94', '135', '15', '155', '104', '241', '32', '274', '249', '15', '197', '104', '44', '259', '69', '191', '15', '198', '193', '133', '17', '258', '248', '166', '197', '200', '250', '204', '3', '225', '238', '163', '69', '15', '15', '26', '202', '19', '209', '78', '212', '15', '78', '243', '110', '191', '15', '18', '107', '38', '208', '166', '239', '237', '199', '15', '204', '258', '173', '172', '200', '274', '243', '19', '191', '135', '203', '69', '149', '148', '196', '256', '204', '97', '191', '26', '114', '274', '200', '69', '69', '64', '69', '239', '25', '274', '203', '78', '135', '196', '202', '17', '2', '2', '3', '98', '98', '62', '2', '95', '96', '2', '207', '78', '200', '26', '64', '18', '93', '49', '57', '154', '200', '69', '94', '98', '98', '56', '56', '166', '98', '212', '226', '226', '15', '69', '15', '273', '272', '104', '193', '3', '98', '199', '98', '25', '125', '259', '249', '108', '40', '26', '198', '218', '73', '148', '69', '204', '241', '35', '3', '274', '70', '18', '71', '35', '172', '133', '106', '69', '107', '199', '219', '19', '133', '69', '135', '109', '57', '191', '135', '252', '104', '3', '94', '3', '72', '126', '16', '36', '84', '241', '3', '218', '98', '108', '46', '19', '196', '118', '104', '29', '238', '69', '10', '204', '264', '259', '124', '242', '107', '133', '25', '64', '30', '194', '259', '159', '264', '274', '52', '200', '104', '29', '30', '193', '98', '94', '114', '204', '40', '18', '198', '97', '3', '99', '274', '239', '177', '15', '18', '126', '126', '208', '148', '15', '104', '29', '36', '218', '70', '15', '23', '278', '23', '36', '191', '210', '51', '245', '167', '72', '237', '25', '203', '36', '196', '193', '196', '15', '151', '207', '203', '103', '166', '135', '110', '27', '13', '27', '191', '98', '57', '166', '50', '21', '15', '197', '23', '2', '70', '69', '193', '274', '104', '166', '258', '126', '238', '161', '18', '193', '69', '140', '170', '197', '204', '165', '203', '170', '15', '58', '35', '83', '212', '150', '73', '25', '15', '64', '133', '210', '108', '79', '193', '15', '126', '165', '15', '208', '98', '3', '200', '126', '3', '40', '165', '98', '69', '15', '165', '133', '96', '173', '203', '133', '238', '248', '15', '7', '38', '207', '108', '165', '199', '18', '162', '27', '3', '72', '65', '250', '209', '166', '64', '209', '237', '212', '64', '225', '120', '108', '203', '135', '248', '69', '251', '135', '204', '160', '160', '135', '148', '15', '15', '23', '207', '274', '29', '28', '194', '150', '40', '104', '208', '125', '70', '18', '3', '19', '102', '104', '23', '29', '207', '133', '204', '111', '2', '125', '126', '7', '194', '209', '203', '83', '126', '126', '258', '119', '162', '241', '133', '18', '208', '197', '209', '15', '78', '80', '110', '237', '104', '30', '208', '103', '15', '200', '15', '238', '254', '108', '173', '40', '69', '175', '17', '155', '136', '26', '3', '135', '13', '241', '161', '166', '170', '162', '133', '53', '211', '204', '15', '161', '238', '110', '18', '191', '18', '50', '71', '30', '17', '208', '248', '203', '203', '196', '210', '258', '204', '3', '18', '196', '125', '173', '199', '249', '18', '104', '165', '241', '2', '193', '238', '78', '15', '166', '15', '126', '3', '237', '161', '52', '207', '3', '3', '50', '56', '22', '56', '69', '93', '135', '207', '69', '99', '3', '133', '68', '133', '161', '108', '126', '250', '60', '155', '98', '18', '16', '78', '78', '240', '235', '176', '22', '199', '160', '235', '274', '209', '64', '173', '137', '128', '25', '18', '160', '203', '70', '165', '193', '110', '207', '64', '85', '252', '78', '51', '2', '99', '19', '22', '198', '204', '166', '166', '151', '56', '133', '210', '26', '149', '109', '30', '209', '209', '51', '3', '193', '119', '208', '93', '165', '248', '55', '203', '204', '58', '18', '199', '211', '83', '32', '135', '212', '156', '19', '235', '173', '191', '209', '203', '146', '208', '58', '158', '102', '199', '200', '200', '173', '70', '52', '107', '211', '213', '237', '196', '205', '104', '135', '258', '255', '96', '19', '124', '128', '57', '199', '98', '166', '102', '40', '15', '238', '3', '148', '225', '126', '135', '114', '193', '204', '15', '209', '209', '193', '259', '64', '161', '26', '274', '126', '18', '196', '0', '60', '207', '17', '71', '218', '239', '248', '38', '154', '61', '128', '199', '196', '173', '25', '46', '98', '258', '108', '3', '108', '196', '93', '93', '99', '172', '69', '241', '196', '30', '135', '278', '109', '18', '5', '209', '57', '159', '70', '193', '163', '249', '196', '196', '72', '103', '133', '17', '204', '199', '166', '202', '29', '57', '193', '150', '98', '119', '25', '205', '18', '173', '166', '209', '57', '25', '58', '258', '26', '15', '15', '101', '26', '74', '1', '4', '74', '263', '']\n",
            "['209', '63', '216', '108', '196', '200', '203', '246', '188', '193', '25', '106', '56', '53', '223', '161', '200', '208', '78', '147', '147', '179', '147', '80', '56', '209', '209', '200', '214', '202', '203', '194', '26', '237', '165', '258', '238', '238', '76', '193', '196', '213', '161', '213', '209', '194', '196', '25', '151', '260', '135', '173', '30', '108', '272', '259', '176', '213', '213', '21', '5', '171', '200', '195', '240', '194', '176', '240', '240', '80', '173', '98', '214', '21', '76', '76', '73', '173', '173', '78', '61', '258', '191', '3', '78', '173', '193', '118', '200', '264', '260', '49', '211', '258', '18', '173', '78', '3', '3', '1', '213', '135', '182', '239', '208', '278', '50', '194', '200', '124', '189', '117', '264', '244', '76', '193', '221', '78', '78', '173', '197', '78', '161', '231', '135', '22', '78', '25', '25', '197', '200', '147', '203', '191', '196', '193', '263', '32', '197', '191', '4', '56', '196', '104', '191', '196', '173', '147', '194', '96', '161', '161', '238', '200', '3', '78', '78', '53', '41', '237', '53', '18', '10', '194', '97', '96', '208', '25', '10', '117', '104', '54', '96', '194', '21', '245', '238', '202', '209', '161', '97', '258', '240', '213', '136', '155', '152', '21', '191', '191', '118', '68', '258', '189', '197', '208', '61', '78', '135', '202', '96', '155', '108', '272', '80', '25', '155', '35', '213', '191', '194', '96', '96', '191', '264', '191', '191', '191', '2', '258', '191', '173', '259', '15', '47', '258', '156', '118', '160', '115', '105', '26', '173', '192', '76', '76', '135', '240', '118', '161', '200', '230', '18', '200', '25', '10', '161', '244', '24', '24', '200', '205', '104', '15', '193', '156', '95', '209', '240', '191', '194', '128', '208', '208', '218', '173', '10', '25', '18', '107', '46', '32', '197', '172', '209', '213', '79', '121', '78', '78', '193', '2', '78', '194', '149', '136', '80', '3', '21', '200', '78', '21', '197', '194', '118', '202', '161', '180', '209', '173', '2', '193', '6', '260', '197', '78', '78', '42', '191', '213', '135', '193', '196', '202', '222', '217', '42', '259', '213', '76', '46', '30', '15', '260', '202', '18', '259', '104', '135', '76', '196', '240', '240', '240', '108', '24', '200', '197', '262', '176', '15', '156', '264', '156', '200', '191', '161', '196', '78', '238', '161', '221', '118', '14', '14', '14', '200', '278', '238', '200', '196', '78', '173', '25', '197', '197', '196', '161', '18', '10', '10', '10', '18', '66', '203', '19', '246', '196', '244', '240', '161', '200', '78', '240', '209', '208', '118', '215', '194', '193', '168', '229', '191', '135', '172', '196', '197', '161', '78', '196', '197', '258', '78', '78', '238', '221', '121', '264', '104', '146', '50', '78', '217', '11', '238', '238', '238', '3', '3', '154', '215', '79', '18', '158', '104', '18', '13', '197', '117', '197', '161', '213', '242', '32', '238', '230', '2', '2', '32', '161', '251', '96', '156', '242', '22', '22', '140', '3', '149', '135', '191', '173', '135', '200', '238', '238', '78', '156', '209', '78', '97', '97', '160', '25', '258', '240', '161', '15', '161', '96', '209', '80', '132', '140', '4', '1', '240', '196', '274', '272', '62', '80', '196', '196', '191', '79', '156', '15', '196', '209', '22', '251', '202', '202', '121', '73', '191', '238', '191', '150', '150', '135', '46', '203', '202', '238', '22', '3', '258', '239', '238', '238', '165', '238', '238', '194', '200', '200', '197', '200', '196', '191', '194', '76', '78', '208', '200', '71', '208', '240', '76', '117', '120', '108', '96', '144', '30', '133', '133', '202', '117', '161', '251', '29', '104', '242', '197', '118', '216', '262', '236', '141', '11', '235', '235', '133', '191', '238', '202', '80', '80', '22', '140', '191', '197', '202', '133', '184', '184', '184', '184', '143', '274', '50', '237', '196', '274', '265', '10', '18', '10', '121', '238', '245', '228', '76', '25', '191', '197', '268', '10', '240', '195', '196', '195', '15', '48', '200', '183', '70', '59', '197', '196', '235', '161', '161', '215', '104', '97', '191', '194', '191', '200', '200', '197', '76', '200', '93', '256', '160', '238', '146', '238', '239', '15', '140', '258', '140', '21', '79', '202', '258', '79', '62', '205', '161', '104', '10', '161', '196', '137', '152', '191', '117', '213', '242', '176', '218', '195', '191', '162', '191', '135', '241', '213', '213', '84', '79', '68', '18', '98', '197', '155', '41', '246', '269', '78', '209', '238', '80', '238', '62', '237', '273', '273', '230', '150', '121', '260', '244', '161', '202', '78', '109', '155', '264', '145', '50', '274', '173', '210', '258', '238', '211', '156', '215', '195', '177', '244', '3', '141', '196', '191', '194', '29', '30', '10', '16', '200', '200', '156', '53', '3', '62', '165', '173', '173', '22', '205', '274', '150', '183', '116', '188', '188', '98', '161', '204', '80', '172', '156', '104', '229', '259', '78', '169', '3', '200', '118', '161', '80', '260', '240', '150', '197', '196', '62', '93', '6', '272', '272', '197', '160', '78', '96', '70', '195', '191', '191', '80', '3', '3', '240', '147', '196', '191', '238', '96', '203', '3', '15', '10', '58', '55', '96', '41', '15', '26', '50', '238', '194', '79', '240', '161', '191', '195', '263', '50', '240', '147', '235', '195', '203', '59', '178', '176', '2', '15', '203', '191', '18', '278', '196', '14', '153', '58', '175', '18', '217', '217', '80', '191', '200', '18', '200', '173', '24', '171', '171', '210', '124', '34', '200', '218', '104', '4', '223', '193', '194', '200', '200', '13', '62', '194', '197', '80', '197', '146', '146', '96', '97', '97', '2', '196', '191', '191', '104', '238', '10', '200', '239', '135', '104', '213', '2', '221', '169', '25', '21', '194', '161', '21', '278', '3', '195', '46', '176', '25', '117', '191', '62', '62', '62', '78', '50', '264', '79', '197', '80', '80', '80', '191', '191', '78', '161', '194', '195', '197', '203', '259', '278', '49', '147', '147', '147', '147', '22', '78', '78', '258', '2', '202', '221', '55', '258', '104', '104', '84', '240', '221', '274', '104', '22', '241', '63', '197', '197', '196', '159', '200', '22', '22', '22', '78', '78', '173', '104', '97', '104', '191', '146', '223', '268', '96', '14', '173', '204', '241', '97', '15', '118', '78', '78', '196', '194', '258', '96', '96', '259', '75', '10', '197', '30', '2', '238', '121', '50', '161', '173', '173', '186', '185', '186', '78', '194', '221', '218', '35', '33', '13', '34', '78', '21', '215', '97', '60', '24', '78', '193', '173', '235', '78', '200', '258', '140', '259', '194', '197', '195', '200', '145', '76', '108', '130', '238', '215', '213', '2', '217', '121', '178', '140', '62', '156', '176', '196', '196', '41', '223', '147', '172', '79', '200', '2', '244', '244', '191', '213', '104', '191', '237', '237', '33', '117', '195', '195', '264', '195', '140', '210', '41', '197', '14', '257', '196', '238', '238', '79', '161', '156', '196', '238', '238', '213', '104', '3', '10', '118', '204', '235', '33', '259', '274', '259', '176', '78', '78', '196', '34', '52', '25', '30', '194', '153', '21', '70', '191', '18', '191', '191', '172', '194', '194', '194', '194', '148', '104', '125', '2', '2', '2', '185', '194', '29', '104', '78', '189', '272', '204', '183', '213', '78', '104', '197', '240', '97', '168', '168', '182', '238', '197', '18', '194', '229', '221', '62', '78', '53', '104', '214', '189', '189', '189', '213', '117', '117', '191', '29', '104', '104', '276', '276', '22', '195', '195', '155', '139', '139', '59', '168', '135', '140', '79', '146', '200', '245', '70', '160', '153', '50', '202', '154', '263', '130', '194', '117', '194', '194', '191', '33', '29', '29', '238', '239', '194', '21', '274', '215', '18', '18', '140', '2', '215', '25', '46', '137', '238', '30', '160', '156', '46', '196', '263', '242', '104', '0', '0', '73', '213', '135', '22', '191', '191', '241', '202', '221', '15', '15', '191', '238', '213', '194', '197', '179', '238', '200', '200', '200', '5', '238', '194', '55', '117', '166', '49', '244', '239', '191', '78', '78', '78', '194', '200', '117', '210', '22', '121', '121', '121', '21', '237', '194', '121', '78', '104', '194', '79', '191', '251', '195', '197', '274', '176', '21', '43', '43', '10', '61', '215', '215', '30', '214', '194', '10', '30', '139', '96', '226', '226', '265', '2', '2', '121', '121', '41', '237', '104', '238', '238', '161', '78', '78', '78', '11', '184', '140', '140', '161', '104', '104', '237', '130', '195', '160', '118', '161', '18', '11', '241', '184', '196', '178', '22', '30', '146', '146', '10', '10', '141', '141', '135', '18', '18', '10', '26', '208', '97', '141', '18', '18', '97', '194', '194', '79', '213', '205', '205', '205', '75', '241', '50', '226', '238', '97', '222', '215', '215', '139', '238', '51', '238', '96', '173', '109', '200', '117', '192', '2', '195', '10', '3', '18', '49', '238', '238', '191', '195', '52', '197', '200', '200', '79', '191', '200', '200', '30', '195', '197', '195', '25', '260', '73', '30', '193', '21', '200', '242', '203', '120', '78', '260', '241', '0', '30', '202', '195', '30', '191', '78', '194', '194', '2', '205', '191', '196', '173', '197', '30', '196', '172', '139', '2', '18', '191', '230', '56', '194', '215', '195', '2', '71', '13', '146', '264', '50', '191', '235', '93', '191', '139', '139', '227', '235', '186', '235', '24', '18', '261', '128', '33', '71', '25', '176', '39', '10', '195', '202', '29', '29', '221', '29', '197', '139', '78', '227', '146', '137', '195', '205', '205', '195', '135', '62', '153', '146', '29', '75', '0', '194', '194', '180', '213', '78', '18', '194', '239', '196', '239', '197', '194', '153', '192', '221', '10', '46', '50', '241', '117', '117', '117', '195', '173', '78', '195', '194', '239', '2', '12', '173', '197', '30', '269', '197', '176', '135', '30', '146', '191', '93', '93', '135', '260', '218', '156', '155', '200', '241', '125', '18', '18', '183', '203', '239', '259', '173', '213', '180', '180', '210', '120', '104', '194', '156', '258', '172', '9', '0', '241', '192', '143', '189', '204', '50', '63', '197', '243', '241', '93', '125', '2', '21', '260', '4', '4', '3', '191', '150', '128', '78', '238', '245', '8', '80', '238', '79', '79', '240', '58', '104', '155', '80', '80', '188', '191', '235', '170', '170', '170', '146', '258', '195', '276', '25', '78', '215', '240', '161', '53', '147', '78', '78', '78', '78', '221', '221', '96', '18', '19', '130', '18', '18', '18', '80', '80', '194', '79', '93', '121', '78', '139', '139', '78', '197', '121', '194', '200', '80', '101', '22', '12', '135', '97', '205', '181', '60', '193', '15', '140', '172', '191', '241', '222', '135', '29', '25', '179', '191', '191', '259', '194', '133', '240', '121', '121', '241', '195', '29', '191', '209', '18', '139', '139', '50', '18', '12', '18', '133', '200', '25', '10', '70', '25', '242', '18', '161', '49', '172', '172', '172', '172', '128', '96', '195', '200', '78', '10', '239', '22', '173', '142', '142', '22', '194', '192', '3', '200', '241', '96', '241', '62', '139', '213', '202', '135', '79', '120', '192', '78', '118', '241', '46', '151', '150', '151', '150', '118', '221', '183', '237', '196', '140', '140', '39', '179', '213', '140', '41', '73', '78', '217', '258', '30', '118', '140', '219', '50', '10', '139', '150', '251', '24', '10', '18', '78', '18', '172', '135', '146', '71', '196', '202', '197', '25', '106', '160', '215', '15', '6', '101', '165', '140', '145', '80', '3', '238', '239', '78', '268', '0', '109', '191', '215', '238', '238', '238', '203', '50', '146', '15', '25', '156', '191', '16', '2', '195', '189', '96', '193', '109', '204', '161', '185', '210', '50', '235', '30', '242', '193', '102', '194', '121', '177', '241', '102', '132', '240', '71', '192', '79', '25', '197', '39', '161', '22', '258', '237', '15', '7', '78', '156', '197', '70', '176', '139', '240', '132', '78', '177', '191', '194', '117', '10', '79', '238', '195', '24', '101', '118', '196', '2', '102', '21', '180', '185', '170', '79', '139', '135', '191', '151', '200', '191', '154', '262', '200', '237', '56', '27', '191', '180', '258', '139', '139', '192', '30', '0', '242', '239', '242', '25', '191', '242', '240', '3', '56', '215', '171', '192', '193', '222', '238', '237', '215', '272', '2', '156', '237', '151', '189', '80', '15', '260', '260', '217', '235', '22', '228', '135', '18', '191', '157', '78', '2', '240', '213', '179', '2', '191', '79', '79', '153', '3', '194', '78', '121', '71', '121', '149', '257', '194', '191', '188', '15', '25', '125', '102', '15', '130', '130', '177', '10', '135', '265', '240', '149', '196', '200', '204', '204', '213', '177', '197', '194', '13', '202', '30', '213', '125', '193', '125', '125', '125', '125', '125', '15', '180', '78', '1', '25', '96', '200', '29', '156', '172', '202', '22', '78', '197', '188', '241', '101', '101', '101', '221', '50', '191', '15', '15', '0', '101', '101', '168', '56', '197', '130', '62', '204', '29', '10', '96', '97', '139', '97', '97', '2', '2', '262', '39', '176', '172', '194', '194', '18', '78', '13', '80', '139', '196', '207', '191', '151', '192', '150', '13', '104', '104', '101', '15', '125', '101', '15', '0', '50', '194', '264', '50', '25', '25', '125', '125', '125', '125', '125', '25', '29', '25', '25', '78', '240', '261', '19', '2', '74', '78', '238', '50', '50', '203', '197', '71', '104', '80', '194', '149', '170', '130', '2', '237', '170', '202', '48', '97', '93', '179', '235', '78', '102', '102', '70', '208', '238', '104', '146', '118', '118', '96', '237', '274', '218', '241', '102', '146', '209', '209', '240', '170', '137', '7', '197', '191', '264', '29', '46', '96', '193', '19', '12', '213', '215', '275', '179', '18', '183', '78', '221', '18', '170', '119', '21', '140', '240', '96', '192', '192', '21', '96', '149', '239', '135', '191', '240', '184', '183', '24', '2', '96', '197', '194', '215', '18', '192', '121', '202', '196', '25', '97', '214', '3', '97', '16', '0', '192', '135', '215', '5', '12', '80', '143', '202', '242', '151', '13', '209', '203', '200', '30', '209', '194', '102', '102', '102', '102', '102', '102', '22', '24', '6', '18', '5', '191', '101', '50', '101', '101', '192', '153', '153', '101', '101', '10', '192', '25', '140', '50', '241', '102', '102', '32', '150', '101', '102', '153', '102', '101', '153', '102', '25', '10', '125', '18', '125', '177', '209', '102', '10', '78', '78', '137', '18', '237', '241', '175', '195', '175', '96', '176', '237', '96', '96', '101', '193', '238', '170', '238', '102', '222', '268', '238', '102', '244', '34', '18', '244', '202', '149', '149', '193', '102', '170', '194', '75', '101', '222', '238', '50', '195', '135', '22', '93', '18', '102', '102', '93', '93', '191', '147', '244', '96', '2', '241', '241', '93', '189', '189', '177', '35', '137', '197', '191', '40', '40', '195', '176', '35', '200', '200', '237', '135', '140', '140', '93', '156', '200', '78', '191', '22', '81', '238', '101', '213', '101', '78', '109', '19', '140', '196', '78', '78', '76', '218', '14', '265', '101', '241', '155', '4', '50', '78', '78', '238', '135', '191', '265', '81', '238', '238', '102', '102', '102', '140', '162', '32', '17', '17', '171', '171', '191', '101', '102', '192', '101', '17', '171', '135', '102', '242', '195', '237', '21', '18', '238', '191', '278', '140', '149', '97', '260', '155', '276', '192', '196', '21', '191', '2', '97', '0', '150', '221', '213', '209', '238', '80', '260', '194', '104', '96', '193', '14', '240', '193', '117', '117', '155', '17', '264', '260', '209', '209', '2', '137', '71', '72', '101', '205', '173', '191', '118', '24', '41', '197', '78', '94', '237', '156', '193', '153', '154', '153', '209', '110', '78', '78', '213', '238', '78', '191', '93', '79', '177', '267', '133', '133', '46', '139', '40', '143', '74', '241', '202', '50', '3', '102', '78', '149', '191', '156', '18', '93', '195', '192', '137', '240', '135', '204', '189', '22', '56', '154', '25', '192', '130', '277', '191', '80', '39', '21', '21', '238', '238', '23', '192', '191', '192', '94', '94', '80', '139', '17', '10', '22', '227', '227', '140', '102', '21', '17', '227', '227', '23', '243', '264', '18', '274', '149', '238', '30', '221', '22', '238', '193', '135', '205', '195', '238', '238', '197', '130', '195', '18', '104', '102', '14', '59', '104', '18', '18', '258', '34', '241', '138', '188', '176', '102', '1', '140', '140', '22', '22', '278', '10', '96', '35', '145', '120', '33', '33', '22', '33', '22', '35', '22', '19', '33', '102', '192', '192', '175', '22', '72', '191', '222', '125', '22', '79', '18', '194', '18', '250', '192', '192', '96', '48', '274', '273', '132', '133', '192', '40', '171', '171', '246', '191', '10', '166', '197', '195', '125', '125', '200', '153', '192', '149', '110', '139', '194', '48', '102', '102', '104', '203', '238', '238', '238', '78', '18', '238', '121', '18', '238', '96', '238', '238', '13', '191', '18', '18', '240', '96', '40', '188', '18', '137', '175', '28', '28', '28', '149', '191', '191', '207', '239', '239', '153', '153', '153', '153', '153', '153', '154', '153', '154', '153', '153', '153', '18', '0', '27', '18', '25', '96', '96', '137', '33', '10', '130', '153', '153', '154', '180', '137', '44', '241', '153', '268', '25', '161', '25', '39', '242', '4', '238', '195', '102', '135', '13', '229', '140', '1', '79', '192', '229', '191', '118', '21', '260', '203', '18', '177', '191', '10', '238', '10', '133', '14', '175', '175', '192', '18', '153', '153', '254', '140', '149', '22', '22', '22', '237', '40', '135', '191', '192', '192', '62', '205', '19', '238', '176', '214', '200', '96', '15', '153', '191', '108', '29', '166', '28', '193', '197', '96', '195', '108', '102', '241', '195', '10', '33', '166', '197', '168', '2', '96', '26', '33', '204', '258', '149', '153', '153', '40', '137', '241', '166', '221', '23', '221', '153', '15', '113', '113', '78', '171', '40', '71', '21', '71', '80', '15', '191', '191', '135', '110', '10', '259', '102', '149', '149', '109', '109', '175', '97', '97', '97', '153', '15', '238', '193', '175', '38', '38', '137', '18', '235', '155', '137', '265', '110', '18', '18', '256', '191', '224', '173', '104', '10', '152', '24', '24', '24', '17', '40', '40', '40', '40', '40', '40', '112', '6', '18', '18', '210', '96', '191', '27', '213', '17', '153', '26', '133', '18', '18', '153', '38', '96', '40', '38', '40', '40', '10', '17', '17', '95', '71', '192', '17', '259', '213', '200', '15', '11', '207', '194', '18', '135', '238', '191', '241', '191', '116', '35', '147', '156', '67', '78', '17', '227', '191', '117', '188', '130', '72', '96', '235', '97', '18', '150', '203', '25', '126', '149', '200', '94', '4', '4', '1', '244', '228', '252', '149', '191', '149', '15', '17', '17', '29', '18', '18', '116', '23', '237', '94', '184', '268', '197', '175', '243', '28', '16', '10', '50', '191', '102', '102', '153', '149', '111', '78', '191', '135', '18', '35', '78', '238', '208', '93', '95', '258', '17', '111', '111', '262', '22', '22', '104', '161', '209', '21', '195', '12', '116', '241', '242', '239', '17', '97', '16', '40', '117', '94', '96', '78', '15', '103', '195', '135', '235', '247', '135', '130', '260', '239', '176', '93', '191', '209', '239', '93', '264', '30', '18', '30', '18', '18', '103', '191', '97', '17', '95', '115', '209', '140', '22', '22', '241', '10', '153', '3', '188', '238', '226', '195', '97', '96', '238', '97', '130', '97', '116', '96', '96', '242', '45', '149', '166', '155', '96', '116', '264', '10', '237', '192', '71', '22', '71', '175', '149', '116', '237', '274', '18', '235', '176', '116', '202', '250', '250', '78', '2', '196', '2', '97', '143', '18', '111', '111', '80', '78', '153', '80', '40', '110', '30', '112', '30', '111', '215', '112', '149', '78', '78', '209', '110', '110', '238', '109', '239', '112', '151', '154', '16', '16', '16', '78', '137', '179', '72', '149', '10', '111', '268', '96', '170', '111', '38', '192', '153', '183', '195', '30', '30', '147', '195', '147', '93', '264', '153', '25', '18', '19', '22', '74', '40', '93', '166', '15', '127', '147', '192', '260', '238', '103', '269', '127', '19', '19', '153', '18', '78', '238', '30', '30', '241', '16', '235', '250', '204', '238', '218', '40', '93', '184', '18', '0', '241', '195', '94', '154', '109', '156', '10', '21', '30', '197', '39', '78', '24', '107', '142', '142', '228', '229', '229', '229', '232', '179', '239', '142', '232', '23', '175', '147', '137', '137', '137', '171', '191', '117', '128', '127', '204', '98', '147', '235', '239', '153', '15', '204', '255', '80', '204', '204', '149', '113', '19', '167', '22', '250', '17', '17', '17', '18', '17', '67', '121', '125', '26', '97', '97', '97', '97', '247', '124', '163', '147', '112', '111', '208', '108', '38', '209', '154', '154', '194', '135', '22', '147', '241', '120', '218', '260', '156', '155', '241', '192', '215', '180', '179', '149', '40', '193', '192', '150', '102', '93', '1', '149', '137', '133', '102', '102', '102', '78', '145', '21', '96', '175', '175', '78', '123', '30', '95', '19', '18', '110', '18', '152', '152', '147', '26', '135', '260', '158', '203', '195', '158', '112', '156', '125', '213', '230', '30', '15', '125', '106', '111', '195', '181', '130', '195', '130', '239', '15', '194', '16', '15', '111', '61', '72', '57', '128', '239', '239', '203', '25', '218', '10', '155', '191', '46', '98', '175', '260', '102', '21', '191', '260', '191', '239', '15', '30', '93', '147', '103', '78', '177', '30', '95', '155', '155', '155', '103', '103', '112', '241', '191', '191', '17', '78', '153', '150', '113', '115', '147', '78', '40', '163', '33', '15', '149', '103', '192', '191', '96', '106', '115', '143', '235', '200', '204', '238', '204', '204', '74', '196', '113', '246', '102', '150', '16', '176', '20', '195', '128', '197', '15', '71', '71', '193', '24', '24', '24', '170', '204', '209', '111', '144', '112', '22', '222', '224', '28', '209', '155', '213', '275', '196', '239', '195', '25', '103', '149', '128', '19', '96', '102', '135', '17', '232', '155', '191', '20', '18', '241', '30', '25', '205', '97', '97', '96', '72', '182', '135', '135', '20', '149', '149', '78', '135', '40', '18', '153', '10', '26', '158', '205', '157', '157', '135', '135', '140', '155', '192', '26', '268', '263', '245', '155', '15', '177', '159', '212', '161', '239', '200', '40', '35', '153', '197', '259', '15', '254', '175', '10', '30', '239', '67', '195', '200', '191', '22', '66', '191', '112', '133', '128', '235', '161', '155', '17', '93', '36', '113', '239', '18', '18', '191', '165', '163', '163', '238', '79', '158', '40', '102', '40', '200', '191', '60', '241', '245', '96', '78', '30', '15', '173', '192', '192', '158', '64', '113', '200', '74', '128', '135', '115', '115', '10', '74', '102', '102', '115', '111', '238', '244', '109', '109', '238', '152', '10', '237', '18', '50', '16', '173', '204', '15', '208', '276', '43', '109', '258', '222', '15', '94', '235', '203', '79', '128', '113', '3', '204', '106', '133', '214', '218', '25', '98', '166', '71', '30', '175', '175', '97', '135', '108', '19', '18', '196', '35', '12', '196', '78', '166', '155', '72', '155', '22', '109', '258', '196', '263', '161', '204', '210', '14', '208', '208', '208', '208', '204', '102', '102', '213', '155', '251', '153', '153', '153', '154', '154', '237', '97', '19', '28', '94', '136', '112', '19', '97', '152', '153', '153', '111', '111', '135', '135', '105', '17', '105', '152', '152', '152', '153', '79', '266', '135', '120', '125', '149', '50', '162', '72', '17', '147', '109', '115', '24', '105', '244', '149', '238', '203', '35', '202', '177', '173', '239', '19', '19', '179', '237', '14', '18', '185', '58', '158', '144', '213', '237', '155', '250', '239', '93', '237', '97', '195', '157', '107', '109', '246', '165', '98', '49', '2', '266', '149', '20', '18', '71', '108', '22', '115', '239', '40', '178', '232', '238', '218', '118', '118', '210', '235', '46', '228', '192', '112', '149', '96', '111', '21', '95', '95', '95', '18', '57', '206', '18', '239', '157', '80', '196', '112', '176', '99', '106', '71', '235', '135', '18', '209', '78', '20', '20', '103', '33', '3', '103', '166', '79', '79', '156', '229', '18', '238', '2', '16', '128', '200', '117', '22', '197', '166', '78', '237', '241', '107', '53', '189', '204', '243', '163', '221', '15', '200', '221', '152', '23', '193', '107', '195', '99', '97', '19', '125', '276', '113', '193', '64', '78', '197', '117', '175', '135', '135', '78', '93', '154', '153', '256', '256', '135', '137', '135', '137', '149', '110', '15', '159', '21', '158', '192', '192', '117', '29', '97', '258', '78', '46', '5', '80', '15', '105', '260', '161', '191', '2', '138', '117', '215', '176', '156', '194', '265', '115', '181', '21', '14', '106', '106', '108', '200', '156', '103', '102', '29', '17', '204', '175', '227', '118', '18', '239', '204', '258', '78', '155', '213', '109', '192', '186', '186', '15', '241', '133', '30', '30', '239', '30', '218', '109', '138', '241', '241', '45', '135', '112', '232', '104', '106', '200', '53', '23', '112', '17', '117', '59', '260', '195', '106', '117', '159', '204', '200', '228', '165', '165', '261', '200', '227', '166', '93', '21', '80', '112', '152', '196', '203', '228', '98', '102', '15', '139', '110', '13', '238', '6', '171', '94', '208', '155', '112', '203', '239', '191', '39', '136', '203', '195', '46', '203', '148', '192', '210', '23', '4', '153', '29', '165', '192', '204', '232', '94', '112', '239', '98', '71', '248', '17', '155', '10', '78', '191', '191', '111', '25', '25', '19', '19', '17', '235', '102', '265', '195', '235', '163', '238', '71', '197', '107', '155', '30', '72', '19', '259', '80', '155', '23', '64', '239', '197', '156', '57', '203', '258', '200', '205', '246', '238', '258', '227', '238', '238', '19', '148', '161', '111', '105', '202', '109', '262', '133', '137', '221', '135', '52', '170', '111', '36', '30', '156', '102', '268', '115', '22', '153', '133', '259', '98', '235', '135', '191', '25', '235', '25', '29', '94', '195', '203', '212', '112', '135', '245', '260', '200', '96', '158', '23', '165', '22', '18', '249', '240', '109', '43', '37', '15', '241', '241', '23', '135', '240', '108', '98', '268', '275', '191', '136', '263', '259', '196', '98', '98', '204', '248', '123', '105', '204', '240', '9', '9', '98', '218', '189', '238', '246', '155', '130', '157', '74', '109', '29', '189', '203', '213', '195', '241', '159', '238', '204', '218', '105', '23', '16', '263', '206', '150', '152', '204', '202', '173', '267', '18', '153', '155', '196', '2', '97', '158', '158', '204', '98', '202', '98', '204', '258', '98', '22', '113', '113', '135', '71', '237', '155', '163', '250', '233', '26', '26', '26', '26', '26', '19', '170', '18', '15', '19', '191', '30', '30', '48', '35', '30', '237', '46', '119', '16', '78', '239', '150', '245', '235', '158', '27', '241', '30', '38', '107', '75', '241', '135', '106', '106', '98', '159', '96', '135', '111', '176', '175', '200', '18', '14', '242', '163', '187', '20', '12', '191', '23', '106', '171', '25', '78', '162', '191', '157', '94', '237', '246', '203', '108', '238', '204', '204', '1', '102', '154', '106', '170', '102', '200', '2', '135', '106', '258', '258', '227', '26', '15', '25', '71', '203', '96', '3', '241', '200', '30', '108', '98', '108', '27', '106', '26', '150', '259', '78', '113', '113', '158', '202', '16', '136', '239', '156', '239', '35', '276', '252', '115', '22', '22', '258', '172', '3', '154', '3', '98', '237', '102', '235', '112', '206', '98', '57', '118', '153', '18', '26', '30', '196', '163', '191', '153', '72', '24', '156', '117', '122', '30', '135', '192', '98', '28', '30', '30', '95', '241', '155', '17', '195', '31', '103', '202', '98', '237', '94', '112', '163', '163', '94', '163', '112', '19', '195', '191', '15', '135', '78', '109', '160', '152', '97', '98', '107', '167', '97', '138', '138', '26', '191', '97', '97', '30', '30', '206', '202', '13', '30', '240', '111', '261', '79', '0', '111', '19', '152', '107', '107', '18', '98', '28', '258', '244', '106', '237', '204', '191', '15', '183', '202', '16', '111', '166', '268', '202', '12', '30', '78', '110', '133', '71', '197', '111', '52', '79', '171', '156', '94', '13', '118', '111', '71', '133', '18', '105', '30', '22', '133', '60', '237', '191', '6', '126', '3', '238', '276', '78', '162', '150', '115', '191', '107', '200', '148', '135', '72', '155', '241', '4', '108', '160', '203', '20', '133', '2', '80', '80', '154', '18', '15', '115', '239', '167', '167', '35', '268', '18', '221', '18', '195', '204', '195', '216', '133', '112', '104', '223', '191', '115', '30', '30', '30', '30', '30', '202', '204', '165', '200', '200', '29', '191', '237', '267', '18', '175', '81', '264', '264', '178', '115', '102', '178', '212', '273', '202', '109', '259', '166', '135', '155', '219', '161', '18', '94', '241', '191', '196', '191', '98', '26', '26', '204', '204', '135', '27', '238', '260', '182', '26', '98', '94', '93', '238', '156', '19', '111', '79', '16', '192', '113', '19', '241', '18', '202', '260', '272', '106', '163', '156', '106', '17', '109', '221', '78', '203', '7', '136', '197', '193', '191', '98', '112', '200', '95', '221', '98', '161', '191', '3', '106', '189', '111', '25', '196', '196', '241', '163', '200', '24', '78', '176', '3', '98', '102', '98', '19', '161', '224', '80', '18', '191', '112', '105', '254', '71', '200', '137', '29', '20', '165', '8', '106', '23', '106', '155', '200', '265', '135', '126', '203', '188', '161', '158', '258', '255', '98', '203', '50', '35', '102', '158', '175', '40', '239', '26', '66', '57', '193', '155', '15', '135', '10', '107', '106', '229', '18', '191', '204', '107', '99', '235', '238', '18', '205', '205', '35', '238', '208', '203', '110', '110', '109', '112', '23', '18', '19', '182', '156', '18', '18', '156', '111', '135', '137', '133', '137', '240', '99', '197', '113', '15', '165', '39', '97', '97', '15', '150', '163', '109', '268', '15', '15', '238', '203', '67', '161', '197', '18', '196', '97', '161', '197', '161', '185', '238', '191', '30', '121', '133', '160', '160', '74', '135', '75', '54', '135', '196', '23', '241', '166', '194', '239', '135', '240', '208', '238', '98', '135', '152', '176', '22', '174', '113', '241', '237', '57', '195', '191', '24', '165', '165', '204', '204', '56', '109', '27', '153', '252', '228', '165', '252', '200', '28', '158', '135', '196', '22', '18', '133', '30', '204', '237', '196', '93', '106', '94', '25', '93', '159', '4', '195', '23', '237', '25', '199', '57', '26', '276', '274', '204', '75', '18', '204', '108', '22', '133', '191', '111', '204', '203', '133', '237', '166', '4', '62', '202', '166', '161', '165', '74', '109', '118', '200', '22', '206', '25', '209', '17', '155', '204', '64', '23', '30', '202', '149', '171', '135', '136', '184', '26', '18', '159', '195', '18', '166', '156', '98', '133', '38', '40', '18', '18', '166', '109', '109', '209', '22', '195', '163', '163', '108', '18', '15', '99', '135', '174', '26', '19', '166', '133', '259', '151', '235', '204', '228', '133', '30', '161', '111', '242', '84', '238', '98', '195', '78', '160', '28', '25', '264', '165', '41', '19', '55', '46', '202', '121', '15', '32', '98', '238', '0', '78', '31', '30', '203', '264', '203', '135', '104', '111', '78', '212', '111', '98', '126', '22', '2', '30', '237', '212', '265', '163', '106', '106', '235', '109', '223', '161', '202', '150', '170', '255', '112', '114', '170', '98', '96', '197', '36', '18', '228', '14', '255', '204', '102', '239', '0', '185', '30', '22', '23', '246', '199', '133', '191', '78', '195', '153', '112', '191', '200', '170', '171', '200', '78', '128', '25', '153', '237', '99', '94', '208', '107', '208', '208', '78', '204', '204', '27', '191', '272', '134', '158', '158', '174', '118', '64', '18', '258', '150', '189', '18', '153', '241', '204', '112', '106', '135', '71', '148', '7', '133', '140', '187', '23', '223', '19', '19', '2', '136', '2', '200', '161', '15', '98', '158', '148', '3', '250', '153', '235', '165', '220', '176', '110', '240', '151', '148', '133', '74', '193', '46', '27', '173', '94', '176', '105', '256', '161', '226', '18', '78', '191', '171', '243', '26', '0', '108', '167', '159', '239', '150', '112', '155', '79', '241', '202', '165', '254', '107', '202', '156', '166', '193', '239', '135', '15', '203', '187', '148', '25', '15', '22', '194', '239', '56', '191', '176', '262', '200', '110', '72', '99', '102', '135', '98', '113', '15', '15', '159', '262', '171', '151', '98', '204', '213', '255', '26', '243', '175', '156', '98', '107', '191', '165', '241', '26', '165', '171', '30', '160', '166', '266', '11', '19', '211', '93', '254', '30', '239', '115', '273', '205', '112', '237', '263', '74', '98', '79', '122', '94', '265', '155', '191', '16', '235', '229', '98', '191', '227', '17', '111', '267', '135', '202', '195', '10', '180', '196', '196', '133', '135', '133', '95', '111', '122', '258', '191', '3', '114', '107', '19', '264', '74', '56', '200', '200', '133', '161', '97', '15', '97', '197', '196', '253', '93', '202', '258', '106', '153', '26', '109', '40', '149', '8', '196', '15', '119', '155', '23', '237', '193', '264', '148', '151', '238', '200', '94', '75', '148', '175', '24', '195', '171', '30', '203', '200', '202', '16', '158', '223', '191', '22', '264', '135', '35', '57', '94', '191', '196', '202', '98', '18', '196', '3', '133', '78', '191', '208', '258', '18', '200', '29', '135', '109', '110', '2', '185', '98', '133', '19', '22', '204', '95', '238', '25', '133', '250', '135', '135', '3', '47', '158', '187', '2', '78', '115', '155', '3', '218', '166', '112', '266', '22', '195', '135', '72', '97', '197', '209', '127', '80', '2', '98', '14', '239', '15', '113', '165', '204', '78', '204', '191', '175', '200', '193', '18', '196', '133', '109', '237', '56', '196', '3', '163', '163', '163', '195', '15', '263', '18', '84', '18', '133', '107', '18', '102', '156', '238', '102', '26', '244', '82', '75', '23', '258', '238', '136', '3', '204', '184', '200', '196', '158', '106', '27', '238', '155', '152', '50', '50', '2', '235', '98', '203', '166', '109', '25', '18', '151', '133', '200', '26', '26', '26', '19', '107', '156', '23', '26', '3', '70', '225', '41', '26', '239', '26', '186', '26', '34', '272', '272', '104', '78', '260', '18', '204', '26', '78', '18', '210', '135', '203', '71', '207', '26', '133', '13', '25', '26', '115', '94', '264', '135', '244', '79', '30', '263', '22', '109', '205', '176', '106', '22', '17', '151', '106', '191', '160', '160', '165', '78', '177', '196', '260', '166', '35', '191', '260', '98', '18', '237', '18', '161', '18', '197', '133', '30', '35', '200', '170', '155', '245', '19', '200', '160', '96', '172', '165', '55', '137', '241', '133', '191', '112', '25', '126', '238', '18', '18', '108', '135', '161', '135', '73', '112', '107', '8', '112', '109', '195', '25', '30', '105', '161', '241', '110', '75', '238', '215', '238', '104', '25', '3', '155', '30', '16', '3', '175', '99', '3', '200', '23', '3', '199', '203', '191', '196', '196', '204', '98', '105', '133', '197', '135', '161', '39', '15', '30', '133', '55', '191', '203', '133', '203', '140', '200', '149', '30', '96', '135', '23', '26', '3', '18', '155', '158', '135', '135', '18', '250', '18', '22', '114', '18', '18', '166', '18', '30', '18', '239', '18', '161', '50', '50', '49', '51', '23', '55', '98', '78', '210', '211', '18', '239', '78', '13', '193', '114', '78', '18', '25', '133', '19', '176', '4', '173', '50', '108', '17', '4', '204', '94', '112', '207', '109', '115', '23', '235', '197', '194', '197', '213', '148', '40', '157', '107', '200', '147', '14', '14', '18', '238', '18', '238', '18', '238', '57', '200', '135', '30', '18', '191', '191', '200', '166', '151', '161', '73', '19', '73', '239', '239', '165', '108', '2', '93', '166', '133', '133', '173', '35', '193', '114', '74', '19', '40', '58', '104', '16', '260', '160', '158', '112', '210', '158', '167', '203', '203', '3', '3', '109', '30', '133', '258', '70', '204', '200', '152', '155', '192', '204', '108', '191', '204', '26', '98', '26', '98', '264', '158', '18', '150', '137', '153', '196', '199', '191', '115', '168', '157', '149', '151', '78', '137', '160', '259', '166', '196', '171', '175', '199', '166', '161', '70', '13', '166', '152', '152', '206', '58', '18', '207', '58', '74', '207', '29', '46', '109', '135', '74', '174', '109', '40', '10', '30', '258', '16', '111', '30', '15', '239', '204', '48', '25', '196', '0', '156', '72', '72', '96', '22', '72', '133', '133', '238', '26', '3', '50', '152', '193', '235', '136', '153', '50', '1', '19', '239', '30', '25', '179', '161', '30', '191', '3', '27', '133', '3', '40', '23', '133', '150', '151', '265', '32', '191', '3', '25', '170', '238', '40', '259', '166', '137', '241', '133', '209', '159', '78', '78', '153', '26', '52', '241', '36', '250', '96', '30', '30', '38', '18', '18', '238', '99', '72', '111', '3', '244', '191', '226', '3', '191', '114', '135', '109', '40', '79', '202', '78', '114', '238', '10', '150', '108', '163', '108', '113', '197', '109', '115', '161', '3', '238', '3', '237', '161', '107', '155', '78', '202', '112', '38', '195', '137', '250', '133', '237', '161', '25', '26', '25', '15', '15', '239', '166', '114', '229', '111', '40', '135', '94', '102', '162', '204', '237', '17', '166', '136', '22', '98', '99', '99', '166', '35', '196', '3', '203', '135', '135', '84', '98', '266', '238', '13', '166', '78', '193', '98', '241', '104', '153', '192', '200', '10', '95', '95', '148', '107', '14', '17', '191', '239', '19', '203', '255', '197', '213', '94', '3', '107', '23', '35', '10', '237', '235', '78', '29', '18', '160', '200', '150', '99', '109', '155', '161', '133', '114', '18', '18', '18', '264', '133', '104', '161', '70', '17', '204', '195', '204', '241', '99', '82', '241', '241', '161', '18', '235', '175', '2', '2', '202', '50', '230', '26', '40', '40', '200', '78', '156', '26', '112', '26', '30', '71', '184', '200', '135', '153', '23', '106', '125', '175', '133', '133', '7', '38', '196', '161', '0', '49', '241', '158', '15', '21', '104', '2', '3', '176', '259', '135', '78', '75', '112', '200', '1', '207', '238', '161', '150', '158', '2', '45', '31', '237', '149', '3', '26', '165', '135', '152', '115', '133', '18', '40', '166', '30', '173', '40', '26', '78', '50', '48', '50', '13', '45', '200', '200', '18', '151', '205', '135', '38', '117', '196', '135', '155', '111', '196', '196', '166', '23', '25', '161', '18', '124', '259', '165', '58', '170', '204', '250', '103', '160', '238', '194', '163', '241', '255', '235', '17', '133', '26', '135', '196', '237', '114', '258', '193', '248', '40', '195', '84', '238', '146', '199', '40', '161', '22', '137', '43', '166', '156', '4', '104', '133', '177', '211', '193', '78', '203', '246', '237', '264', '3', '149', '78', '78', '111', '239', '13', '238', '176', '112', '150', '156', '3', '6', '118', '178', '178', '21', '186', '150', '241', '50', '243', '191', '121', '278', '29', '200', '18', '3', '197', '191', '98', '173', '195', '25', '195', '264', '221', '15', '156', '25', '200', '211', '195', '26', '35', '2', '135', '126', '113', '167', '191', '264', '196', '133', '18', '26', '146', '203', '219', '23', '160', '160', '135', '259', '3', '159', '120', '160', '106', '240', '133', '115', '258', '38', '22', '3', '26', '22', '107', '49', '48', '25', '237', '72', '219', '196', '196', '111', '135', '196', '18', '166', '172', '135', '26', '258', '70', '98', '40', '166', '23', '96', '3', '30', '155', '126', '146', '114', '242', '237', '157', '238', '241', '40', '133', '200', '161', '175', '200', '175', '84', '266', '18', '99', '30', '242', '151', '153', '166', '199', '228', '3', '237', '132', '110', '135', '80', '15', '15', '15', '84', '133', '15', '177', '3', '202', '255', '71', '115', '235', '19', '18', '18', '102', '58', '78', '165', '166', '264', '12', '150', '15', '2', '18', '135', '109', '200', '258', '128', '3', '199', '197', '239', '109', '222', '239', '107', '125', '132', '204', '204', '133', '135', '30', '158', '203', '15', '264', '237', '133', '196', '3', '252', '26', '98', '79', '199', '210', '0', '99', '30', '26', '259', '6', '49', '166', '98', '108', '9', '133', '102', '155', '98', '114', '78', '35', '166', '188', '202', '235', '30', '149', '165', '176', '138', '29', '153', '25', '25', '137', '133', '166', '135', '125', '71', '193', '154', '165', '98', '23', '56', '198', '26', '26', '0', '109', '136', '163', '151', '25', '15', '241', '195', '204', '191', '195', '161', '175', '133', '237', '78', '165', '258', '135', '78', '135', '258', '18', '235', '30', '187', '150', '275', '149', '2', '36', '112', '22', '161', '238', '203', '112', '265', '75', '133', '205', '258', '158', '18', '26', '18', '25', '26', '15', '26', '239', '15', '240', '18', '71', '71', '78', '135', '3', '148', '166', '26', '176', '2', '197', '150', '176', '19', '18', '97', '166', '18', '153', '250', '133', '210', '20', '192', '196', '26', '191', '2', '81', '70', '5', '79', '3', '30', '193', '193', '69', '160', '176', '246', '108', '54', '250', '135', '200', '80', '3', '167', '18', '166', '200', '152', '23', '149', '19', '14', '13', '237', '196', '19', '112', '254', '204', '191', '23', '264', '199', '209', '137', '31', '135', '238', '78', '18', '191', '15', '164', '195', '74', '163', '163', '41', '99', '26', '135', '26', '40', '40', '250', '200', '200', '26', '237', '4', '195', '158', '35', '71', '191', '3', '218', '274', '58', '3', '23', '177', '132', '49', '62', '238', '166', '97', '264', '193', '29', '26', '49', '241', '197', '200', '64', '227', '258', '44', '15', '133', '175', '238', '109', '254', '237', '198', '175', '166', '213', '174', '196', '107', '259', '98', '10', '196', '210', '191', '96', '166', '135', '15', '197', '78', '78', '195', '176', '18', '98', '129', '170', '18', '247', '203', '26', '158', '40', '64', '200', '163', '196', '19', '205', '197', '193', '191', '69', '96', '74', '35', '36', '64', '133', '172', '19', '266', '96', '94', '103', '258', '193', '264', '98', '254', '50', '193', '25', '170', '75', '84', '27', '252', '203', '94', '94', '19', '98', '137', '12', '153', '3', '155', '135', '238', '15', '196', '15', '26', '84', '49', '27', '135', '203', '48', '133', '129', '133', '258', '155', '35', '195', '71', '241', '135', '166', '193', '165', '104', '196', '117', '196', '26', '237', '109', '95', '196', '60', '40', '158', '193', '250', '104', '94', '178', '27', '193', '78', '106', '79', '204', '133', '237', '235', '135', '202', '70', '15', '264', '241', '254', '110', '199', '49', '255', '30', '193', '238', '204', '102', '97', '161', '43', '228', '1', '6', '196', '193', '26', '15', '128', '46', '155', '32', '133', '69', '47', '135', '78', '205', '200', '200', '202', '3', '95', '191', '78', '26', '248', '258', '191', '240', '49', '110', '38', '193', '200', '200', '96', '200', '204', '135', '9', '200', '258', '15', '46', '15', '149', '155', '197', '2', '166', '151', '72', '135', '97', '211', '238', '135', '163', '161', '26', '163', '129', '241', '239', '241', '16', '26', '18', '263', '260', '166', '3', '264', '195', '79', '132', '251', '50', '133', '15', '25', '15', '191', '15', '78', '50', '78', '71', '227', '69', '200', '69', '109', '64', '133', '197', '196', '3', '45', '238', '26', '132', '104', '196', '193', '136', '80', '26', '75', '117', '14', '78', '75', '98', '15', '2', '15', '50', '255', '136', '3', '112', '173', '69', '191', '19', '239', '235', '109', '70', '135', '78', '165', '238', '262', '199', '35', '203', '224', '193', '15', '18', '150', '238', '197', '238', '193', '135', '3', '150', '187', '199', '239', '109', '152', '27', '257', '6', '235', '104', '191', '133', '166', '155', '26', '16', '136', '198', '98', '154', '235', '27', '102', '30', '191', '15', '132', '132', '114', '107', '241', '241', '156', '199', '241', '49', '62', '117', '258', '30', '163', '133', '238', '200', '3', '15', '109', '2', '196', '165', '166', '159', '210', '35', '15', '17', '200', '241', '106', '203', '197', '197', '79', '98', '98', '23', '204', '48', '129', '57', '258', '202', '215', '238', '70', '64', '193', '189', '57', '200', '0', '98', '135', '204', '25', '30', '204', '27', '26', '78', '135', '109', '3', '197', '3', '167', '35', '196', '225', '2', '194', '200', '15', '161', '198', '23', '133', '238', '195', '35', '50', '135', '152', '163', '21', '15', '94', '26', '94', '53', '260', '196', '154', '277', '200', '118', '6', '238', '50', '209', '50', '3', '238', '3', '15', '107', '135', '199', '15', '135', '26', '258', '251', '203', '19', '259', '195', '199', '238', '15', '254', '112', '211', '200', '129', '84', '196', '3', '208', '259', '135', '104', '29', '136', '31', '78', '13', '3', '64', '75', '55', '114', '160', '96', '14', '18', '207', '75', '48', '15', '49', '97', '117', '196', '99', '258', '195', '264', '264', '78', '197', '30', '238', '109', '158', '32', '195', '149', '227', '65', '167', '162', '204', '250', '151', '151', '102', '26', '29', '26', '193', '74', '50', '193', '186', '15', '74', '259', '57', '188', '277', '277', '195', '195', '212', '78', '197', '64', '129', '259', '200', '199', '129', '4', '235', '197', '196', '161', '3', '255', '199', '102', '3', '13', '94', '53', '98', '15', '129', '15', '155', '71', '179', '50', '29', '136', '80', '254', '198', '162', '237', '3', '18', '260', '238', '117', '179', '156', '19', '45', '197', '78', '199', '102', '102', '237', '155', '69', '69', '163', '239', '15', '196', '80', '191', '30', '71', '158', '259', '2', '106', '148', '15', '135', '135', '135', '196', '19', '123', '15', '104', '19', '128', '230', '69', '98', '197', '241', '161', '195', '200', '3', '98', '78', '165', '163', '19', '196', '31', '173', '203', '55', '69', '209', '239', '79', '158', '254', '135', '51', '94', '162', '255', '95', '18', '133', '109', '50', '94', '199', '15', '197', '200', '133', '116', '50', '196', '161', '69', '133', '3', '258', '3', '197', '29', '3', '157', '197', '193', '4', '94', '195', '17', '48', '152', '8', '15', '155', '133', '195', '136', '150', '102', '237', '203', '164', '164', '250', '154', '13', '191', '58', '94', '70', '93', '264', '15', '30', '133', '237', '267', '199', '193', '76', '241', '157', '26', '200', '109', '179', '55', '98', '200', '129', '238', '235', '208', '109', '239', '258', '196', '193', '18', '107', '210', '70', '40', '129', '259', '193', '30', '158', '252', '40', '210', '78', '15', '78', '96', '208', '113', '258', '64', '80', '15', '133', '241', '15', '15', '258', '135', '109', '251', '251', '109', '74', '196', '170', '240', '254', '258', '204', '199', '26', '109', '197', '57', '45', '161', '57', '114', '3', '15', '264', '106', '57', '27', '18', '258', '29', '84', '161', '135', '114', '24', '30', '107', '107', '172', '209', '170', '18', '19', '133', '191', '195', '135', '156', '196', '94', '3', '84', '225', '199', '238', '106', '104', '70', '50', '12', '155', '114', '176', '237', '200', '108', '70', '240', '196', '19', '22', '276', '199', '238', '205', '104', '193', '108', '115', '259', '102', '19', '258', '18', '106', '161', '191', '114', '64', '15', '204', '35', '155', '40', '238', '136', '264', '133', '186', '15', '258', '166', '26', '6', '114', '114', '133', '114', '114', '13', '13', '13', '237', '250', '245', '136', '78', '13', '48', '18', '13', '25', '26', '93', '125', '50', '94', '148', '78', '194', '200', '25', '93', '274', '266', '70', '22', '3', '208', '240', '211', '135', '31', '18', '153', '18', '2', '15', '15', '15', '156', '15', '70', '73', '173', '13', '112', '69', '204', '70', '248', '135', '70', '3', '193', '193', '22', '162', '238', '57', '98', '15', '199', '199', '52', '151', '73', '5', '108', '23', '69', '95', '220', '106', '163', '152', '163', '108', '15', '58', '250', '19', '177', '245', '264', '93', '204', '25', '78', '155', '69', '155', '239', '241', '155', '200', '18', '3', '13', '196', '197', '204', '272', '95', '135', '165', '22', '210', '35', '69', '18', '170', '3', '25', '156', '78', '135', '102', '107', '255', '19', '70', '70', '109', '198', '156', '64', '170', '138', '78', '203', '166', '108', '114', '179', '228', '74', '49', '14', '264', '26', '195', '78', '107', '75', '18', '106', '151', '70', '58', '15', '196', '238', '109', '104', '104', '109', '136', '148', '210', '3', '161', '15', '104', '238', '208', '133', '108', '241', '18', '209', '74', '156', '114', '199', '174', '35', '18', '170', '135', '238', '244', '155', '162', '197', '3', '98', '227', '267', '238', '193', '50', '238', '98', '112', '124', '25', '114', '241', '159', '154', '78', '161', '136', '114', '153', '225', '154', '13', '50', '11', '25', '15', '209', '11', '166', '203', '259', '237', '22', '114', '276', '98', '278', '150', '198', '18', '15', '78', '15', '162', '26', '3', '196', '3', '102', '189', '71', '69', '16', '18', '152', '133', '135', '162', '241', '15', '104', '15', '259', '202', '2', '238', '207', '15', '204', '30', '215', '112', '210', '42', '96', '193', '15', '110', '163', '211', '78', '108', '15', '31', '27', '15', '31', '196', '195', '96', '15', '161', '94', '193', '195', '198', '98', '261', '258', '76', '15', '104', '258', '135', '18', '15', '202', '258', '108', '156', '13', '22', '251', '199', '155', '258', '211', '3', '207', '202', '259', '107', '202', '259', '135', '135', '148', '158', '154', '99', '99', '3', '207', '94', '25', '162', '31', '241', '25', '15', '114', '18', '114', '148', '19', '15', '203', '191', '19', '13', '193', '204', '25', '166', '155', '196', '98', '162', '197', '29', '152', '191', '133', '199', '230', '40', '25', '204', '79', '3', '19', '165', '204', '108', '208', '3', '251', '274', '15', '166', '264', '35', '13', '193', '114', '258', '13', '19', '133', '23', '17', '199', '204', '153', '250', '204', '16', '12', '107', '191', '18', '3', '15', '165', '15', '69', '17', '165', '17', '15', '15', '156', '23', '23', '15', '197', '104', '197', '26', '208', '135', '211', '250', '18', '204', '25', '196', '53', '69', '104', '104', '19', '78', '241', '230', '30', '195', '96', '26', '109', '196', '69', '238', '15', '18', '133', '26', '106', '109', '98', '15', '258', '274', '193', '203', '58', '57', '274', '2', '202', '161', '136', '135', '155', '241', '35', '151', '137', '5', '114', '96', '16', '209', '158', '15', '266', '161', '15', '25', '274', '202', '3', '161', '237', '267', '204', '125', '15', '151', '155', '259', '151', '210', '238', '15', '26', '132', '50', '173', '197', '114', '241', '15', '258', '109', '166', '274', '13', '191', '15', '26', '164', '258', '163', '15', '264', '11', '260', '148', '150', '207', '196', '98', '272', '133', '196', '107', '26', '191', '163', '191', '135', '3', '18', '18', '119', '69', '196', '15', '156', '135', '112', '258', '155', '256', '199', '204', '212', '18', '135', '254', '72', '258', '254', '15', '133', '18', '259', '193', '258', '204', '40', '18', '29', '155', '104', '107', '274', '241', '25', '26', '69', '148', '166', '46', '94', '135', '15', '155', '104', '241', '32', '274', '249', '15', '197', '104', '44', '259', '69', '191', '15', '198', '193', '133', '17', '258', '248', '166', '197', '200', '250', '204', '3', '225', '238', '163', '69', '15', '15', '26', '202', '19', '209', '78', '212', '15', '78', '243', '110', '191', '15', '18', '107', '38', '208', '166', '239', '237', '199', '15', '204', '258', '173', '172', '200', '274', '243', '19', '191', '135', '203', '69', '149', '148', '196', '256', '204', '97', '191', '26', '114', '274', '200', '69', '69', '64', '69', '239', '25', '274', '203', '78', '135', '196', '202', '17', '2', '2', '3', '98', '98', '62', '2', '95', '96', '2', '207', '78', '200', '26', '64', '18', '93', '49', '57', '154', '200', '69', '94', '98', '98', '56', '56', '166', '98', '212', '226', '226', '15', '69', '15', '273', '272', '104', '193', '3', '98', '199', '98', '25', '125', '259', '249', '108', '40', '26', '198', '218', '73', '148', '69', '204', '241', '35', '3', '274', '70', '18', '71', '35', '172', '133', '106', '69', '107', '199', '219', '19', '133', '69', '135', '109', '57', '191', '135', '252', '104', '3', '94', '3', '72', '126', '16', '36', '84', '241', '3', '218', '98', '108', '46', '19', '196', '118', '104', '29', '238', '69', '10', '204', '264', '259', '124', '242', '107', '133', '25', '64', '30', '194', '259', '159', '264', '274', '52', '200', '104', '29', '30', '193', '98', '94', '114', '204', '40', '18', '198', '97', '3', '99', '274', '239', '177', '15', '18', '126', '126', '208', '148', '15', '104', '29', '36', '218', '70', '15', '23', '278', '23', '36', '191', '210', '51', '245', '167', '72', '237', '25', '203', '36', '196', '193', '196', '15', '151', '207', '203', '103', '166', '135', '110', '27', '13', '27', '191', '98', '57', '166', '50', '21', '15', '197', '23', '2', '70', '69', '193', '274', '104', '166', '258', '126', '238', '161', '18', '193', '69', '140', '170', '197', '204', '165', '203', '170', '15', '58', '35', '83', '212', '150', '73', '25', '15', '64', '133', '210', '108', '79', '193', '15', '126', '165', '15', '208', '98', '3', '200', '126', '3', '40', '165', '98', '69', '15', '165', '133', '96', '173', '203', '133', '238', '248', '15', '7', '38', '207', '108', '165', '199', '18', '162', '27', '3', '72', '65', '250', '209', '166', '64', '209', '237', '212', '64', '225', '120', '108', '203', '135', '248', '69', '251', '135', '204', '160', '160', '135', '148', '15', '15', '23', '207', '274', '29', '28', '194', '150', '40', '104', '208', '125', '70', '18', '3', '19', '102', '104', '23', '29', '207', '133', '204', '111', '2', '125', '126', '7', '194', '209', '203', '83', '126', '126', '258', '119', '162', '241', '133', '18', '208', '197', '209', '15', '78', '80', '110', '237', '104', '30', '208', '103', '15', '200', '15', '238', '254', '108', '173', '40', '69', '175', '17', '155', '136', '26', '3', '135', '13', '241', '161', '166', '170', '162', '133', '53', '211', '204', '15', '161', '238', '110', '18', '191', '18', '50', '71', '30', '17', '208', '248', '203', '203', '196', '210', '258', '204', '3', '18', '196', '125', '173', '199', '249', '18', '104', '165', '241', '2', '193', '238', '78', '15', '166', '15', '126', '3', '237', '161', '52', '207', '3', '3', '50', '56', '22', '56', '69', '93', '135', '207', '69', '99', '3', '133', '68', '133', '161', '108', '126', '250', '60', '155', '98', '18', '16', '78', '78', '240', '235', '176', '22', '199', '160', '235', '274', '209', '64', '173', '137', '128', '25', '18', '160', '203', '70', '165', '193', '110', '207', '64', '85', '252', '78', '51', '2', '99', '19', '22', '198', '204', '166', '166', '151', '56', '133', '210', '26', '149', '109', '30', '209', '209', '51', '3', '193', '119', '208', '93', '165', '248', '55', '203', '204', '58', '18', '199', '211', '83', '32', '135', '212', '156', '19', '235', '173', '191', '209', '203', '146', '208', '58', '158', '102', '199', '200', '200', '173', '70', '52', '107', '211', '213', '237', '196', '205', '104', '135', '258', '255', '96', '19', '124', '128', '57', '199', '98', '166', '102', '40', '15', '238', '3', '148', '225', '126', '135', '114', '193', '204', '15', '209', '209', '193', '259', '64', '161', '26', '274', '126', '18', '196', '0', '60', '207', '17', '71', '218', '239', '248', '38', '154', '61', '128', '199', '196', '173', '25', '46', '98', '258', '108', '3', '108', '196', '93', '93', '99', '172', '69', '241', '196', '30', '135', '278', '109', '18', '5', '209', '57', '159', '70', '193', '163', '249', '196', '196', '72', '103', '133', '17', '204', '199', '166', '202', '29', '57', '193', '150', '98', '119', '25', '205', '18', '173', '166', '209', '57', '25', '58', '258', '26', '15', '15', '101', '26', '74', '1', '4', '74', '263']\n",
            "     filenum                                               text  label\n",
            "0          1  only had this sound-echo-time method been long...    209\n",
            "1          2  has no such implied limitation. In common unde...     63\n",
            "2          3  to apply its conclusion to Champlin. The contr...    216\n",
            "3          4  size of the reservation; in Congress by statut...    108\n",
            "4          5  to them.' Consequently, the Government cannot ...    196\n",
            "...      ...                                                ...    ...\n",
            "7327    8413  process of law secured by the Fourteenth Amend...    101\n",
            "7328    8414  other shootings and repeatedly expressed an in...     26\n",
            "7329    8417  U.S.C. (b), the United States in April asked l...      4\n",
            "7330    8418  AAA pursuant to agreement of the parties. That...     74\n",
            "7331    8419  Bd. v. Corvallis Sand & Gravel Co., . In the e...    263\n",
            "\n",
            "[7332 rows x 3 columns]\n",
            "0       209\n",
            "1        63\n",
            "2       216\n",
            "3       108\n",
            "4       196\n",
            "       ... \n",
            "7327    101\n",
            "7328     26\n",
            "7329      4\n",
            "7330     74\n",
            "7331    263\n",
            "Name: label, Length: 7332, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "fh = open('sc_model_1.txt')\n",
        "filenums=[]\n",
        "records=[]\n",
        "for record in fh:\n",
        "    record=record.split(\"---\")\n",
        "    filenum=record[0]\n",
        "    modelnum=record[1]\n",
        "    text=record[2]\n",
        "    filenums.append(filenum)\n",
        "    records.append(text)\n",
        "    \n",
        "print(len(filenums))\n",
        "print(len(records))\n",
        "\n",
        "summarized_data = pd.DataFrame(list(zip(filenums, records)),columns =['filenum','text'])\n",
        "len_list = [len(ele.split()) for ele in records]\n",
        "res = 0 if len(len_list) == 0 else (float(sum(len_list)) / len(len_list))\n",
        "print(\"Average Length %s\" % res)\n",
        "\n",
        "#temp_file = open(\"labels_sc.txt\", \"r\")\n",
        "temp_file = open(\"labels_sc_279.txt\", \"r\")\n",
        "\n",
        "data = temp_file.read()\n",
        "\n",
        "label_list = data.split(\"\\n\")\n",
        "print(label_list)\n",
        "label_list = label_list[0:-1]\n",
        "print(label_list)\n",
        "label_list = [int(i) for i in label_list]\n",
        "temp_file.close()\n",
        "\n",
        "\n",
        "new_label_list=[]\n",
        "for num in filenums:\n",
        "  new_label_list.append(label_list[int(num)-1])\n",
        "summarized_data['label'] = new_label_list\n",
        "\n",
        "print(summarized_data)\n",
        "print(summarized_data['label'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQbAYvIIvxo4",
        "outputId": "6e866b2f-361d-4288-8613-baded711b342"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_1[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_roberta_model[0][0]']       \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 279)          143127      ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[209  63 216 ...   4  74 263]\n",
            "[   1    2    3 ... 8417 8418 8419]\n",
            "[6622   35 6713 4989 8233 7444 5061 3942 8344  454 5464  317 2883 4515\n",
            " 3001 3410 6991 2875 1125 3885 6647 4857 4504  214 3230 8340 2969 3847\n",
            " 4609 5092 1346 5993 7358 2752 7651 7301 8188 7073 4782 6069 2346 6445\n",
            " 6483  542 1663 4952 5072 1967 4071 7590  860 6418 5030 7297  626 5463\n",
            " 6029 6302 3819 7256 2950 5667 1235 3993 1439 1323 4263 3835 2134 2624\n",
            " 5801 4293 8129  320 5866 3247 5777   19 6059 6520 4837 3843 5695 1666\n",
            "  858 1418 5409 5274 6205 3167 2356 8418 5997 6685  223 3444 3310 7721\n",
            " 5928 2360 7197 4252 8343  909 4093 4354 4752 1421  649  829 1916 1003\n",
            " 3488 2496 5765 6612 4004 4052 1403 6697 5268 1010 6293 6380 4602 6057\n",
            " 1851 4577 4236 7978 1350 5083 6552 1058  361 7706 4186 2590 4452 4396\n",
            "  749  870 8351 3834  812 5108  463 1743 6600 8264 8064 7121 8279 5556\n",
            " 1845 5119 2516 4673 7947 8215 1882 3350 7617 7152 5199 4282 1092 3342\n",
            " 8003 5025 5051  535  933 3969 1927 3615 1221 8062 1489 3296 7442 5459\n",
            " 2031 7351  234 2313 8015 7416 7921 6574 6257 8396  941 4520 3300 5640\n",
            "  240 7810 7425 1321 6580  928 1170 5404 8273 6742 7005   94 7459    9\n",
            " 7861 7975 1018 5029 2322 2140 1415 7061 7767 4990 8124 5927 6051 3269\n",
            " 6916 5959 2846 6818 7052 7775 7582 8116 4240 4767 5248  430 1812 2175\n",
            " 6055 8331 3916  998 7271 1564 7326 1077 1559  292 3065 6764 6854 4915\n",
            " 5552 3583 1917  868 5902 4229 2144 7565 3712 7224 4502 1406 1651 6256\n",
            " 3827 6525 6318 2686 3616 5369 8235 1704 1011 3351 8089 8021  102  224\n",
            " 5671 8294 1510 7707 2985 6624 5146 1428 7381 3709 6824 6308  701 2281\n",
            " 6837 5436 1741 3520 5995 2649 4886  966 2863 5476 4058 1246 2406 1199\n",
            " 8387  513 5449 1028 2226 6166 4387  350 6123  678 5236 5041  734 3412\n",
            " 6085  587  489 5760 3163 3376 4982 5115 4655 5575 3839 6743 7173 5724\n",
            " 7689 7787  520  713 3214 8302 3150 5713 2247 8128 4607 6660 3505 4610\n",
            " 2866 4161 6905 7096  768 1062 4411 6087 2096   21 4903 7546 6630 2928\n",
            " 4640 7125 1792 2063 1700 2305 7928 8095  737 5849  922 3423 2359 6015\n",
            " 4489 4361 6081 6346 5014 5167 1023 8009 3335  268 4777 5221 4323 6168\n",
            "  759  711 5702 6129 7181 1860 7192 1347 1692 5467 3378 6579   67 2693\n",
            " 6134 4841 4270 3073 8397 4611 2943 6746 7696 3466 3950 4693 6125 8370\n",
            " 6142 6131  995 5548 2186 2112 1721 5067 4015 5587 7092 3156   25 1749\n",
            " 5554 4878 1197 8248  458 8035 6836 1006 5310 2698 6960 2339 1650 6227\n",
            " 4104 5130 7089 6993 2400 1289 4305 5585 2524 7933 6284 7057 5324 3807\n",
            " 5790 8347 2390 5280 2841 7220 1384 7693 8090 2671 4171 5260  133 7451\n",
            " 3957 6793 1719 7379 4743 4746 5435 6216 4858 4417 8355 4492 2648 5104\n",
            " 5278 6539  743 7308  158 7543 2168 5913 7111  864 3414 5788 7592   39\n",
            " 7157 5647 4555 3956 1045 3617 4630 4384 6596   96 4539 6749 7202 4710\n",
            " 2413 4749 5122 5052 1598 5929  252 5565  336 6008 4564 5580 7237 1833\n",
            " 1466 1568 5958 7657 4327 5091 6918 6487 4082 2989 7448 4715   81 3382\n",
            "  467 3784 4624  764 4419 1117 2962 7954 7122 1430 6392  505 3174 1534\n",
            "  323 4164 3659 7526  498 5944 3510 4572 2307 2929 7480 4412  731 4563\n",
            " 7692 1869 2472 6966 4238 4702 6549 1594 2276 7269 2545 2216  605 6587\n",
            " 6315 5297 5282 4816 3200 1619 5504 2536 6399 3459 3917 6342 5754 5721\n",
            " 7385 7909 6126 7382 8382 4383 5502 7130  485 5862 4936 7674 3575 5372\n",
            " 3893 7109 4605 5211 5462 8065   50 4080 3334 8167 8378 5595 1873 8207\n",
            " 7076 4106 3719 4627  395 2685  231 2405 5419 6973 3480 3246 5414 3593\n",
            " 5931 6252 8208 4825 6203 3019 1105 5822   63 5854 6543 5787 4764 6701\n",
            "  334 6562 3822 4861 3051 1315 4805  246 8318 1089 8169 8218 2756 7549\n",
            "  707 1284 1076 2465 3815  540 4636 3188  697   80 1455  381 7025 7127\n",
            " 3992 4026 2777 4961  383 7531  744 6640 1047 6684 4297 7939  150 7384\n",
            "  446 7738 3287 6675 7752 5279 8196 3020 1203 4192 1552 4253 2887 2556\n",
            " 5729 1898 3208 5973 7033 1431 5915 8312 5799 7105  511 6404 2852 1561\n",
            " 4150 4835 1178]\n"
          ]
        }
      ],
      "source": [
        "gc.collect()\n",
        "tf.keras.backend.clear_session()\n",
        "dbert_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "dbert_model = TFRobertaModel.from_pretrained('roberta-base')\n",
        "max_len=512\n",
        "sentences=summarized_data['text']\n",
        "labels=summarized_data['label']\n",
        "filenumbers=summarized_data['filenum']\n",
        "len(sentences),len(labels),len(filenumbers)\n",
        "\n",
        "model_1=create_model()\n",
        "input_ids=[]\n",
        "attention_masks=[]\n",
        "\n",
        "for sent in sentences:\n",
        "  dbert_inps=dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n",
        "  input_ids.append(dbert_inps['input_ids'])\n",
        "  attention_masks.append(dbert_inps['attention_mask'])\n",
        "input_ids=np.asarray(input_ids)\n",
        "\n",
        "attention_masks=np.array(attention_masks)\n",
        "labels=np.array(labels)\n",
        "filenumbers=[int(i) for i in filenumbers]\n",
        "filenumbers=np.array(filenumbers)\n",
        "print(labels)\n",
        "print(filenumbers)\n",
        "\n",
        "#train_inp,val_inp,train_label,val_label,train_mask,val_mask,train_fnum,val_fnum=train_test_split(input_ids,labels,attention_masks,filenumbers,test_size=0.1,random_state=42)\n",
        "\n",
        "new_train_inp_1=[]\n",
        "new_val_inp_1=[]\n",
        "new_train_label_1=[]\n",
        "new_val_label_1=[]\n",
        "new_train_mask_1=[]\n",
        "new_train_fnum_1=[]\n",
        "new_val_fnum_1=[]\n",
        "new_val_mask_1=[]\n",
        "for i in range(len(train_fnum)):\n",
        "  if train_fnum[i] in filenumbers:\n",
        "    new_train_inp_1.append(train_inp[i])\n",
        "    #new_val_inp.append(val_inp[i])\n",
        "    new_train_label_1.append(train_label[i])\n",
        "    #new_val_label.append(val_label[i])\n",
        "    new_train_mask_1.append(train_mask[i])\n",
        "    #new_val_mask.append(val_mask[i])\n",
        "    new_train_fnum_1.append(train_fnum[i])\n",
        "\n",
        "for i in range(len(val_fnum)):\n",
        "  if val_fnum[i] in filenumbers:\n",
        "    #new_train_inp.append(train_inp[i])\n",
        "    new_val_inp_1.append(val_inp[i])\n",
        "    #new_train_label.append(train_label[i])\n",
        "    new_val_label_1.append(val_label[i])\n",
        "    #new_train_mask.append(train_mask[i])\n",
        "    new_val_mask_1.append(val_mask[i])\n",
        "    new_val_fnum_1.append(val_fnum[i])\n",
        "\n",
        "# print(new_train_label)\n",
        "# print(len(new_train_label))\n",
        "# print(new_val_label)\n",
        "# print(len(new_val_label))\n",
        "new_train_inp_1=np.array(new_train_inp_1)\n",
        "new_val_inp_1=np.array(new_val_inp_1)\n",
        "new_train_label_1=np.array(new_train_label_1)\n",
        "new_val_label_1=np.array(new_val_label_1)\n",
        "new_train_mask_1=np.array(new_train_mask_1)\n",
        "new_train_fnum_1=np.array(new_train_fnum_1)\n",
        "new_val_fnum_1=np.array(new_val_fnum_1)\n",
        "new_val_mask_1=np.array(new_val_mask_1)\n",
        "\n",
        "print(new_val_fnum_1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "U5WPkKimLztr",
        "outputId": "3d1fb1ef-c5b4-4d96-9e7d-9e358dfb9a37"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_1[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_roberta_model[0][0]']       \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 279)          143127      ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[209  63 216 ...   4  74 263]\n",
            "[   1    2    3 ... 8417 8418 8419]\n",
            "[6622   35 6713 4989 8233 7444 5061 3942 8344  454 5464  317 2883 4515\n",
            " 3001 3410 6991 2875 1125 3885 6647 4857 4504  214 3230 8340 2969 3847\n",
            " 4609 5092 1346 5993 7358 2752 7651 7301 8188 7073 4782 6069 2346 6445\n",
            " 6483  542 1663 4952 5072 1967 4071 7590  860 6418 5030 7297  626 5463\n",
            " 6029 6302 3819 7256 2950 5667 1235 3993 1439 1323 4263 3835 2134 2624\n",
            " 5801 4293 8129  320 5866 3247 5777   19 6059 6520 4837 3843 5695 1666\n",
            "  858 1418 5409 5274 6205 3167 2356 8418 5997 6685  223 3444 3310 7721\n",
            " 5928 2360 7197 4252 8343  909 4093 4354 4752 1421  649  829 1916 1003\n",
            " 3488 2496 5765 6612 4004 4052 1403 6697 5268 1010 6293 6380 4602 6057\n",
            " 1851 4577 4236 7978 1350 5083 6552 1058  361 7706 4186 2590 4452 4396\n",
            "  749  870 8351 3834  812 5108  463 1743 6600 8264 8064 7121 8279 5556\n",
            " 1845 5119 2516 4673 7947 8215 1882 3350 7617 7152 5199 4282 1092 3342\n",
            " 8003 5025 5051  535  933 3969 1927 3615 1221 8062 1489 3296 7442 5459\n",
            " 2031 7351  234 2313 8015 7416 7921 6574 6257 8396  941 4520 3300 5640\n",
            "  240 7810 7425 1321 6580  928 1170 5404 8273 6742 7005   94 7459    9\n",
            " 7861 7975 1018 5029 2322 2140 1415 7061 7767 4990 8124 5927 6051 3269\n",
            " 6916 5959 2846 6818 7052 7775 7582 8116 4240 4767 5248  430 1812 2175\n",
            " 6055 8331 3916  998 7271 1564 7326 1077 1559  292 3065 6764 6854 4915\n",
            " 5552 3583 1917  868 5902 4229 2144 7565 3712 7224 4502 1406 1651 6256\n",
            " 3827 6525 6318 2686 3616 5369 8235 1704 1011 3351 8089 8021  102  224\n",
            " 5671 8294 1510 7707 2985 6624 5146 1428 7381 3709 6824 6308  701 2281\n",
            " 6837 5436 1741 3520 5995 2649 4886  966 2863 5476 4058 1246 2406 1199\n",
            " 8387  513 5449 1028 2226 6166 4387  350 6123  678 5236 5041  734 3412\n",
            " 6085  587  489 5760 3163 3376 4982 5115 4655 5575 3839 6743 7173 5724\n",
            " 7689 7787  520  713 3214 8302 3150 5713 2247 8128 4607 6660 3505 4610\n",
            " 2866 4161 6905 7096  768 1062 4411 6087 2096   21 4903 7546 6630 2928\n",
            " 4640 7125 1792 2063 1700 2305 7928 8095  737 5849  922 3423 2359 6015\n",
            " 4489 4361 6081 6346 5014 5167 1023 8009 3335  268 4777 5221 4323 6168\n",
            "  759  711 5702 6129 7181 1860 7192 1347 1692 5467 3378 6579   67 2693\n",
            " 6134 4841 4270 3073 8397 4611 2943 6746 7696 3466 3950 4693 6125 8370\n",
            " 6142 6131  995 5548 2186 2112 1721 5067 4015 5587 7092 3156   25 1749\n",
            " 5554 4878 1197 8248  458 8035 6836 1006 5310 2698 6960 2339 1650 6227\n",
            " 4104 5130 7089 6993 2400 1289 4305 5585 2524 7933 6284 7057 5324 3807\n",
            " 5790 8347 2390 5280 2841 7220 1384 7693 8090 2671 4171 5260  133 7451\n",
            " 3957 6793 1719 7379 4743 4746 5435 6216 4858 4417 8355 4492 2648 5104\n",
            " 5278 6539  743 7308  158 7543 2168 5913 7111  864 3414 5788 7592   39\n",
            " 7157 5647 4555 3956 1045 3617 4630 4384 6596   96 4539 6749 7202 4710\n",
            " 2413 4749 5122 5052 1598 5929  252 5565  336 6008 4564 5580 7237 1833\n",
            " 1466 1568 5958 7657 4327 5091 6918 6487 4082 2989 7448 4715   81 3382\n",
            "  467 3784 4624  764 4419 1117 2962 7954 7122 1430 6392  505 3174 1534\n",
            "  323 4164 3659 7526  498 5944 3510 4572 2307 2929 7480 4412  731 4563\n",
            " 7692 1869 2472 6966 4238 4702 6549 1594 2276 7269 2545 2216  605 6587\n",
            " 6315 5297 5282 4816 3200 1619 5504 2536 6399 3459 3917 6342 5754 5721\n",
            " 7385 7909 6126 7382 8382 4383 5502 7130  485 5862 4936 7674 3575 5372\n",
            " 3893 7109 4605 5211 5462 8065   50 4080 3334 8167 8378 5595 1873 8207\n",
            " 7076 4106 3719 4627  395 2685  231 2405 5419 6973 3480 3246 5414 3593\n",
            " 5931 6252 8208 4825 6203 3019 1105 5822   63 5854 6543 5787 4764 6701\n",
            "  334 6562 3822 4861 3051 1315 4805  246 8318 1089 8169 8218 2756 7549\n",
            "  707 1284 1076 2465 3815  540 4636 3188  697   80 1455  381 7025 7127\n",
            " 3992 4026 2777 4961  383 7531  744 6640 1047 6684 4297 7939  150 7384\n",
            "  446 7738 3287 6675 7752 5279 8196 3020 1203 4192 1552 4253 2887 2556\n",
            " 5729 1898 3208 5973 7033 1431 5915 8312 5799 7105  511 6404 2852 1561\n",
            " 4150 4835 1178]\n",
            "Mon Jun  6 05:18:34 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   42C    P0    39W / 300W |  15237MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "826/826 [==============================] - 303s 347ms/step - loss: 14.1407 - accuracy: 0.0848 - val_loss: 12.4654 - val_accuracy: 0.2668\n",
            "Epoch 2/5\n",
            "826/826 [==============================] - 287s 347ms/step - loss: 11.8261 - accuracy: 0.3330 - val_loss: 11.0254 - val_accuracy: 0.4172\n",
            "Epoch 3/5\n",
            "826/826 [==============================] - 286s 346ms/step - loss: 10.6549 - accuracy: 0.4586 - val_loss: 10.2964 - val_accuracy: 0.4733\n",
            "Epoch 4/5\n",
            "826/826 [==============================] - 286s 347ms/step - loss: 9.8152 - accuracy: 0.5428 - val_loss: 9.7755 - val_accuracy: 0.5116\n",
            "Epoch 5/5\n",
            "826/826 [==============================] - 286s 347ms/step - loss: 9.1275 - accuracy: 0.6064 - val_loss: 9.4381 - val_accuracy: 0.5130\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_3[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_roberta_model[1][0]']       \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 279)          143127      ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.49643705463182897\n",
            "Weighted F1: 0.4464579045635286\n",
            "Micro F1: 0.49643705463182897\n",
            "Weighted Precision: 0.44044529529758447\n",
            "Micro Precision: 0.49643705463182897\n",
            "Weighted Recall: 0.49643705463182897\n",
            "Micro Recall: 0.49643705463182897\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_1[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_roberta_model[0][0]']       \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 279)          143127      ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[209  63 216 ...   4  74 263]\n",
            "[   1    2    3 ... 8417 8418 8419]\n",
            "[6622   35 6713 4989 8233 7444 5061 3942 8344  454 5464  317 2883 4515\n",
            " 3001 3410 6991 2875 1125 3885 6647 4857 4504  214 3230 8340 2969 3847\n",
            " 4609 5092 1346 5993 7358 2752 7651 7301 8188 7073 4782 6069 2346 6445\n",
            " 6483  542 1663 4952 5072 1967 4071 7590  860 6418 5030 7297  626 5463\n",
            " 6029 6302 3819 7256 2950 5667 1235 3993 1439 1323 4263 3835 2134 2624\n",
            " 5801 4293 8129  320 5866 3247 5777   19 6059 6520 4837 3843 5695 1666\n",
            "  858 1418 5409 5274 6205 3167 2356 8418 5997 6685  223 3444 3310 7721\n",
            " 5928 2360 7197 4252 8343  909 4093 4354 4752 1421  649  829 1916 1003\n",
            " 3488 2496 5765 6612 4004 4052 1403 6697 5268 1010 6293 6380 4602 6057\n",
            " 1851 4577 4236 7978 1350 5083 6552 1058  361 7706 4186 2590 4452 4396\n",
            "  749  870 8351 3834  812 5108  463 1743 6600 8264 8064 7121 8279 5556\n",
            " 1845 5119 2516 4673 7947 8215 1882 3350 7617 7152 5199 4282 1092 3342\n",
            " 8003 5025 5051  535  933 3969 1927 3615 1221 8062 1489 3296 7442 5459\n",
            " 2031 7351  234 2313 8015 7416 7921 6574 6257 8396  941 4520 3300 5640\n",
            "  240 7810 7425 1321 6580  928 1170 5404 8273 6742 7005   94 7459    9\n",
            " 7861 7975 1018 5029 2322 2140 1415 7061 7767 4990 8124 5927 6051 3269\n",
            " 6916 5959 2846 6818 7052 7775 7582 8116 4240 4767 5248  430 1812 2175\n",
            " 6055 8331 3916  998 7271 1564 7326 1077 1559  292 3065 6764 6854 4915\n",
            " 5552 3583 1917  868 5902 4229 2144 7565 3712 7224 4502 1406 1651 6256\n",
            " 3827 6525 6318 2686 3616 5369 8235 1704 1011 3351 8089 8021  102  224\n",
            " 5671 8294 1510 7707 2985 6624 5146 1428 7381 3709 6824 6308  701 2281\n",
            " 6837 5436 1741 3520 5995 2649 4886  966 2863 5476 4058 1246 2406 1199\n",
            " 8387  513 5449 1028 2226 6166 4387  350 6123  678 5236 5041  734 3412\n",
            " 6085  587  489 5760 3163 3376 4982 5115 4655 5575 3839 6743 7173 5724\n",
            " 7689 7787  520  713 3214 8302 3150 5713 2247 8128 4607 6660 3505 4610\n",
            " 2866 4161 6905 7096  768 1062 4411 6087 2096   21 4903 7546 6630 2928\n",
            " 4640 7125 1792 2063 1700 2305 7928 8095  737 5849  922 3423 2359 6015\n",
            " 4489 4361 6081 6346 5014 5167 1023 8009 3335  268 4777 5221 4323 6168\n",
            "  759  711 5702 6129 7181 1860 7192 1347 1692 5467 3378 6579   67 2693\n",
            " 6134 4841 4270 3073 8397 4611 2943 6746 7696 3466 3950 4693 6125 8370\n",
            " 6142 6131  995 5548 2186 2112 1721 5067 4015 5587 7092 3156   25 1749\n",
            " 5554 4878 1197 8248  458 8035 6836 1006 5310 2698 6960 2339 1650 6227\n",
            " 4104 5130 7089 6993 2400 1289 4305 5585 2524 7933 6284 7057 5324 3807\n",
            " 5790 8347 2390 5280 2841 7220 1384 7693 8090 2671 4171 5260  133 7451\n",
            " 3957 6793 1719 7379 4743 4746 5435 6216 4858 4417 8355 4492 2648 5104\n",
            " 5278 6539  743 7308  158 7543 2168 5913 7111  864 3414 5788 7592   39\n",
            " 7157 5647 4555 3956 1045 3617 4630 4384 6596   96 4539 6749 7202 4710\n",
            " 2413 4749 5122 5052 1598 5929  252 5565  336 6008 4564 5580 7237 1833\n",
            " 1466 1568 5958 7657 4327 5091 6918 6487 4082 2989 7448 4715   81 3382\n",
            "  467 3784 4624  764 4419 1117 2962 7954 7122 1430 6392  505 3174 1534\n",
            "  323 4164 3659 7526  498 5944 3510 4572 2307 2929 7480 4412  731 4563\n",
            " 7692 1869 2472 6966 4238 4702 6549 1594 2276 7269 2545 2216  605 6587\n",
            " 6315 5297 5282 4816 3200 1619 5504 2536 6399 3459 3917 6342 5754 5721\n",
            " 7385 7909 6126 7382 8382 4383 5502 7130  485 5862 4936 7674 3575 5372\n",
            " 3893 7109 4605 5211 5462 8065   50 4080 3334 8167 8378 5595 1873 8207\n",
            " 7076 4106 3719 4627  395 2685  231 2405 5419 6973 3480 3246 5414 3593\n",
            " 5931 6252 8208 4825 6203 3019 1105 5822   63 5854 6543 5787 4764 6701\n",
            "  334 6562 3822 4861 3051 1315 4805  246 8318 1089 8169 8218 2756 7549\n",
            "  707 1284 1076 2465 3815  540 4636 3188  697   80 1455  381 7025 7127\n",
            " 3992 4026 2777 4961  383 7531  744 6640 1047 6684 4297 7939  150 7384\n",
            "  446 7738 3287 6675 7752 5279 8196 3020 1203 4192 1552 4253 2887 2556\n",
            " 5729 1898 3208 5973 7033 1431 5915 8312 5799 7105  511 6404 2852 1561\n",
            " 4150 4835 1178]\n",
            "Mon Jun  6 05:44:38 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P0    39W / 300W |  15237MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "826/826 [==============================] - 303s 347ms/step - loss: 14.1039 - accuracy: 0.0904 - val_loss: 12.4328 - val_accuracy: 0.2859\n",
            "Epoch 2/5\n",
            "826/826 [==============================] - 286s 346ms/step - loss: 11.8282 - accuracy: 0.3345 - val_loss: 11.0940 - val_accuracy: 0.4077\n",
            "Epoch 3/5\n",
            "826/826 [==============================] - 286s 346ms/step - loss: 10.6631 - accuracy: 0.4514 - val_loss: 10.3669 - val_accuracy: 0.4651\n",
            "Epoch 4/5\n",
            "826/826 [==============================] - 286s 347ms/step - loss: 9.8653 - accuracy: 0.5275 - val_loss: 9.8592 - val_accuracy: 0.4911\n",
            "Epoch 5/5\n",
            "826/826 [==============================] - 287s 348ms/step - loss: 9.1885 - accuracy: 0.5967 - val_loss: 9.4219 - val_accuracy: 0.5376\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_3[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_roberta_model[1][0]']       \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 279)          143127      ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.5130641330166271\n",
            "Weighted F1: 0.46116701066423854\n",
            "Micro F1: 0.5130641330166271\n",
            "Weighted Precision: 0.44293702620585396\n",
            "Micro Precision: 0.5130641330166271\n",
            "Weighted Recall: 0.5130641330166271\n",
            "Micro Recall: 0.5130641330166271\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_1[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_roberta_model[0][0]']       \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 279)          143127      ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[209  63 216 ...   4  74 263]\n",
            "[   1    2    3 ... 8417 8418 8419]\n",
            "[6622   35 6713 4989 8233 7444 5061 3942 8344  454 5464  317 2883 4515\n",
            " 3001 3410 6991 2875 1125 3885 6647 4857 4504  214 3230 8340 2969 3847\n",
            " 4609 5092 1346 5993 7358 2752 7651 7301 8188 7073 4782 6069 2346 6445\n",
            " 6483  542 1663 4952 5072 1967 4071 7590  860 6418 5030 7297  626 5463\n",
            " 6029 6302 3819 7256 2950 5667 1235 3993 1439 1323 4263 3835 2134 2624\n",
            " 5801 4293 8129  320 5866 3247 5777   19 6059 6520 4837 3843 5695 1666\n",
            "  858 1418 5409 5274 6205 3167 2356 8418 5997 6685  223 3444 3310 7721\n",
            " 5928 2360 7197 4252 8343  909 4093 4354 4752 1421  649  829 1916 1003\n",
            " 3488 2496 5765 6612 4004 4052 1403 6697 5268 1010 6293 6380 4602 6057\n",
            " 1851 4577 4236 7978 1350 5083 6552 1058  361 7706 4186 2590 4452 4396\n",
            "  749  870 8351 3834  812 5108  463 1743 6600 8264 8064 7121 8279 5556\n",
            " 1845 5119 2516 4673 7947 8215 1882 3350 7617 7152 5199 4282 1092 3342\n",
            " 8003 5025 5051  535  933 3969 1927 3615 1221 8062 1489 3296 7442 5459\n",
            " 2031 7351  234 2313 8015 7416 7921 6574 6257 8396  941 4520 3300 5640\n",
            "  240 7810 7425 1321 6580  928 1170 5404 8273 6742 7005   94 7459    9\n",
            " 7861 7975 1018 5029 2322 2140 1415 7061 7767 4990 8124 5927 6051 3269\n",
            " 6916 5959 2846 6818 7052 7775 7582 8116 4240 4767 5248  430 1812 2175\n",
            " 6055 8331 3916  998 7271 1564 7326 1077 1559  292 3065 6764 6854 4915\n",
            " 5552 3583 1917  868 5902 4229 2144 7565 3712 7224 4502 1406 1651 6256\n",
            " 3827 6525 6318 2686 3616 5369 8235 1704 1011 3351 8089 8021  102  224\n",
            " 5671 8294 1510 7707 2985 6624 5146 1428 7381 3709 6824 6308  701 2281\n",
            " 6837 5436 1741 3520 5995 2649 4886  966 2863 5476 4058 1246 2406 1199\n",
            " 8387  513 5449 1028 2226 6166 4387  350 6123  678 5236 5041  734 3412\n",
            " 6085  587  489 5760 3163 3376 4982 5115 4655 5575 3839 6743 7173 5724\n",
            " 7689 7787  520  713 3214 8302 3150 5713 2247 8128 4607 6660 3505 4610\n",
            " 2866 4161 6905 7096  768 1062 4411 6087 2096   21 4903 7546 6630 2928\n",
            " 4640 7125 1792 2063 1700 2305 7928 8095  737 5849  922 3423 2359 6015\n",
            " 4489 4361 6081 6346 5014 5167 1023 8009 3335  268 4777 5221 4323 6168\n",
            "  759  711 5702 6129 7181 1860 7192 1347 1692 5467 3378 6579   67 2693\n",
            " 6134 4841 4270 3073 8397 4611 2943 6746 7696 3466 3950 4693 6125 8370\n",
            " 6142 6131  995 5548 2186 2112 1721 5067 4015 5587 7092 3156   25 1749\n",
            " 5554 4878 1197 8248  458 8035 6836 1006 5310 2698 6960 2339 1650 6227\n",
            " 4104 5130 7089 6993 2400 1289 4305 5585 2524 7933 6284 7057 5324 3807\n",
            " 5790 8347 2390 5280 2841 7220 1384 7693 8090 2671 4171 5260  133 7451\n",
            " 3957 6793 1719 7379 4743 4746 5435 6216 4858 4417 8355 4492 2648 5104\n",
            " 5278 6539  743 7308  158 7543 2168 5913 7111  864 3414 5788 7592   39\n",
            " 7157 5647 4555 3956 1045 3617 4630 4384 6596   96 4539 6749 7202 4710\n",
            " 2413 4749 5122 5052 1598 5929  252 5565  336 6008 4564 5580 7237 1833\n",
            " 1466 1568 5958 7657 4327 5091 6918 6487 4082 2989 7448 4715   81 3382\n",
            "  467 3784 4624  764 4419 1117 2962 7954 7122 1430 6392  505 3174 1534\n",
            "  323 4164 3659 7526  498 5944 3510 4572 2307 2929 7480 4412  731 4563\n",
            " 7692 1869 2472 6966 4238 4702 6549 1594 2276 7269 2545 2216  605 6587\n",
            " 6315 5297 5282 4816 3200 1619 5504 2536 6399 3459 3917 6342 5754 5721\n",
            " 7385 7909 6126 7382 8382 4383 5502 7130  485 5862 4936 7674 3575 5372\n",
            " 3893 7109 4605 5211 5462 8065   50 4080 3334 8167 8378 5595 1873 8207\n",
            " 7076 4106 3719 4627  395 2685  231 2405 5419 6973 3480 3246 5414 3593\n",
            " 5931 6252 8208 4825 6203 3019 1105 5822   63 5854 6543 5787 4764 6701\n",
            "  334 6562 3822 4861 3051 1315 4805  246 8318 1089 8169 8218 2756 7549\n",
            "  707 1284 1076 2465 3815  540 4636 3188  697   80 1455  381 7025 7127\n",
            " 3992 4026 2777 4961  383 7531  744 6640 1047 6684 4297 7939  150 7384\n",
            "  446 7738 3287 6675 7752 5279 8196 3020 1203 4192 1552 4253 2887 2556\n",
            " 5729 1898 3208 5973 7033 1431 5915 8312 5799 7105  511 6404 2852 1561\n",
            " 4150 4835 1178]\n",
            "Mon Jun  6 06:10:41 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   42C    P0    39W / 300W |  15237MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "826/826 [==============================] - 304s 348ms/step - loss: 13.9519 - accuracy: 0.1132 - val_loss: 12.2194 - val_accuracy: 0.2996\n",
            "Epoch 2/5\n",
            "826/826 [==============================] - 286s 346ms/step - loss: 11.6663 - accuracy: 0.3499 - val_loss: 10.9229 - val_accuracy: 0.4172\n",
            "Epoch 3/5\n",
            "826/826 [==============================] - 286s 346ms/step - loss: 10.5405 - accuracy: 0.4701 - val_loss: 10.2588 - val_accuracy: 0.4651\n",
            "Epoch 4/5\n",
            "826/826 [==============================] - 286s 346ms/step - loss: 9.7146 - accuracy: 0.5457 - val_loss: 9.7409 - val_accuracy: 0.5103\n",
            "Epoch 5/5\n",
            "826/826 [==============================] - 286s 346ms/step - loss: 9.0711 - accuracy: 0.6082 - val_loss: 9.3714 - val_accuracy: 0.5253\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_3[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_roberta_model[1][0]']       \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 279)          143127      ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.502375296912114\n",
            "Weighted F1: 0.45249288930831705\n",
            "Micro F1: 0.502375296912114\n",
            "Weighted Precision: 0.44890392114876054\n",
            "Micro Precision: 0.502375296912114\n",
            "Weighted Recall: 0.502375296912114\n",
            "Micro Recall: 0.502375296912114\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_1[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_roberta_model[0][0]']       \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 279)          143127      ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[209  63 216 ...   4  74 263]\n",
            "[   1    2    3 ... 8417 8418 8419]\n",
            "[6622   35 6713 4989 8233 7444 5061 3942 8344  454 5464  317 2883 4515\n",
            " 3001 3410 6991 2875 1125 3885 6647 4857 4504  214 3230 8340 2969 3847\n",
            " 4609 5092 1346 5993 7358 2752 7651 7301 8188 7073 4782 6069 2346 6445\n",
            " 6483  542 1663 4952 5072 1967 4071 7590  860 6418 5030 7297  626 5463\n",
            " 6029 6302 3819 7256 2950 5667 1235 3993 1439 1323 4263 3835 2134 2624\n",
            " 5801 4293 8129  320 5866 3247 5777   19 6059 6520 4837 3843 5695 1666\n",
            "  858 1418 5409 5274 6205 3167 2356 8418 5997 6685  223 3444 3310 7721\n",
            " 5928 2360 7197 4252 8343  909 4093 4354 4752 1421  649  829 1916 1003\n",
            " 3488 2496 5765 6612 4004 4052 1403 6697 5268 1010 6293 6380 4602 6057\n",
            " 1851 4577 4236 7978 1350 5083 6552 1058  361 7706 4186 2590 4452 4396\n",
            "  749  870 8351 3834  812 5108  463 1743 6600 8264 8064 7121 8279 5556\n",
            " 1845 5119 2516 4673 7947 8215 1882 3350 7617 7152 5199 4282 1092 3342\n",
            " 8003 5025 5051  535  933 3969 1927 3615 1221 8062 1489 3296 7442 5459\n",
            " 2031 7351  234 2313 8015 7416 7921 6574 6257 8396  941 4520 3300 5640\n",
            "  240 7810 7425 1321 6580  928 1170 5404 8273 6742 7005   94 7459    9\n",
            " 7861 7975 1018 5029 2322 2140 1415 7061 7767 4990 8124 5927 6051 3269\n",
            " 6916 5959 2846 6818 7052 7775 7582 8116 4240 4767 5248  430 1812 2175\n",
            " 6055 8331 3916  998 7271 1564 7326 1077 1559  292 3065 6764 6854 4915\n",
            " 5552 3583 1917  868 5902 4229 2144 7565 3712 7224 4502 1406 1651 6256\n",
            " 3827 6525 6318 2686 3616 5369 8235 1704 1011 3351 8089 8021  102  224\n",
            " 5671 8294 1510 7707 2985 6624 5146 1428 7381 3709 6824 6308  701 2281\n",
            " 6837 5436 1741 3520 5995 2649 4886  966 2863 5476 4058 1246 2406 1199\n",
            " 8387  513 5449 1028 2226 6166 4387  350 6123  678 5236 5041  734 3412\n",
            " 6085  587  489 5760 3163 3376 4982 5115 4655 5575 3839 6743 7173 5724\n",
            " 7689 7787  520  713 3214 8302 3150 5713 2247 8128 4607 6660 3505 4610\n",
            " 2866 4161 6905 7096  768 1062 4411 6087 2096   21 4903 7546 6630 2928\n",
            " 4640 7125 1792 2063 1700 2305 7928 8095  737 5849  922 3423 2359 6015\n",
            " 4489 4361 6081 6346 5014 5167 1023 8009 3335  268 4777 5221 4323 6168\n",
            "  759  711 5702 6129 7181 1860 7192 1347 1692 5467 3378 6579   67 2693\n",
            " 6134 4841 4270 3073 8397 4611 2943 6746 7696 3466 3950 4693 6125 8370\n",
            " 6142 6131  995 5548 2186 2112 1721 5067 4015 5587 7092 3156   25 1749\n",
            " 5554 4878 1197 8248  458 8035 6836 1006 5310 2698 6960 2339 1650 6227\n",
            " 4104 5130 7089 6993 2400 1289 4305 5585 2524 7933 6284 7057 5324 3807\n",
            " 5790 8347 2390 5280 2841 7220 1384 7693 8090 2671 4171 5260  133 7451\n",
            " 3957 6793 1719 7379 4743 4746 5435 6216 4858 4417 8355 4492 2648 5104\n",
            " 5278 6539  743 7308  158 7543 2168 5913 7111  864 3414 5788 7592   39\n",
            " 7157 5647 4555 3956 1045 3617 4630 4384 6596   96 4539 6749 7202 4710\n",
            " 2413 4749 5122 5052 1598 5929  252 5565  336 6008 4564 5580 7237 1833\n",
            " 1466 1568 5958 7657 4327 5091 6918 6487 4082 2989 7448 4715   81 3382\n",
            "  467 3784 4624  764 4419 1117 2962 7954 7122 1430 6392  505 3174 1534\n",
            "  323 4164 3659 7526  498 5944 3510 4572 2307 2929 7480 4412  731 4563\n",
            " 7692 1869 2472 6966 4238 4702 6549 1594 2276 7269 2545 2216  605 6587\n",
            " 6315 5297 5282 4816 3200 1619 5504 2536 6399 3459 3917 6342 5754 5721\n",
            " 7385 7909 6126 7382 8382 4383 5502 7130  485 5862 4936 7674 3575 5372\n",
            " 3893 7109 4605 5211 5462 8065   50 4080 3334 8167 8378 5595 1873 8207\n",
            " 7076 4106 3719 4627  395 2685  231 2405 5419 6973 3480 3246 5414 3593\n",
            " 5931 6252 8208 4825 6203 3019 1105 5822   63 5854 6543 5787 4764 6701\n",
            "  334 6562 3822 4861 3051 1315 4805  246 8318 1089 8169 8218 2756 7549\n",
            "  707 1284 1076 2465 3815  540 4636 3188  697   80 1455  381 7025 7127\n",
            " 3992 4026 2777 4961  383 7531  744 6640 1047 6684 4297 7939  150 7384\n",
            "  446 7738 3287 6675 7752 5279 8196 3020 1203 4192 1552 4253 2887 2556\n",
            " 5729 1898 3208 5973 7033 1431 5915 8312 5799 7105  511 6404 2852 1561\n",
            " 4150 4835 1178]\n",
            "Mon Jun  6 06:36:44 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P0    39W / 300W |  15237MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "826/826 [==============================] - 304s 348ms/step - loss: 14.0501 - accuracy: 0.1153 - val_loss: 12.3186 - val_accuracy: 0.2914\n",
            "Epoch 2/5\n",
            "826/826 [==============================] - 287s 347ms/step - loss: 11.8575 - accuracy: 0.3365 - val_loss: 11.1523 - val_accuracy: 0.3830\n",
            "Epoch 3/5\n",
            "826/826 [==============================] - 286s 347ms/step - loss: 10.7574 - accuracy: 0.4422 - val_loss: 10.4134 - val_accuracy: 0.4446\n",
            "Epoch 4/5\n",
            "826/826 [==============================] - 286s 346ms/step - loss: 9.9231 - accuracy: 0.5255 - val_loss: 9.9062 - val_accuracy: 0.4815\n",
            "Epoch 5/5\n",
            "826/826 [==============================] - 286s 347ms/step - loss: 9.2811 - accuracy: 0.5920 - val_loss: 9.5375 - val_accuracy: 0.5021\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_3[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_roberta_model[1][0]']       \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 279)          143127      ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.47743467933491684\n",
            "Weighted F1: 0.4200663700372353\n",
            "Micro F1: 0.47743467933491684\n",
            "Weighted Precision: 0.4028119705385957\n",
            "Micro Precision: 0.47743467933491684\n",
            "Weighted Recall: 0.47743467933491684\n",
            "Micro Recall: 0.47743467933491684\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_1[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_roberta_model[0][0]']       \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 279)          143127      ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[209  63 216 ...   4  74 263]\n",
            "[   1    2    3 ... 8417 8418 8419]\n",
            "[6622   35 6713 4989 8233 7444 5061 3942 8344  454 5464  317 2883 4515\n",
            " 3001 3410 6991 2875 1125 3885 6647 4857 4504  214 3230 8340 2969 3847\n",
            " 4609 5092 1346 5993 7358 2752 7651 7301 8188 7073 4782 6069 2346 6445\n",
            " 6483  542 1663 4952 5072 1967 4071 7590  860 6418 5030 7297  626 5463\n",
            " 6029 6302 3819 7256 2950 5667 1235 3993 1439 1323 4263 3835 2134 2624\n",
            " 5801 4293 8129  320 5866 3247 5777   19 6059 6520 4837 3843 5695 1666\n",
            "  858 1418 5409 5274 6205 3167 2356 8418 5997 6685  223 3444 3310 7721\n",
            " 5928 2360 7197 4252 8343  909 4093 4354 4752 1421  649  829 1916 1003\n",
            " 3488 2496 5765 6612 4004 4052 1403 6697 5268 1010 6293 6380 4602 6057\n",
            " 1851 4577 4236 7978 1350 5083 6552 1058  361 7706 4186 2590 4452 4396\n",
            "  749  870 8351 3834  812 5108  463 1743 6600 8264 8064 7121 8279 5556\n",
            " 1845 5119 2516 4673 7947 8215 1882 3350 7617 7152 5199 4282 1092 3342\n",
            " 8003 5025 5051  535  933 3969 1927 3615 1221 8062 1489 3296 7442 5459\n",
            " 2031 7351  234 2313 8015 7416 7921 6574 6257 8396  941 4520 3300 5640\n",
            "  240 7810 7425 1321 6580  928 1170 5404 8273 6742 7005   94 7459    9\n",
            " 7861 7975 1018 5029 2322 2140 1415 7061 7767 4990 8124 5927 6051 3269\n",
            " 6916 5959 2846 6818 7052 7775 7582 8116 4240 4767 5248  430 1812 2175\n",
            " 6055 8331 3916  998 7271 1564 7326 1077 1559  292 3065 6764 6854 4915\n",
            " 5552 3583 1917  868 5902 4229 2144 7565 3712 7224 4502 1406 1651 6256\n",
            " 3827 6525 6318 2686 3616 5369 8235 1704 1011 3351 8089 8021  102  224\n",
            " 5671 8294 1510 7707 2985 6624 5146 1428 7381 3709 6824 6308  701 2281\n",
            " 6837 5436 1741 3520 5995 2649 4886  966 2863 5476 4058 1246 2406 1199\n",
            " 8387  513 5449 1028 2226 6166 4387  350 6123  678 5236 5041  734 3412\n",
            " 6085  587  489 5760 3163 3376 4982 5115 4655 5575 3839 6743 7173 5724\n",
            " 7689 7787  520  713 3214 8302 3150 5713 2247 8128 4607 6660 3505 4610\n",
            " 2866 4161 6905 7096  768 1062 4411 6087 2096   21 4903 7546 6630 2928\n",
            " 4640 7125 1792 2063 1700 2305 7928 8095  737 5849  922 3423 2359 6015\n",
            " 4489 4361 6081 6346 5014 5167 1023 8009 3335  268 4777 5221 4323 6168\n",
            "  759  711 5702 6129 7181 1860 7192 1347 1692 5467 3378 6579   67 2693\n",
            " 6134 4841 4270 3073 8397 4611 2943 6746 7696 3466 3950 4693 6125 8370\n",
            " 6142 6131  995 5548 2186 2112 1721 5067 4015 5587 7092 3156   25 1749\n",
            " 5554 4878 1197 8248  458 8035 6836 1006 5310 2698 6960 2339 1650 6227\n",
            " 4104 5130 7089 6993 2400 1289 4305 5585 2524 7933 6284 7057 5324 3807\n",
            " 5790 8347 2390 5280 2841 7220 1384 7693 8090 2671 4171 5260  133 7451\n",
            " 3957 6793 1719 7379 4743 4746 5435 6216 4858 4417 8355 4492 2648 5104\n",
            " 5278 6539  743 7308  158 7543 2168 5913 7111  864 3414 5788 7592   39\n",
            " 7157 5647 4555 3956 1045 3617 4630 4384 6596   96 4539 6749 7202 4710\n",
            " 2413 4749 5122 5052 1598 5929  252 5565  336 6008 4564 5580 7237 1833\n",
            " 1466 1568 5958 7657 4327 5091 6918 6487 4082 2989 7448 4715   81 3382\n",
            "  467 3784 4624  764 4419 1117 2962 7954 7122 1430 6392  505 3174 1534\n",
            "  323 4164 3659 7526  498 5944 3510 4572 2307 2929 7480 4412  731 4563\n",
            " 7692 1869 2472 6966 4238 4702 6549 1594 2276 7269 2545 2216  605 6587\n",
            " 6315 5297 5282 4816 3200 1619 5504 2536 6399 3459 3917 6342 5754 5721\n",
            " 7385 7909 6126 7382 8382 4383 5502 7130  485 5862 4936 7674 3575 5372\n",
            " 3893 7109 4605 5211 5462 8065   50 4080 3334 8167 8378 5595 1873 8207\n",
            " 7076 4106 3719 4627  395 2685  231 2405 5419 6973 3480 3246 5414 3593\n",
            " 5931 6252 8208 4825 6203 3019 1105 5822   63 5854 6543 5787 4764 6701\n",
            "  334 6562 3822 4861 3051 1315 4805  246 8318 1089 8169 8218 2756 7549\n",
            "  707 1284 1076 2465 3815  540 4636 3188  697   80 1455  381 7025 7127\n",
            " 3992 4026 2777 4961  383 7531  744 6640 1047 6684 4297 7939  150 7384\n",
            "  446 7738 3287 6675 7752 5279 8196 3020 1203 4192 1552 4253 2887 2556\n",
            " 5729 1898 3208 5973 7033 1431 5915 8312 5799 7105  511 6404 2852 1561\n",
            " 4150 4835 1178]\n",
            "Mon Jun  6 07:02:48 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   42C    P0    39W / 300W |  15237MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "826/826 [==============================] - 304s 348ms/step - loss: 14.2237 - accuracy: 0.0677 - val_loss: 12.6424 - val_accuracy: 0.2503\n",
            "Epoch 2/5\n",
            "826/826 [==============================] - 287s 348ms/step - loss: 11.8456 - accuracy: 0.3309 - val_loss: 11.1748 - val_accuracy: 0.3926\n",
            "Epoch 3/5\n",
            "826/826 [==============================] - 287s 347ms/step - loss: 10.6272 - accuracy: 0.4421 - val_loss: 10.3175 - val_accuracy: 0.4637\n",
            "Epoch 4/5\n",
            "826/826 [==============================] - 287s 348ms/step - loss: 9.7763 - accuracy: 0.5248 - val_loss: 9.7933 - val_accuracy: 0.4829\n",
            "Epoch 5/5\n",
            "826/826 [==============================] - 286s 346ms/step - loss: 9.1557 - accuracy: 0.5795 - val_loss: 9.3928 - val_accuracy: 0.4979\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_3[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_roberta_model[1][0]']       \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 279)          143127      ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.47743467933491684\n",
            "Weighted F1: 0.42113659853721314\n",
            "Micro F1: 0.47743467933491684\n",
            "Weighted Precision: 0.4025473864815928\n",
            "Micro Precision: 0.47743467933491684\n",
            "Weighted Recall: 0.47743467933491684\n",
            "Micro Recall: 0.47743467933491684\n",
            "Average Accuracy: 0.49334916864608075\n",
            "Average Weighted F1: 0.44026415462210655\n",
            "Average Micro F1: 0.49334916864608075\n",
            "Average Weighted Precision: 0.42752911993447756\n",
            "Average Micro Precision: 0.49334916864608075\n",
            "Average Weighted Recall: 0.49334916864608075\n",
            "Average Micro Recall: 0.49334916864608075\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "total_accuracy=0\n",
        "total_weighted_f1=0\n",
        "total_micro_f1=0\n",
        "total_weighted_precision=0\n",
        "total_micro_precision=0\n",
        "total_weighted_recall=0\n",
        "total_micro_recall=0\n",
        "\n",
        "for f in range(5):\n",
        "  gc.collect()\n",
        "  tf.keras.backend.clear_session()\n",
        "  dbert_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "  dbert_model = TFRobertaModel.from_pretrained('roberta-base')\n",
        "  max_len=512\n",
        "  sentences=summarized_data['text']\n",
        "  labels=summarized_data['label']\n",
        "  filenumbers=summarized_data['filenum']\n",
        "  len(sentences),len(labels),len(filenumbers)\n",
        "  \n",
        "  model_1=create_model()\n",
        "  input_ids=[]\n",
        "  attention_masks=[]\n",
        "\n",
        "  for sent in sentences:\n",
        "    dbert_inps=dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n",
        "    input_ids.append(dbert_inps['input_ids'])\n",
        "    attention_masks.append(dbert_inps['attention_mask'])\n",
        "  input_ids=np.asarray(input_ids)\n",
        "\n",
        "  attention_masks=np.array(attention_masks)\n",
        "  labels=np.array(labels)\n",
        "  filenumbers=[int(i) for i in filenumbers]\n",
        "  filenumbers=np.array(filenumbers)\n",
        "  print(labels)\n",
        "  print(filenumbers)\n",
        "\n",
        "  #train_inp,val_inp,train_label,val_label,train_mask,val_mask,train_fnum,val_fnum=train_test_split(input_ids,labels,attention_masks,filenumbers,test_size=0.1,random_state=42)\n",
        "\n",
        "  new_train_inp_1=[]\n",
        "  new_val_inp_1=[]\n",
        "  new_train_label_1=[]\n",
        "  new_val_label_1=[]\n",
        "  new_train_mask_1=[]\n",
        "  new_train_fnum_1=[]\n",
        "  new_val_fnum_1=[]\n",
        "  new_val_mask_1=[]\n",
        "  for i in range(len(train_fnum)):\n",
        "    if train_fnum[i] in filenumbers:\n",
        "      new_train_inp_1.append(train_inp[i])\n",
        "      #new_val_inp.append(val_inp[i])\n",
        "      new_train_label_1.append(train_label[i])\n",
        "      #new_val_label.append(val_label[i])\n",
        "      new_train_mask_1.append(train_mask[i])\n",
        "      #new_val_mask.append(val_mask[i])\n",
        "      new_train_fnum_1.append(train_fnum[i])\n",
        "\n",
        "  for i in range(len(val_fnum)):\n",
        "    if val_fnum[i] in filenumbers:\n",
        "      #new_train_inp.append(train_inp[i])\n",
        "      new_val_inp_1.append(val_inp[i])\n",
        "      #new_train_label.append(train_label[i])\n",
        "      new_val_label_1.append(val_label[i])\n",
        "      #new_train_mask.append(train_mask[i])\n",
        "      new_val_mask_1.append(val_mask[i])\n",
        "      new_val_fnum_1.append(val_fnum[i])\n",
        "\n",
        "  # print(new_train_label)\n",
        "  # print(len(new_train_label))\n",
        "  # print(new_val_label)\n",
        "  # print(len(new_val_label))\n",
        "  new_train_inp_1=np.array(new_train_inp_1)\n",
        "  new_val_inp_1=np.array(new_val_inp_1)\n",
        "  new_train_label_1=np.array(new_train_label_1)\n",
        "  new_val_label_1=np.array(new_val_label_1)\n",
        "  new_train_mask_1=np.array(new_train_mask_1)\n",
        "  new_train_fnum_1=np.array(new_train_fnum_1)\n",
        "  new_val_fnum_1=np.array(new_val_fnum_1)\n",
        "  new_val_mask_1=np.array(new_val_mask_1)\n",
        "\n",
        "  print(new_val_fnum_1)\n",
        "  \n",
        "  log_dir='dbert_model'\n",
        "\n",
        "  model_save_path='./drive/MyDrive/Ensemble/roberta-ensemble-512-model_1-'+str(f)+'-279labels.h5'\n",
        "\n",
        "  loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "  accuracy = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
        "  callbacks= [tf.keras.callbacks.ModelCheckpoint(filepath=model_save_path,monitor='val_accuracy',mode='max',save_best_only=True,save_weights_only=True),keras.callbacks.TensorBoard(log_dir=log_dir)]\n",
        "  model_1.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])\n",
        "  gpu_info = !nvidia-smi\n",
        "  gpu_info = '\\n'.join(gpu_info)\n",
        "  if gpu_info.find('failed') >= 0:\n",
        "    print('Not connected to a GPU')\n",
        "  else:\n",
        "    print(gpu_info)\n",
        "  \n",
        "  history=model_1.fit([new_train_inp_1,new_train_mask_1],new_train_label_1,batch_size=8,epochs=5,validation_data=([new_val_inp_1,new_val_mask_1],new_val_label_1),callbacks=callbacks)\n",
        "\n",
        "  pred_labels=[]\n",
        "\n",
        "  model_saved_1= create_model()\n",
        "  model_saved_1.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])\n",
        "  model_saved_1.load_weights('./drive/MyDrive/Ensemble/roberta-ensemble-512-model_1-'+str(f)+'-279labels.h5')\n",
        "\n",
        "  for i in range(0,len(val_inp)):\n",
        "    pred=model_saved_1.predict([val_inp[i].reshape(1,512),val_mask[i].reshape(1,512)])\n",
        "    pred_label = pred.argmax(axis=1)\n",
        "    pred_labels.append(pred_label)\n",
        "  accuracy=accuracy_score(val_label, pred_labels)\n",
        "  print(\"Accuracy: \"+str(accuracy))\n",
        "  total_accuracy=total_accuracy+accuracy\n",
        "  \n",
        "  weighted_f1=f1_score(val_label,pred_labels, average='weighted')\n",
        "  print(\"Weighted F1: \"+ str(weighted_f1))\n",
        "  total_weighted_f1=total_weighted_f1+weighted_f1\n",
        "  micro_f1=f1_score(val_label,pred_labels, average='micro')\n",
        "  print(\"Micro F1: \"+ str(micro_f1))\n",
        "  total_micro_f1=total_micro_f1+micro_f1\n",
        "\n",
        "  weighted_precision=precision_score(val_label, pred_labels, average='weighted')\n",
        "  print(\"Weighted Precision: \" + str(weighted_precision))\n",
        "  total_weighted_precision=total_weighted_precision+weighted_precision\n",
        "  micro_precision=precision_score(val_label, pred_labels, average='micro')\n",
        "  print(\"Micro Precision: \" + str(micro_precision))\n",
        "  total_micro_precision=total_micro_precision+micro_precision\n",
        "\n",
        "  weighted_recall=recall_score(val_label, pred_labels, average='weighted')\n",
        "  print(\"Weighted Recall: \" + str(weighted_recall))\n",
        "  total_weighted_recall=total_weighted_recall+weighted_recall\n",
        "  micro_recall=recall_score(val_label, pred_labels, average='micro')\n",
        "  print(\"Micro Recall: \" + str(micro_recall))\n",
        "  total_micro_recall=total_micro_recall+micro_recall\n",
        "\n",
        "\n",
        "print(\"Average Accuracy: \"+str(total_accuracy/5))\n",
        "print(\"Average Weighted F1: \"+str(total_weighted_f1/5))\n",
        "print(\"Average Micro F1: \"+str(total_micro_f1/5))\n",
        "print(\"Average Weighted Precision: \"+str(total_weighted_precision/5))\n",
        "print(\"Average Micro Precision: \"+str(total_micro_precision/5))\n",
        "print(\"Average Weighted Recall: \"+str(total_weighted_recall/5))\n",
        "print(\"Average Micro Recall: \"+str(total_micro_recall/5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4Yijn8aNjtR",
        "outputId": "18da8d82-ff8b-4988-ede0-5460e44f725a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6722\n",
            "6722\n",
            "Average Length 511.5861350788456\n",
            "['209', '63', '216', '108', '196', '200', '203', '246', '188', '193', '25', '106', '56', '53', '223', '161', '200', '208', '78', '147', '147', '179', '147', '80', '56', '209', '209', '200', '214', '202', '203', '194', '26', '237', '165', '258', '238', '238', '76', '193', '196', '213', '161', '213', '209', '194', '196', '25', '151', '260', '135', '173', '30', '108', '272', '259', '176', '213', '213', '21', '5', '171', '200', '195', '240', '194', '176', '240', '240', '80', '173', '98', '214', '21', '76', '76', '73', '173', '173', '78', '61', '258', '191', '3', '78', '173', '193', '118', '200', '264', '260', '49', '211', '258', '18', '173', '78', '3', '3', '1', '213', '135', '182', '239', '208', '278', '50', '194', '200', '124', '189', '117', '264', '244', '76', '193', '221', '78', '78', '173', '197', '78', '161', '231', '135', '22', '78', '25', '25', '197', '200', '147', '203', '191', '196', '193', '263', '32', '197', '191', '4', '56', '196', '104', '191', '196', '173', '147', '194', '96', '161', '161', '238', '200', '3', '78', '78', '53', '41', '237', '53', '18', '10', '194', '97', '96', '208', '25', '10', '117', '104', '54', '96', '194', '21', '245', '238', '202', '209', '161', '97', '258', '240', '213', '136', '155', '152', '21', '191', '191', '118', '68', '258', '189', '197', '208', '61', '78', '135', '202', '96', '155', '108', '272', '80', '25', '155', '35', '213', '191', '194', '96', '96', '191', '264', '191', '191', '191', '2', '258', '191', '173', '259', '15', '47', '258', '156', '118', '160', '115', '105', '26', '173', '192', '76', '76', '135', '240', '118', '161', '200', '230', '18', '200', '25', '10', '161', '244', '24', '24', '200', '205', '104', '15', '193', '156', '95', '209', '240', '191', '194', '128', '208', '208', '218', '173', '10', '25', '18', '107', '46', '32', '197', '172', '209', '213', '79', '121', '78', '78', '193', '2', '78', '194', '149', '136', '80', '3', '21', '200', '78', '21', '197', '194', '118', '202', '161', '180', '209', '173', '2', '193', '6', '260', '197', '78', '78', '42', '191', '213', '135', '193', '196', '202', '222', '217', '42', '259', '213', '76', '46', '30', '15', '260', '202', '18', '259', '104', '135', '76', '196', '240', '240', '240', '108', '24', '200', '197', '262', '176', '15', '156', '264', '156', '200', '191', '161', '196', '78', '238', '161', '221', '118', '14', '14', '14', '200', '278', '238', '200', '196', '78', '173', '25', '197', '197', '196', '161', '18', '10', '10', '10', '18', '66', '203', '19', '246', '196', '244', '240', '161', '200', '78', '240', '209', '208', '118', '215', '194', '193', '168', '229', '191', '135', '172', '196', '197', '161', '78', '196', '197', '258', '78', '78', '238', '221', '121', '264', '104', '146', '50', '78', '217', '11', '238', '238', '238', '3', '3', '154', '215', '79', '18', '158', '104', '18', '13', '197', '117', '197', '161', '213', '242', '32', '238', '230', '2', '2', '32', '161', '251', '96', '156', '242', '22', '22', '140', '3', '149', '135', '191', '173', '135', '200', '238', '238', '78', '156', '209', '78', '97', '97', '160', '25', '258', '240', '161', '15', '161', '96', '209', '80', '132', '140', '4', '1', '240', '196', '274', '272', '62', '80', '196', '196', '191', '79', '156', '15', '196', '209', '22', '251', '202', '202', '121', '73', '191', '238', '191', '150', '150', '135', '46', '203', '202', '238', '22', '3', '258', '239', '238', '238', '165', '238', '238', '194', '200', '200', '197', '200', '196', '191', '194', '76', '78', '208', '200', '71', '208', '240', '76', '117', '120', '108', '96', '144', '30', '133', '133', '202', '117', '161', '251', '29', '104', '242', '197', '118', '216', '262', '236', '141', '11', '235', '235', '133', '191', '238', '202', '80', '80', '22', '140', '191', '197', '202', '133', '184', '184', '184', '184', '143', '274', '50', '237', '196', '274', '265', '10', '18', '10', '121', '238', '245', '228', '76', '25', '191', '197', '268', '10', '240', '195', '196', '195', '15', '48', '200', '183', '70', '59', '197', '196', '235', '161', '161', '215', '104', '97', '191', '194', '191', '200', '200', '197', '76', '200', '93', '256', '160', '238', '146', '238', '239', '15', '140', '258', '140', '21', '79', '202', '258', '79', '62', '205', '161', '104', '10', '161', '196', '137', '152', '191', '117', '213', '242', '176', '218', '195', '191', '162', '191', '135', '241', '213', '213', '84', '79', '68', '18', '98', '197', '155', '41', '246', '269', '78', '209', '238', '80', '238', '62', '237', '273', '273', '230', '150', '121', '260', '244', '161', '202', '78', '109', '155', '264', '145', '50', '274', '173', '210', '258', '238', '211', '156', '215', '195', '177', '244', '3', '141', '196', '191', '194', '29', '30', '10', '16', '200', '200', '156', '53', '3', '62', '165', '173', '173', '22', '205', '274', '150', '183', '116', '188', '188', '98', '161', '204', '80', '172', '156', '104', '229', '259', '78', '169', '3', '200', '118', '161', '80', '260', '240', '150', '197', '196', '62', '93', '6', '272', '272', '197', '160', '78', '96', '70', '195', '191', '191', '80', '3', '3', '240', '147', '196', '191', '238', '96', '203', '3', '15', '10', '58', '55', '96', '41', '15', '26', '50', '238', '194', '79', '240', '161', '191', '195', '263', '50', '240', '147', '235', '195', '203', '59', '178', '176', '2', '15', '203', '191', '18', '278', '196', '14', '153', '58', '175', '18', '217', '217', '80', '191', '200', '18', '200', '173', '24', '171', '171', '210', '124', '34', '200', '218', '104', '4', '223', '193', '194', '200', '200', '13', '62', '194', '197', '80', '197', '146', '146', '96', '97', '97', '2', '196', '191', '191', '104', '238', '10', '200', '239', '135', '104', '213', '2', '221', '169', '25', '21', '194', '161', '21', '278', '3', '195', '46', '176', '25', '117', '191', '62', '62', '62', '78', '50', '264', '79', '197', '80', '80', '80', '191', '191', '78', '161', '194', '195', '197', '203', '259', '278', '49', '147', '147', '147', '147', '22', '78', '78', '258', '2', '202', '221', '55', '258', '104', '104', '84', '240', '221', '274', '104', '22', '241', '63', '197', '197', '196', '159', '200', '22', '22', '22', '78', '78', '173', '104', '97', '104', '191', '146', '223', '268', '96', '14', '173', '204', '241', '97', '15', '118', '78', '78', '196', '194', '258', '96', '96', '259', '75', '10', '197', '30', '2', '238', '121', '50', '161', '173', '173', '186', '185', '186', '78', '194', '221', '218', '35', '33', '13', '34', '78', '21', '215', '97', '60', '24', '78', '193', '173', '235', '78', '200', '258', '140', '259', '194', '197', '195', '200', '145', '76', '108', '130', '238', '215', '213', '2', '217', '121', '178', '140', '62', '156', '176', '196', '196', '41', '223', '147', '172', '79', '200', '2', '244', '244', '191', '213', '104', '191', '237', '237', '33', '117', '195', '195', '264', '195', '140', '210', '41', '197', '14', '257', '196', '238', '238', '79', '161', '156', '196', '238', '238', '213', '104', '3', '10', '118', '204', '235', '33', '259', '274', '259', '176', '78', '78', '196', '34', '52', '25', '30', '194', '153', '21', '70', '191', '18', '191', '191', '172', '194', '194', '194', '194', '148', '104', '125', '2', '2', '2', '185', '194', '29', '104', '78', '189', '272', '204', '183', '213', '78', '104', '197', '240', '97', '168', '168', '182', '238', '197', '18', '194', '229', '221', '62', '78', '53', '104', '214', '189', '189', '189', '213', '117', '117', '191', '29', '104', '104', '276', '276', '22', '195', '195', '155', '139', '139', '59', '168', '135', '140', '79', '146', '200', '245', '70', '160', '153', '50', '202', '154', '263', '130', '194', '117', '194', '194', '191', '33', '29', '29', '238', '239', '194', '21', '274', '215', '18', '18', '140', '2', '215', '25', '46', '137', '238', '30', '160', '156', '46', '196', '263', '242', '104', '0', '0', '73', '213', '135', '22', '191', '191', '241', '202', '221', '15', '15', '191', '238', '213', '194', '197', '179', '238', '200', '200', '200', '5', '238', '194', '55', '117', '166', '49', '244', '239', '191', '78', '78', '78', '194', '200', '117', '210', '22', '121', '121', '121', '21', '237', '194', '121', '78', '104', '194', '79', '191', '251', '195', '197', '274', '176', '21', '43', '43', '10', '61', '215', '215', '30', '214', '194', '10', '30', '139', '96', '226', '226', '265', '2', '2', '121', '121', '41', '237', '104', '238', '238', '161', '78', '78', '78', '11', '184', '140', '140', '161', '104', '104', '237', '130', '195', '160', '118', '161', '18', '11', '241', '184', '196', '178', '22', '30', '146', '146', '10', '10', '141', '141', '135', '18', '18', '10', '26', '208', '97', '141', '18', '18', '97', '194', '194', '79', '213', '205', '205', '205', '75', '241', '50', '226', '238', '97', '222', '215', '215', '139', '238', '51', '238', '96', '173', '109', '200', '117', '192', '2', '195', '10', '3', '18', '49', '238', '238', '191', '195', '52', '197', '200', '200', '79', '191', '200', '200', '30', '195', '197', '195', '25', '260', '73', '30', '193', '21', '200', '242', '203', '120', '78', '260', '241', '0', '30', '202', '195', '30', '191', '78', '194', '194', '2', '205', '191', '196', '173', '197', '30', '196', '172', '139', '2', '18', '191', '230', '56', '194', '215', '195', '2', '71', '13', '146', '264', '50', '191', '235', '93', '191', '139', '139', '227', '235', '186', '235', '24', '18', '261', '128', '33', '71', '25', '176', '39', '10', '195', '202', '29', '29', '221', '29', '197', '139', '78', '227', '146', '137', '195', '205', '205', '195', '135', '62', '153', '146', '29', '75', '0', '194', '194', '180', '213', '78', '18', '194', '239', '196', '239', '197', '194', '153', '192', '221', '10', '46', '50', '241', '117', '117', '117', '195', '173', '78', '195', '194', '239', '2', '12', '173', '197', '30', '269', '197', '176', '135', '30', '146', '191', '93', '93', '135', '260', '218', '156', '155', '200', '241', '125', '18', '18', '183', '203', '239', '259', '173', '213', '180', '180', '210', '120', '104', '194', '156', '258', '172', '9', '0', '241', '192', '143', '189', '204', '50', '63', '197', '243', '241', '93', '125', '2', '21', '260', '4', '4', '3', '191', '150', '128', '78', '238', '245', '8', '80', '238', '79', '79', '240', '58', '104', '155', '80', '80', '188', '191', '235', '170', '170', '170', '146', '258', '195', '276', '25', '78', '215', '240', '161', '53', '147', '78', '78', '78', '78', '221', '221', '96', '18', '19', '130', '18', '18', '18', '80', '80', '194', '79', '93', '121', '78', '139', '139', '78', '197', '121', '194', '200', '80', '101', '22', '12', '135', '97', '205', '181', '60', '193', '15', '140', '172', '191', '241', '222', '135', '29', '25', '179', '191', '191', '259', '194', '133', '240', '121', '121', '241', '195', '29', '191', '209', '18', '139', '139', '50', '18', '12', '18', '133', '200', '25', '10', '70', '25', '242', '18', '161', '49', '172', '172', '172', '172', '128', '96', '195', '200', '78', '10', '239', '22', '173', '142', '142', '22', '194', '192', '3', '200', '241', '96', '241', '62', '139', '213', '202', '135', '79', '120', '192', '78', '118', '241', '46', '151', '150', '151', '150', '118', '221', '183', '237', '196', '140', '140', '39', '179', '213', '140', '41', '73', '78', '217', '258', '30', '118', '140', '219', '50', '10', '139', '150', '251', '24', '10', '18', '78', '18', '172', '135', '146', '71', '196', '202', '197', '25', '106', '160', '215', '15', '6', '101', '165', '140', '145', '80', '3', '238', '239', '78', '268', '0', '109', '191', '215', '238', '238', '238', '203', '50', '146', '15', '25', '156', '191', '16', '2', '195', '189', '96', '193', '109', '204', '161', '185', '210', '50', '235', '30', '242', '193', '102', '194', '121', '177', '241', '102', '132', '240', '71', '192', '79', '25', '197', '39', '161', '22', '258', '237', '15', '7', '78', '156', '197', '70', '176', '139', '240', '132', '78', '177', '191', '194', '117', '10', '79', '238', '195', '24', '101', '118', '196', '2', '102', '21', '180', '185', '170', '79', '139', '135', '191', '151', '200', '191', '154', '262', '200', '237', '56', '27', '191', '180', '258', '139', '139', '192', '30', '0', '242', '239', '242', '25', '191', '242', '240', '3', '56', '215', '171', '192', '193', '222', '238', '237', '215', '272', '2', '156', '237', '151', '189', '80', '15', '260', '260', '217', '235', '22', '228', '135', '18', '191', '157', '78', '2', '240', '213', '179', '2', '191', '79', '79', '153', '3', '194', '78', '121', '71', '121', '149', '257', '194', '191', '188', '15', '25', '125', '102', '15', '130', '130', '177', '10', '135', '265', '240', '149', '196', '200', '204', '204', '213', '177', '197', '194', '13', '202', '30', '213', '125', '193', '125', '125', '125', '125', '125', '15', '180', '78', '1', '25', '96', '200', '29', '156', '172', '202', '22', '78', '197', '188', '241', '101', '101', '101', '221', '50', '191', '15', '15', '0', '101', '101', '168', '56', '197', '130', '62', '204', '29', '10', '96', '97', '139', '97', '97', '2', '2', '262', '39', '176', '172', '194', '194', '18', '78', '13', '80', '139', '196', '207', '191', '151', '192', '150', '13', '104', '104', '101', '15', '125', '101', '15', '0', '50', '194', '264', '50', '25', '25', '125', '125', '125', '125', '125', '25', '29', '25', '25', '78', '240', '261', '19', '2', '74', '78', '238', '50', '50', '203', '197', '71', '104', '80', '194', '149', '170', '130', '2', '237', '170', '202', '48', '97', '93', '179', '235', '78', '102', '102', '70', '208', '238', '104', '146', '118', '118', '96', '237', '274', '218', '241', '102', '146', '209', '209', '240', '170', '137', '7', '197', '191', '264', '29', '46', '96', '193', '19', '12', '213', '215', '275', '179', '18', '183', '78', '221', '18', '170', '119', '21', '140', '240', '96', '192', '192', '21', '96', '149', '239', '135', '191', '240', '184', '183', '24', '2', '96', '197', '194', '215', '18', '192', '121', '202', '196', '25', '97', '214', '3', '97', '16', '0', '192', '135', '215', '5', '12', '80', '143', '202', '242', '151', '13', '209', '203', '200', '30', '209', '194', '102', '102', '102', '102', '102', '102', '22', '24', '6', '18', '5', '191', '101', '50', '101', '101', '192', '153', '153', '101', '101', '10', '192', '25', '140', '50', '241', '102', '102', '32', '150', '101', '102', '153', '102', '101', '153', '102', '25', '10', '125', '18', '125', '177', '209', '102', '10', '78', '78', '137', '18', '237', '241', '175', '195', '175', '96', '176', '237', '96', '96', '101', '193', '238', '170', '238', '102', '222', '268', '238', '102', '244', '34', '18', '244', '202', '149', '149', '193', '102', '170', '194', '75', '101', '222', '238', '50', '195', '135', '22', '93', '18', '102', '102', '93', '93', '191', '147', '244', '96', '2', '241', '241', '93', '189', '189', '177', '35', '137', '197', '191', '40', '40', '195', '176', '35', '200', '200', '237', '135', '140', '140', '93', '156', '200', '78', '191', '22', '81', '238', '101', '213', '101', '78', '109', '19', '140', '196', '78', '78', '76', '218', '14', '265', '101', '241', '155', '4', '50', '78', '78', '238', '135', '191', '265', '81', '238', '238', '102', '102', '102', '140', '162', '32', '17', '17', '171', '171', '191', '101', '102', '192', '101', '17', '171', '135', '102', '242', '195', '237', '21', '18', '238', '191', '278', '140', '149', '97', '260', '155', '276', '192', '196', '21', '191', '2', '97', '0', '150', '221', '213', '209', '238', '80', '260', '194', '104', '96', '193', '14', '240', '193', '117', '117', '155', '17', '264', '260', '209', '209', '2', '137', '71', '72', '101', '205', '173', '191', '118', '24', '41', '197', '78', '94', '237', '156', '193', '153', '154', '153', '209', '110', '78', '78', '213', '238', '78', '191', '93', '79', '177', '267', '133', '133', '46', '139', '40', '143', '74', '241', '202', '50', '3', '102', '78', '149', '191', '156', '18', '93', '195', '192', '137', '240', '135', '204', '189', '22', '56', '154', '25', '192', '130', '277', '191', '80', '39', '21', '21', '238', '238', '23', '192', '191', '192', '94', '94', '80', '139', '17', '10', '22', '227', '227', '140', '102', '21', '17', '227', '227', '23', '243', '264', '18', '274', '149', '238', '30', '221', '22', '238', '193', '135', '205', '195', '238', '238', '197', '130', '195', '18', '104', '102', '14', '59', '104', '18', '18', '258', '34', '241', '138', '188', '176', '102', '1', '140', '140', '22', '22', '278', '10', '96', '35', '145', '120', '33', '33', '22', '33', '22', '35', '22', '19', '33', '102', '192', '192', '175', '22', '72', '191', '222', '125', '22', '79', '18', '194', '18', '250', '192', '192', '96', '48', '274', '273', '132', '133', '192', '40', '171', '171', '246', '191', '10', '166', '197', '195', '125', '125', '200', '153', '192', '149', '110', '139', '194', '48', '102', '102', '104', '203', '238', '238', '238', '78', '18', '238', '121', '18', '238', '96', '238', '238', '13', '191', '18', '18', '240', '96', '40', '188', '18', '137', '175', '28', '28', '28', '149', '191', '191', '207', '239', '239', '153', '153', '153', '153', '153', '153', '154', '153', '154', '153', '153', '153', '18', '0', '27', '18', '25', '96', '96', '137', '33', '10', '130', '153', '153', '154', '180', '137', '44', '241', '153', '268', '25', '161', '25', '39', '242', '4', '238', '195', '102', '135', '13', '229', '140', '1', '79', '192', '229', '191', '118', '21', '260', '203', '18', '177', '191', '10', '238', '10', '133', '14', '175', '175', '192', '18', '153', '153', '254', '140', '149', '22', '22', '22', '237', '40', '135', '191', '192', '192', '62', '205', '19', '238', '176', '214', '200', '96', '15', '153', '191', '108', '29', '166', '28', '193', '197', '96', '195', '108', '102', '241', '195', '10', '33', '166', '197', '168', '2', '96', '26', '33', '204', '258', '149', '153', '153', '40', '137', '241', '166', '221', '23', '221', '153', '15', '113', '113', '78', '171', '40', '71', '21', '71', '80', '15', '191', '191', '135', '110', '10', '259', '102', '149', '149', '109', '109', '175', '97', '97', '97', '153', '15', '238', '193', '175', '38', '38', '137', '18', '235', '155', '137', '265', '110', '18', '18', '256', '191', '224', '173', '104', '10', '152', '24', '24', '24', '17', '40', '40', '40', '40', '40', '40', '112', '6', '18', '18', '210', '96', '191', '27', '213', '17', '153', '26', '133', '18', '18', '153', '38', '96', '40', '38', '40', '40', '10', '17', '17', '95', '71', '192', '17', '259', '213', '200', '15', '11', '207', '194', '18', '135', '238', '191', '241', '191', '116', '35', '147', '156', '67', '78', '17', '227', '191', '117', '188', '130', '72', '96', '235', '97', '18', '150', '203', '25', '126', '149', '200', '94', '4', '4', '1', '244', '228', '252', '149', '191', '149', '15', '17', '17', '29', '18', '18', '116', '23', '237', '94', '184', '268', '197', '175', '243', '28', '16', '10', '50', '191', '102', '102', '153', '149', '111', '78', '191', '135', '18', '35', '78', '238', '208', '93', '95', '258', '17', '111', '111', '262', '22', '22', '104', '161', '209', '21', '195', '12', '116', '241', '242', '239', '17', '97', '16', '40', '117', '94', '96', '78', '15', '103', '195', '135', '235', '247', '135', '130', '260', '239', '176', '93', '191', '209', '239', '93', '264', '30', '18', '30', '18', '18', '103', '191', '97', '17', '95', '115', '209', '140', '22', '22', '241', '10', '153', '3', '188', '238', '226', '195', '97', '96', '238', '97', '130', '97', '116', '96', '96', '242', '45', '149', '166', '155', '96', '116', '264', '10', '237', '192', '71', '22', '71', '175', '149', '116', '237', '274', '18', '235', '176', '116', '202', '250', '250', '78', '2', '196', '2', '97', '143', '18', '111', '111', '80', '78', '153', '80', '40', '110', '30', '112', '30', '111', '215', '112', '149', '78', '78', '209', '110', '110', '238', '109', '239', '112', '151', '154', '16', '16', '16', '78', '137', '179', '72', '149', '10', '111', '268', '96', '170', '111', '38', '192', '153', '183', '195', '30', '30', '147', '195', '147', '93', '264', '153', '25', '18', '19', '22', '74', '40', '93', '166', '15', '127', '147', '192', '260', '238', '103', '269', '127', '19', '19', '153', '18', '78', '238', '30', '30', '241', '16', '235', '250', '204', '238', '218', '40', '93', '184', '18', '0', '241', '195', '94', '154', '109', '156', '10', '21', '30', '197', '39', '78', '24', '107', '142', '142', '228', '229', '229', '229', '232', '179', '239', '142', '232', '23', '175', '147', '137', '137', '137', '171', '191', '117', '128', '127', '204', '98', '147', '235', '239', '153', '15', '204', '255', '80', '204', '204', '149', '113', '19', '167', '22', '250', '17', '17', '17', '18', '17', '67', '121', '125', '26', '97', '97', '97', '97', '247', '124', '163', '147', '112', '111', '208', '108', '38', '209', '154', '154', '194', '135', '22', '147', '241', '120', '218', '260', '156', '155', '241', '192', '215', '180', '179', '149', '40', '193', '192', '150', '102', '93', '1', '149', '137', '133', '102', '102', '102', '78', '145', '21', '96', '175', '175', '78', '123', '30', '95', '19', '18', '110', '18', '152', '152', '147', '26', '135', '260', '158', '203', '195', '158', '112', '156', '125', '213', '230', '30', '15', '125', '106', '111', '195', '181', '130', '195', '130', '239', '15', '194', '16', '15', '111', '61', '72', '57', '128', '239', '239', '203', '25', '218', '10', '155', '191', '46', '98', '175', '260', '102', '21', '191', '260', '191', '239', '15', '30', '93', '147', '103', '78', '177', '30', '95', '155', '155', '155', '103', '103', '112', '241', '191', '191', '17', '78', '153', '150', '113', '115', '147', '78', '40', '163', '33', '15', '149', '103', '192', '191', '96', '106', '115', '143', '235', '200', '204', '238', '204', '204', '74', '196', '113', '246', '102', '150', '16', '176', '20', '195', '128', '197', '15', '71', '71', '193', '24', '24', '24', '170', '204', '209', '111', '144', '112', '22', '222', '224', '28', '209', '155', '213', '275', '196', '239', '195', '25', '103', '149', '128', '19', '96', '102', '135', '17', '232', '155', '191', '20', '18', '241', '30', '25', '205', '97', '97', '96', '72', '182', '135', '135', '20', '149', '149', '78', '135', '40', '18', '153', '10', '26', '158', '205', '157', '157', '135', '135', '140', '155', '192', '26', '268', '263', '245', '155', '15', '177', '159', '212', '161', '239', '200', '40', '35', '153', '197', '259', '15', '254', '175', '10', '30', '239', '67', '195', '200', '191', '22', '66', '191', '112', '133', '128', '235', '161', '155', '17', '93', '36', '113', '239', '18', '18', '191', '165', '163', '163', '238', '79', '158', '40', '102', '40', '200', '191', '60', '241', '245', '96', '78', '30', '15', '173', '192', '192', '158', '64', '113', '200', '74', '128', '135', '115', '115', '10', '74', '102', '102', '115', '111', '238', '244', '109', '109', '238', '152', '10', '237', '18', '50', '16', '173', '204', '15', '208', '276', '43', '109', '258', '222', '15', '94', '235', '203', '79', '128', '113', '3', '204', '106', '133', '214', '218', '25', '98', '166', '71', '30', '175', '175', '97', '135', '108', '19', '18', '196', '35', '12', '196', '78', '166', '155', '72', '155', '22', '109', '258', '196', '263', '161', '204', '210', '14', '208', '208', '208', '208', '204', '102', '102', '213', '155', '251', '153', '153', '153', '154', '154', '237', '97', '19', '28', '94', '136', '112', '19', '97', '152', '153', '153', '111', '111', '135', '135', '105', '17', '105', '152', '152', '152', '153', '79', '266', '135', '120', '125', '149', '50', '162', '72', '17', '147', '109', '115', '24', '105', '244', '149', '238', '203', '35', '202', '177', '173', '239', '19', '19', '179', '237', '14', '18', '185', '58', '158', '144', '213', '237', '155', '250', '239', '93', '237', '97', '195', '157', '107', '109', '246', '165', '98', '49', '2', '266', '149', '20', '18', '71', '108', '22', '115', '239', '40', '178', '232', '238', '218', '118', '118', '210', '235', '46', '228', '192', '112', '149', '96', '111', '21', '95', '95', '95', '18', '57', '206', '18', '239', '157', '80', '196', '112', '176', '99', '106', '71', '235', '135', '18', '209', '78', '20', '20', '103', '33', '3', '103', '166', '79', '79', '156', '229', '18', '238', '2', '16', '128', '200', '117', '22', '197', '166', '78', '237', '241', '107', '53', '189', '204', '243', '163', '221', '15', '200', '221', '152', '23', '193', '107', '195', '99', '97', '19', '125', '276', '113', '193', '64', '78', '197', '117', '175', '135', '135', '78', '93', '154', '153', '256', '256', '135', '137', '135', '137', '149', '110', '15', '159', '21', '158', '192', '192', '117', '29', '97', '258', '78', '46', '5', '80', '15', '105', '260', '161', '191', '2', '138', '117', '215', '176', '156', '194', '265', '115', '181', '21', '14', '106', '106', '108', '200', '156', '103', '102', '29', '17', '204', '175', '227', '118', '18', '239', '204', '258', '78', '155', '213', '109', '192', '186', '186', '15', '241', '133', '30', '30', '239', '30', '218', '109', '138', '241', '241', '45', '135', '112', '232', '104', '106', '200', '53', '23', '112', '17', '117', '59', '260', '195', '106', '117', '159', '204', '200', '228', '165', '165', '261', '200', '227', '166', '93', '21', '80', '112', '152', '196', '203', '228', '98', '102', '15', '139', '110', '13', '238', '6', '171', '94', '208', '155', '112', '203', '239', '191', '39', '136', '203', '195', '46', '203', '148', '192', '210', '23', '4', '153', '29', '165', '192', '204', '232', '94', '112', '239', '98', '71', '248', '17', '155', '10', '78', '191', '191', '111', '25', '25', '19', '19', '17', '235', '102', '265', '195', '235', '163', '238', '71', '197', '107', '155', '30', '72', '19', '259', '80', '155', '23', '64', '239', '197', '156', '57', '203', '258', '200', '205', '246', '238', '258', '227', '238', '238', '19', '148', '161', '111', '105', '202', '109', '262', '133', '137', '221', '135', '52', '170', '111', '36', '30', '156', '102', '268', '115', '22', '153', '133', '259', '98', '235', '135', '191', '25', '235', '25', '29', '94', '195', '203', '212', '112', '135', '245', '260', '200', '96', '158', '23', '165', '22', '18', '249', '240', '109', '43', '37', '15', '241', '241', '23', '135', '240', '108', '98', '268', '275', '191', '136', '263', '259', '196', '98', '98', '204', '248', '123', '105', '204', '240', '9', '9', '98', '218', '189', '238', '246', '155', '130', '157', '74', '109', '29', '189', '203', '213', '195', '241', '159', '238', '204', '218', '105', '23', '16', '263', '206', '150', '152', '204', '202', '173', '267', '18', '153', '155', '196', '2', '97', '158', '158', '204', '98', '202', '98', '204', '258', '98', '22', '113', '113', '135', '71', '237', '155', '163', '250', '233', '26', '26', '26', '26', '26', '19', '170', '18', '15', '19', '191', '30', '30', '48', '35', '30', '237', '46', '119', '16', '78', '239', '150', '245', '235', '158', '27', '241', '30', '38', '107', '75', '241', '135', '106', '106', '98', '159', '96', '135', '111', '176', '175', '200', '18', '14', '242', '163', '187', '20', '12', '191', '23', '106', '171', '25', '78', '162', '191', '157', '94', '237', '246', '203', '108', '238', '204', '204', '1', '102', '154', '106', '170', '102', '200', '2', '135', '106', '258', '258', '227', '26', '15', '25', '71', '203', '96', '3', '241', '200', '30', '108', '98', '108', '27', '106', '26', '150', '259', '78', '113', '113', '158', '202', '16', '136', '239', '156', '239', '35', '276', '252', '115', '22', '22', '258', '172', '3', '154', '3', '98', '237', '102', '235', '112', '206', '98', '57', '118', '153', '18', '26', '30', '196', '163', '191', '153', '72', '24', '156', '117', '122', '30', '135', '192', '98', '28', '30', '30', '95', '241', '155', '17', '195', '31', '103', '202', '98', '237', '94', '112', '163', '163', '94', '163', '112', '19', '195', '191', '15', '135', '78', '109', '160', '152', '97', '98', '107', '167', '97', '138', '138', '26', '191', '97', '97', '30', '30', '206', '202', '13', '30', '240', '111', '261', '79', '0', '111', '19', '152', '107', '107', '18', '98', '28', '258', '244', '106', '237', '204', '191', '15', '183', '202', '16', '111', '166', '268', '202', '12', '30', '78', '110', '133', '71', '197', '111', '52', '79', '171', '156', '94', '13', '118', '111', '71', '133', '18', '105', '30', '22', '133', '60', '237', '191', '6', '126', '3', '238', '276', '78', '162', '150', '115', '191', '107', '200', '148', '135', '72', '155', '241', '4', '108', '160', '203', '20', '133', '2', '80', '80', '154', '18', '15', '115', '239', '167', '167', '35', '268', '18', '221', '18', '195', '204', '195', '216', '133', '112', '104', '223', '191', '115', '30', '30', '30', '30', '30', '202', '204', '165', '200', '200', '29', '191', '237', '267', '18', '175', '81', '264', '264', '178', '115', '102', '178', '212', '273', '202', '109', '259', '166', '135', '155', '219', '161', '18', '94', '241', '191', '196', '191', '98', '26', '26', '204', '204', '135', '27', '238', '260', '182', '26', '98', '94', '93', '238', '156', '19', '111', '79', '16', '192', '113', '19', '241', '18', '202', '260', '272', '106', '163', '156', '106', '17', '109', '221', '78', '203', '7', '136', '197', '193', '191', '98', '112', '200', '95', '221', '98', '161', '191', '3', '106', '189', '111', '25', '196', '196', '241', '163', '200', '24', '78', '176', '3', '98', '102', '98', '19', '161', '224', '80', '18', '191', '112', '105', '254', '71', '200', '137', '29', '20', '165', '8', '106', '23', '106', '155', '200', '265', '135', '126', '203', '188', '161', '158', '258', '255', '98', '203', '50', '35', '102', '158', '175', '40', '239', '26', '66', '57', '193', '155', '15', '135', '10', '107', '106', '229', '18', '191', '204', '107', '99', '235', '238', '18', '205', '205', '35', '238', '208', '203', '110', '110', '109', '112', '23', '18', '19', '182', '156', '18', '18', '156', '111', '135', '137', '133', '137', '240', '99', '197', '113', '15', '165', '39', '97', '97', '15', '150', '163', '109', '268', '15', '15', '238', '203', '67', '161', '197', '18', '196', '97', '161', '197', '161', '185', '238', '191', '30', '121', '133', '160', '160', '74', '135', '75', '54', '135', '196', '23', '241', '166', '194', '239', '135', '240', '208', '238', '98', '135', '152', '176', '22', '174', '113', '241', '237', '57', '195', '191', '24', '165', '165', '204', '204', '56', '109', '27', '153', '252', '228', '165', '252', '200', '28', '158', '135', '196', '22', '18', '133', '30', '204', '237', '196', '93', '106', '94', '25', '93', '159', '4', '195', '23', '237', '25', '199', '57', '26', '276', '274', '204', '75', '18', '204', '108', '22', '133', '191', '111', '204', '203', '133', '237', '166', '4', '62', '202', '166', '161', '165', '74', '109', '118', '200', '22', '206', '25', '209', '17', '155', '204', '64', '23', '30', '202', '149', '171', '135', '136', '184', '26', '18', '159', '195', '18', '166', '156', '98', '133', '38', '40', '18', '18', '166', '109', '109', '209', '22', '195', '163', '163', '108', '18', '15', '99', '135', '174', '26', '19', '166', '133', '259', '151', '235', '204', '228', '133', '30', '161', '111', '242', '84', '238', '98', '195', '78', '160', '28', '25', '264', '165', '41', '19', '55', '46', '202', '121', '15', '32', '98', '238', '0', '78', '31', '30', '203', '264', '203', '135', '104', '111', '78', '212', '111', '98', '126', '22', '2', '30', '237', '212', '265', '163', '106', '106', '235', '109', '223', '161', '202', '150', '170', '255', '112', '114', '170', '98', '96', '197', '36', '18', '228', '14', '255', '204', '102', '239', '0', '185', '30', '22', '23', '246', '199', '133', '191', '78', '195', '153', '112', '191', '200', '170', '171', '200', '78', '128', '25', '153', '237', '99', '94', '208', '107', '208', '208', '78', '204', '204', '27', '191', '272', '134', '158', '158', '174', '118', '64', '18', '258', '150', '189', '18', '153', '241', '204', '112', '106', '135', '71', '148', '7', '133', '140', '187', '23', '223', '19', '19', '2', '136', '2', '200', '161', '15', '98', '158', '148', '3', '250', '153', '235', '165', '220', '176', '110', '240', '151', '148', '133', '74', '193', '46', '27', '173', '94', '176', '105', '256', '161', '226', '18', '78', '191', '171', '243', '26', '0', '108', '167', '159', '239', '150', '112', '155', '79', '241', '202', '165', '254', '107', '202', '156', '166', '193', '239', '135', '15', '203', '187', '148', '25', '15', '22', '194', '239', '56', '191', '176', '262', '200', '110', '72', '99', '102', '135', '98', '113', '15', '15', '159', '262', '171', '151', '98', '204', '213', '255', '26', '243', '175', '156', '98', '107', '191', '165', '241', '26', '165', '171', '30', '160', '166', '266', '11', '19', '211', '93', '254', '30', '239', '115', '273', '205', '112', '237', '263', '74', '98', '79', '122', '94', '265', '155', '191', '16', '235', '229', '98', '191', '227', '17', '111', '267', '135', '202', '195', '10', '180', '196', '196', '133', '135', '133', '95', '111', '122', '258', '191', '3', '114', '107', '19', '264', '74', '56', '200', '200', '133', '161', '97', '15', '97', '197', '196', '253', '93', '202', '258', '106', '153', '26', '109', '40', '149', '8', '196', '15', '119', '155', '23', '237', '193', '264', '148', '151', '238', '200', '94', '75', '148', '175', '24', '195', '171', '30', '203', '200', '202', '16', '158', '223', '191', '22', '264', '135', '35', '57', '94', '191', '196', '202', '98', '18', '196', '3', '133', '78', '191', '208', '258', '18', '200', '29', '135', '109', '110', '2', '185', '98', '133', '19', '22', '204', '95', '238', '25', '133', '250', '135', '135', '3', '47', '158', '187', '2', '78', '115', '155', '3', '218', '166', '112', '266', '22', '195', '135', '72', '97', '197', '209', '127', '80', '2', '98', '14', '239', '15', '113', '165', '204', '78', '204', '191', '175', '200', '193', '18', '196', '133', '109', '237', '56', '196', '3', '163', '163', '163', '195', '15', '263', '18', '84', '18', '133', '107', '18', '102', '156', '238', '102', '26', '244', '82', '75', '23', '258', '238', '136', '3', '204', '184', '200', '196', '158', '106', '27', '238', '155', '152', '50', '50', '2', '235', '98', '203', '166', '109', '25', '18', '151', '133', '200', '26', '26', '26', '19', '107', '156', '23', '26', '3', '70', '225', '41', '26', '239', '26', '186', '26', '34', '272', '272', '104', '78', '260', '18', '204', '26', '78', '18', '210', '135', '203', '71', '207', '26', '133', '13', '25', '26', '115', '94', '264', '135', '244', '79', '30', '263', '22', '109', '205', '176', '106', '22', '17', '151', '106', '191', '160', '160', '165', '78', '177', '196', '260', '166', '35', '191', '260', '98', '18', '237', '18', '161', '18', '197', '133', '30', '35', '200', '170', '155', '245', '19', '200', '160', '96', '172', '165', '55', '137', '241', '133', '191', '112', '25', '126', '238', '18', '18', '108', '135', '161', '135', '73', '112', '107', '8', '112', '109', '195', '25', '30', '105', '161', '241', '110', '75', '238', '215', '238', '104', '25', '3', '155', '30', '16', '3', '175', '99', '3', '200', '23', '3', '199', '203', '191', '196', '196', '204', '98', '105', '133', '197', '135', '161', '39', '15', '30', '133', '55', '191', '203', '133', '203', '140', '200', '149', '30', '96', '135', '23', '26', '3', '18', '155', '158', '135', '135', '18', '250', '18', '22', '114', '18', '18', '166', '18', '30', '18', '239', '18', '161', '50', '50', '49', '51', '23', '55', '98', '78', '210', '211', '18', '239', '78', '13', '193', '114', '78', '18', '25', '133', '19', '176', '4', '173', '50', '108', '17', '4', '204', '94', '112', '207', '109', '115', '23', '235', '197', '194', '197', '213', '148', '40', '157', '107', '200', '147', '14', '14', '18', '238', '18', '238', '18', '238', '57', '200', '135', '30', '18', '191', '191', '200', '166', '151', '161', '73', '19', '73', '239', '239', '165', '108', '2', '93', '166', '133', '133', '173', '35', '193', '114', '74', '19', '40', '58', '104', '16', '260', '160', '158', '112', '210', '158', '167', '203', '203', '3', '3', '109', '30', '133', '258', '70', '204', '200', '152', '155', '192', '204', '108', '191', '204', '26', '98', '26', '98', '264', '158', '18', '150', '137', '153', '196', '199', '191', '115', '168', '157', '149', '151', '78', '137', '160', '259', '166', '196', '171', '175', '199', '166', '161', '70', '13', '166', '152', '152', '206', '58', '18', '207', '58', '74', '207', '29', '46', '109', '135', '74', '174', '109', '40', '10', '30', '258', '16', '111', '30', '15', '239', '204', '48', '25', '196', '0', '156', '72', '72', '96', '22', '72', '133', '133', '238', '26', '3', '50', '152', '193', '235', '136', '153', '50', '1', '19', '239', '30', '25', '179', '161', '30', '191', '3', '27', '133', '3', '40', '23', '133', '150', '151', '265', '32', '191', '3', '25', '170', '238', '40', '259', '166', '137', '241', '133', '209', '159', '78', '78', '153', '26', '52', '241', '36', '250', '96', '30', '30', '38', '18', '18', '238', '99', '72', '111', '3', '244', '191', '226', '3', '191', '114', '135', '109', '40', '79', '202', '78', '114', '238', '10', '150', '108', '163', '108', '113', '197', '109', '115', '161', '3', '238', '3', '237', '161', '107', '155', '78', '202', '112', '38', '195', '137', '250', '133', '237', '161', '25', '26', '25', '15', '15', '239', '166', '114', '229', '111', '40', '135', '94', '102', '162', '204', '237', '17', '166', '136', '22', '98', '99', '99', '166', '35', '196', '3', '203', '135', '135', '84', '98', '266', '238', '13', '166', '78', '193', '98', '241', '104', '153', '192', '200', '10', '95', '95', '148', '107', '14', '17', '191', '239', '19', '203', '255', '197', '213', '94', '3', '107', '23', '35', '10', '237', '235', '78', '29', '18', '160', '200', '150', '99', '109', '155', '161', '133', '114', '18', '18', '18', '264', '133', '104', '161', '70', '17', '204', '195', '204', '241', '99', '82', '241', '241', '161', '18', '235', '175', '2', '2', '202', '50', '230', '26', '40', '40', '200', '78', '156', '26', '112', '26', '30', '71', '184', '200', '135', '153', '23', '106', '125', '175', '133', '133', '7', '38', '196', '161', '0', '49', '241', '158', '15', '21', '104', '2', '3', '176', '259', '135', '78', '75', '112', '200', '1', '207', '238', '161', '150', '158', '2', '45', '31', '237', '149', '3', '26', '165', '135', '152', '115', '133', '18', '40', '166', '30', '173', '40', '26', '78', '50', '48', '50', '13', '45', '200', '200', '18', '151', '205', '135', '38', '117', '196', '135', '155', '111', '196', '196', '166', '23', '25', '161', '18', '124', '259', '165', '58', '170', '204', '250', '103', '160', '238', '194', '163', '241', '255', '235', '17', '133', '26', '135', '196', '237', '114', '258', '193', '248', '40', '195', '84', '238', '146', '199', '40', '161', '22', '137', '43', '166', '156', '4', '104', '133', '177', '211', '193', '78', '203', '246', '237', '264', '3', '149', '78', '78', '111', '239', '13', '238', '176', '112', '150', '156', '3', '6', '118', '178', '178', '21', '186', '150', '241', '50', '243', '191', '121', '278', '29', '200', '18', '3', '197', '191', '98', '173', '195', '25', '195', '264', '221', '15', '156', '25', '200', '211', '195', '26', '35', '2', '135', '126', '113', '167', '191', '264', '196', '133', '18', '26', '146', '203', '219', '23', '160', '160', '135', '259', '3', '159', '120', '160', '106', '240', '133', '115', '258', '38', '22', '3', '26', '22', '107', '49', '48', '25', '237', '72', '219', '196', '196', '111', '135', '196', '18', '166', '172', '135', '26', '258', '70', '98', '40', '166', '23', '96', '3', '30', '155', '126', '146', '114', '242', '237', '157', '238', '241', '40', '133', '200', '161', '175', '200', '175', '84', '266', '18', '99', '30', '242', '151', '153', '166', '199', '228', '3', '237', '132', '110', '135', '80', '15', '15', '15', '84', '133', '15', '177', '3', '202', '255', '71', '115', '235', '19', '18', '18', '102', '58', '78', '165', '166', '264', '12', '150', '15', '2', '18', '135', '109', '200', '258', '128', '3', '199', '197', '239', '109', '222', '239', '107', '125', '132', '204', '204', '133', '135', '30', '158', '203', '15', '264', '237', '133', '196', '3', '252', '26', '98', '79', '199', '210', '0', '99', '30', '26', '259', '6', '49', '166', '98', '108', '9', '133', '102', '155', '98', '114', '78', '35', '166', '188', '202', '235', '30', '149', '165', '176', '138', '29', '153', '25', '25', '137', '133', '166', '135', '125', '71', '193', '154', '165', '98', '23', '56', '198', '26', '26', '0', '109', '136', '163', '151', '25', '15', '241', '195', '204', '191', '195', '161', '175', '133', '237', '78', '165', '258', '135', '78', '135', '258', '18', '235', '30', '187', '150', '275', '149', '2', '36', '112', '22', '161', '238', '203', '112', '265', '75', '133', '205', '258', '158', '18', '26', '18', '25', '26', '15', '26', '239', '15', '240', '18', '71', '71', '78', '135', '3', '148', '166', '26', '176', '2', '197', '150', '176', '19', '18', '97', '166', '18', '153', '250', '133', '210', '20', '192', '196', '26', '191', '2', '81', '70', '5', '79', '3', '30', '193', '193', '69', '160', '176', '246', '108', '54', '250', '135', '200', '80', '3', '167', '18', '166', '200', '152', '23', '149', '19', '14', '13', '237', '196', '19', '112', '254', '204', '191', '23', '264', '199', '209', '137', '31', '135', '238', '78', '18', '191', '15', '164', '195', '74', '163', '163', '41', '99', '26', '135', '26', '40', '40', '250', '200', '200', '26', '237', '4', '195', '158', '35', '71', '191', '3', '218', '274', '58', '3', '23', '177', '132', '49', '62', '238', '166', '97', '264', '193', '29', '26', '49', '241', '197', '200', '64', '227', '258', '44', '15', '133', '175', '238', '109', '254', '237', '198', '175', '166', '213', '174', '196', '107', '259', '98', '10', '196', '210', '191', '96', '166', '135', '15', '197', '78', '78', '195', '176', '18', '98', '129', '170', '18', '247', '203', '26', '158', '40', '64', '200', '163', '196', '19', '205', '197', '193', '191', '69', '96', '74', '35', '36', '64', '133', '172', '19', '266', '96', '94', '103', '258', '193', '264', '98', '254', '50', '193', '25', '170', '75', '84', '27', '252', '203', '94', '94', '19', '98', '137', '12', '153', '3', '155', '135', '238', '15', '196', '15', '26', '84', '49', '27', '135', '203', '48', '133', '129', '133', '258', '155', '35', '195', '71', '241', '135', '166', '193', '165', '104', '196', '117', '196', '26', '237', '109', '95', '196', '60', '40', '158', '193', '250', '104', '94', '178', '27', '193', '78', '106', '79', '204', '133', '237', '235', '135', '202', '70', '15', '264', '241', '254', '110', '199', '49', '255', '30', '193', '238', '204', '102', '97', '161', '43', '228', '1', '6', '196', '193', '26', '15', '128', '46', '155', '32', '133', '69', '47', '135', '78', '205', '200', '200', '202', '3', '95', '191', '78', '26', '248', '258', '191', '240', '49', '110', '38', '193', '200', '200', '96', '200', '204', '135', '9', '200', '258', '15', '46', '15', '149', '155', '197', '2', '166', '151', '72', '135', '97', '211', '238', '135', '163', '161', '26', '163', '129', '241', '239', '241', '16', '26', '18', '263', '260', '166', '3', '264', '195', '79', '132', '251', '50', '133', '15', '25', '15', '191', '15', '78', '50', '78', '71', '227', '69', '200', '69', '109', '64', '133', '197', '196', '3', '45', '238', '26', '132', '104', '196', '193', '136', '80', '26', '75', '117', '14', '78', '75', '98', '15', '2', '15', '50', '255', '136', '3', '112', '173', '69', '191', '19', '239', '235', '109', '70', '135', '78', '165', '238', '262', '199', '35', '203', '224', '193', '15', '18', '150', '238', '197', '238', '193', '135', '3', '150', '187', '199', '239', '109', '152', '27', '257', '6', '235', '104', '191', '133', '166', '155', '26', '16', '136', '198', '98', '154', '235', '27', '102', '30', '191', '15', '132', '132', '114', '107', '241', '241', '156', '199', '241', '49', '62', '117', '258', '30', '163', '133', '238', '200', '3', '15', '109', '2', '196', '165', '166', '159', '210', '35', '15', '17', '200', '241', '106', '203', '197', '197', '79', '98', '98', '23', '204', '48', '129', '57', '258', '202', '215', '238', '70', '64', '193', '189', '57', '200', '0', '98', '135', '204', '25', '30', '204', '27', '26', '78', '135', '109', '3', '197', '3', '167', '35', '196', '225', '2', '194', '200', '15', '161', '198', '23', '133', '238', '195', '35', '50', '135', '152', '163', '21', '15', '94', '26', '94', '53', '260', '196', '154', '277', '200', '118', '6', '238', '50', '209', '50', '3', '238', '3', '15', '107', '135', '199', '15', '135', '26', '258', '251', '203', '19', '259', '195', '199', '238', '15', '254', '112', '211', '200', '129', '84', '196', '3', '208', '259', '135', '104', '29', '136', '31', '78', '13', '3', '64', '75', '55', '114', '160', '96', '14', '18', '207', '75', '48', '15', '49', '97', '117', '196', '99', '258', '195', '264', '264', '78', '197', '30', '238', '109', '158', '32', '195', '149', '227', '65', '167', '162', '204', '250', '151', '151', '102', '26', '29', '26', '193', '74', '50', '193', '186', '15', '74', '259', '57', '188', '277', '277', '195', '195', '212', '78', '197', '64', '129', '259', '200', '199', '129', '4', '235', '197', '196', '161', '3', '255', '199', '102', '3', '13', '94', '53', '98', '15', '129', '15', '155', '71', '179', '50', '29', '136', '80', '254', '198', '162', '237', '3', '18', '260', '238', '117', '179', '156', '19', '45', '197', '78', '199', '102', '102', '237', '155', '69', '69', '163', '239', '15', '196', '80', '191', '30', '71', '158', '259', '2', '106', '148', '15', '135', '135', '135', '196', '19', '123', '15', '104', '19', '128', '230', '69', '98', '197', '241', '161', '195', '200', '3', '98', '78', '165', '163', '19', '196', '31', '173', '203', '55', '69', '209', '239', '79', '158', '254', '135', '51', '94', '162', '255', '95', '18', '133', '109', '50', '94', '199', '15', '197', '200', '133', '116', '50', '196', '161', '69', '133', '3', '258', '3', '197', '29', '3', '157', '197', '193', '4', '94', '195', '17', '48', '152', '8', '15', '155', '133', '195', '136', '150', '102', '237', '203', '164', '164', '250', '154', '13', '191', '58', '94', '70', '93', '264', '15', '30', '133', '237', '267', '199', '193', '76', '241', '157', '26', '200', '109', '179', '55', '98', '200', '129', '238', '235', '208', '109', '239', '258', '196', '193', '18', '107', '210', '70', '40', '129', '259', '193', '30', '158', '252', '40', '210', '78', '15', '78', '96', '208', '113', '258', '64', '80', '15', '133', '241', '15', '15', '258', '135', '109', '251', '251', '109', '74', '196', '170', '240', '254', '258', '204', '199', '26', '109', '197', '57', '45', '161', '57', '114', '3', '15', '264', '106', '57', '27', '18', '258', '29', '84', '161', '135', '114', '24', '30', '107', '107', '172', '209', '170', '18', '19', '133', '191', '195', '135', '156', '196', '94', '3', '84', '225', '199', '238', '106', '104', '70', '50', '12', '155', '114', '176', '237', '200', '108', '70', '240', '196', '19', '22', '276', '199', '238', '205', '104', '193', '108', '115', '259', '102', '19', '258', '18', '106', '161', '191', '114', '64', '15', '204', '35', '155', '40', '238', '136', '264', '133', '186', '15', '258', '166', '26', '6', '114', '114', '133', '114', '114', '13', '13', '13', '237', '250', '245', '136', '78', '13', '48', '18', '13', '25', '26', '93', '125', '50', '94', '148', '78', '194', '200', '25', '93', '274', '266', '70', '22', '3', '208', '240', '211', '135', '31', '18', '153', '18', '2', '15', '15', '15', '156', '15', '70', '73', '173', '13', '112', '69', '204', '70', '248', '135', '70', '3', '193', '193', '22', '162', '238', '57', '98', '15', '199', '199', '52', '151', '73', '5', '108', '23', '69', '95', '220', '106', '163', '152', '163', '108', '15', '58', '250', '19', '177', '245', '264', '93', '204', '25', '78', '155', '69', '155', '239', '241', '155', '200', '18', '3', '13', '196', '197', '204', '272', '95', '135', '165', '22', '210', '35', '69', '18', '170', '3', '25', '156', '78', '135', '102', '107', '255', '19', '70', '70', '109', '198', '156', '64', '170', '138', '78', '203', '166', '108', '114', '179', '228', '74', '49', '14', '264', '26', '195', '78', '107', '75', '18', '106', '151', '70', '58', '15', '196', '238', '109', '104', '104', '109', '136', '148', '210', '3', '161', '15', '104', '238', '208', '133', '108', '241', '18', '209', '74', '156', '114', '199', '174', '35', '18', '170', '135', '238', '244', '155', '162', '197', '3', '98', '227', '267', '238', '193', '50', '238', '98', '112', '124', '25', '114', '241', '159', '154', '78', '161', '136', '114', '153', '225', '154', '13', '50', '11', '25', '15', '209', '11', '166', '203', '259', '237', '22', '114', '276', '98', '278', '150', '198', '18', '15', '78', '15', '162', '26', '3', '196', '3', '102', '189', '71', '69', '16', '18', '152', '133', '135', '162', '241', '15', '104', '15', '259', '202', '2', '238', '207', '15', '204', '30', '215', '112', '210', '42', '96', '193', '15', '110', '163', '211', '78', '108', '15', '31', '27', '15', '31', '196', '195', '96', '15', '161', '94', '193', '195', '198', '98', '261', '258', '76', '15', '104', '258', '135', '18', '15', '202', '258', '108', '156', '13', '22', '251', '199', '155', '258', '211', '3', '207', '202', '259', '107', '202', '259', '135', '135', '148', '158', '154', '99', '99', '3', '207', '94', '25', '162', '31', '241', '25', '15', '114', '18', '114', '148', '19', '15', '203', '191', '19', '13', '193', '204', '25', '166', '155', '196', '98', '162', '197', '29', '152', '191', '133', '199', '230', '40', '25', '204', '79', '3', '19', '165', '204', '108', '208', '3', '251', '274', '15', '166', '264', '35', '13', '193', '114', '258', '13', '19', '133', '23', '17', '199', '204', '153', '250', '204', '16', '12', '107', '191', '18', '3', '15', '165', '15', '69', '17', '165', '17', '15', '15', '156', '23', '23', '15', '197', '104', '197', '26', '208', '135', '211', '250', '18', '204', '25', '196', '53', '69', '104', '104', '19', '78', '241', '230', '30', '195', '96', '26', '109', '196', '69', '238', '15', '18', '133', '26', '106', '109', '98', '15', '258', '274', '193', '203', '58', '57', '274', '2', '202', '161', '136', '135', '155', '241', '35', '151', '137', '5', '114', '96', '16', '209', '158', '15', '266', '161', '15', '25', '274', '202', '3', '161', '237', '267', '204', '125', '15', '151', '155', '259', '151', '210', '238', '15', '26', '132', '50', '173', '197', '114', '241', '15', '258', '109', '166', '274', '13', '191', '15', '26', '164', '258', '163', '15', '264', '11', '260', '148', '150', '207', '196', '98', '272', '133', '196', '107', '26', '191', '163', '191', '135', '3', '18', '18', '119', '69', '196', '15', '156', '135', '112', '258', '155', '256', '199', '204', '212', '18', '135', '254', '72', '258', '254', '15', '133', '18', '259', '193', '258', '204', '40', '18', '29', '155', '104', '107', '274', '241', '25', '26', '69', '148', '166', '46', '94', '135', '15', '155', '104', '241', '32', '274', '249', '15', '197', '104', '44', '259', '69', '191', '15', '198', '193', '133', '17', '258', '248', '166', '197', '200', '250', '204', '3', '225', '238', '163', '69', '15', '15', '26', '202', '19', '209', '78', '212', '15', '78', '243', '110', '191', '15', '18', '107', '38', '208', '166', '239', '237', '199', '15', '204', '258', '173', '172', '200', '274', '243', '19', '191', '135', '203', '69', '149', '148', '196', '256', '204', '97', '191', '26', '114', '274', '200', '69', '69', '64', '69', '239', '25', '274', '203', '78', '135', '196', '202', '17', '2', '2', '3', '98', '98', '62', '2', '95', '96', '2', '207', '78', '200', '26', '64', '18', '93', '49', '57', '154', '200', '69', '94', '98', '98', '56', '56', '166', '98', '212', '226', '226', '15', '69', '15', '273', '272', '104', '193', '3', '98', '199', '98', '25', '125', '259', '249', '108', '40', '26', '198', '218', '73', '148', '69', '204', '241', '35', '3', '274', '70', '18', '71', '35', '172', '133', '106', '69', '107', '199', '219', '19', '133', '69', '135', '109', '57', '191', '135', '252', '104', '3', '94', '3', '72', '126', '16', '36', '84', '241', '3', '218', '98', '108', '46', '19', '196', '118', '104', '29', '238', '69', '10', '204', '264', '259', '124', '242', '107', '133', '25', '64', '30', '194', '259', '159', '264', '274', '52', '200', '104', '29', '30', '193', '98', '94', '114', '204', '40', '18', '198', '97', '3', '99', '274', '239', '177', '15', '18', '126', '126', '208', '148', '15', '104', '29', '36', '218', '70', '15', '23', '278', '23', '36', '191', '210', '51', '245', '167', '72', '237', '25', '203', '36', '196', '193', '196', '15', '151', '207', '203', '103', '166', '135', '110', '27', '13', '27', '191', '98', '57', '166', '50', '21', '15', '197', '23', '2', '70', '69', '193', '274', '104', '166', '258', '126', '238', '161', '18', '193', '69', '140', '170', '197', '204', '165', '203', '170', '15', '58', '35', '83', '212', '150', '73', '25', '15', '64', '133', '210', '108', '79', '193', '15', '126', '165', '15', '208', '98', '3', '200', '126', '3', '40', '165', '98', '69', '15', '165', '133', '96', '173', '203', '133', '238', '248', '15', '7', '38', '207', '108', '165', '199', '18', '162', '27', '3', '72', '65', '250', '209', '166', '64', '209', '237', '212', '64', '225', '120', '108', '203', '135', '248', '69', '251', '135', '204', '160', '160', '135', '148', '15', '15', '23', '207', '274', '29', '28', '194', '150', '40', '104', '208', '125', '70', '18', '3', '19', '102', '104', '23', '29', '207', '133', '204', '111', '2', '125', '126', '7', '194', '209', '203', '83', '126', '126', '258', '119', '162', '241', '133', '18', '208', '197', '209', '15', '78', '80', '110', '237', '104', '30', '208', '103', '15', '200', '15', '238', '254', '108', '173', '40', '69', '175', '17', '155', '136', '26', '3', '135', '13', '241', '161', '166', '170', '162', '133', '53', '211', '204', '15', '161', '238', '110', '18', '191', '18', '50', '71', '30', '17', '208', '248', '203', '203', '196', '210', '258', '204', '3', '18', '196', '125', '173', '199', '249', '18', '104', '165', '241', '2', '193', '238', '78', '15', '166', '15', '126', '3', '237', '161', '52', '207', '3', '3', '50', '56', '22', '56', '69', '93', '135', '207', '69', '99', '3', '133', '68', '133', '161', '108', '126', '250', '60', '155', '98', '18', '16', '78', '78', '240', '235', '176', '22', '199', '160', '235', '274', '209', '64', '173', '137', '128', '25', '18', '160', '203', '70', '165', '193', '110', '207', '64', '85', '252', '78', '51', '2', '99', '19', '22', '198', '204', '166', '166', '151', '56', '133', '210', '26', '149', '109', '30', '209', '209', '51', '3', '193', '119', '208', '93', '165', '248', '55', '203', '204', '58', '18', '199', '211', '83', '32', '135', '212', '156', '19', '235', '173', '191', '209', '203', '146', '208', '58', '158', '102', '199', '200', '200', '173', '70', '52', '107', '211', '213', '237', '196', '205', '104', '135', '258', '255', '96', '19', '124', '128', '57', '199', '98', '166', '102', '40', '15', '238', '3', '148', '225', '126', '135', '114', '193', '204', '15', '209', '209', '193', '259', '64', '161', '26', '274', '126', '18', '196', '0', '60', '207', '17', '71', '218', '239', '248', '38', '154', '61', '128', '199', '196', '173', '25', '46', '98', '258', '108', '3', '108', '196', '93', '93', '99', '172', '69', '241', '196', '30', '135', '278', '109', '18', '5', '209', '57', '159', '70', '193', '163', '249', '196', '196', '72', '103', '133', '17', '204', '199', '166', '202', '29', '57', '193', '150', '98', '119', '25', '205', '18', '173', '166', '209', '57', '25', '58', '258', '26', '15', '15', '101', '26', '74', '1', '4', '74', '263', '']\n",
            "['209', '63', '216', '108', '196', '200', '203', '246', '188', '193', '25', '106', '56', '53', '223', '161', '200', '208', '78', '147', '147', '179', '147', '80', '56', '209', '209', '200', '214', '202', '203', '194', '26', '237', '165', '258', '238', '238', '76', '193', '196', '213', '161', '213', '209', '194', '196', '25', '151', '260', '135', '173', '30', '108', '272', '259', '176', '213', '213', '21', '5', '171', '200', '195', '240', '194', '176', '240', '240', '80', '173', '98', '214', '21', '76', '76', '73', '173', '173', '78', '61', '258', '191', '3', '78', '173', '193', '118', '200', '264', '260', '49', '211', '258', '18', '173', '78', '3', '3', '1', '213', '135', '182', '239', '208', '278', '50', '194', '200', '124', '189', '117', '264', '244', '76', '193', '221', '78', '78', '173', '197', '78', '161', '231', '135', '22', '78', '25', '25', '197', '200', '147', '203', '191', '196', '193', '263', '32', '197', '191', '4', '56', '196', '104', '191', '196', '173', '147', '194', '96', '161', '161', '238', '200', '3', '78', '78', '53', '41', '237', '53', '18', '10', '194', '97', '96', '208', '25', '10', '117', '104', '54', '96', '194', '21', '245', '238', '202', '209', '161', '97', '258', '240', '213', '136', '155', '152', '21', '191', '191', '118', '68', '258', '189', '197', '208', '61', '78', '135', '202', '96', '155', '108', '272', '80', '25', '155', '35', '213', '191', '194', '96', '96', '191', '264', '191', '191', '191', '2', '258', '191', '173', '259', '15', '47', '258', '156', '118', '160', '115', '105', '26', '173', '192', '76', '76', '135', '240', '118', '161', '200', '230', '18', '200', '25', '10', '161', '244', '24', '24', '200', '205', '104', '15', '193', '156', '95', '209', '240', '191', '194', '128', '208', '208', '218', '173', '10', '25', '18', '107', '46', '32', '197', '172', '209', '213', '79', '121', '78', '78', '193', '2', '78', '194', '149', '136', '80', '3', '21', '200', '78', '21', '197', '194', '118', '202', '161', '180', '209', '173', '2', '193', '6', '260', '197', '78', '78', '42', '191', '213', '135', '193', '196', '202', '222', '217', '42', '259', '213', '76', '46', '30', '15', '260', '202', '18', '259', '104', '135', '76', '196', '240', '240', '240', '108', '24', '200', '197', '262', '176', '15', '156', '264', '156', '200', '191', '161', '196', '78', '238', '161', '221', '118', '14', '14', '14', '200', '278', '238', '200', '196', '78', '173', '25', '197', '197', '196', '161', '18', '10', '10', '10', '18', '66', '203', '19', '246', '196', '244', '240', '161', '200', '78', '240', '209', '208', '118', '215', '194', '193', '168', '229', '191', '135', '172', '196', '197', '161', '78', '196', '197', '258', '78', '78', '238', '221', '121', '264', '104', '146', '50', '78', '217', '11', '238', '238', '238', '3', '3', '154', '215', '79', '18', '158', '104', '18', '13', '197', '117', '197', '161', '213', '242', '32', '238', '230', '2', '2', '32', '161', '251', '96', '156', '242', '22', '22', '140', '3', '149', '135', '191', '173', '135', '200', '238', '238', '78', '156', '209', '78', '97', '97', '160', '25', '258', '240', '161', '15', '161', '96', '209', '80', '132', '140', '4', '1', '240', '196', '274', '272', '62', '80', '196', '196', '191', '79', '156', '15', '196', '209', '22', '251', '202', '202', '121', '73', '191', '238', '191', '150', '150', '135', '46', '203', '202', '238', '22', '3', '258', '239', '238', '238', '165', '238', '238', '194', '200', '200', '197', '200', '196', '191', '194', '76', '78', '208', '200', '71', '208', '240', '76', '117', '120', '108', '96', '144', '30', '133', '133', '202', '117', '161', '251', '29', '104', '242', '197', '118', '216', '262', '236', '141', '11', '235', '235', '133', '191', '238', '202', '80', '80', '22', '140', '191', '197', '202', '133', '184', '184', '184', '184', '143', '274', '50', '237', '196', '274', '265', '10', '18', '10', '121', '238', '245', '228', '76', '25', '191', '197', '268', '10', '240', '195', '196', '195', '15', '48', '200', '183', '70', '59', '197', '196', '235', '161', '161', '215', '104', '97', '191', '194', '191', '200', '200', '197', '76', '200', '93', '256', '160', '238', '146', '238', '239', '15', '140', '258', '140', '21', '79', '202', '258', '79', '62', '205', '161', '104', '10', '161', '196', '137', '152', '191', '117', '213', '242', '176', '218', '195', '191', '162', '191', '135', '241', '213', '213', '84', '79', '68', '18', '98', '197', '155', '41', '246', '269', '78', '209', '238', '80', '238', '62', '237', '273', '273', '230', '150', '121', '260', '244', '161', '202', '78', '109', '155', '264', '145', '50', '274', '173', '210', '258', '238', '211', '156', '215', '195', '177', '244', '3', '141', '196', '191', '194', '29', '30', '10', '16', '200', '200', '156', '53', '3', '62', '165', '173', '173', '22', '205', '274', '150', '183', '116', '188', '188', '98', '161', '204', '80', '172', '156', '104', '229', '259', '78', '169', '3', '200', '118', '161', '80', '260', '240', '150', '197', '196', '62', '93', '6', '272', '272', '197', '160', '78', '96', '70', '195', '191', '191', '80', '3', '3', '240', '147', '196', '191', '238', '96', '203', '3', '15', '10', '58', '55', '96', '41', '15', '26', '50', '238', '194', '79', '240', '161', '191', '195', '263', '50', '240', '147', '235', '195', '203', '59', '178', '176', '2', '15', '203', '191', '18', '278', '196', '14', '153', '58', '175', '18', '217', '217', '80', '191', '200', '18', '200', '173', '24', '171', '171', '210', '124', '34', '200', '218', '104', '4', '223', '193', '194', '200', '200', '13', '62', '194', '197', '80', '197', '146', '146', '96', '97', '97', '2', '196', '191', '191', '104', '238', '10', '200', '239', '135', '104', '213', '2', '221', '169', '25', '21', '194', '161', '21', '278', '3', '195', '46', '176', '25', '117', '191', '62', '62', '62', '78', '50', '264', '79', '197', '80', '80', '80', '191', '191', '78', '161', '194', '195', '197', '203', '259', '278', '49', '147', '147', '147', '147', '22', '78', '78', '258', '2', '202', '221', '55', '258', '104', '104', '84', '240', '221', '274', '104', '22', '241', '63', '197', '197', '196', '159', '200', '22', '22', '22', '78', '78', '173', '104', '97', '104', '191', '146', '223', '268', '96', '14', '173', '204', '241', '97', '15', '118', '78', '78', '196', '194', '258', '96', '96', '259', '75', '10', '197', '30', '2', '238', '121', '50', '161', '173', '173', '186', '185', '186', '78', '194', '221', '218', '35', '33', '13', '34', '78', '21', '215', '97', '60', '24', '78', '193', '173', '235', '78', '200', '258', '140', '259', '194', '197', '195', '200', '145', '76', '108', '130', '238', '215', '213', '2', '217', '121', '178', '140', '62', '156', '176', '196', '196', '41', '223', '147', '172', '79', '200', '2', '244', '244', '191', '213', '104', '191', '237', '237', '33', '117', '195', '195', '264', '195', '140', '210', '41', '197', '14', '257', '196', '238', '238', '79', '161', '156', '196', '238', '238', '213', '104', '3', '10', '118', '204', '235', '33', '259', '274', '259', '176', '78', '78', '196', '34', '52', '25', '30', '194', '153', '21', '70', '191', '18', '191', '191', '172', '194', '194', '194', '194', '148', '104', '125', '2', '2', '2', '185', '194', '29', '104', '78', '189', '272', '204', '183', '213', '78', '104', '197', '240', '97', '168', '168', '182', '238', '197', '18', '194', '229', '221', '62', '78', '53', '104', '214', '189', '189', '189', '213', '117', '117', '191', '29', '104', '104', '276', '276', '22', '195', '195', '155', '139', '139', '59', '168', '135', '140', '79', '146', '200', '245', '70', '160', '153', '50', '202', '154', '263', '130', '194', '117', '194', '194', '191', '33', '29', '29', '238', '239', '194', '21', '274', '215', '18', '18', '140', '2', '215', '25', '46', '137', '238', '30', '160', '156', '46', '196', '263', '242', '104', '0', '0', '73', '213', '135', '22', '191', '191', '241', '202', '221', '15', '15', '191', '238', '213', '194', '197', '179', '238', '200', '200', '200', '5', '238', '194', '55', '117', '166', '49', '244', '239', '191', '78', '78', '78', '194', '200', '117', '210', '22', '121', '121', '121', '21', '237', '194', '121', '78', '104', '194', '79', '191', '251', '195', '197', '274', '176', '21', '43', '43', '10', '61', '215', '215', '30', '214', '194', '10', '30', '139', '96', '226', '226', '265', '2', '2', '121', '121', '41', '237', '104', '238', '238', '161', '78', '78', '78', '11', '184', '140', '140', '161', '104', '104', '237', '130', '195', '160', '118', '161', '18', '11', '241', '184', '196', '178', '22', '30', '146', '146', '10', '10', '141', '141', '135', '18', '18', '10', '26', '208', '97', '141', '18', '18', '97', '194', '194', '79', '213', '205', '205', '205', '75', '241', '50', '226', '238', '97', '222', '215', '215', '139', '238', '51', '238', '96', '173', '109', '200', '117', '192', '2', '195', '10', '3', '18', '49', '238', '238', '191', '195', '52', '197', '200', '200', '79', '191', '200', '200', '30', '195', '197', '195', '25', '260', '73', '30', '193', '21', '200', '242', '203', '120', '78', '260', '241', '0', '30', '202', '195', '30', '191', '78', '194', '194', '2', '205', '191', '196', '173', '197', '30', '196', '172', '139', '2', '18', '191', '230', '56', '194', '215', '195', '2', '71', '13', '146', '264', '50', '191', '235', '93', '191', '139', '139', '227', '235', '186', '235', '24', '18', '261', '128', '33', '71', '25', '176', '39', '10', '195', '202', '29', '29', '221', '29', '197', '139', '78', '227', '146', '137', '195', '205', '205', '195', '135', '62', '153', '146', '29', '75', '0', '194', '194', '180', '213', '78', '18', '194', '239', '196', '239', '197', '194', '153', '192', '221', '10', '46', '50', '241', '117', '117', '117', '195', '173', '78', '195', '194', '239', '2', '12', '173', '197', '30', '269', '197', '176', '135', '30', '146', '191', '93', '93', '135', '260', '218', '156', '155', '200', '241', '125', '18', '18', '183', '203', '239', '259', '173', '213', '180', '180', '210', '120', '104', '194', '156', '258', '172', '9', '0', '241', '192', '143', '189', '204', '50', '63', '197', '243', '241', '93', '125', '2', '21', '260', '4', '4', '3', '191', '150', '128', '78', '238', '245', '8', '80', '238', '79', '79', '240', '58', '104', '155', '80', '80', '188', '191', '235', '170', '170', '170', '146', '258', '195', '276', '25', '78', '215', '240', '161', '53', '147', '78', '78', '78', '78', '221', '221', '96', '18', '19', '130', '18', '18', '18', '80', '80', '194', '79', '93', '121', '78', '139', '139', '78', '197', '121', '194', '200', '80', '101', '22', '12', '135', '97', '205', '181', '60', '193', '15', '140', '172', '191', '241', '222', '135', '29', '25', '179', '191', '191', '259', '194', '133', '240', '121', '121', '241', '195', '29', '191', '209', '18', '139', '139', '50', '18', '12', '18', '133', '200', '25', '10', '70', '25', '242', '18', '161', '49', '172', '172', '172', '172', '128', '96', '195', '200', '78', '10', '239', '22', '173', '142', '142', '22', '194', '192', '3', '200', '241', '96', '241', '62', '139', '213', '202', '135', '79', '120', '192', '78', '118', '241', '46', '151', '150', '151', '150', '118', '221', '183', '237', '196', '140', '140', '39', '179', '213', '140', '41', '73', '78', '217', '258', '30', '118', '140', '219', '50', '10', '139', '150', '251', '24', '10', '18', '78', '18', '172', '135', '146', '71', '196', '202', '197', '25', '106', '160', '215', '15', '6', '101', '165', '140', '145', '80', '3', '238', '239', '78', '268', '0', '109', '191', '215', '238', '238', '238', '203', '50', '146', '15', '25', '156', '191', '16', '2', '195', '189', '96', '193', '109', '204', '161', '185', '210', '50', '235', '30', '242', '193', '102', '194', '121', '177', '241', '102', '132', '240', '71', '192', '79', '25', '197', '39', '161', '22', '258', '237', '15', '7', '78', '156', '197', '70', '176', '139', '240', '132', '78', '177', '191', '194', '117', '10', '79', '238', '195', '24', '101', '118', '196', '2', '102', '21', '180', '185', '170', '79', '139', '135', '191', '151', '200', '191', '154', '262', '200', '237', '56', '27', '191', '180', '258', '139', '139', '192', '30', '0', '242', '239', '242', '25', '191', '242', '240', '3', '56', '215', '171', '192', '193', '222', '238', '237', '215', '272', '2', '156', '237', '151', '189', '80', '15', '260', '260', '217', '235', '22', '228', '135', '18', '191', '157', '78', '2', '240', '213', '179', '2', '191', '79', '79', '153', '3', '194', '78', '121', '71', '121', '149', '257', '194', '191', '188', '15', '25', '125', '102', '15', '130', '130', '177', '10', '135', '265', '240', '149', '196', '200', '204', '204', '213', '177', '197', '194', '13', '202', '30', '213', '125', '193', '125', '125', '125', '125', '125', '15', '180', '78', '1', '25', '96', '200', '29', '156', '172', '202', '22', '78', '197', '188', '241', '101', '101', '101', '221', '50', '191', '15', '15', '0', '101', '101', '168', '56', '197', '130', '62', '204', '29', '10', '96', '97', '139', '97', '97', '2', '2', '262', '39', '176', '172', '194', '194', '18', '78', '13', '80', '139', '196', '207', '191', '151', '192', '150', '13', '104', '104', '101', '15', '125', '101', '15', '0', '50', '194', '264', '50', '25', '25', '125', '125', '125', '125', '125', '25', '29', '25', '25', '78', '240', '261', '19', '2', '74', '78', '238', '50', '50', '203', '197', '71', '104', '80', '194', '149', '170', '130', '2', '237', '170', '202', '48', '97', '93', '179', '235', '78', '102', '102', '70', '208', '238', '104', '146', '118', '118', '96', '237', '274', '218', '241', '102', '146', '209', '209', '240', '170', '137', '7', '197', '191', '264', '29', '46', '96', '193', '19', '12', '213', '215', '275', '179', '18', '183', '78', '221', '18', '170', '119', '21', '140', '240', '96', '192', '192', '21', '96', '149', '239', '135', '191', '240', '184', '183', '24', '2', '96', '197', '194', '215', '18', '192', '121', '202', '196', '25', '97', '214', '3', '97', '16', '0', '192', '135', '215', '5', '12', '80', '143', '202', '242', '151', '13', '209', '203', '200', '30', '209', '194', '102', '102', '102', '102', '102', '102', '22', '24', '6', '18', '5', '191', '101', '50', '101', '101', '192', '153', '153', '101', '101', '10', '192', '25', '140', '50', '241', '102', '102', '32', '150', '101', '102', '153', '102', '101', '153', '102', '25', '10', '125', '18', '125', '177', '209', '102', '10', '78', '78', '137', '18', '237', '241', '175', '195', '175', '96', '176', '237', '96', '96', '101', '193', '238', '170', '238', '102', '222', '268', '238', '102', '244', '34', '18', '244', '202', '149', '149', '193', '102', '170', '194', '75', '101', '222', '238', '50', '195', '135', '22', '93', '18', '102', '102', '93', '93', '191', '147', '244', '96', '2', '241', '241', '93', '189', '189', '177', '35', '137', '197', '191', '40', '40', '195', '176', '35', '200', '200', '237', '135', '140', '140', '93', '156', '200', '78', '191', '22', '81', '238', '101', '213', '101', '78', '109', '19', '140', '196', '78', '78', '76', '218', '14', '265', '101', '241', '155', '4', '50', '78', '78', '238', '135', '191', '265', '81', '238', '238', '102', '102', '102', '140', '162', '32', '17', '17', '171', '171', '191', '101', '102', '192', '101', '17', '171', '135', '102', '242', '195', '237', '21', '18', '238', '191', '278', '140', '149', '97', '260', '155', '276', '192', '196', '21', '191', '2', '97', '0', '150', '221', '213', '209', '238', '80', '260', '194', '104', '96', '193', '14', '240', '193', '117', '117', '155', '17', '264', '260', '209', '209', '2', '137', '71', '72', '101', '205', '173', '191', '118', '24', '41', '197', '78', '94', '237', '156', '193', '153', '154', '153', '209', '110', '78', '78', '213', '238', '78', '191', '93', '79', '177', '267', '133', '133', '46', '139', '40', '143', '74', '241', '202', '50', '3', '102', '78', '149', '191', '156', '18', '93', '195', '192', '137', '240', '135', '204', '189', '22', '56', '154', '25', '192', '130', '277', '191', '80', '39', '21', '21', '238', '238', '23', '192', '191', '192', '94', '94', '80', '139', '17', '10', '22', '227', '227', '140', '102', '21', '17', '227', '227', '23', '243', '264', '18', '274', '149', '238', '30', '221', '22', '238', '193', '135', '205', '195', '238', '238', '197', '130', '195', '18', '104', '102', '14', '59', '104', '18', '18', '258', '34', '241', '138', '188', '176', '102', '1', '140', '140', '22', '22', '278', '10', '96', '35', '145', '120', '33', '33', '22', '33', '22', '35', '22', '19', '33', '102', '192', '192', '175', '22', '72', '191', '222', '125', '22', '79', '18', '194', '18', '250', '192', '192', '96', '48', '274', '273', '132', '133', '192', '40', '171', '171', '246', '191', '10', '166', '197', '195', '125', '125', '200', '153', '192', '149', '110', '139', '194', '48', '102', '102', '104', '203', '238', '238', '238', '78', '18', '238', '121', '18', '238', '96', '238', '238', '13', '191', '18', '18', '240', '96', '40', '188', '18', '137', '175', '28', '28', '28', '149', '191', '191', '207', '239', '239', '153', '153', '153', '153', '153', '153', '154', '153', '154', '153', '153', '153', '18', '0', '27', '18', '25', '96', '96', '137', '33', '10', '130', '153', '153', '154', '180', '137', '44', '241', '153', '268', '25', '161', '25', '39', '242', '4', '238', '195', '102', '135', '13', '229', '140', '1', '79', '192', '229', '191', '118', '21', '260', '203', '18', '177', '191', '10', '238', '10', '133', '14', '175', '175', '192', '18', '153', '153', '254', '140', '149', '22', '22', '22', '237', '40', '135', '191', '192', '192', '62', '205', '19', '238', '176', '214', '200', '96', '15', '153', '191', '108', '29', '166', '28', '193', '197', '96', '195', '108', '102', '241', '195', '10', '33', '166', '197', '168', '2', '96', '26', '33', '204', '258', '149', '153', '153', '40', '137', '241', '166', '221', '23', '221', '153', '15', '113', '113', '78', '171', '40', '71', '21', '71', '80', '15', '191', '191', '135', '110', '10', '259', '102', '149', '149', '109', '109', '175', '97', '97', '97', '153', '15', '238', '193', '175', '38', '38', '137', '18', '235', '155', '137', '265', '110', '18', '18', '256', '191', '224', '173', '104', '10', '152', '24', '24', '24', '17', '40', '40', '40', '40', '40', '40', '112', '6', '18', '18', '210', '96', '191', '27', '213', '17', '153', '26', '133', '18', '18', '153', '38', '96', '40', '38', '40', '40', '10', '17', '17', '95', '71', '192', '17', '259', '213', '200', '15', '11', '207', '194', '18', '135', '238', '191', '241', '191', '116', '35', '147', '156', '67', '78', '17', '227', '191', '117', '188', '130', '72', '96', '235', '97', '18', '150', '203', '25', '126', '149', '200', '94', '4', '4', '1', '244', '228', '252', '149', '191', '149', '15', '17', '17', '29', '18', '18', '116', '23', '237', '94', '184', '268', '197', '175', '243', '28', '16', '10', '50', '191', '102', '102', '153', '149', '111', '78', '191', '135', '18', '35', '78', '238', '208', '93', '95', '258', '17', '111', '111', '262', '22', '22', '104', '161', '209', '21', '195', '12', '116', '241', '242', '239', '17', '97', '16', '40', '117', '94', '96', '78', '15', '103', '195', '135', '235', '247', '135', '130', '260', '239', '176', '93', '191', '209', '239', '93', '264', '30', '18', '30', '18', '18', '103', '191', '97', '17', '95', '115', '209', '140', '22', '22', '241', '10', '153', '3', '188', '238', '226', '195', '97', '96', '238', '97', '130', '97', '116', '96', '96', '242', '45', '149', '166', '155', '96', '116', '264', '10', '237', '192', '71', '22', '71', '175', '149', '116', '237', '274', '18', '235', '176', '116', '202', '250', '250', '78', '2', '196', '2', '97', '143', '18', '111', '111', '80', '78', '153', '80', '40', '110', '30', '112', '30', '111', '215', '112', '149', '78', '78', '209', '110', '110', '238', '109', '239', '112', '151', '154', '16', '16', '16', '78', '137', '179', '72', '149', '10', '111', '268', '96', '170', '111', '38', '192', '153', '183', '195', '30', '30', '147', '195', '147', '93', '264', '153', '25', '18', '19', '22', '74', '40', '93', '166', '15', '127', '147', '192', '260', '238', '103', '269', '127', '19', '19', '153', '18', '78', '238', '30', '30', '241', '16', '235', '250', '204', '238', '218', '40', '93', '184', '18', '0', '241', '195', '94', '154', '109', '156', '10', '21', '30', '197', '39', '78', '24', '107', '142', '142', '228', '229', '229', '229', '232', '179', '239', '142', '232', '23', '175', '147', '137', '137', '137', '171', '191', '117', '128', '127', '204', '98', '147', '235', '239', '153', '15', '204', '255', '80', '204', '204', '149', '113', '19', '167', '22', '250', '17', '17', '17', '18', '17', '67', '121', '125', '26', '97', '97', '97', '97', '247', '124', '163', '147', '112', '111', '208', '108', '38', '209', '154', '154', '194', '135', '22', '147', '241', '120', '218', '260', '156', '155', '241', '192', '215', '180', '179', '149', '40', '193', '192', '150', '102', '93', '1', '149', '137', '133', '102', '102', '102', '78', '145', '21', '96', '175', '175', '78', '123', '30', '95', '19', '18', '110', '18', '152', '152', '147', '26', '135', '260', '158', '203', '195', '158', '112', '156', '125', '213', '230', '30', '15', '125', '106', '111', '195', '181', '130', '195', '130', '239', '15', '194', '16', '15', '111', '61', '72', '57', '128', '239', '239', '203', '25', '218', '10', '155', '191', '46', '98', '175', '260', '102', '21', '191', '260', '191', '239', '15', '30', '93', '147', '103', '78', '177', '30', '95', '155', '155', '155', '103', '103', '112', '241', '191', '191', '17', '78', '153', '150', '113', '115', '147', '78', '40', '163', '33', '15', '149', '103', '192', '191', '96', '106', '115', '143', '235', '200', '204', '238', '204', '204', '74', '196', '113', '246', '102', '150', '16', '176', '20', '195', '128', '197', '15', '71', '71', '193', '24', '24', '24', '170', '204', '209', '111', '144', '112', '22', '222', '224', '28', '209', '155', '213', '275', '196', '239', '195', '25', '103', '149', '128', '19', '96', '102', '135', '17', '232', '155', '191', '20', '18', '241', '30', '25', '205', '97', '97', '96', '72', '182', '135', '135', '20', '149', '149', '78', '135', '40', '18', '153', '10', '26', '158', '205', '157', '157', '135', '135', '140', '155', '192', '26', '268', '263', '245', '155', '15', '177', '159', '212', '161', '239', '200', '40', '35', '153', '197', '259', '15', '254', '175', '10', '30', '239', '67', '195', '200', '191', '22', '66', '191', '112', '133', '128', '235', '161', '155', '17', '93', '36', '113', '239', '18', '18', '191', '165', '163', '163', '238', '79', '158', '40', '102', '40', '200', '191', '60', '241', '245', '96', '78', '30', '15', '173', '192', '192', '158', '64', '113', '200', '74', '128', '135', '115', '115', '10', '74', '102', '102', '115', '111', '238', '244', '109', '109', '238', '152', '10', '237', '18', '50', '16', '173', '204', '15', '208', '276', '43', '109', '258', '222', '15', '94', '235', '203', '79', '128', '113', '3', '204', '106', '133', '214', '218', '25', '98', '166', '71', '30', '175', '175', '97', '135', '108', '19', '18', '196', '35', '12', '196', '78', '166', '155', '72', '155', '22', '109', '258', '196', '263', '161', '204', '210', '14', '208', '208', '208', '208', '204', '102', '102', '213', '155', '251', '153', '153', '153', '154', '154', '237', '97', '19', '28', '94', '136', '112', '19', '97', '152', '153', '153', '111', '111', '135', '135', '105', '17', '105', '152', '152', '152', '153', '79', '266', '135', '120', '125', '149', '50', '162', '72', '17', '147', '109', '115', '24', '105', '244', '149', '238', '203', '35', '202', '177', '173', '239', '19', '19', '179', '237', '14', '18', '185', '58', '158', '144', '213', '237', '155', '250', '239', '93', '237', '97', '195', '157', '107', '109', '246', '165', '98', '49', '2', '266', '149', '20', '18', '71', '108', '22', '115', '239', '40', '178', '232', '238', '218', '118', '118', '210', '235', '46', '228', '192', '112', '149', '96', '111', '21', '95', '95', '95', '18', '57', '206', '18', '239', '157', '80', '196', '112', '176', '99', '106', '71', '235', '135', '18', '209', '78', '20', '20', '103', '33', '3', '103', '166', '79', '79', '156', '229', '18', '238', '2', '16', '128', '200', '117', '22', '197', '166', '78', '237', '241', '107', '53', '189', '204', '243', '163', '221', '15', '200', '221', '152', '23', '193', '107', '195', '99', '97', '19', '125', '276', '113', '193', '64', '78', '197', '117', '175', '135', '135', '78', '93', '154', '153', '256', '256', '135', '137', '135', '137', '149', '110', '15', '159', '21', '158', '192', '192', '117', '29', '97', '258', '78', '46', '5', '80', '15', '105', '260', '161', '191', '2', '138', '117', '215', '176', '156', '194', '265', '115', '181', '21', '14', '106', '106', '108', '200', '156', '103', '102', '29', '17', '204', '175', '227', '118', '18', '239', '204', '258', '78', '155', '213', '109', '192', '186', '186', '15', '241', '133', '30', '30', '239', '30', '218', '109', '138', '241', '241', '45', '135', '112', '232', '104', '106', '200', '53', '23', '112', '17', '117', '59', '260', '195', '106', '117', '159', '204', '200', '228', '165', '165', '261', '200', '227', '166', '93', '21', '80', '112', '152', '196', '203', '228', '98', '102', '15', '139', '110', '13', '238', '6', '171', '94', '208', '155', '112', '203', '239', '191', '39', '136', '203', '195', '46', '203', '148', '192', '210', '23', '4', '153', '29', '165', '192', '204', '232', '94', '112', '239', '98', '71', '248', '17', '155', '10', '78', '191', '191', '111', '25', '25', '19', '19', '17', '235', '102', '265', '195', '235', '163', '238', '71', '197', '107', '155', '30', '72', '19', '259', '80', '155', '23', '64', '239', '197', '156', '57', '203', '258', '200', '205', '246', '238', '258', '227', '238', '238', '19', '148', '161', '111', '105', '202', '109', '262', '133', '137', '221', '135', '52', '170', '111', '36', '30', '156', '102', '268', '115', '22', '153', '133', '259', '98', '235', '135', '191', '25', '235', '25', '29', '94', '195', '203', '212', '112', '135', '245', '260', '200', '96', '158', '23', '165', '22', '18', '249', '240', '109', '43', '37', '15', '241', '241', '23', '135', '240', '108', '98', '268', '275', '191', '136', '263', '259', '196', '98', '98', '204', '248', '123', '105', '204', '240', '9', '9', '98', '218', '189', '238', '246', '155', '130', '157', '74', '109', '29', '189', '203', '213', '195', '241', '159', '238', '204', '218', '105', '23', '16', '263', '206', '150', '152', '204', '202', '173', '267', '18', '153', '155', '196', '2', '97', '158', '158', '204', '98', '202', '98', '204', '258', '98', '22', '113', '113', '135', '71', '237', '155', '163', '250', '233', '26', '26', '26', '26', '26', '19', '170', '18', '15', '19', '191', '30', '30', '48', '35', '30', '237', '46', '119', '16', '78', '239', '150', '245', '235', '158', '27', '241', '30', '38', '107', '75', '241', '135', '106', '106', '98', '159', '96', '135', '111', '176', '175', '200', '18', '14', '242', '163', '187', '20', '12', '191', '23', '106', '171', '25', '78', '162', '191', '157', '94', '237', '246', '203', '108', '238', '204', '204', '1', '102', '154', '106', '170', '102', '200', '2', '135', '106', '258', '258', '227', '26', '15', '25', '71', '203', '96', '3', '241', '200', '30', '108', '98', '108', '27', '106', '26', '150', '259', '78', '113', '113', '158', '202', '16', '136', '239', '156', '239', '35', '276', '252', '115', '22', '22', '258', '172', '3', '154', '3', '98', '237', '102', '235', '112', '206', '98', '57', '118', '153', '18', '26', '30', '196', '163', '191', '153', '72', '24', '156', '117', '122', '30', '135', '192', '98', '28', '30', '30', '95', '241', '155', '17', '195', '31', '103', '202', '98', '237', '94', '112', '163', '163', '94', '163', '112', '19', '195', '191', '15', '135', '78', '109', '160', '152', '97', '98', '107', '167', '97', '138', '138', '26', '191', '97', '97', '30', '30', '206', '202', '13', '30', '240', '111', '261', '79', '0', '111', '19', '152', '107', '107', '18', '98', '28', '258', '244', '106', '237', '204', '191', '15', '183', '202', '16', '111', '166', '268', '202', '12', '30', '78', '110', '133', '71', '197', '111', '52', '79', '171', '156', '94', '13', '118', '111', '71', '133', '18', '105', '30', '22', '133', '60', '237', '191', '6', '126', '3', '238', '276', '78', '162', '150', '115', '191', '107', '200', '148', '135', '72', '155', '241', '4', '108', '160', '203', '20', '133', '2', '80', '80', '154', '18', '15', '115', '239', '167', '167', '35', '268', '18', '221', '18', '195', '204', '195', '216', '133', '112', '104', '223', '191', '115', '30', '30', '30', '30', '30', '202', '204', '165', '200', '200', '29', '191', '237', '267', '18', '175', '81', '264', '264', '178', '115', '102', '178', '212', '273', '202', '109', '259', '166', '135', '155', '219', '161', '18', '94', '241', '191', '196', '191', '98', '26', '26', '204', '204', '135', '27', '238', '260', '182', '26', '98', '94', '93', '238', '156', '19', '111', '79', '16', '192', '113', '19', '241', '18', '202', '260', '272', '106', '163', '156', '106', '17', '109', '221', '78', '203', '7', '136', '197', '193', '191', '98', '112', '200', '95', '221', '98', '161', '191', '3', '106', '189', '111', '25', '196', '196', '241', '163', '200', '24', '78', '176', '3', '98', '102', '98', '19', '161', '224', '80', '18', '191', '112', '105', '254', '71', '200', '137', '29', '20', '165', '8', '106', '23', '106', '155', '200', '265', '135', '126', '203', '188', '161', '158', '258', '255', '98', '203', '50', '35', '102', '158', '175', '40', '239', '26', '66', '57', '193', '155', '15', '135', '10', '107', '106', '229', '18', '191', '204', '107', '99', '235', '238', '18', '205', '205', '35', '238', '208', '203', '110', '110', '109', '112', '23', '18', '19', '182', '156', '18', '18', '156', '111', '135', '137', '133', '137', '240', '99', '197', '113', '15', '165', '39', '97', '97', '15', '150', '163', '109', '268', '15', '15', '238', '203', '67', '161', '197', '18', '196', '97', '161', '197', '161', '185', '238', '191', '30', '121', '133', '160', '160', '74', '135', '75', '54', '135', '196', '23', '241', '166', '194', '239', '135', '240', '208', '238', '98', '135', '152', '176', '22', '174', '113', '241', '237', '57', '195', '191', '24', '165', '165', '204', '204', '56', '109', '27', '153', '252', '228', '165', '252', '200', '28', '158', '135', '196', '22', '18', '133', '30', '204', '237', '196', '93', '106', '94', '25', '93', '159', '4', '195', '23', '237', '25', '199', '57', '26', '276', '274', '204', '75', '18', '204', '108', '22', '133', '191', '111', '204', '203', '133', '237', '166', '4', '62', '202', '166', '161', '165', '74', '109', '118', '200', '22', '206', '25', '209', '17', '155', '204', '64', '23', '30', '202', '149', '171', '135', '136', '184', '26', '18', '159', '195', '18', '166', '156', '98', '133', '38', '40', '18', '18', '166', '109', '109', '209', '22', '195', '163', '163', '108', '18', '15', '99', '135', '174', '26', '19', '166', '133', '259', '151', '235', '204', '228', '133', '30', '161', '111', '242', '84', '238', '98', '195', '78', '160', '28', '25', '264', '165', '41', '19', '55', '46', '202', '121', '15', '32', '98', '238', '0', '78', '31', '30', '203', '264', '203', '135', '104', '111', '78', '212', '111', '98', '126', '22', '2', '30', '237', '212', '265', '163', '106', '106', '235', '109', '223', '161', '202', '150', '170', '255', '112', '114', '170', '98', '96', '197', '36', '18', '228', '14', '255', '204', '102', '239', '0', '185', '30', '22', '23', '246', '199', '133', '191', '78', '195', '153', '112', '191', '200', '170', '171', '200', '78', '128', '25', '153', '237', '99', '94', '208', '107', '208', '208', '78', '204', '204', '27', '191', '272', '134', '158', '158', '174', '118', '64', '18', '258', '150', '189', '18', '153', '241', '204', '112', '106', '135', '71', '148', '7', '133', '140', '187', '23', '223', '19', '19', '2', '136', '2', '200', '161', '15', '98', '158', '148', '3', '250', '153', '235', '165', '220', '176', '110', '240', '151', '148', '133', '74', '193', '46', '27', '173', '94', '176', '105', '256', '161', '226', '18', '78', '191', '171', '243', '26', '0', '108', '167', '159', '239', '150', '112', '155', '79', '241', '202', '165', '254', '107', '202', '156', '166', '193', '239', '135', '15', '203', '187', '148', '25', '15', '22', '194', '239', '56', '191', '176', '262', '200', '110', '72', '99', '102', '135', '98', '113', '15', '15', '159', '262', '171', '151', '98', '204', '213', '255', '26', '243', '175', '156', '98', '107', '191', '165', '241', '26', '165', '171', '30', '160', '166', '266', '11', '19', '211', '93', '254', '30', '239', '115', '273', '205', '112', '237', '263', '74', '98', '79', '122', '94', '265', '155', '191', '16', '235', '229', '98', '191', '227', '17', '111', '267', '135', '202', '195', '10', '180', '196', '196', '133', '135', '133', '95', '111', '122', '258', '191', '3', '114', '107', '19', '264', '74', '56', '200', '200', '133', '161', '97', '15', '97', '197', '196', '253', '93', '202', '258', '106', '153', '26', '109', '40', '149', '8', '196', '15', '119', '155', '23', '237', '193', '264', '148', '151', '238', '200', '94', '75', '148', '175', '24', '195', '171', '30', '203', '200', '202', '16', '158', '223', '191', '22', '264', '135', '35', '57', '94', '191', '196', '202', '98', '18', '196', '3', '133', '78', '191', '208', '258', '18', '200', '29', '135', '109', '110', '2', '185', '98', '133', '19', '22', '204', '95', '238', '25', '133', '250', '135', '135', '3', '47', '158', '187', '2', '78', '115', '155', '3', '218', '166', '112', '266', '22', '195', '135', '72', '97', '197', '209', '127', '80', '2', '98', '14', '239', '15', '113', '165', '204', '78', '204', '191', '175', '200', '193', '18', '196', '133', '109', '237', '56', '196', '3', '163', '163', '163', '195', '15', '263', '18', '84', '18', '133', '107', '18', '102', '156', '238', '102', '26', '244', '82', '75', '23', '258', '238', '136', '3', '204', '184', '200', '196', '158', '106', '27', '238', '155', '152', '50', '50', '2', '235', '98', '203', '166', '109', '25', '18', '151', '133', '200', '26', '26', '26', '19', '107', '156', '23', '26', '3', '70', '225', '41', '26', '239', '26', '186', '26', '34', '272', '272', '104', '78', '260', '18', '204', '26', '78', '18', '210', '135', '203', '71', '207', '26', '133', '13', '25', '26', '115', '94', '264', '135', '244', '79', '30', '263', '22', '109', '205', '176', '106', '22', '17', '151', '106', '191', '160', '160', '165', '78', '177', '196', '260', '166', '35', '191', '260', '98', '18', '237', '18', '161', '18', '197', '133', '30', '35', '200', '170', '155', '245', '19', '200', '160', '96', '172', '165', '55', '137', '241', '133', '191', '112', '25', '126', '238', '18', '18', '108', '135', '161', '135', '73', '112', '107', '8', '112', '109', '195', '25', '30', '105', '161', '241', '110', '75', '238', '215', '238', '104', '25', '3', '155', '30', '16', '3', '175', '99', '3', '200', '23', '3', '199', '203', '191', '196', '196', '204', '98', '105', '133', '197', '135', '161', '39', '15', '30', '133', '55', '191', '203', '133', '203', '140', '200', '149', '30', '96', '135', '23', '26', '3', '18', '155', '158', '135', '135', '18', '250', '18', '22', '114', '18', '18', '166', '18', '30', '18', '239', '18', '161', '50', '50', '49', '51', '23', '55', '98', '78', '210', '211', '18', '239', '78', '13', '193', '114', '78', '18', '25', '133', '19', '176', '4', '173', '50', '108', '17', '4', '204', '94', '112', '207', '109', '115', '23', '235', '197', '194', '197', '213', '148', '40', '157', '107', '200', '147', '14', '14', '18', '238', '18', '238', '18', '238', '57', '200', '135', '30', '18', '191', '191', '200', '166', '151', '161', '73', '19', '73', '239', '239', '165', '108', '2', '93', '166', '133', '133', '173', '35', '193', '114', '74', '19', '40', '58', '104', '16', '260', '160', '158', '112', '210', '158', '167', '203', '203', '3', '3', '109', '30', '133', '258', '70', '204', '200', '152', '155', '192', '204', '108', '191', '204', '26', '98', '26', '98', '264', '158', '18', '150', '137', '153', '196', '199', '191', '115', '168', '157', '149', '151', '78', '137', '160', '259', '166', '196', '171', '175', '199', '166', '161', '70', '13', '166', '152', '152', '206', '58', '18', '207', '58', '74', '207', '29', '46', '109', '135', '74', '174', '109', '40', '10', '30', '258', '16', '111', '30', '15', '239', '204', '48', '25', '196', '0', '156', '72', '72', '96', '22', '72', '133', '133', '238', '26', '3', '50', '152', '193', '235', '136', '153', '50', '1', '19', '239', '30', '25', '179', '161', '30', '191', '3', '27', '133', '3', '40', '23', '133', '150', '151', '265', '32', '191', '3', '25', '170', '238', '40', '259', '166', '137', '241', '133', '209', '159', '78', '78', '153', '26', '52', '241', '36', '250', '96', '30', '30', '38', '18', '18', '238', '99', '72', '111', '3', '244', '191', '226', '3', '191', '114', '135', '109', '40', '79', '202', '78', '114', '238', '10', '150', '108', '163', '108', '113', '197', '109', '115', '161', '3', '238', '3', '237', '161', '107', '155', '78', '202', '112', '38', '195', '137', '250', '133', '237', '161', '25', '26', '25', '15', '15', '239', '166', '114', '229', '111', '40', '135', '94', '102', '162', '204', '237', '17', '166', '136', '22', '98', '99', '99', '166', '35', '196', '3', '203', '135', '135', '84', '98', '266', '238', '13', '166', '78', '193', '98', '241', '104', '153', '192', '200', '10', '95', '95', '148', '107', '14', '17', '191', '239', '19', '203', '255', '197', '213', '94', '3', '107', '23', '35', '10', '237', '235', '78', '29', '18', '160', '200', '150', '99', '109', '155', '161', '133', '114', '18', '18', '18', '264', '133', '104', '161', '70', '17', '204', '195', '204', '241', '99', '82', '241', '241', '161', '18', '235', '175', '2', '2', '202', '50', '230', '26', '40', '40', '200', '78', '156', '26', '112', '26', '30', '71', '184', '200', '135', '153', '23', '106', '125', '175', '133', '133', '7', '38', '196', '161', '0', '49', '241', '158', '15', '21', '104', '2', '3', '176', '259', '135', '78', '75', '112', '200', '1', '207', '238', '161', '150', '158', '2', '45', '31', '237', '149', '3', '26', '165', '135', '152', '115', '133', '18', '40', '166', '30', '173', '40', '26', '78', '50', '48', '50', '13', '45', '200', '200', '18', '151', '205', '135', '38', '117', '196', '135', '155', '111', '196', '196', '166', '23', '25', '161', '18', '124', '259', '165', '58', '170', '204', '250', '103', '160', '238', '194', '163', '241', '255', '235', '17', '133', '26', '135', '196', '237', '114', '258', '193', '248', '40', '195', '84', '238', '146', '199', '40', '161', '22', '137', '43', '166', '156', '4', '104', '133', '177', '211', '193', '78', '203', '246', '237', '264', '3', '149', '78', '78', '111', '239', '13', '238', '176', '112', '150', '156', '3', '6', '118', '178', '178', '21', '186', '150', '241', '50', '243', '191', '121', '278', '29', '200', '18', '3', '197', '191', '98', '173', '195', '25', '195', '264', '221', '15', '156', '25', '200', '211', '195', '26', '35', '2', '135', '126', '113', '167', '191', '264', '196', '133', '18', '26', '146', '203', '219', '23', '160', '160', '135', '259', '3', '159', '120', '160', '106', '240', '133', '115', '258', '38', '22', '3', '26', '22', '107', '49', '48', '25', '237', '72', '219', '196', '196', '111', '135', '196', '18', '166', '172', '135', '26', '258', '70', '98', '40', '166', '23', '96', '3', '30', '155', '126', '146', '114', '242', '237', '157', '238', '241', '40', '133', '200', '161', '175', '200', '175', '84', '266', '18', '99', '30', '242', '151', '153', '166', '199', '228', '3', '237', '132', '110', '135', '80', '15', '15', '15', '84', '133', '15', '177', '3', '202', '255', '71', '115', '235', '19', '18', '18', '102', '58', '78', '165', '166', '264', '12', '150', '15', '2', '18', '135', '109', '200', '258', '128', '3', '199', '197', '239', '109', '222', '239', '107', '125', '132', '204', '204', '133', '135', '30', '158', '203', '15', '264', '237', '133', '196', '3', '252', '26', '98', '79', '199', '210', '0', '99', '30', '26', '259', '6', '49', '166', '98', '108', '9', '133', '102', '155', '98', '114', '78', '35', '166', '188', '202', '235', '30', '149', '165', '176', '138', '29', '153', '25', '25', '137', '133', '166', '135', '125', '71', '193', '154', '165', '98', '23', '56', '198', '26', '26', '0', '109', '136', '163', '151', '25', '15', '241', '195', '204', '191', '195', '161', '175', '133', '237', '78', '165', '258', '135', '78', '135', '258', '18', '235', '30', '187', '150', '275', '149', '2', '36', '112', '22', '161', '238', '203', '112', '265', '75', '133', '205', '258', '158', '18', '26', '18', '25', '26', '15', '26', '239', '15', '240', '18', '71', '71', '78', '135', '3', '148', '166', '26', '176', '2', '197', '150', '176', '19', '18', '97', '166', '18', '153', '250', '133', '210', '20', '192', '196', '26', '191', '2', '81', '70', '5', '79', '3', '30', '193', '193', '69', '160', '176', '246', '108', '54', '250', '135', '200', '80', '3', '167', '18', '166', '200', '152', '23', '149', '19', '14', '13', '237', '196', '19', '112', '254', '204', '191', '23', '264', '199', '209', '137', '31', '135', '238', '78', '18', '191', '15', '164', '195', '74', '163', '163', '41', '99', '26', '135', '26', '40', '40', '250', '200', '200', '26', '237', '4', '195', '158', '35', '71', '191', '3', '218', '274', '58', '3', '23', '177', '132', '49', '62', '238', '166', '97', '264', '193', '29', '26', '49', '241', '197', '200', '64', '227', '258', '44', '15', '133', '175', '238', '109', '254', '237', '198', '175', '166', '213', '174', '196', '107', '259', '98', '10', '196', '210', '191', '96', '166', '135', '15', '197', '78', '78', '195', '176', '18', '98', '129', '170', '18', '247', '203', '26', '158', '40', '64', '200', '163', '196', '19', '205', '197', '193', '191', '69', '96', '74', '35', '36', '64', '133', '172', '19', '266', '96', '94', '103', '258', '193', '264', '98', '254', '50', '193', '25', '170', '75', '84', '27', '252', '203', '94', '94', '19', '98', '137', '12', '153', '3', '155', '135', '238', '15', '196', '15', '26', '84', '49', '27', '135', '203', '48', '133', '129', '133', '258', '155', '35', '195', '71', '241', '135', '166', '193', '165', '104', '196', '117', '196', '26', '237', '109', '95', '196', '60', '40', '158', '193', '250', '104', '94', '178', '27', '193', '78', '106', '79', '204', '133', '237', '235', '135', '202', '70', '15', '264', '241', '254', '110', '199', '49', '255', '30', '193', '238', '204', '102', '97', '161', '43', '228', '1', '6', '196', '193', '26', '15', '128', '46', '155', '32', '133', '69', '47', '135', '78', '205', '200', '200', '202', '3', '95', '191', '78', '26', '248', '258', '191', '240', '49', '110', '38', '193', '200', '200', '96', '200', '204', '135', '9', '200', '258', '15', '46', '15', '149', '155', '197', '2', '166', '151', '72', '135', '97', '211', '238', '135', '163', '161', '26', '163', '129', '241', '239', '241', '16', '26', '18', '263', '260', '166', '3', '264', '195', '79', '132', '251', '50', '133', '15', '25', '15', '191', '15', '78', '50', '78', '71', '227', '69', '200', '69', '109', '64', '133', '197', '196', '3', '45', '238', '26', '132', '104', '196', '193', '136', '80', '26', '75', '117', '14', '78', '75', '98', '15', '2', '15', '50', '255', '136', '3', '112', '173', '69', '191', '19', '239', '235', '109', '70', '135', '78', '165', '238', '262', '199', '35', '203', '224', '193', '15', '18', '150', '238', '197', '238', '193', '135', '3', '150', '187', '199', '239', '109', '152', '27', '257', '6', '235', '104', '191', '133', '166', '155', '26', '16', '136', '198', '98', '154', '235', '27', '102', '30', '191', '15', '132', '132', '114', '107', '241', '241', '156', '199', '241', '49', '62', '117', '258', '30', '163', '133', '238', '200', '3', '15', '109', '2', '196', '165', '166', '159', '210', '35', '15', '17', '200', '241', '106', '203', '197', '197', '79', '98', '98', '23', '204', '48', '129', '57', '258', '202', '215', '238', '70', '64', '193', '189', '57', '200', '0', '98', '135', '204', '25', '30', '204', '27', '26', '78', '135', '109', '3', '197', '3', '167', '35', '196', '225', '2', '194', '200', '15', '161', '198', '23', '133', '238', '195', '35', '50', '135', '152', '163', '21', '15', '94', '26', '94', '53', '260', '196', '154', '277', '200', '118', '6', '238', '50', '209', '50', '3', '238', '3', '15', '107', '135', '199', '15', '135', '26', '258', '251', '203', '19', '259', '195', '199', '238', '15', '254', '112', '211', '200', '129', '84', '196', '3', '208', '259', '135', '104', '29', '136', '31', '78', '13', '3', '64', '75', '55', '114', '160', '96', '14', '18', '207', '75', '48', '15', '49', '97', '117', '196', '99', '258', '195', '264', '264', '78', '197', '30', '238', '109', '158', '32', '195', '149', '227', '65', '167', '162', '204', '250', '151', '151', '102', '26', '29', '26', '193', '74', '50', '193', '186', '15', '74', '259', '57', '188', '277', '277', '195', '195', '212', '78', '197', '64', '129', '259', '200', '199', '129', '4', '235', '197', '196', '161', '3', '255', '199', '102', '3', '13', '94', '53', '98', '15', '129', '15', '155', '71', '179', '50', '29', '136', '80', '254', '198', '162', '237', '3', '18', '260', '238', '117', '179', '156', '19', '45', '197', '78', '199', '102', '102', '237', '155', '69', '69', '163', '239', '15', '196', '80', '191', '30', '71', '158', '259', '2', '106', '148', '15', '135', '135', '135', '196', '19', '123', '15', '104', '19', '128', '230', '69', '98', '197', '241', '161', '195', '200', '3', '98', '78', '165', '163', '19', '196', '31', '173', '203', '55', '69', '209', '239', '79', '158', '254', '135', '51', '94', '162', '255', '95', '18', '133', '109', '50', '94', '199', '15', '197', '200', '133', '116', '50', '196', '161', '69', '133', '3', '258', '3', '197', '29', '3', '157', '197', '193', '4', '94', '195', '17', '48', '152', '8', '15', '155', '133', '195', '136', '150', '102', '237', '203', '164', '164', '250', '154', '13', '191', '58', '94', '70', '93', '264', '15', '30', '133', '237', '267', '199', '193', '76', '241', '157', '26', '200', '109', '179', '55', '98', '200', '129', '238', '235', '208', '109', '239', '258', '196', '193', '18', '107', '210', '70', '40', '129', '259', '193', '30', '158', '252', '40', '210', '78', '15', '78', '96', '208', '113', '258', '64', '80', '15', '133', '241', '15', '15', '258', '135', '109', '251', '251', '109', '74', '196', '170', '240', '254', '258', '204', '199', '26', '109', '197', '57', '45', '161', '57', '114', '3', '15', '264', '106', '57', '27', '18', '258', '29', '84', '161', '135', '114', '24', '30', '107', '107', '172', '209', '170', '18', '19', '133', '191', '195', '135', '156', '196', '94', '3', '84', '225', '199', '238', '106', '104', '70', '50', '12', '155', '114', '176', '237', '200', '108', '70', '240', '196', '19', '22', '276', '199', '238', '205', '104', '193', '108', '115', '259', '102', '19', '258', '18', '106', '161', '191', '114', '64', '15', '204', '35', '155', '40', '238', '136', '264', '133', '186', '15', '258', '166', '26', '6', '114', '114', '133', '114', '114', '13', '13', '13', '237', '250', '245', '136', '78', '13', '48', '18', '13', '25', '26', '93', '125', '50', '94', '148', '78', '194', '200', '25', '93', '274', '266', '70', '22', '3', '208', '240', '211', '135', '31', '18', '153', '18', '2', '15', '15', '15', '156', '15', '70', '73', '173', '13', '112', '69', '204', '70', '248', '135', '70', '3', '193', '193', '22', '162', '238', '57', '98', '15', '199', '199', '52', '151', '73', '5', '108', '23', '69', '95', '220', '106', '163', '152', '163', '108', '15', '58', '250', '19', '177', '245', '264', '93', '204', '25', '78', '155', '69', '155', '239', '241', '155', '200', '18', '3', '13', '196', '197', '204', '272', '95', '135', '165', '22', '210', '35', '69', '18', '170', '3', '25', '156', '78', '135', '102', '107', '255', '19', '70', '70', '109', '198', '156', '64', '170', '138', '78', '203', '166', '108', '114', '179', '228', '74', '49', '14', '264', '26', '195', '78', '107', '75', '18', '106', '151', '70', '58', '15', '196', '238', '109', '104', '104', '109', '136', '148', '210', '3', '161', '15', '104', '238', '208', '133', '108', '241', '18', '209', '74', '156', '114', '199', '174', '35', '18', '170', '135', '238', '244', '155', '162', '197', '3', '98', '227', '267', '238', '193', '50', '238', '98', '112', '124', '25', '114', '241', '159', '154', '78', '161', '136', '114', '153', '225', '154', '13', '50', '11', '25', '15', '209', '11', '166', '203', '259', '237', '22', '114', '276', '98', '278', '150', '198', '18', '15', '78', '15', '162', '26', '3', '196', '3', '102', '189', '71', '69', '16', '18', '152', '133', '135', '162', '241', '15', '104', '15', '259', '202', '2', '238', '207', '15', '204', '30', '215', '112', '210', '42', '96', '193', '15', '110', '163', '211', '78', '108', '15', '31', '27', '15', '31', '196', '195', '96', '15', '161', '94', '193', '195', '198', '98', '261', '258', '76', '15', '104', '258', '135', '18', '15', '202', '258', '108', '156', '13', '22', '251', '199', '155', '258', '211', '3', '207', '202', '259', '107', '202', '259', '135', '135', '148', '158', '154', '99', '99', '3', '207', '94', '25', '162', '31', '241', '25', '15', '114', '18', '114', '148', '19', '15', '203', '191', '19', '13', '193', '204', '25', '166', '155', '196', '98', '162', '197', '29', '152', '191', '133', '199', '230', '40', '25', '204', '79', '3', '19', '165', '204', '108', '208', '3', '251', '274', '15', '166', '264', '35', '13', '193', '114', '258', '13', '19', '133', '23', '17', '199', '204', '153', '250', '204', '16', '12', '107', '191', '18', '3', '15', '165', '15', '69', '17', '165', '17', '15', '15', '156', '23', '23', '15', '197', '104', '197', '26', '208', '135', '211', '250', '18', '204', '25', '196', '53', '69', '104', '104', '19', '78', '241', '230', '30', '195', '96', '26', '109', '196', '69', '238', '15', '18', '133', '26', '106', '109', '98', '15', '258', '274', '193', '203', '58', '57', '274', '2', '202', '161', '136', '135', '155', '241', '35', '151', '137', '5', '114', '96', '16', '209', '158', '15', '266', '161', '15', '25', '274', '202', '3', '161', '237', '267', '204', '125', '15', '151', '155', '259', '151', '210', '238', '15', '26', '132', '50', '173', '197', '114', '241', '15', '258', '109', '166', '274', '13', '191', '15', '26', '164', '258', '163', '15', '264', '11', '260', '148', '150', '207', '196', '98', '272', '133', '196', '107', '26', '191', '163', '191', '135', '3', '18', '18', '119', '69', '196', '15', '156', '135', '112', '258', '155', '256', '199', '204', '212', '18', '135', '254', '72', '258', '254', '15', '133', '18', '259', '193', '258', '204', '40', '18', '29', '155', '104', '107', '274', '241', '25', '26', '69', '148', '166', '46', '94', '135', '15', '155', '104', '241', '32', '274', '249', '15', '197', '104', '44', '259', '69', '191', '15', '198', '193', '133', '17', '258', '248', '166', '197', '200', '250', '204', '3', '225', '238', '163', '69', '15', '15', '26', '202', '19', '209', '78', '212', '15', '78', '243', '110', '191', '15', '18', '107', '38', '208', '166', '239', '237', '199', '15', '204', '258', '173', '172', '200', '274', '243', '19', '191', '135', '203', '69', '149', '148', '196', '256', '204', '97', '191', '26', '114', '274', '200', '69', '69', '64', '69', '239', '25', '274', '203', '78', '135', '196', '202', '17', '2', '2', '3', '98', '98', '62', '2', '95', '96', '2', '207', '78', '200', '26', '64', '18', '93', '49', '57', '154', '200', '69', '94', '98', '98', '56', '56', '166', '98', '212', '226', '226', '15', '69', '15', '273', '272', '104', '193', '3', '98', '199', '98', '25', '125', '259', '249', '108', '40', '26', '198', '218', '73', '148', '69', '204', '241', '35', '3', '274', '70', '18', '71', '35', '172', '133', '106', '69', '107', '199', '219', '19', '133', '69', '135', '109', '57', '191', '135', '252', '104', '3', '94', '3', '72', '126', '16', '36', '84', '241', '3', '218', '98', '108', '46', '19', '196', '118', '104', '29', '238', '69', '10', '204', '264', '259', '124', '242', '107', '133', '25', '64', '30', '194', '259', '159', '264', '274', '52', '200', '104', '29', '30', '193', '98', '94', '114', '204', '40', '18', '198', '97', '3', '99', '274', '239', '177', '15', '18', '126', '126', '208', '148', '15', '104', '29', '36', '218', '70', '15', '23', '278', '23', '36', '191', '210', '51', '245', '167', '72', '237', '25', '203', '36', '196', '193', '196', '15', '151', '207', '203', '103', '166', '135', '110', '27', '13', '27', '191', '98', '57', '166', '50', '21', '15', '197', '23', '2', '70', '69', '193', '274', '104', '166', '258', '126', '238', '161', '18', '193', '69', '140', '170', '197', '204', '165', '203', '170', '15', '58', '35', '83', '212', '150', '73', '25', '15', '64', '133', '210', '108', '79', '193', '15', '126', '165', '15', '208', '98', '3', '200', '126', '3', '40', '165', '98', '69', '15', '165', '133', '96', '173', '203', '133', '238', '248', '15', '7', '38', '207', '108', '165', '199', '18', '162', '27', '3', '72', '65', '250', '209', '166', '64', '209', '237', '212', '64', '225', '120', '108', '203', '135', '248', '69', '251', '135', '204', '160', '160', '135', '148', '15', '15', '23', '207', '274', '29', '28', '194', '150', '40', '104', '208', '125', '70', '18', '3', '19', '102', '104', '23', '29', '207', '133', '204', '111', '2', '125', '126', '7', '194', '209', '203', '83', '126', '126', '258', '119', '162', '241', '133', '18', '208', '197', '209', '15', '78', '80', '110', '237', '104', '30', '208', '103', '15', '200', '15', '238', '254', '108', '173', '40', '69', '175', '17', '155', '136', '26', '3', '135', '13', '241', '161', '166', '170', '162', '133', '53', '211', '204', '15', '161', '238', '110', '18', '191', '18', '50', '71', '30', '17', '208', '248', '203', '203', '196', '210', '258', '204', '3', '18', '196', '125', '173', '199', '249', '18', '104', '165', '241', '2', '193', '238', '78', '15', '166', '15', '126', '3', '237', '161', '52', '207', '3', '3', '50', '56', '22', '56', '69', '93', '135', '207', '69', '99', '3', '133', '68', '133', '161', '108', '126', '250', '60', '155', '98', '18', '16', '78', '78', '240', '235', '176', '22', '199', '160', '235', '274', '209', '64', '173', '137', '128', '25', '18', '160', '203', '70', '165', '193', '110', '207', '64', '85', '252', '78', '51', '2', '99', '19', '22', '198', '204', '166', '166', '151', '56', '133', '210', '26', '149', '109', '30', '209', '209', '51', '3', '193', '119', '208', '93', '165', '248', '55', '203', '204', '58', '18', '199', '211', '83', '32', '135', '212', '156', '19', '235', '173', '191', '209', '203', '146', '208', '58', '158', '102', '199', '200', '200', '173', '70', '52', '107', '211', '213', '237', '196', '205', '104', '135', '258', '255', '96', '19', '124', '128', '57', '199', '98', '166', '102', '40', '15', '238', '3', '148', '225', '126', '135', '114', '193', '204', '15', '209', '209', '193', '259', '64', '161', '26', '274', '126', '18', '196', '0', '60', '207', '17', '71', '218', '239', '248', '38', '154', '61', '128', '199', '196', '173', '25', '46', '98', '258', '108', '3', '108', '196', '93', '93', '99', '172', '69', '241', '196', '30', '135', '278', '109', '18', '5', '209', '57', '159', '70', '193', '163', '249', '196', '196', '72', '103', '133', '17', '204', '199', '166', '202', '29', '57', '193', '150', '98', '119', '25', '205', '18', '173', '166', '209', '57', '25', '58', '258', '26', '15', '15', '101', '26', '74', '1', '4', '74', '263']\n",
            "     filenum                                               text  label\n",
            "0          1  each other. But the section length and therefo...    209\n",
            "1          2  v. United States, L.R.A.,N.S. see Athanasaw v....     63\n",
            "2          3  has made no order which changes the appellant'...    216\n",
            "3          4  to withhold from judicial scrutiny has now bee...    108\n",
            "4          6  instrumentality whose sole purpose is to expor...    200\n",
            "...      ...                                                ...    ...\n",
            "6717    8411  substantially similar to those to which we giv...     15\n",
            "6718    8412  is \"strong evidence\" to the contrary. Ylst v. ...     15\n",
            "6719    8413  \"for the purpose of disturbing the public peac...    101\n",
            "6720    8414  that the sentencing phase of his trial violate...     26\n",
            "6721    8417  detailed exceptions. The controversy is now be...      4\n",
            "\n",
            "[6722 rows x 3 columns]\n"
          ]
        }
      ],
      "source": [
        "fh = open('sc_model_2.txt')\n",
        "filenums=[]\n",
        "records=[]\n",
        "for record in fh:\n",
        "    record=record.split(\"---\")\n",
        "    filenum=record[0]\n",
        "    modelnum=record[1]\n",
        "    text=record[2]\n",
        "    filenums.append(filenum)\n",
        "    records.append(text)\n",
        "    \n",
        "print(len(filenums))\n",
        "print(len(records))\n",
        "\n",
        "summarized_data = pd.DataFrame(list(zip(filenums, records)),columns =['filenum','text'])\n",
        "len_list = [len(ele.split()) for ele in records]\n",
        "res = 0 if len(len_list) == 0 else (float(sum(len_list)) / len(len_list))\n",
        "print(\"Average Length %s\" % res)\n",
        "\n",
        "#temp_file = open(\"labels_sc.txt\", \"r\")\n",
        "temp_file = open(\"labels_sc_279.txt\", \"r\")\n",
        "\n",
        "data = temp_file.read()\n",
        "\n",
        "label_list = data.split(\"\\n\")\n",
        "print(label_list)\n",
        "label_list = label_list[0:-1]\n",
        "print(label_list)\n",
        "label_list = [int(i) for i in label_list]\n",
        "temp_file.close()\n",
        "\n",
        "\n",
        "new_label_list=[]\n",
        "for num in filenums:\n",
        "  new_label_list.append(label_list[int(num)-1])\n",
        "summarized_data['label'] = new_label_list\n",
        "\n",
        "print(summarized_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bK-mvfm1v8-i",
        "outputId": "eebf9e99-ac95-471b-9eea-4bb19387ba46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_1[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_roberta_model[0][0]']       \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 279)          143127      ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[209  63 216 ... 101  26   4]\n",
            "[   1    2    3 ... 8413 8414 8417]\n",
            "[6622   35 6713 4989 8233 7444 5061 3942 8344  454 5464  317 4515 3001\n",
            " 6991 2875 1125 3885 6647 4857 4504  214 8340 2969 3847 4609 5092 1346\n",
            " 5993 7358 2752 7651 7301 8188 7073 4782 6069 2346 6445 6483 1663 4952\n",
            " 5072 1967 4071 7590  860 6418 5030 7297  626 5463 6029 6302 3819 7256\n",
            " 5667 1235 3993 1439 1323 4263 3835 2624 5801 4293 8129  320 5866 3247\n",
            " 5777   19 6059 4837 3843 1666  858 1418 5409 5274 6205 3167 2356 5997\n",
            " 6685  223 3444 3310 7721 5928 2360 7197 4252 8343  909 4354 4752 1421\n",
            "  649  829 1916 3488 2496 5765 6612 4004 4052 1403 6697 5268 1010 6293\n",
            " 6380 4602 6057 1851 4577 4236 7978 5083 6552 1058  361 7706 4186 4452\n",
            " 4396  749 8351 3834  812 5108  463 1743 6600 8264 8064 7121 8279 5556\n",
            " 5119 4673 7947 1882 3350 7617 7152 5199 4282 1092 8003 5025 5051  933\n",
            " 3969 1927 3615 1221 8062 1489 3296 7442 5459 2031 7351  234 2313 8015\n",
            " 7416 7921 6574 6257 8396  941 4520 3300 5640  240 7810 7425 1321 6580\n",
            "  928 1170 5404 8273 6742 7005   94 7459    9 7861 7975 1018 5029 2140\n",
            " 1415 7061 7767 4990 8124 5927 6051 3269 6916 5959 2846 6818 7052 7775\n",
            " 7582 8116 4240 4767 5248  430 1812 2175 6055 8331 3916  998 1564 7326\n",
            " 1559  292 3065 6764 6854 4915 5552 3583 1917 5902 4229 2144 7565 3712\n",
            " 7224 4502 1406 1651 6256 3827 6525 6318 2686 3616 5369 8235 1704 1011\n",
            " 3351 8089 8021  102  224 5671 8294 7707 2985 6624 5146 1428 7381 3709\n",
            " 6824 6308  701 2281 6837 5436 1741 3520 5995 2649 4886  966 5476 2406\n",
            " 1199 8387  513 5449 1028 2226 6166 4387  350 6123  678 5236 5041  734\n",
            " 3412 6085  587  489 5760 3376 4982 5115 4655 5575 3839 6743 7173 5724\n",
            " 7689 7787  520 8302 3150 5713 2247 8128 4607 6660 3505 4610 2866 4161\n",
            " 6905 7096  768 1062 4411 6087 2096 4903 7546 6630 2928 4640 7125 1792\n",
            " 2063 1700 2305 7928 8095  737 5849  922 2359 6015 4489 4361 6081 6346\n",
            " 5014 5167 1023 8009 3335  268 4777 5221 4323 6168  759  711 5702 6129\n",
            " 7181 1860 7192 1347 5467 3378 6579   67 2693 6134 4841 4270 4611 2943\n",
            " 6746 7696 3466 3950 4693 6125 8370 6142 6131  995 5548 2112 1721 5067\n",
            " 4015 7092 3156   25 1749 5554 4878 8248  458 6836 1006 5310 2698 6960\n",
            " 2339 1650 6227 4104 5130 7089 6993 2400 1289 4305 5585 7933 6284 7057\n",
            " 3807 5790 8347 2390 5280 2841 7220 1384 7693 8090 4171 5260  133 7451\n",
            " 3957 6793 1719 7379 4743 4746 5435 6216 4858 4417 8355 4492 2648 5104\n",
            " 5278 6539  743 7308  158 7543 2168 5913 7111 3414 5788 7592   39 7157\n",
            " 5647 4555 3956 1045 3617 4630 4384 6596   96 4539 6749 7202 2413 4749\n",
            " 5122 5052 1598 5929  252 5565  336 6008 4564 5580 7237 1466 1568 5958\n",
            " 7657 4327 5091 6918 6487 4082 2989 7448 4715 3382  467 3784 4624  764\n",
            " 4419 1117 7954 7122 6392  505 3174 1534  323 4164 3659 7526  498 5944\n",
            " 4572 2307 2929 7480 4412  731 4563 7692 1869 2472 6966 4238 6549 1594\n",
            " 2276 7269 2545 2216  605 6587 6315 5297 5282 4816 1619 5504 2536 6399\n",
            " 3917 6342 5754 5721 7385 7909 6126 7382 8382 4383 5502 7130  485 5862\n",
            " 4936 7674 3575 5372 3893 7109 4605 5211 5462 8065   50 4080 3334 8167\n",
            " 8378 5595 1873 8207 7076 4106 3719 4627  395  231 2405 5419 6973 3480\n",
            " 3246 5414 5931 8208 4825 6203 3019 5822   63 5854 6543 5787 4764 6701\n",
            " 6562 3822 4861 3051 1315 4805 8318 1089 8169 8218 7549  707 1284 1076\n",
            " 2465 3815 4636 3188  697   80 1455  381 7025 7127 3992 4026 2777 4961\n",
            " 7531  744 6640 1047 6684 4297 7939 7384  446 3287 6675 7752 5279 8196\n",
            " 3020 1203 4192 1552 4253 2887 2556 5729 1898 3208 5973 7033 1431 5915\n",
            " 8312 5799 7105  511 6404 2852 1561 4150 4835 1178]\n"
          ]
        }
      ],
      "source": [
        "gc.collect()\n",
        "tf.keras.backend.clear_session()\n",
        "dbert_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "dbert_model = TFRobertaModel.from_pretrained('roberta-base')\n",
        "max_len=512\n",
        "sentences=summarized_data['text']\n",
        "labels=summarized_data['label']\n",
        "filenumbers=summarized_data['filenum']\n",
        "len(sentences),len(labels),len(filenumbers)\n",
        "\n",
        "model_2=create_model()\n",
        "input_ids=[]\n",
        "attention_masks=[]\n",
        "\n",
        "for sent in sentences:\n",
        "  dbert_inps=dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n",
        "  input_ids.append(dbert_inps['input_ids'])\n",
        "  attention_masks.append(dbert_inps['attention_mask'])\n",
        "input_ids=np.asarray(input_ids)\n",
        "\n",
        "attention_masks=np.array(attention_masks)\n",
        "labels=np.array(labels)\n",
        "filenumbers=[int(i) for i in filenumbers]\n",
        "filenumbers=np.array(filenumbers)\n",
        "print(labels)\n",
        "print(filenumbers)\n",
        "\n",
        "#train_inp,val_inp,train_label,val_label,train_mask,val_mask,train_fnum,val_fnum=train_test_split(input_ids,labels,attention_masks,filenumbers,test_size=0.1,random_state=42)\n",
        "\n",
        "new_train_inp_2=[]\n",
        "new_val_inp_2=[]\n",
        "new_train_label_2=[]\n",
        "new_val_label_2=[]\n",
        "new_train_mask_2=[]\n",
        "new_train_fnum_2=[]\n",
        "new_val_fnum_2=[]\n",
        "new_val_mask_2=[]\n",
        "for i in range(len(train_fnum)):\n",
        "  if train_fnum[i] in filenumbers:\n",
        "    new_train_inp_2.append(train_inp[i])\n",
        "    #new_val_inp.append(val_inp[i])\n",
        "    new_train_label_2.append(train_label[i])\n",
        "    #new_val_label.append(val_label[i])\n",
        "    new_train_mask_2.append(train_mask[i])\n",
        "    #new_val_mask.append(val_mask[i])\n",
        "    new_train_fnum_2.append(train_fnum[i])\n",
        "\n",
        "for i in range(len(val_fnum)):\n",
        "  if val_fnum[i] in filenumbers:\n",
        "    #new_train_inp.append(train_inp[i])\n",
        "    new_val_inp_2.append(val_inp[i])\n",
        "    #new_train_label.append(train_label[i])\n",
        "    new_val_label_2.append(val_label[i])\n",
        "    #new_train_mask.append(train_mask[i])\n",
        "    new_val_mask_2.append(val_mask[i])\n",
        "    new_val_fnum_2.append(val_fnum[i])\n",
        "\n",
        "# print(new_train_label)\n",
        "# print(len(new_train_label))\n",
        "# print(new_val_label)\n",
        "# print(len(new_val_label))\n",
        "new_train_inp_2=np.array(new_train_inp_2)\n",
        "new_val_inp_2=np.array(new_val_inp_2)\n",
        "new_train_label_2=np.array(new_train_label_2)\n",
        "new_val_label_2=np.array(new_val_label_2)\n",
        "new_train_mask_2=np.array(new_train_mask_2)\n",
        "new_train_fnum_2=np.array(new_train_fnum_2)\n",
        "new_val_fnum_2=np.array(new_val_fnum_2)\n",
        "new_val_mask_2=np.array(new_val_mask_2)\n",
        "\n",
        "print(new_val_fnum_2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "RFTWfoELNpYx",
        "outputId": "05b328e1-f6c5-40fd-caf4-2dc8a4f2d067"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_1[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_roberta_model[0][0]']       \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 279)          143127      ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[209  63 216 ... 101  26   4]\n",
            "[   1    2    3 ... 8413 8414 8417]\n",
            "[6622   35 6713 4989 8233 7444 5061 3942 8344  454 5464  317 4515 3001\n",
            " 6991 2875 1125 3885 6647 4857 4504  214 8340 2969 3847 4609 5092 1346\n",
            " 5993 7358 2752 7651 7301 8188 7073 4782 6069 2346 6445 6483 1663 4952\n",
            " 5072 1967 4071 7590  860 6418 5030 7297  626 5463 6029 6302 3819 7256\n",
            " 5667 1235 3993 1439 1323 4263 3835 2624 5801 4293 8129  320 5866 3247\n",
            " 5777   19 6059 4837 3843 1666  858 1418 5409 5274 6205 3167 2356 5997\n",
            " 6685  223 3444 3310 7721 5928 2360 7197 4252 8343  909 4354 4752 1421\n",
            "  649  829 1916 3488 2496 5765 6612 4004 4052 1403 6697 5268 1010 6293\n",
            " 6380 4602 6057 1851 4577 4236 7978 5083 6552 1058  361 7706 4186 4452\n",
            " 4396  749 8351 3834  812 5108  463 1743 6600 8264 8064 7121 8279 5556\n",
            " 5119 4673 7947 1882 3350 7617 7152 5199 4282 1092 8003 5025 5051  933\n",
            " 3969 1927 3615 1221 8062 1489 3296 7442 5459 2031 7351  234 2313 8015\n",
            " 7416 7921 6574 6257 8396  941 4520 3300 5640  240 7810 7425 1321 6580\n",
            "  928 1170 5404 8273 6742 7005   94 7459    9 7861 7975 1018 5029 2140\n",
            " 1415 7061 7767 4990 8124 5927 6051 3269 6916 5959 2846 6818 7052 7775\n",
            " 7582 8116 4240 4767 5248  430 1812 2175 6055 8331 3916  998 1564 7326\n",
            " 1559  292 3065 6764 6854 4915 5552 3583 1917 5902 4229 2144 7565 3712\n",
            " 7224 4502 1406 1651 6256 3827 6525 6318 2686 3616 5369 8235 1704 1011\n",
            " 3351 8089 8021  102  224 5671 8294 7707 2985 6624 5146 1428 7381 3709\n",
            " 6824 6308  701 2281 6837 5436 1741 3520 5995 2649 4886  966 5476 2406\n",
            " 1199 8387  513 5449 1028 2226 6166 4387  350 6123  678 5236 5041  734\n",
            " 3412 6085  587  489 5760 3376 4982 5115 4655 5575 3839 6743 7173 5724\n",
            " 7689 7787  520 8302 3150 5713 2247 8128 4607 6660 3505 4610 2866 4161\n",
            " 6905 7096  768 1062 4411 6087 2096 4903 7546 6630 2928 4640 7125 1792\n",
            " 2063 1700 2305 7928 8095  737 5849  922 2359 6015 4489 4361 6081 6346\n",
            " 5014 5167 1023 8009 3335  268 4777 5221 4323 6168  759  711 5702 6129\n",
            " 7181 1860 7192 1347 5467 3378 6579   67 2693 6134 4841 4270 4611 2943\n",
            " 6746 7696 3466 3950 4693 6125 8370 6142 6131  995 5548 2112 1721 5067\n",
            " 4015 7092 3156   25 1749 5554 4878 8248  458 6836 1006 5310 2698 6960\n",
            " 2339 1650 6227 4104 5130 7089 6993 2400 1289 4305 5585 7933 6284 7057\n",
            " 3807 5790 8347 2390 5280 2841 7220 1384 7693 8090 4171 5260  133 7451\n",
            " 3957 6793 1719 7379 4743 4746 5435 6216 4858 4417 8355 4492 2648 5104\n",
            " 5278 6539  743 7308  158 7543 2168 5913 7111 3414 5788 7592   39 7157\n",
            " 5647 4555 3956 1045 3617 4630 4384 6596   96 4539 6749 7202 2413 4749\n",
            " 5122 5052 1598 5929  252 5565  336 6008 4564 5580 7237 1466 1568 5958\n",
            " 7657 4327 5091 6918 6487 4082 2989 7448 4715 3382  467 3784 4624  764\n",
            " 4419 1117 7954 7122 6392  505 3174 1534  323 4164 3659 7526  498 5944\n",
            " 4572 2307 2929 7480 4412  731 4563 7692 1869 2472 6966 4238 6549 1594\n",
            " 2276 7269 2545 2216  605 6587 6315 5297 5282 4816 1619 5504 2536 6399\n",
            " 3917 6342 5754 5721 7385 7909 6126 7382 8382 4383 5502 7130  485 5862\n",
            " 4936 7674 3575 5372 3893 7109 4605 5211 5462 8065   50 4080 3334 8167\n",
            " 8378 5595 1873 8207 7076 4106 3719 4627  395  231 2405 5419 6973 3480\n",
            " 3246 5414 5931 8208 4825 6203 3019 5822   63 5854 6543 5787 4764 6701\n",
            " 6562 3822 4861 3051 1315 4805 8318 1089 8169 8218 7549  707 1284 1076\n",
            " 2465 3815 4636 3188  697   80 1455  381 7025 7127 3992 4026 2777 4961\n",
            " 7531  744 6640 1047 6684 4297 7939 7384  446 3287 6675 7752 5279 8196\n",
            " 3020 1203 4192 1552 4253 2887 2556 5729 1898 3208 5973 7033 1431 5915\n",
            " 8312 5799 7105  511 6404 2852 1561 4150 4835 1178]\n",
            "Mon Jun  6 07:28:53 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P0    39W / 300W |  15237MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "757/757 [==============================] - 281s 349ms/step - loss: 14.1551 - accuracy: 0.1006 - val_loss: 12.5214 - val_accuracy: 0.2889\n",
            "Epoch 2/5\n",
            "757/757 [==============================] - 263s 348ms/step - loss: 11.9669 - accuracy: 0.3370 - val_loss: 11.2289 - val_accuracy: 0.4251\n",
            "Epoch 3/5\n",
            "757/757 [==============================] - 263s 347ms/step - loss: 10.8010 - accuracy: 0.4604 - val_loss: 10.5001 - val_accuracy: 0.4701\n",
            "Epoch 4/5\n",
            "757/757 [==============================] - 264s 349ms/step - loss: 9.9944 - accuracy: 0.5418 - val_loss: 10.0196 - val_accuracy: 0.5105\n",
            "Epoch 5/5\n",
            "757/757 [==============================] - 263s 348ms/step - loss: 9.3525 - accuracy: 0.6079 - val_loss: 9.6356 - val_accuracy: 0.5135\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_3[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_roberta_model[1][0]']       \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 279)          143127      ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.48812351543942994\n",
            "Weighted F1: 0.4276898953506736\n",
            "Micro F1: 0.48812351543942994\n",
            "Weighted Precision: 0.4063307570246469\n",
            "Micro Precision: 0.48812351543942994\n",
            "Weighted Recall: 0.48812351543942994\n",
            "Micro Recall: 0.48812351543942994\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_1[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_roberta_model[0][0]']       \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 279)          143127      ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[209  63 216 ... 101  26   4]\n",
            "[   1    2    3 ... 8413 8414 8417]\n",
            "[6622   35 6713 4989 8233 7444 5061 3942 8344  454 5464  317 4515 3001\n",
            " 6991 2875 1125 3885 6647 4857 4504  214 8340 2969 3847 4609 5092 1346\n",
            " 5993 7358 2752 7651 7301 8188 7073 4782 6069 2346 6445 6483 1663 4952\n",
            " 5072 1967 4071 7590  860 6418 5030 7297  626 5463 6029 6302 3819 7256\n",
            " 5667 1235 3993 1439 1323 4263 3835 2624 5801 4293 8129  320 5866 3247\n",
            " 5777   19 6059 4837 3843 1666  858 1418 5409 5274 6205 3167 2356 5997\n",
            " 6685  223 3444 3310 7721 5928 2360 7197 4252 8343  909 4354 4752 1421\n",
            "  649  829 1916 3488 2496 5765 6612 4004 4052 1403 6697 5268 1010 6293\n",
            " 6380 4602 6057 1851 4577 4236 7978 5083 6552 1058  361 7706 4186 4452\n",
            " 4396  749 8351 3834  812 5108  463 1743 6600 8264 8064 7121 8279 5556\n",
            " 5119 4673 7947 1882 3350 7617 7152 5199 4282 1092 8003 5025 5051  933\n",
            " 3969 1927 3615 1221 8062 1489 3296 7442 5459 2031 7351  234 2313 8015\n",
            " 7416 7921 6574 6257 8396  941 4520 3300 5640  240 7810 7425 1321 6580\n",
            "  928 1170 5404 8273 6742 7005   94 7459    9 7861 7975 1018 5029 2140\n",
            " 1415 7061 7767 4990 8124 5927 6051 3269 6916 5959 2846 6818 7052 7775\n",
            " 7582 8116 4240 4767 5248  430 1812 2175 6055 8331 3916  998 1564 7326\n",
            " 1559  292 3065 6764 6854 4915 5552 3583 1917 5902 4229 2144 7565 3712\n",
            " 7224 4502 1406 1651 6256 3827 6525 6318 2686 3616 5369 8235 1704 1011\n",
            " 3351 8089 8021  102  224 5671 8294 7707 2985 6624 5146 1428 7381 3709\n",
            " 6824 6308  701 2281 6837 5436 1741 3520 5995 2649 4886  966 5476 2406\n",
            " 1199 8387  513 5449 1028 2226 6166 4387  350 6123  678 5236 5041  734\n",
            " 3412 6085  587  489 5760 3376 4982 5115 4655 5575 3839 6743 7173 5724\n",
            " 7689 7787  520 8302 3150 5713 2247 8128 4607 6660 3505 4610 2866 4161\n",
            " 6905 7096  768 1062 4411 6087 2096 4903 7546 6630 2928 4640 7125 1792\n",
            " 2063 1700 2305 7928 8095  737 5849  922 2359 6015 4489 4361 6081 6346\n",
            " 5014 5167 1023 8009 3335  268 4777 5221 4323 6168  759  711 5702 6129\n",
            " 7181 1860 7192 1347 5467 3378 6579   67 2693 6134 4841 4270 4611 2943\n",
            " 6746 7696 3466 3950 4693 6125 8370 6142 6131  995 5548 2112 1721 5067\n",
            " 4015 7092 3156   25 1749 5554 4878 8248  458 6836 1006 5310 2698 6960\n",
            " 2339 1650 6227 4104 5130 7089 6993 2400 1289 4305 5585 7933 6284 7057\n",
            " 3807 5790 8347 2390 5280 2841 7220 1384 7693 8090 4171 5260  133 7451\n",
            " 3957 6793 1719 7379 4743 4746 5435 6216 4858 4417 8355 4492 2648 5104\n",
            " 5278 6539  743 7308  158 7543 2168 5913 7111 3414 5788 7592   39 7157\n",
            " 5647 4555 3956 1045 3617 4630 4384 6596   96 4539 6749 7202 2413 4749\n",
            " 5122 5052 1598 5929  252 5565  336 6008 4564 5580 7237 1466 1568 5958\n",
            " 7657 4327 5091 6918 6487 4082 2989 7448 4715 3382  467 3784 4624  764\n",
            " 4419 1117 7954 7122 6392  505 3174 1534  323 4164 3659 7526  498 5944\n",
            " 4572 2307 2929 7480 4412  731 4563 7692 1869 2472 6966 4238 6549 1594\n",
            " 2276 7269 2545 2216  605 6587 6315 5297 5282 4816 1619 5504 2536 6399\n",
            " 3917 6342 5754 5721 7385 7909 6126 7382 8382 4383 5502 7130  485 5862\n",
            " 4936 7674 3575 5372 3893 7109 4605 5211 5462 8065   50 4080 3334 8167\n",
            " 8378 5595 1873 8207 7076 4106 3719 4627  395  231 2405 5419 6973 3480\n",
            " 3246 5414 5931 8208 4825 6203 3019 5822   63 5854 6543 5787 4764 6701\n",
            " 6562 3822 4861 3051 1315 4805 8318 1089 8169 8218 7549  707 1284 1076\n",
            " 2465 3815 4636 3188  697   80 1455  381 7025 7127 3992 4026 2777 4961\n",
            " 7531  744 6640 1047 6684 4297 7939 7384  446 3287 6675 7752 5279 8196\n",
            " 3020 1203 4192 1552 4253 2887 2556 5729 1898 3208 5973 7033 1431 5915\n",
            " 8312 5799 7105  511 6404 2852 1561 4150 4835 1178]\n",
            "Mon Jun  6 07:52:59 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P0    37W / 300W |  15237MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "757/757 [==============================] - 279s 347ms/step - loss: 13.9324 - accuracy: 0.1397 - val_loss: 12.3680 - val_accuracy: 0.3129\n",
            "Epoch 2/5\n",
            "757/757 [==============================] - 263s 347ms/step - loss: 11.8946 - accuracy: 0.3565 - val_loss: 11.2129 - val_accuracy: 0.4401\n",
            "Epoch 3/5\n",
            "757/757 [==============================] - 263s 347ms/step - loss: 10.8047 - accuracy: 0.4673 - val_loss: 10.5234 - val_accuracy: 0.4955\n",
            "Epoch 4/5\n",
            "757/757 [==============================] - 262s 347ms/step - loss: 10.0332 - accuracy: 0.5375 - val_loss: 9.9923 - val_accuracy: 0.5030\n",
            "Epoch 5/5\n",
            "757/757 [==============================] - 263s 348ms/step - loss: 9.3667 - accuracy: 0.6120 - val_loss: 9.6368 - val_accuracy: 0.5105\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_3[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_roberta_model[1][0]']       \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 279)          143127      ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.48931116389548696\n",
            "Weighted F1: 0.4284195924303802\n",
            "Micro F1: 0.48931116389548696\n",
            "Weighted Precision: 0.40191810813608647\n",
            "Micro Precision: 0.48931116389548696\n",
            "Weighted Recall: 0.48931116389548696\n",
            "Micro Recall: 0.48931116389548696\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_1[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_roberta_model[0][0]']       \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 279)          143127      ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[209  63 216 ... 101  26   4]\n",
            "[   1    2    3 ... 8413 8414 8417]\n",
            "[6622   35 6713 4989 8233 7444 5061 3942 8344  454 5464  317 4515 3001\n",
            " 6991 2875 1125 3885 6647 4857 4504  214 8340 2969 3847 4609 5092 1346\n",
            " 5993 7358 2752 7651 7301 8188 7073 4782 6069 2346 6445 6483 1663 4952\n",
            " 5072 1967 4071 7590  860 6418 5030 7297  626 5463 6029 6302 3819 7256\n",
            " 5667 1235 3993 1439 1323 4263 3835 2624 5801 4293 8129  320 5866 3247\n",
            " 5777   19 6059 4837 3843 1666  858 1418 5409 5274 6205 3167 2356 5997\n",
            " 6685  223 3444 3310 7721 5928 2360 7197 4252 8343  909 4354 4752 1421\n",
            "  649  829 1916 3488 2496 5765 6612 4004 4052 1403 6697 5268 1010 6293\n",
            " 6380 4602 6057 1851 4577 4236 7978 5083 6552 1058  361 7706 4186 4452\n",
            " 4396  749 8351 3834  812 5108  463 1743 6600 8264 8064 7121 8279 5556\n",
            " 5119 4673 7947 1882 3350 7617 7152 5199 4282 1092 8003 5025 5051  933\n",
            " 3969 1927 3615 1221 8062 1489 3296 7442 5459 2031 7351  234 2313 8015\n",
            " 7416 7921 6574 6257 8396  941 4520 3300 5640  240 7810 7425 1321 6580\n",
            "  928 1170 5404 8273 6742 7005   94 7459    9 7861 7975 1018 5029 2140\n",
            " 1415 7061 7767 4990 8124 5927 6051 3269 6916 5959 2846 6818 7052 7775\n",
            " 7582 8116 4240 4767 5248  430 1812 2175 6055 8331 3916  998 1564 7326\n",
            " 1559  292 3065 6764 6854 4915 5552 3583 1917 5902 4229 2144 7565 3712\n",
            " 7224 4502 1406 1651 6256 3827 6525 6318 2686 3616 5369 8235 1704 1011\n",
            " 3351 8089 8021  102  224 5671 8294 7707 2985 6624 5146 1428 7381 3709\n",
            " 6824 6308  701 2281 6837 5436 1741 3520 5995 2649 4886  966 5476 2406\n",
            " 1199 8387  513 5449 1028 2226 6166 4387  350 6123  678 5236 5041  734\n",
            " 3412 6085  587  489 5760 3376 4982 5115 4655 5575 3839 6743 7173 5724\n",
            " 7689 7787  520 8302 3150 5713 2247 8128 4607 6660 3505 4610 2866 4161\n",
            " 6905 7096  768 1062 4411 6087 2096 4903 7546 6630 2928 4640 7125 1792\n",
            " 2063 1700 2305 7928 8095  737 5849  922 2359 6015 4489 4361 6081 6346\n",
            " 5014 5167 1023 8009 3335  268 4777 5221 4323 6168  759  711 5702 6129\n",
            " 7181 1860 7192 1347 5467 3378 6579   67 2693 6134 4841 4270 4611 2943\n",
            " 6746 7696 3466 3950 4693 6125 8370 6142 6131  995 5548 2112 1721 5067\n",
            " 4015 7092 3156   25 1749 5554 4878 8248  458 6836 1006 5310 2698 6960\n",
            " 2339 1650 6227 4104 5130 7089 6993 2400 1289 4305 5585 7933 6284 7057\n",
            " 3807 5790 8347 2390 5280 2841 7220 1384 7693 8090 4171 5260  133 7451\n",
            " 3957 6793 1719 7379 4743 4746 5435 6216 4858 4417 8355 4492 2648 5104\n",
            " 5278 6539  743 7308  158 7543 2168 5913 7111 3414 5788 7592   39 7157\n",
            " 5647 4555 3956 1045 3617 4630 4384 6596   96 4539 6749 7202 2413 4749\n",
            " 5122 5052 1598 5929  252 5565  336 6008 4564 5580 7237 1466 1568 5958\n",
            " 7657 4327 5091 6918 6487 4082 2989 7448 4715 3382  467 3784 4624  764\n",
            " 4419 1117 7954 7122 6392  505 3174 1534  323 4164 3659 7526  498 5944\n",
            " 4572 2307 2929 7480 4412  731 4563 7692 1869 2472 6966 4238 6549 1594\n",
            " 2276 7269 2545 2216  605 6587 6315 5297 5282 4816 1619 5504 2536 6399\n",
            " 3917 6342 5754 5721 7385 7909 6126 7382 8382 4383 5502 7130  485 5862\n",
            " 4936 7674 3575 5372 3893 7109 4605 5211 5462 8065   50 4080 3334 8167\n",
            " 8378 5595 1873 8207 7076 4106 3719 4627  395  231 2405 5419 6973 3480\n",
            " 3246 5414 5931 8208 4825 6203 3019 5822   63 5854 6543 5787 4764 6701\n",
            " 6562 3822 4861 3051 1315 4805 8318 1089 8169 8218 7549  707 1284 1076\n",
            " 2465 3815 4636 3188  697   80 1455  381 7025 7127 3992 4026 2777 4961\n",
            " 7531  744 6640 1047 6684 4297 7939 7384  446 3287 6675 7752 5279 8196\n",
            " 3020 1203 4192 1552 4253 2887 2556 5729 1898 3208 5973 7033 1431 5915\n",
            " 8312 5799 7105  511 6404 2852 1561 4150 4835 1178]\n",
            "Mon Jun  6 08:17:02 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P0    38W / 300W |  15237MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "757/757 [==============================] - 280s 348ms/step - loss: 14.1158 - accuracy: 0.1188 - val_loss: 12.5665 - val_accuracy: 0.3099\n",
            "Epoch 2/5\n",
            "757/757 [==============================] - 263s 348ms/step - loss: 12.0115 - accuracy: 0.3499 - val_loss: 11.2848 - val_accuracy: 0.4162\n",
            "Epoch 3/5\n",
            "757/757 [==============================] - 263s 348ms/step - loss: 10.8983 - accuracy: 0.4523 - val_loss: 10.6040 - val_accuracy: 0.4820\n",
            "Epoch 4/5\n",
            "757/757 [==============================] - 263s 347ms/step - loss: 10.0920 - accuracy: 0.5314 - val_loss: 10.1085 - val_accuracy: 0.4940\n",
            "Epoch 5/5\n",
            "757/757 [==============================] - 263s 347ms/step - loss: 9.4356 - accuracy: 0.5940 - val_loss: 9.7095 - val_accuracy: 0.5165\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_3[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_roberta_model[1][0]']       \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 279)          143127      ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.48812351543942994\n",
            "Weighted F1: 0.4275310022945432\n",
            "Micro F1: 0.48812351543942994\n",
            "Weighted Precision: 0.41886923949450783\n",
            "Micro Precision: 0.48812351543942994\n",
            "Weighted Recall: 0.48812351543942994\n",
            "Micro Recall: 0.48812351543942994\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_1[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_roberta_model[0][0]']       \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 279)          143127      ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[209  63 216 ... 101  26   4]\n",
            "[   1    2    3 ... 8413 8414 8417]\n",
            "[6622   35 6713 4989 8233 7444 5061 3942 8344  454 5464  317 4515 3001\n",
            " 6991 2875 1125 3885 6647 4857 4504  214 8340 2969 3847 4609 5092 1346\n",
            " 5993 7358 2752 7651 7301 8188 7073 4782 6069 2346 6445 6483 1663 4952\n",
            " 5072 1967 4071 7590  860 6418 5030 7297  626 5463 6029 6302 3819 7256\n",
            " 5667 1235 3993 1439 1323 4263 3835 2624 5801 4293 8129  320 5866 3247\n",
            " 5777   19 6059 4837 3843 1666  858 1418 5409 5274 6205 3167 2356 5997\n",
            " 6685  223 3444 3310 7721 5928 2360 7197 4252 8343  909 4354 4752 1421\n",
            "  649  829 1916 3488 2496 5765 6612 4004 4052 1403 6697 5268 1010 6293\n",
            " 6380 4602 6057 1851 4577 4236 7978 5083 6552 1058  361 7706 4186 4452\n",
            " 4396  749 8351 3834  812 5108  463 1743 6600 8264 8064 7121 8279 5556\n",
            " 5119 4673 7947 1882 3350 7617 7152 5199 4282 1092 8003 5025 5051  933\n",
            " 3969 1927 3615 1221 8062 1489 3296 7442 5459 2031 7351  234 2313 8015\n",
            " 7416 7921 6574 6257 8396  941 4520 3300 5640  240 7810 7425 1321 6580\n",
            "  928 1170 5404 8273 6742 7005   94 7459    9 7861 7975 1018 5029 2140\n",
            " 1415 7061 7767 4990 8124 5927 6051 3269 6916 5959 2846 6818 7052 7775\n",
            " 7582 8116 4240 4767 5248  430 1812 2175 6055 8331 3916  998 1564 7326\n",
            " 1559  292 3065 6764 6854 4915 5552 3583 1917 5902 4229 2144 7565 3712\n",
            " 7224 4502 1406 1651 6256 3827 6525 6318 2686 3616 5369 8235 1704 1011\n",
            " 3351 8089 8021  102  224 5671 8294 7707 2985 6624 5146 1428 7381 3709\n",
            " 6824 6308  701 2281 6837 5436 1741 3520 5995 2649 4886  966 5476 2406\n",
            " 1199 8387  513 5449 1028 2226 6166 4387  350 6123  678 5236 5041  734\n",
            " 3412 6085  587  489 5760 3376 4982 5115 4655 5575 3839 6743 7173 5724\n",
            " 7689 7787  520 8302 3150 5713 2247 8128 4607 6660 3505 4610 2866 4161\n",
            " 6905 7096  768 1062 4411 6087 2096 4903 7546 6630 2928 4640 7125 1792\n",
            " 2063 1700 2305 7928 8095  737 5849  922 2359 6015 4489 4361 6081 6346\n",
            " 5014 5167 1023 8009 3335  268 4777 5221 4323 6168  759  711 5702 6129\n",
            " 7181 1860 7192 1347 5467 3378 6579   67 2693 6134 4841 4270 4611 2943\n",
            " 6746 7696 3466 3950 4693 6125 8370 6142 6131  995 5548 2112 1721 5067\n",
            " 4015 7092 3156   25 1749 5554 4878 8248  458 6836 1006 5310 2698 6960\n",
            " 2339 1650 6227 4104 5130 7089 6993 2400 1289 4305 5585 7933 6284 7057\n",
            " 3807 5790 8347 2390 5280 2841 7220 1384 7693 8090 4171 5260  133 7451\n",
            " 3957 6793 1719 7379 4743 4746 5435 6216 4858 4417 8355 4492 2648 5104\n",
            " 5278 6539  743 7308  158 7543 2168 5913 7111 3414 5788 7592   39 7157\n",
            " 5647 4555 3956 1045 3617 4630 4384 6596   96 4539 6749 7202 2413 4749\n",
            " 5122 5052 1598 5929  252 5565  336 6008 4564 5580 7237 1466 1568 5958\n",
            " 7657 4327 5091 6918 6487 4082 2989 7448 4715 3382  467 3784 4624  764\n",
            " 4419 1117 7954 7122 6392  505 3174 1534  323 4164 3659 7526  498 5944\n",
            " 4572 2307 2929 7480 4412  731 4563 7692 1869 2472 6966 4238 6549 1594\n",
            " 2276 7269 2545 2216  605 6587 6315 5297 5282 4816 1619 5504 2536 6399\n",
            " 3917 6342 5754 5721 7385 7909 6126 7382 8382 4383 5502 7130  485 5862\n",
            " 4936 7674 3575 5372 3893 7109 4605 5211 5462 8065   50 4080 3334 8167\n",
            " 8378 5595 1873 8207 7076 4106 3719 4627  395  231 2405 5419 6973 3480\n",
            " 3246 5414 5931 8208 4825 6203 3019 5822   63 5854 6543 5787 4764 6701\n",
            " 6562 3822 4861 3051 1315 4805 8318 1089 8169 8218 7549  707 1284 1076\n",
            " 2465 3815 4636 3188  697   80 1455  381 7025 7127 3992 4026 2777 4961\n",
            " 7531  744 6640 1047 6684 4297 7939 7384  446 3287 6675 7752 5279 8196\n",
            " 3020 1203 4192 1552 4253 2887 2556 5729 1898 3208 5973 7033 1431 5915\n",
            " 8312 5799 7105  511 6404 2852 1561 4150 4835 1178]\n",
            "Mon Jun  6 08:41:07 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P0    38W / 300W |  15237MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "757/757 [==============================] - 279s 347ms/step - loss: 14.0895 - accuracy: 0.1117 - val_loss: 12.5011 - val_accuracy: 0.2934\n",
            "Epoch 2/5\n",
            "757/757 [==============================] - 262s 347ms/step - loss: 11.9247 - accuracy: 0.3495 - val_loss: 11.1569 - val_accuracy: 0.4311\n",
            "Epoch 3/5\n",
            "757/757 [==============================] - 263s 347ms/step - loss: 10.7766 - accuracy: 0.4577 - val_loss: 10.4775 - val_accuracy: 0.4686\n",
            "Epoch 4/5\n",
            "757/757 [==============================] - 262s 347ms/step - loss: 9.9734 - accuracy: 0.5408 - val_loss: 9.9203 - val_accuracy: 0.5165\n",
            "Epoch 5/5\n",
            "757/757 [==============================] - 263s 347ms/step - loss: 9.3174 - accuracy: 0.6054 - val_loss: 9.6124 - val_accuracy: 0.5210\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_3[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_roberta_model[1][0]']       \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 279)          143127      ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.4916864608076009\n",
            "Weighted F1: 0.43577134296359543\n",
            "Micro F1: 0.4916864608076009\n",
            "Weighted Precision: 0.42211143901677006\n",
            "Micro Precision: 0.4916864608076009\n",
            "Weighted Recall: 0.4916864608076009\n",
            "Micro Recall: 0.4916864608076009\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_1[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_roberta_model[0][0]']       \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 279)          143127      ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[209  63 216 ... 101  26   4]\n",
            "[   1    2    3 ... 8413 8414 8417]\n",
            "[6622   35 6713 4989 8233 7444 5061 3942 8344  454 5464  317 4515 3001\n",
            " 6991 2875 1125 3885 6647 4857 4504  214 8340 2969 3847 4609 5092 1346\n",
            " 5993 7358 2752 7651 7301 8188 7073 4782 6069 2346 6445 6483 1663 4952\n",
            " 5072 1967 4071 7590  860 6418 5030 7297  626 5463 6029 6302 3819 7256\n",
            " 5667 1235 3993 1439 1323 4263 3835 2624 5801 4293 8129  320 5866 3247\n",
            " 5777   19 6059 4837 3843 1666  858 1418 5409 5274 6205 3167 2356 5997\n",
            " 6685  223 3444 3310 7721 5928 2360 7197 4252 8343  909 4354 4752 1421\n",
            "  649  829 1916 3488 2496 5765 6612 4004 4052 1403 6697 5268 1010 6293\n",
            " 6380 4602 6057 1851 4577 4236 7978 5083 6552 1058  361 7706 4186 4452\n",
            " 4396  749 8351 3834  812 5108  463 1743 6600 8264 8064 7121 8279 5556\n",
            " 5119 4673 7947 1882 3350 7617 7152 5199 4282 1092 8003 5025 5051  933\n",
            " 3969 1927 3615 1221 8062 1489 3296 7442 5459 2031 7351  234 2313 8015\n",
            " 7416 7921 6574 6257 8396  941 4520 3300 5640  240 7810 7425 1321 6580\n",
            "  928 1170 5404 8273 6742 7005   94 7459    9 7861 7975 1018 5029 2140\n",
            " 1415 7061 7767 4990 8124 5927 6051 3269 6916 5959 2846 6818 7052 7775\n",
            " 7582 8116 4240 4767 5248  430 1812 2175 6055 8331 3916  998 1564 7326\n",
            " 1559  292 3065 6764 6854 4915 5552 3583 1917 5902 4229 2144 7565 3712\n",
            " 7224 4502 1406 1651 6256 3827 6525 6318 2686 3616 5369 8235 1704 1011\n",
            " 3351 8089 8021  102  224 5671 8294 7707 2985 6624 5146 1428 7381 3709\n",
            " 6824 6308  701 2281 6837 5436 1741 3520 5995 2649 4886  966 5476 2406\n",
            " 1199 8387  513 5449 1028 2226 6166 4387  350 6123  678 5236 5041  734\n",
            " 3412 6085  587  489 5760 3376 4982 5115 4655 5575 3839 6743 7173 5724\n",
            " 7689 7787  520 8302 3150 5713 2247 8128 4607 6660 3505 4610 2866 4161\n",
            " 6905 7096  768 1062 4411 6087 2096 4903 7546 6630 2928 4640 7125 1792\n",
            " 2063 1700 2305 7928 8095  737 5849  922 2359 6015 4489 4361 6081 6346\n",
            " 5014 5167 1023 8009 3335  268 4777 5221 4323 6168  759  711 5702 6129\n",
            " 7181 1860 7192 1347 5467 3378 6579   67 2693 6134 4841 4270 4611 2943\n",
            " 6746 7696 3466 3950 4693 6125 8370 6142 6131  995 5548 2112 1721 5067\n",
            " 4015 7092 3156   25 1749 5554 4878 8248  458 6836 1006 5310 2698 6960\n",
            " 2339 1650 6227 4104 5130 7089 6993 2400 1289 4305 5585 7933 6284 7057\n",
            " 3807 5790 8347 2390 5280 2841 7220 1384 7693 8090 4171 5260  133 7451\n",
            " 3957 6793 1719 7379 4743 4746 5435 6216 4858 4417 8355 4492 2648 5104\n",
            " 5278 6539  743 7308  158 7543 2168 5913 7111 3414 5788 7592   39 7157\n",
            " 5647 4555 3956 1045 3617 4630 4384 6596   96 4539 6749 7202 2413 4749\n",
            " 5122 5052 1598 5929  252 5565  336 6008 4564 5580 7237 1466 1568 5958\n",
            " 7657 4327 5091 6918 6487 4082 2989 7448 4715 3382  467 3784 4624  764\n",
            " 4419 1117 7954 7122 6392  505 3174 1534  323 4164 3659 7526  498 5944\n",
            " 4572 2307 2929 7480 4412  731 4563 7692 1869 2472 6966 4238 6549 1594\n",
            " 2276 7269 2545 2216  605 6587 6315 5297 5282 4816 1619 5504 2536 6399\n",
            " 3917 6342 5754 5721 7385 7909 6126 7382 8382 4383 5502 7130  485 5862\n",
            " 4936 7674 3575 5372 3893 7109 4605 5211 5462 8065   50 4080 3334 8167\n",
            " 8378 5595 1873 8207 7076 4106 3719 4627  395  231 2405 5419 6973 3480\n",
            " 3246 5414 5931 8208 4825 6203 3019 5822   63 5854 6543 5787 4764 6701\n",
            " 6562 3822 4861 3051 1315 4805 8318 1089 8169 8218 7549  707 1284 1076\n",
            " 2465 3815 4636 3188  697   80 1455  381 7025 7127 3992 4026 2777 4961\n",
            " 7531  744 6640 1047 6684 4297 7939 7384  446 3287 6675 7752 5279 8196\n",
            " 3020 1203 4192 1552 4253 2887 2556 5729 1898 3208 5973 7033 1431 5915\n",
            " 8312 5799 7105  511 6404 2852 1561 4150 4835 1178]\n",
            "Mon Jun  6 09:05:09 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P0    38W / 300W |  15237MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "757/757 [==============================] - 280s 347ms/step - loss: 14.1302 - accuracy: 0.0943 - val_loss: 12.5350 - val_accuracy: 0.2575\n",
            "Epoch 2/5\n",
            "757/757 [==============================] - 263s 348ms/step - loss: 11.8890 - accuracy: 0.3299 - val_loss: 11.1472 - val_accuracy: 0.4356\n",
            "Epoch 3/5\n",
            "757/757 [==============================] - 263s 348ms/step - loss: 10.7447 - accuracy: 0.4516 - val_loss: 10.4812 - val_accuracy: 0.4731\n",
            "Epoch 4/5\n",
            "757/757 [==============================] - 263s 347ms/step - loss: 9.9258 - accuracy: 0.5329 - val_loss: 9.9004 - val_accuracy: 0.5210\n",
            "Epoch 5/5\n",
            "757/757 [==============================] - 258s 341ms/step - loss: 9.2601 - accuracy: 0.6014 - val_loss: 9.5653 - val_accuracy: 0.5135\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_3[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_roberta_model[1][0]']       \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 279)          143127      ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.497624703087886\n",
            "Weighted F1: 0.428792559901646\n",
            "Micro F1: 0.497624703087886\n",
            "Weighted Precision: 0.4088691507677405\n",
            "Micro Precision: 0.497624703087886\n",
            "Weighted Recall: 0.497624703087886\n",
            "Micro Recall: 0.497624703087886\n",
            "Average Accuracy: 0.4909738717339668\n",
            "Average Weighted F1: 0.42964087858816774\n",
            "Average Micro F1: 0.4909738717339668\n",
            "Average Weighted Precision: 0.41161973888795045\n",
            "Average Micro Precision: 0.4909738717339668\n",
            "Average Weighted Recall: 0.4909738717339668\n",
            "Average Micro Recall: 0.4909738717339668\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "total_accuracy=0\n",
        "total_weighted_f1=0\n",
        "total_micro_f1=0\n",
        "total_weighted_precision=0\n",
        "total_micro_precision=0\n",
        "total_weighted_recall=0\n",
        "total_micro_recall=0\n",
        "\n",
        "for f in range(5):\n",
        "  gc.collect()\n",
        "  tf.keras.backend.clear_session()\n",
        "  dbert_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "  dbert_model = TFRobertaModel.from_pretrained('roberta-base')\n",
        "  max_len=512\n",
        "  sentences=summarized_data['text']\n",
        "  labels=summarized_data['label']\n",
        "  filenumbers=summarized_data['filenum']\n",
        "  len(sentences),len(labels),len(filenumbers)\n",
        "  \n",
        "  model_2=create_model()\n",
        "  input_ids=[]\n",
        "  attention_masks=[]\n",
        "\n",
        "  for sent in sentences:\n",
        "    dbert_inps=dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n",
        "    input_ids.append(dbert_inps['input_ids'])\n",
        "    attention_masks.append(dbert_inps['attention_mask'])\n",
        "  input_ids=np.asarray(input_ids)\n",
        "\n",
        "  attention_masks=np.array(attention_masks)\n",
        "  labels=np.array(labels)\n",
        "  filenumbers=[int(i) for i in filenumbers]\n",
        "  filenumbers=np.array(filenumbers)\n",
        "  print(labels)\n",
        "  print(filenumbers)\n",
        "\n",
        "  #train_inp,val_inp,train_label,val_label,train_mask,val_mask,train_fnum,val_fnum=train_test_split(input_ids,labels,attention_masks,filenumbers,test_size=0.1,random_state=42)\n",
        "\n",
        "  new_train_inp_2=[]\n",
        "  new_val_inp_2=[]\n",
        "  new_train_label_2=[]\n",
        "  new_val_label_2=[]\n",
        "  new_train_mask_2=[]\n",
        "  new_train_fnum_2=[]\n",
        "  new_val_fnum_2=[]\n",
        "  new_val_mask_2=[]\n",
        "  for i in range(len(train_fnum)):\n",
        "    if train_fnum[i] in filenumbers:\n",
        "      new_train_inp_2.append(train_inp[i])\n",
        "      #new_val_inp.append(val_inp[i])\n",
        "      new_train_label_2.append(train_label[i])\n",
        "      #new_val_label.append(val_label[i])\n",
        "      new_train_mask_2.append(train_mask[i])\n",
        "      #new_val_mask.append(val_mask[i])\n",
        "      new_train_fnum_2.append(train_fnum[i])\n",
        "\n",
        "  for i in range(len(val_fnum)):\n",
        "    if val_fnum[i] in filenumbers:\n",
        "      #new_train_inp.append(train_inp[i])\n",
        "      new_val_inp_2.append(val_inp[i])\n",
        "      #new_train_label.append(train_label[i])\n",
        "      new_val_label_2.append(val_label[i])\n",
        "      #new_train_mask.append(train_mask[i])\n",
        "      new_val_mask_2.append(val_mask[i])\n",
        "      new_val_fnum_2.append(val_fnum[i])\n",
        "\n",
        "  # print(new_train_label)\n",
        "  # print(len(new_train_label))\n",
        "  # print(new_val_label)\n",
        "  # print(len(new_val_label))\n",
        "  new_train_inp_2=np.array(new_train_inp_2)\n",
        "  new_val_inp_2=np.array(new_val_inp_2)\n",
        "  new_train_label_2=np.array(new_train_label_2)\n",
        "  new_val_label_2=np.array(new_val_label_2)\n",
        "  new_train_mask_2=np.array(new_train_mask_2)\n",
        "  new_train_fnum_2=np.array(new_train_fnum_2)\n",
        "  new_val_fnum_2=np.array(new_val_fnum_2)\n",
        "  new_val_mask_2=np.array(new_val_mask_2)\n",
        "  \n",
        "  print(new_val_fnum_2)\n",
        "  \n",
        "  log_dir='dbert_model'\n",
        "\n",
        "  model_save_path='./drive/MyDrive/Ensemble/roberta-ensemble-512-model_2-'+str(f)+'-279labels.h5'\n",
        "\n",
        "  loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "  accuracy = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
        "  callbacks= [tf.keras.callbacks.ModelCheckpoint(filepath=model_save_path,monitor='val_accuracy',mode='max',save_best_only=True,save_weights_only=True),keras.callbacks.TensorBoard(log_dir=log_dir)]\n",
        "  model_2.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])\n",
        "  gpu_info = !nvidia-smi\n",
        "  gpu_info = '\\n'.join(gpu_info)\n",
        "  if gpu_info.find('failed') >= 0:\n",
        "    print('Not connected to a GPU')\n",
        "  else:\n",
        "    print(gpu_info)\n",
        "  \n",
        "  history=model_2.fit([new_train_inp_2,new_train_mask_2],new_train_label_2,batch_size=8,epochs=5,validation_data=([new_val_inp_2,new_val_mask_2],new_val_label_2),callbacks=callbacks)\n",
        "\n",
        "  pred_labels=[]\n",
        "\n",
        "  model_saved_2= create_model()\n",
        "  model_saved_2.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])\n",
        "  model_saved_2.load_weights('./drive/MyDrive/Ensemble/roberta-ensemble-512-model_2-'+str(f)+'-279labels.h5')\n",
        "\n",
        "  for i in range(0,len(val_inp)):\n",
        "    pred=model_saved_2.predict([val_inp[i].reshape(1,512),val_mask[i].reshape(1,512)])\n",
        "    pred_label = pred.argmax(axis=1)\n",
        "    pred_labels.append(pred_label)\n",
        "  accuracy=accuracy_score(val_label, pred_labels)\n",
        "  print(\"Accuracy: \"+str(accuracy))\n",
        "  total_accuracy=total_accuracy+accuracy\n",
        "  \n",
        "  weighted_f1=f1_score(val_label,pred_labels, average='weighted')\n",
        "  print(\"Weighted F1: \"+ str(weighted_f1))\n",
        "  total_weighted_f1=total_weighted_f1+weighted_f1\n",
        "  micro_f1=f1_score(val_label,pred_labels, average='micro')\n",
        "  print(\"Micro F1: \"+ str(micro_f1))\n",
        "  total_micro_f1=total_micro_f1+micro_f1\n",
        "\n",
        "  weighted_precision=precision_score(val_label, pred_labels, average='weighted')\n",
        "  print(\"Weighted Precision: \" + str(weighted_precision))\n",
        "  total_weighted_precision=total_weighted_precision+weighted_precision\n",
        "  micro_precision=precision_score(val_label, pred_labels, average='micro')\n",
        "  print(\"Micro Precision: \" + str(micro_precision))\n",
        "  total_micro_precision=total_micro_precision+micro_precision\n",
        "\n",
        "  weighted_recall=recall_score(val_label, pred_labels, average='weighted')\n",
        "  print(\"Weighted Recall: \" + str(weighted_recall))\n",
        "  total_weighted_recall=total_weighted_recall+weighted_recall\n",
        "  micro_recall=recall_score(val_label, pred_labels, average='micro')\n",
        "  print(\"Micro Recall: \" + str(micro_recall))\n",
        "  total_micro_recall=total_micro_recall+micro_recall\n",
        "\n",
        "\n",
        "print(\"Average Accuracy: \"+str(total_accuracy/5))\n",
        "print(\"Average Weighted F1: \"+str(total_weighted_f1/5))\n",
        "print(\"Average Micro F1: \"+str(total_micro_f1/5))\n",
        "print(\"Average Weighted Precision: \"+str(total_weighted_precision/5))\n",
        "print(\"Average Micro Precision: \"+str(total_micro_precision/5))\n",
        "print(\"Average Weighted Recall: \"+str(total_weighted_recall/5))\n",
        "print(\"Average Micro Recall: \"+str(total_micro_recall/5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5uUYcxMONKM",
        "outputId": "3d736f67-2b97-4fb0-997f-644f889bc170"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6005\n",
            "6005\n",
            "Average Length 511.7708576186511\n",
            "['209', '63', '216', '108', '196', '200', '203', '246', '188', '193', '25', '106', '56', '53', '223', '161', '200', '208', '78', '147', '147', '179', '147', '80', '56', '209', '209', '200', '214', '202', '203', '194', '26', '237', '165', '258', '238', '238', '76', '193', '196', '213', '161', '213', '209', '194', '196', '25', '151', '260', '135', '173', '30', '108', '272', '259', '176', '213', '213', '21', '5', '171', '200', '195', '240', '194', '176', '240', '240', '80', '173', '98', '214', '21', '76', '76', '73', '173', '173', '78', '61', '258', '191', '3', '78', '173', '193', '118', '200', '264', '260', '49', '211', '258', '18', '173', '78', '3', '3', '1', '213', '135', '182', '239', '208', '278', '50', '194', '200', '124', '189', '117', '264', '244', '76', '193', '221', '78', '78', '173', '197', '78', '161', '231', '135', '22', '78', '25', '25', '197', '200', '147', '203', '191', '196', '193', '263', '32', '197', '191', '4', '56', '196', '104', '191', '196', '173', '147', '194', '96', '161', '161', '238', '200', '3', '78', '78', '53', '41', '237', '53', '18', '10', '194', '97', '96', '208', '25', '10', '117', '104', '54', '96', '194', '21', '245', '238', '202', '209', '161', '97', '258', '240', '213', '136', '155', '152', '21', '191', '191', '118', '68', '258', '189', '197', '208', '61', '78', '135', '202', '96', '155', '108', '272', '80', '25', '155', '35', '213', '191', '194', '96', '96', '191', '264', '191', '191', '191', '2', '258', '191', '173', '259', '15', '47', '258', '156', '118', '160', '115', '105', '26', '173', '192', '76', '76', '135', '240', '118', '161', '200', '230', '18', '200', '25', '10', '161', '244', '24', '24', '200', '205', '104', '15', '193', '156', '95', '209', '240', '191', '194', '128', '208', '208', '218', '173', '10', '25', '18', '107', '46', '32', '197', '172', '209', '213', '79', '121', '78', '78', '193', '2', '78', '194', '149', '136', '80', '3', '21', '200', '78', '21', '197', '194', '118', '202', '161', '180', '209', '173', '2', '193', '6', '260', '197', '78', '78', '42', '191', '213', '135', '193', '196', '202', '222', '217', '42', '259', '213', '76', '46', '30', '15', '260', '202', '18', '259', '104', '135', '76', '196', '240', '240', '240', '108', '24', '200', '197', '262', '176', '15', '156', '264', '156', '200', '191', '161', '196', '78', '238', '161', '221', '118', '14', '14', '14', '200', '278', '238', '200', '196', '78', '173', '25', '197', '197', '196', '161', '18', '10', '10', '10', '18', '66', '203', '19', '246', '196', '244', '240', '161', '200', '78', '240', '209', '208', '118', '215', '194', '193', '168', '229', '191', '135', '172', '196', '197', '161', '78', '196', '197', '258', '78', '78', '238', '221', '121', '264', '104', '146', '50', '78', '217', '11', '238', '238', '238', '3', '3', '154', '215', '79', '18', '158', '104', '18', '13', '197', '117', '197', '161', '213', '242', '32', '238', '230', '2', '2', '32', '161', '251', '96', '156', '242', '22', '22', '140', '3', '149', '135', '191', '173', '135', '200', '238', '238', '78', '156', '209', '78', '97', '97', '160', '25', '258', '240', '161', '15', '161', '96', '209', '80', '132', '140', '4', '1', '240', '196', '274', '272', '62', '80', '196', '196', '191', '79', '156', '15', '196', '209', '22', '251', '202', '202', '121', '73', '191', '238', '191', '150', '150', '135', '46', '203', '202', '238', '22', '3', '258', '239', '238', '238', '165', '238', '238', '194', '200', '200', '197', '200', '196', '191', '194', '76', '78', '208', '200', '71', '208', '240', '76', '117', '120', '108', '96', '144', '30', '133', '133', '202', '117', '161', '251', '29', '104', '242', '197', '118', '216', '262', '236', '141', '11', '235', '235', '133', '191', '238', '202', '80', '80', '22', '140', '191', '197', '202', '133', '184', '184', '184', '184', '143', '274', '50', '237', '196', '274', '265', '10', '18', '10', '121', '238', '245', '228', '76', '25', '191', '197', '268', '10', '240', '195', '196', '195', '15', '48', '200', '183', '70', '59', '197', '196', '235', '161', '161', '215', '104', '97', '191', '194', '191', '200', '200', '197', '76', '200', '93', '256', '160', '238', '146', '238', '239', '15', '140', '258', '140', '21', '79', '202', '258', '79', '62', '205', '161', '104', '10', '161', '196', '137', '152', '191', '117', '213', '242', '176', '218', '195', '191', '162', '191', '135', '241', '213', '213', '84', '79', '68', '18', '98', '197', '155', '41', '246', '269', '78', '209', '238', '80', '238', '62', '237', '273', '273', '230', '150', '121', '260', '244', '161', '202', '78', '109', '155', '264', '145', '50', '274', '173', '210', '258', '238', '211', '156', '215', '195', '177', '244', '3', '141', '196', '191', '194', '29', '30', '10', '16', '200', '200', '156', '53', '3', '62', '165', '173', '173', '22', '205', '274', '150', '183', '116', '188', '188', '98', '161', '204', '80', '172', '156', '104', '229', '259', '78', '169', '3', '200', '118', '161', '80', '260', '240', '150', '197', '196', '62', '93', '6', '272', '272', '197', '160', '78', '96', '70', '195', '191', '191', '80', '3', '3', '240', '147', '196', '191', '238', '96', '203', '3', '15', '10', '58', '55', '96', '41', '15', '26', '50', '238', '194', '79', '240', '161', '191', '195', '263', '50', '240', '147', '235', '195', '203', '59', '178', '176', '2', '15', '203', '191', '18', '278', '196', '14', '153', '58', '175', '18', '217', '217', '80', '191', '200', '18', '200', '173', '24', '171', '171', '210', '124', '34', '200', '218', '104', '4', '223', '193', '194', '200', '200', '13', '62', '194', '197', '80', '197', '146', '146', '96', '97', '97', '2', '196', '191', '191', '104', '238', '10', '200', '239', '135', '104', '213', '2', '221', '169', '25', '21', '194', '161', '21', '278', '3', '195', '46', '176', '25', '117', '191', '62', '62', '62', '78', '50', '264', '79', '197', '80', '80', '80', '191', '191', '78', '161', '194', '195', '197', '203', '259', '278', '49', '147', '147', '147', '147', '22', '78', '78', '258', '2', '202', '221', '55', '258', '104', '104', '84', '240', '221', '274', '104', '22', '241', '63', '197', '197', '196', '159', '200', '22', '22', '22', '78', '78', '173', '104', '97', '104', '191', '146', '223', '268', '96', '14', '173', '204', '241', '97', '15', '118', '78', '78', '196', '194', '258', '96', '96', '259', '75', '10', '197', '30', '2', '238', '121', '50', '161', '173', '173', '186', '185', '186', '78', '194', '221', '218', '35', '33', '13', '34', '78', '21', '215', '97', '60', '24', '78', '193', '173', '235', '78', '200', '258', '140', '259', '194', '197', '195', '200', '145', '76', '108', '130', '238', '215', '213', '2', '217', '121', '178', '140', '62', '156', '176', '196', '196', '41', '223', '147', '172', '79', '200', '2', '244', '244', '191', '213', '104', '191', '237', '237', '33', '117', '195', '195', '264', '195', '140', '210', '41', '197', '14', '257', '196', '238', '238', '79', '161', '156', '196', '238', '238', '213', '104', '3', '10', '118', '204', '235', '33', '259', '274', '259', '176', '78', '78', '196', '34', '52', '25', '30', '194', '153', '21', '70', '191', '18', '191', '191', '172', '194', '194', '194', '194', '148', '104', '125', '2', '2', '2', '185', '194', '29', '104', '78', '189', '272', '204', '183', '213', '78', '104', '197', '240', '97', '168', '168', '182', '238', '197', '18', '194', '229', '221', '62', '78', '53', '104', '214', '189', '189', '189', '213', '117', '117', '191', '29', '104', '104', '276', '276', '22', '195', '195', '155', '139', '139', '59', '168', '135', '140', '79', '146', '200', '245', '70', '160', '153', '50', '202', '154', '263', '130', '194', '117', '194', '194', '191', '33', '29', '29', '238', '239', '194', '21', '274', '215', '18', '18', '140', '2', '215', '25', '46', '137', '238', '30', '160', '156', '46', '196', '263', '242', '104', '0', '0', '73', '213', '135', '22', '191', '191', '241', '202', '221', '15', '15', '191', '238', '213', '194', '197', '179', '238', '200', '200', '200', '5', '238', '194', '55', '117', '166', '49', '244', '239', '191', '78', '78', '78', '194', '200', '117', '210', '22', '121', '121', '121', '21', '237', '194', '121', '78', '104', '194', '79', '191', '251', '195', '197', '274', '176', '21', '43', '43', '10', '61', '215', '215', '30', '214', '194', '10', '30', '139', '96', '226', '226', '265', '2', '2', '121', '121', '41', '237', '104', '238', '238', '161', '78', '78', '78', '11', '184', '140', '140', '161', '104', '104', '237', '130', '195', '160', '118', '161', '18', '11', '241', '184', '196', '178', '22', '30', '146', '146', '10', '10', '141', '141', '135', '18', '18', '10', '26', '208', '97', '141', '18', '18', '97', '194', '194', '79', '213', '205', '205', '205', '75', '241', '50', '226', '238', '97', '222', '215', '215', '139', '238', '51', '238', '96', '173', '109', '200', '117', '192', '2', '195', '10', '3', '18', '49', '238', '238', '191', '195', '52', '197', '200', '200', '79', '191', '200', '200', '30', '195', '197', '195', '25', '260', '73', '30', '193', '21', '200', '242', '203', '120', '78', '260', '241', '0', '30', '202', '195', '30', '191', '78', '194', '194', '2', '205', '191', '196', '173', '197', '30', '196', '172', '139', '2', '18', '191', '230', '56', '194', '215', '195', '2', '71', '13', '146', '264', '50', '191', '235', '93', '191', '139', '139', '227', '235', '186', '235', '24', '18', '261', '128', '33', '71', '25', '176', '39', '10', '195', '202', '29', '29', '221', '29', '197', '139', '78', '227', '146', '137', '195', '205', '205', '195', '135', '62', '153', '146', '29', '75', '0', '194', '194', '180', '213', '78', '18', '194', '239', '196', '239', '197', '194', '153', '192', '221', '10', '46', '50', '241', '117', '117', '117', '195', '173', '78', '195', '194', '239', '2', '12', '173', '197', '30', '269', '197', '176', '135', '30', '146', '191', '93', '93', '135', '260', '218', '156', '155', '200', '241', '125', '18', '18', '183', '203', '239', '259', '173', '213', '180', '180', '210', '120', '104', '194', '156', '258', '172', '9', '0', '241', '192', '143', '189', '204', '50', '63', '197', '243', '241', '93', '125', '2', '21', '260', '4', '4', '3', '191', '150', '128', '78', '238', '245', '8', '80', '238', '79', '79', '240', '58', '104', '155', '80', '80', '188', '191', '235', '170', '170', '170', '146', '258', '195', '276', '25', '78', '215', '240', '161', '53', '147', '78', '78', '78', '78', '221', '221', '96', '18', '19', '130', '18', '18', '18', '80', '80', '194', '79', '93', '121', '78', '139', '139', '78', '197', '121', '194', '200', '80', '101', '22', '12', '135', '97', '205', '181', '60', '193', '15', '140', '172', '191', '241', '222', '135', '29', '25', '179', '191', '191', '259', '194', '133', '240', '121', '121', '241', '195', '29', '191', '209', '18', '139', '139', '50', '18', '12', '18', '133', '200', '25', '10', '70', '25', '242', '18', '161', '49', '172', '172', '172', '172', '128', '96', '195', '200', '78', '10', '239', '22', '173', '142', '142', '22', '194', '192', '3', '200', '241', '96', '241', '62', '139', '213', '202', '135', '79', '120', '192', '78', '118', '241', '46', '151', '150', '151', '150', '118', '221', '183', '237', '196', '140', '140', '39', '179', '213', '140', '41', '73', '78', '217', '258', '30', '118', '140', '219', '50', '10', '139', '150', '251', '24', '10', '18', '78', '18', '172', '135', '146', '71', '196', '202', '197', '25', '106', '160', '215', '15', '6', '101', '165', '140', '145', '80', '3', '238', '239', '78', '268', '0', '109', '191', '215', '238', '238', '238', '203', '50', '146', '15', '25', '156', '191', '16', '2', '195', '189', '96', '193', '109', '204', '161', '185', '210', '50', '235', '30', '242', '193', '102', '194', '121', '177', '241', '102', '132', '240', '71', '192', '79', '25', '197', '39', '161', '22', '258', '237', '15', '7', '78', '156', '197', '70', '176', '139', '240', '132', '78', '177', '191', '194', '117', '10', '79', '238', '195', '24', '101', '118', '196', '2', '102', '21', '180', '185', '170', '79', '139', '135', '191', '151', '200', '191', '154', '262', '200', '237', '56', '27', '191', '180', '258', '139', '139', '192', '30', '0', '242', '239', '242', '25', '191', '242', '240', '3', '56', '215', '171', '192', '193', '222', '238', '237', '215', '272', '2', '156', '237', '151', '189', '80', '15', '260', '260', '217', '235', '22', '228', '135', '18', '191', '157', '78', '2', '240', '213', '179', '2', '191', '79', '79', '153', '3', '194', '78', '121', '71', '121', '149', '257', '194', '191', '188', '15', '25', '125', '102', '15', '130', '130', '177', '10', '135', '265', '240', '149', '196', '200', '204', '204', '213', '177', '197', '194', '13', '202', '30', '213', '125', '193', '125', '125', '125', '125', '125', '15', '180', '78', '1', '25', '96', '200', '29', '156', '172', '202', '22', '78', '197', '188', '241', '101', '101', '101', '221', '50', '191', '15', '15', '0', '101', '101', '168', '56', '197', '130', '62', '204', '29', '10', '96', '97', '139', '97', '97', '2', '2', '262', '39', '176', '172', '194', '194', '18', '78', '13', '80', '139', '196', '207', '191', '151', '192', '150', '13', '104', '104', '101', '15', '125', '101', '15', '0', '50', '194', '264', '50', '25', '25', '125', '125', '125', '125', '125', '25', '29', '25', '25', '78', '240', '261', '19', '2', '74', '78', '238', '50', '50', '203', '197', '71', '104', '80', '194', '149', '170', '130', '2', '237', '170', '202', '48', '97', '93', '179', '235', '78', '102', '102', '70', '208', '238', '104', '146', '118', '118', '96', '237', '274', '218', '241', '102', '146', '209', '209', '240', '170', '137', '7', '197', '191', '264', '29', '46', '96', '193', '19', '12', '213', '215', '275', '179', '18', '183', '78', '221', '18', '170', '119', '21', '140', '240', '96', '192', '192', '21', '96', '149', '239', '135', '191', '240', '184', '183', '24', '2', '96', '197', '194', '215', '18', '192', '121', '202', '196', '25', '97', '214', '3', '97', '16', '0', '192', '135', '215', '5', '12', '80', '143', '202', '242', '151', '13', '209', '203', '200', '30', '209', '194', '102', '102', '102', '102', '102', '102', '22', '24', '6', '18', '5', '191', '101', '50', '101', '101', '192', '153', '153', '101', '101', '10', '192', '25', '140', '50', '241', '102', '102', '32', '150', '101', '102', '153', '102', '101', '153', '102', '25', '10', '125', '18', '125', '177', '209', '102', '10', '78', '78', '137', '18', '237', '241', '175', '195', '175', '96', '176', '237', '96', '96', '101', '193', '238', '170', '238', '102', '222', '268', '238', '102', '244', '34', '18', '244', '202', '149', '149', '193', '102', '170', '194', '75', '101', '222', '238', '50', '195', '135', '22', '93', '18', '102', '102', '93', '93', '191', '147', '244', '96', '2', '241', '241', '93', '189', '189', '177', '35', '137', '197', '191', '40', '40', '195', '176', '35', '200', '200', '237', '135', '140', '140', '93', '156', '200', '78', '191', '22', '81', '238', '101', '213', '101', '78', '109', '19', '140', '196', '78', '78', '76', '218', '14', '265', '101', '241', '155', '4', '50', '78', '78', '238', '135', '191', '265', '81', '238', '238', '102', '102', '102', '140', '162', '32', '17', '17', '171', '171', '191', '101', '102', '192', '101', '17', '171', '135', '102', '242', '195', '237', '21', '18', '238', '191', '278', '140', '149', '97', '260', '155', '276', '192', '196', '21', '191', '2', '97', '0', '150', '221', '213', '209', '238', '80', '260', '194', '104', '96', '193', '14', '240', '193', '117', '117', '155', '17', '264', '260', '209', '209', '2', '137', '71', '72', '101', '205', '173', '191', '118', '24', '41', '197', '78', '94', '237', '156', '193', '153', '154', '153', '209', '110', '78', '78', '213', '238', '78', '191', '93', '79', '177', '267', '133', '133', '46', '139', '40', '143', '74', '241', '202', '50', '3', '102', '78', '149', '191', '156', '18', '93', '195', '192', '137', '240', '135', '204', '189', '22', '56', '154', '25', '192', '130', '277', '191', '80', '39', '21', '21', '238', '238', '23', '192', '191', '192', '94', '94', '80', '139', '17', '10', '22', '227', '227', '140', '102', '21', '17', '227', '227', '23', '243', '264', '18', '274', '149', '238', '30', '221', '22', '238', '193', '135', '205', '195', '238', '238', '197', '130', '195', '18', '104', '102', '14', '59', '104', '18', '18', '258', '34', '241', '138', '188', '176', '102', '1', '140', '140', '22', '22', '278', '10', '96', '35', '145', '120', '33', '33', '22', '33', '22', '35', '22', '19', '33', '102', '192', '192', '175', '22', '72', '191', '222', '125', '22', '79', '18', '194', '18', '250', '192', '192', '96', '48', '274', '273', '132', '133', '192', '40', '171', '171', '246', '191', '10', '166', '197', '195', '125', '125', '200', '153', '192', '149', '110', '139', '194', '48', '102', '102', '104', '203', '238', '238', '238', '78', '18', '238', '121', '18', '238', '96', '238', '238', '13', '191', '18', '18', '240', '96', '40', '188', '18', '137', '175', '28', '28', '28', '149', '191', '191', '207', '239', '239', '153', '153', '153', '153', '153', '153', '154', '153', '154', '153', '153', '153', '18', '0', '27', '18', '25', '96', '96', '137', '33', '10', '130', '153', '153', '154', '180', '137', '44', '241', '153', '268', '25', '161', '25', '39', '242', '4', '238', '195', '102', '135', '13', '229', '140', '1', '79', '192', '229', '191', '118', '21', '260', '203', '18', '177', '191', '10', '238', '10', '133', '14', '175', '175', '192', '18', '153', '153', '254', '140', '149', '22', '22', '22', '237', '40', '135', '191', '192', '192', '62', '205', '19', '238', '176', '214', '200', '96', '15', '153', '191', '108', '29', '166', '28', '193', '197', '96', '195', '108', '102', '241', '195', '10', '33', '166', '197', '168', '2', '96', '26', '33', '204', '258', '149', '153', '153', '40', '137', '241', '166', '221', '23', '221', '153', '15', '113', '113', '78', '171', '40', '71', '21', '71', '80', '15', '191', '191', '135', '110', '10', '259', '102', '149', '149', '109', '109', '175', '97', '97', '97', '153', '15', '238', '193', '175', '38', '38', '137', '18', '235', '155', '137', '265', '110', '18', '18', '256', '191', '224', '173', '104', '10', '152', '24', '24', '24', '17', '40', '40', '40', '40', '40', '40', '112', '6', '18', '18', '210', '96', '191', '27', '213', '17', '153', '26', '133', '18', '18', '153', '38', '96', '40', '38', '40', '40', '10', '17', '17', '95', '71', '192', '17', '259', '213', '200', '15', '11', '207', '194', '18', '135', '238', '191', '241', '191', '116', '35', '147', '156', '67', '78', '17', '227', '191', '117', '188', '130', '72', '96', '235', '97', '18', '150', '203', '25', '126', '149', '200', '94', '4', '4', '1', '244', '228', '252', '149', '191', '149', '15', '17', '17', '29', '18', '18', '116', '23', '237', '94', '184', '268', '197', '175', '243', '28', '16', '10', '50', '191', '102', '102', '153', '149', '111', '78', '191', '135', '18', '35', '78', '238', '208', '93', '95', '258', '17', '111', '111', '262', '22', '22', '104', '161', '209', '21', '195', '12', '116', '241', '242', '239', '17', '97', '16', '40', '117', '94', '96', '78', '15', '103', '195', '135', '235', '247', '135', '130', '260', '239', '176', '93', '191', '209', '239', '93', '264', '30', '18', '30', '18', '18', '103', '191', '97', '17', '95', '115', '209', '140', '22', '22', '241', '10', '153', '3', '188', '238', '226', '195', '97', '96', '238', '97', '130', '97', '116', '96', '96', '242', '45', '149', '166', '155', '96', '116', '264', '10', '237', '192', '71', '22', '71', '175', '149', '116', '237', '274', '18', '235', '176', '116', '202', '250', '250', '78', '2', '196', '2', '97', '143', '18', '111', '111', '80', '78', '153', '80', '40', '110', '30', '112', '30', '111', '215', '112', '149', '78', '78', '209', '110', '110', '238', '109', '239', '112', '151', '154', '16', '16', '16', '78', '137', '179', '72', '149', '10', '111', '268', '96', '170', '111', '38', '192', '153', '183', '195', '30', '30', '147', '195', '147', '93', '264', '153', '25', '18', '19', '22', '74', '40', '93', '166', '15', '127', '147', '192', '260', '238', '103', '269', '127', '19', '19', '153', '18', '78', '238', '30', '30', '241', '16', '235', '250', '204', '238', '218', '40', '93', '184', '18', '0', '241', '195', '94', '154', '109', '156', '10', '21', '30', '197', '39', '78', '24', '107', '142', '142', '228', '229', '229', '229', '232', '179', '239', '142', '232', '23', '175', '147', '137', '137', '137', '171', '191', '117', '128', '127', '204', '98', '147', '235', '239', '153', '15', '204', '255', '80', '204', '204', '149', '113', '19', '167', '22', '250', '17', '17', '17', '18', '17', '67', '121', '125', '26', '97', '97', '97', '97', '247', '124', '163', '147', '112', '111', '208', '108', '38', '209', '154', '154', '194', '135', '22', '147', '241', '120', '218', '260', '156', '155', '241', '192', '215', '180', '179', '149', '40', '193', '192', '150', '102', '93', '1', '149', '137', '133', '102', '102', '102', '78', '145', '21', '96', '175', '175', '78', '123', '30', '95', '19', '18', '110', '18', '152', '152', '147', '26', '135', '260', '158', '203', '195', '158', '112', '156', '125', '213', '230', '30', '15', '125', '106', '111', '195', '181', '130', '195', '130', '239', '15', '194', '16', '15', '111', '61', '72', '57', '128', '239', '239', '203', '25', '218', '10', '155', '191', '46', '98', '175', '260', '102', '21', '191', '260', '191', '239', '15', '30', '93', '147', '103', '78', '177', '30', '95', '155', '155', '155', '103', '103', '112', '241', '191', '191', '17', '78', '153', '150', '113', '115', '147', '78', '40', '163', '33', '15', '149', '103', '192', '191', '96', '106', '115', '143', '235', '200', '204', '238', '204', '204', '74', '196', '113', '246', '102', '150', '16', '176', '20', '195', '128', '197', '15', '71', '71', '193', '24', '24', '24', '170', '204', '209', '111', '144', '112', '22', '222', '224', '28', '209', '155', '213', '275', '196', '239', '195', '25', '103', '149', '128', '19', '96', '102', '135', '17', '232', '155', '191', '20', '18', '241', '30', '25', '205', '97', '97', '96', '72', '182', '135', '135', '20', '149', '149', '78', '135', '40', '18', '153', '10', '26', '158', '205', '157', '157', '135', '135', '140', '155', '192', '26', '268', '263', '245', '155', '15', '177', '159', '212', '161', '239', '200', '40', '35', '153', '197', '259', '15', '254', '175', '10', '30', '239', '67', '195', '200', '191', '22', '66', '191', '112', '133', '128', '235', '161', '155', '17', '93', '36', '113', '239', '18', '18', '191', '165', '163', '163', '238', '79', '158', '40', '102', '40', '200', '191', '60', '241', '245', '96', '78', '30', '15', '173', '192', '192', '158', '64', '113', '200', '74', '128', '135', '115', '115', '10', '74', '102', '102', '115', '111', '238', '244', '109', '109', '238', '152', '10', '237', '18', '50', '16', '173', '204', '15', '208', '276', '43', '109', '258', '222', '15', '94', '235', '203', '79', '128', '113', '3', '204', '106', '133', '214', '218', '25', '98', '166', '71', '30', '175', '175', '97', '135', '108', '19', '18', '196', '35', '12', '196', '78', '166', '155', '72', '155', '22', '109', '258', '196', '263', '161', '204', '210', '14', '208', '208', '208', '208', '204', '102', '102', '213', '155', '251', '153', '153', '153', '154', '154', '237', '97', '19', '28', '94', '136', '112', '19', '97', '152', '153', '153', '111', '111', '135', '135', '105', '17', '105', '152', '152', '152', '153', '79', '266', '135', '120', '125', '149', '50', '162', '72', '17', '147', '109', '115', '24', '105', '244', '149', '238', '203', '35', '202', '177', '173', '239', '19', '19', '179', '237', '14', '18', '185', '58', '158', '144', '213', '237', '155', '250', '239', '93', '237', '97', '195', '157', '107', '109', '246', '165', '98', '49', '2', '266', '149', '20', '18', '71', '108', '22', '115', '239', '40', '178', '232', '238', '218', '118', '118', '210', '235', '46', '228', '192', '112', '149', '96', '111', '21', '95', '95', '95', '18', '57', '206', '18', '239', '157', '80', '196', '112', '176', '99', '106', '71', '235', '135', '18', '209', '78', '20', '20', '103', '33', '3', '103', '166', '79', '79', '156', '229', '18', '238', '2', '16', '128', '200', '117', '22', '197', '166', '78', '237', '241', '107', '53', '189', '204', '243', '163', '221', '15', '200', '221', '152', '23', '193', '107', '195', '99', '97', '19', '125', '276', '113', '193', '64', '78', '197', '117', '175', '135', '135', '78', '93', '154', '153', '256', '256', '135', '137', '135', '137', '149', '110', '15', '159', '21', '158', '192', '192', '117', '29', '97', '258', '78', '46', '5', '80', '15', '105', '260', '161', '191', '2', '138', '117', '215', '176', '156', '194', '265', '115', '181', '21', '14', '106', '106', '108', '200', '156', '103', '102', '29', '17', '204', '175', '227', '118', '18', '239', '204', '258', '78', '155', '213', '109', '192', '186', '186', '15', '241', '133', '30', '30', '239', '30', '218', '109', '138', '241', '241', '45', '135', '112', '232', '104', '106', '200', '53', '23', '112', '17', '117', '59', '260', '195', '106', '117', '159', '204', '200', '228', '165', '165', '261', '200', '227', '166', '93', '21', '80', '112', '152', '196', '203', '228', '98', '102', '15', '139', '110', '13', '238', '6', '171', '94', '208', '155', '112', '203', '239', '191', '39', '136', '203', '195', '46', '203', '148', '192', '210', '23', '4', '153', '29', '165', '192', '204', '232', '94', '112', '239', '98', '71', '248', '17', '155', '10', '78', '191', '191', '111', '25', '25', '19', '19', '17', '235', '102', '265', '195', '235', '163', '238', '71', '197', '107', '155', '30', '72', '19', '259', '80', '155', '23', '64', '239', '197', '156', '57', '203', '258', '200', '205', '246', '238', '258', '227', '238', '238', '19', '148', '161', '111', '105', '202', '109', '262', '133', '137', '221', '135', '52', '170', '111', '36', '30', '156', '102', '268', '115', '22', '153', '133', '259', '98', '235', '135', '191', '25', '235', '25', '29', '94', '195', '203', '212', '112', '135', '245', '260', '200', '96', '158', '23', '165', '22', '18', '249', '240', '109', '43', '37', '15', '241', '241', '23', '135', '240', '108', '98', '268', '275', '191', '136', '263', '259', '196', '98', '98', '204', '248', '123', '105', '204', '240', '9', '9', '98', '218', '189', '238', '246', '155', '130', '157', '74', '109', '29', '189', '203', '213', '195', '241', '159', '238', '204', '218', '105', '23', '16', '263', '206', '150', '152', '204', '202', '173', '267', '18', '153', '155', '196', '2', '97', '158', '158', '204', '98', '202', '98', '204', '258', '98', '22', '113', '113', '135', '71', '237', '155', '163', '250', '233', '26', '26', '26', '26', '26', '19', '170', '18', '15', '19', '191', '30', '30', '48', '35', '30', '237', '46', '119', '16', '78', '239', '150', '245', '235', '158', '27', '241', '30', '38', '107', '75', '241', '135', '106', '106', '98', '159', '96', '135', '111', '176', '175', '200', '18', '14', '242', '163', '187', '20', '12', '191', '23', '106', '171', '25', '78', '162', '191', '157', '94', '237', '246', '203', '108', '238', '204', '204', '1', '102', '154', '106', '170', '102', '200', '2', '135', '106', '258', '258', '227', '26', '15', '25', '71', '203', '96', '3', '241', '200', '30', '108', '98', '108', '27', '106', '26', '150', '259', '78', '113', '113', '158', '202', '16', '136', '239', '156', '239', '35', '276', '252', '115', '22', '22', '258', '172', '3', '154', '3', '98', '237', '102', '235', '112', '206', '98', '57', '118', '153', '18', '26', '30', '196', '163', '191', '153', '72', '24', '156', '117', '122', '30', '135', '192', '98', '28', '30', '30', '95', '241', '155', '17', '195', '31', '103', '202', '98', '237', '94', '112', '163', '163', '94', '163', '112', '19', '195', '191', '15', '135', '78', '109', '160', '152', '97', '98', '107', '167', '97', '138', '138', '26', '191', '97', '97', '30', '30', '206', '202', '13', '30', '240', '111', '261', '79', '0', '111', '19', '152', '107', '107', '18', '98', '28', '258', '244', '106', '237', '204', '191', '15', '183', '202', '16', '111', '166', '268', '202', '12', '30', '78', '110', '133', '71', '197', '111', '52', '79', '171', '156', '94', '13', '118', '111', '71', '133', '18', '105', '30', '22', '133', '60', '237', '191', '6', '126', '3', '238', '276', '78', '162', '150', '115', '191', '107', '200', '148', '135', '72', '155', '241', '4', '108', '160', '203', '20', '133', '2', '80', '80', '154', '18', '15', '115', '239', '167', '167', '35', '268', '18', '221', '18', '195', '204', '195', '216', '133', '112', '104', '223', '191', '115', '30', '30', '30', '30', '30', '202', '204', '165', '200', '200', '29', '191', '237', '267', '18', '175', '81', '264', '264', '178', '115', '102', '178', '212', '273', '202', '109', '259', '166', '135', '155', '219', '161', '18', '94', '241', '191', '196', '191', '98', '26', '26', '204', '204', '135', '27', '238', '260', '182', '26', '98', '94', '93', '238', '156', '19', '111', '79', '16', '192', '113', '19', '241', '18', '202', '260', '272', '106', '163', '156', '106', '17', '109', '221', '78', '203', '7', '136', '197', '193', '191', '98', '112', '200', '95', '221', '98', '161', '191', '3', '106', '189', '111', '25', '196', '196', '241', '163', '200', '24', '78', '176', '3', '98', '102', '98', '19', '161', '224', '80', '18', '191', '112', '105', '254', '71', '200', '137', '29', '20', '165', '8', '106', '23', '106', '155', '200', '265', '135', '126', '203', '188', '161', '158', '258', '255', '98', '203', '50', '35', '102', '158', '175', '40', '239', '26', '66', '57', '193', '155', '15', '135', '10', '107', '106', '229', '18', '191', '204', '107', '99', '235', '238', '18', '205', '205', '35', '238', '208', '203', '110', '110', '109', '112', '23', '18', '19', '182', '156', '18', '18', '156', '111', '135', '137', '133', '137', '240', '99', '197', '113', '15', '165', '39', '97', '97', '15', '150', '163', '109', '268', '15', '15', '238', '203', '67', '161', '197', '18', '196', '97', '161', '197', '161', '185', '238', '191', '30', '121', '133', '160', '160', '74', '135', '75', '54', '135', '196', '23', '241', '166', '194', '239', '135', '240', '208', '238', '98', '135', '152', '176', '22', '174', '113', '241', '237', '57', '195', '191', '24', '165', '165', '204', '204', '56', '109', '27', '153', '252', '228', '165', '252', '200', '28', '158', '135', '196', '22', '18', '133', '30', '204', '237', '196', '93', '106', '94', '25', '93', '159', '4', '195', '23', '237', '25', '199', '57', '26', '276', '274', '204', '75', '18', '204', '108', '22', '133', '191', '111', '204', '203', '133', '237', '166', '4', '62', '202', '166', '161', '165', '74', '109', '118', '200', '22', '206', '25', '209', '17', '155', '204', '64', '23', '30', '202', '149', '171', '135', '136', '184', '26', '18', '159', '195', '18', '166', '156', '98', '133', '38', '40', '18', '18', '166', '109', '109', '209', '22', '195', '163', '163', '108', '18', '15', '99', '135', '174', '26', '19', '166', '133', '259', '151', '235', '204', '228', '133', '30', '161', '111', '242', '84', '238', '98', '195', '78', '160', '28', '25', '264', '165', '41', '19', '55', '46', '202', '121', '15', '32', '98', '238', '0', '78', '31', '30', '203', '264', '203', '135', '104', '111', '78', '212', '111', '98', '126', '22', '2', '30', '237', '212', '265', '163', '106', '106', '235', '109', '223', '161', '202', '150', '170', '255', '112', '114', '170', '98', '96', '197', '36', '18', '228', '14', '255', '204', '102', '239', '0', '185', '30', '22', '23', '246', '199', '133', '191', '78', '195', '153', '112', '191', '200', '170', '171', '200', '78', '128', '25', '153', '237', '99', '94', '208', '107', '208', '208', '78', '204', '204', '27', '191', '272', '134', '158', '158', '174', '118', '64', '18', '258', '150', '189', '18', '153', '241', '204', '112', '106', '135', '71', '148', '7', '133', '140', '187', '23', '223', '19', '19', '2', '136', '2', '200', '161', '15', '98', '158', '148', '3', '250', '153', '235', '165', '220', '176', '110', '240', '151', '148', '133', '74', '193', '46', '27', '173', '94', '176', '105', '256', '161', '226', '18', '78', '191', '171', '243', '26', '0', '108', '167', '159', '239', '150', '112', '155', '79', '241', '202', '165', '254', '107', '202', '156', '166', '193', '239', '135', '15', '203', '187', '148', '25', '15', '22', '194', '239', '56', '191', '176', '262', '200', '110', '72', '99', '102', '135', '98', '113', '15', '15', '159', '262', '171', '151', '98', '204', '213', '255', '26', '243', '175', '156', '98', '107', '191', '165', '241', '26', '165', '171', '30', '160', '166', '266', '11', '19', '211', '93', '254', '30', '239', '115', '273', '205', '112', '237', '263', '74', '98', '79', '122', '94', '265', '155', '191', '16', '235', '229', '98', '191', '227', '17', '111', '267', '135', '202', '195', '10', '180', '196', '196', '133', '135', '133', '95', '111', '122', '258', '191', '3', '114', '107', '19', '264', '74', '56', '200', '200', '133', '161', '97', '15', '97', '197', '196', '253', '93', '202', '258', '106', '153', '26', '109', '40', '149', '8', '196', '15', '119', '155', '23', '237', '193', '264', '148', '151', '238', '200', '94', '75', '148', '175', '24', '195', '171', '30', '203', '200', '202', '16', '158', '223', '191', '22', '264', '135', '35', '57', '94', '191', '196', '202', '98', '18', '196', '3', '133', '78', '191', '208', '258', '18', '200', '29', '135', '109', '110', '2', '185', '98', '133', '19', '22', '204', '95', '238', '25', '133', '250', '135', '135', '3', '47', '158', '187', '2', '78', '115', '155', '3', '218', '166', '112', '266', '22', '195', '135', '72', '97', '197', '209', '127', '80', '2', '98', '14', '239', '15', '113', '165', '204', '78', '204', '191', '175', '200', '193', '18', '196', '133', '109', '237', '56', '196', '3', '163', '163', '163', '195', '15', '263', '18', '84', '18', '133', '107', '18', '102', '156', '238', '102', '26', '244', '82', '75', '23', '258', '238', '136', '3', '204', '184', '200', '196', '158', '106', '27', '238', '155', '152', '50', '50', '2', '235', '98', '203', '166', '109', '25', '18', '151', '133', '200', '26', '26', '26', '19', '107', '156', '23', '26', '3', '70', '225', '41', '26', '239', '26', '186', '26', '34', '272', '272', '104', '78', '260', '18', '204', '26', '78', '18', '210', '135', '203', '71', '207', '26', '133', '13', '25', '26', '115', '94', '264', '135', '244', '79', '30', '263', '22', '109', '205', '176', '106', '22', '17', '151', '106', '191', '160', '160', '165', '78', '177', '196', '260', '166', '35', '191', '260', '98', '18', '237', '18', '161', '18', '197', '133', '30', '35', '200', '170', '155', '245', '19', '200', '160', '96', '172', '165', '55', '137', '241', '133', '191', '112', '25', '126', '238', '18', '18', '108', '135', '161', '135', '73', '112', '107', '8', '112', '109', '195', '25', '30', '105', '161', '241', '110', '75', '238', '215', '238', '104', '25', '3', '155', '30', '16', '3', '175', '99', '3', '200', '23', '3', '199', '203', '191', '196', '196', '204', '98', '105', '133', '197', '135', '161', '39', '15', '30', '133', '55', '191', '203', '133', '203', '140', '200', '149', '30', '96', '135', '23', '26', '3', '18', '155', '158', '135', '135', '18', '250', '18', '22', '114', '18', '18', '166', '18', '30', '18', '239', '18', '161', '50', '50', '49', '51', '23', '55', '98', '78', '210', '211', '18', '239', '78', '13', '193', '114', '78', '18', '25', '133', '19', '176', '4', '173', '50', '108', '17', '4', '204', '94', '112', '207', '109', '115', '23', '235', '197', '194', '197', '213', '148', '40', '157', '107', '200', '147', '14', '14', '18', '238', '18', '238', '18', '238', '57', '200', '135', '30', '18', '191', '191', '200', '166', '151', '161', '73', '19', '73', '239', '239', '165', '108', '2', '93', '166', '133', '133', '173', '35', '193', '114', '74', '19', '40', '58', '104', '16', '260', '160', '158', '112', '210', '158', '167', '203', '203', '3', '3', '109', '30', '133', '258', '70', '204', '200', '152', '155', '192', '204', '108', '191', '204', '26', '98', '26', '98', '264', '158', '18', '150', '137', '153', '196', '199', '191', '115', '168', '157', '149', '151', '78', '137', '160', '259', '166', '196', '171', '175', '199', '166', '161', '70', '13', '166', '152', '152', '206', '58', '18', '207', '58', '74', '207', '29', '46', '109', '135', '74', '174', '109', '40', '10', '30', '258', '16', '111', '30', '15', '239', '204', '48', '25', '196', '0', '156', '72', '72', '96', '22', '72', '133', '133', '238', '26', '3', '50', '152', '193', '235', '136', '153', '50', '1', '19', '239', '30', '25', '179', '161', '30', '191', '3', '27', '133', '3', '40', '23', '133', '150', '151', '265', '32', '191', '3', '25', '170', '238', '40', '259', '166', '137', '241', '133', '209', '159', '78', '78', '153', '26', '52', '241', '36', '250', '96', '30', '30', '38', '18', '18', '238', '99', '72', '111', '3', '244', '191', '226', '3', '191', '114', '135', '109', '40', '79', '202', '78', '114', '238', '10', '150', '108', '163', '108', '113', '197', '109', '115', '161', '3', '238', '3', '237', '161', '107', '155', '78', '202', '112', '38', '195', '137', '250', '133', '237', '161', '25', '26', '25', '15', '15', '239', '166', '114', '229', '111', '40', '135', '94', '102', '162', '204', '237', '17', '166', '136', '22', '98', '99', '99', '166', '35', '196', '3', '203', '135', '135', '84', '98', '266', '238', '13', '166', '78', '193', '98', '241', '104', '153', '192', '200', '10', '95', '95', '148', '107', '14', '17', '191', '239', '19', '203', '255', '197', '213', '94', '3', '107', '23', '35', '10', '237', '235', '78', '29', '18', '160', '200', '150', '99', '109', '155', '161', '133', '114', '18', '18', '18', '264', '133', '104', '161', '70', '17', '204', '195', '204', '241', '99', '82', '241', '241', '161', '18', '235', '175', '2', '2', '202', '50', '230', '26', '40', '40', '200', '78', '156', '26', '112', '26', '30', '71', '184', '200', '135', '153', '23', '106', '125', '175', '133', '133', '7', '38', '196', '161', '0', '49', '241', '158', '15', '21', '104', '2', '3', '176', '259', '135', '78', '75', '112', '200', '1', '207', '238', '161', '150', '158', '2', '45', '31', '237', '149', '3', '26', '165', '135', '152', '115', '133', '18', '40', '166', '30', '173', '40', '26', '78', '50', '48', '50', '13', '45', '200', '200', '18', '151', '205', '135', '38', '117', '196', '135', '155', '111', '196', '196', '166', '23', '25', '161', '18', '124', '259', '165', '58', '170', '204', '250', '103', '160', '238', '194', '163', '241', '255', '235', '17', '133', '26', '135', '196', '237', '114', '258', '193', '248', '40', '195', '84', '238', '146', '199', '40', '161', '22', '137', '43', '166', '156', '4', '104', '133', '177', '211', '193', '78', '203', '246', '237', '264', '3', '149', '78', '78', '111', '239', '13', '238', '176', '112', '150', '156', '3', '6', '118', '178', '178', '21', '186', '150', '241', '50', '243', '191', '121', '278', '29', '200', '18', '3', '197', '191', '98', '173', '195', '25', '195', '264', '221', '15', '156', '25', '200', '211', '195', '26', '35', '2', '135', '126', '113', '167', '191', '264', '196', '133', '18', '26', '146', '203', '219', '23', '160', '160', '135', '259', '3', '159', '120', '160', '106', '240', '133', '115', '258', '38', '22', '3', '26', '22', '107', '49', '48', '25', '237', '72', '219', '196', '196', '111', '135', '196', '18', '166', '172', '135', '26', '258', '70', '98', '40', '166', '23', '96', '3', '30', '155', '126', '146', '114', '242', '237', '157', '238', '241', '40', '133', '200', '161', '175', '200', '175', '84', '266', '18', '99', '30', '242', '151', '153', '166', '199', '228', '3', '237', '132', '110', '135', '80', '15', '15', '15', '84', '133', '15', '177', '3', '202', '255', '71', '115', '235', '19', '18', '18', '102', '58', '78', '165', '166', '264', '12', '150', '15', '2', '18', '135', '109', '200', '258', '128', '3', '199', '197', '239', '109', '222', '239', '107', '125', '132', '204', '204', '133', '135', '30', '158', '203', '15', '264', '237', '133', '196', '3', '252', '26', '98', '79', '199', '210', '0', '99', '30', '26', '259', '6', '49', '166', '98', '108', '9', '133', '102', '155', '98', '114', '78', '35', '166', '188', '202', '235', '30', '149', '165', '176', '138', '29', '153', '25', '25', '137', '133', '166', '135', '125', '71', '193', '154', '165', '98', '23', '56', '198', '26', '26', '0', '109', '136', '163', '151', '25', '15', '241', '195', '204', '191', '195', '161', '175', '133', '237', '78', '165', '258', '135', '78', '135', '258', '18', '235', '30', '187', '150', '275', '149', '2', '36', '112', '22', '161', '238', '203', '112', '265', '75', '133', '205', '258', '158', '18', '26', '18', '25', '26', '15', '26', '239', '15', '240', '18', '71', '71', '78', '135', '3', '148', '166', '26', '176', '2', '197', '150', '176', '19', '18', '97', '166', '18', '153', '250', '133', '210', '20', '192', '196', '26', '191', '2', '81', '70', '5', '79', '3', '30', '193', '193', '69', '160', '176', '246', '108', '54', '250', '135', '200', '80', '3', '167', '18', '166', '200', '152', '23', '149', '19', '14', '13', '237', '196', '19', '112', '254', '204', '191', '23', '264', '199', '209', '137', '31', '135', '238', '78', '18', '191', '15', '164', '195', '74', '163', '163', '41', '99', '26', '135', '26', '40', '40', '250', '200', '200', '26', '237', '4', '195', '158', '35', '71', '191', '3', '218', '274', '58', '3', '23', '177', '132', '49', '62', '238', '166', '97', '264', '193', '29', '26', '49', '241', '197', '200', '64', '227', '258', '44', '15', '133', '175', '238', '109', '254', '237', '198', '175', '166', '213', '174', '196', '107', '259', '98', '10', '196', '210', '191', '96', '166', '135', '15', '197', '78', '78', '195', '176', '18', '98', '129', '170', '18', '247', '203', '26', '158', '40', '64', '200', '163', '196', '19', '205', '197', '193', '191', '69', '96', '74', '35', '36', '64', '133', '172', '19', '266', '96', '94', '103', '258', '193', '264', '98', '254', '50', '193', '25', '170', '75', '84', '27', '252', '203', '94', '94', '19', '98', '137', '12', '153', '3', '155', '135', '238', '15', '196', '15', '26', '84', '49', '27', '135', '203', '48', '133', '129', '133', '258', '155', '35', '195', '71', '241', '135', '166', '193', '165', '104', '196', '117', '196', '26', '237', '109', '95', '196', '60', '40', '158', '193', '250', '104', '94', '178', '27', '193', '78', '106', '79', '204', '133', '237', '235', '135', '202', '70', '15', '264', '241', '254', '110', '199', '49', '255', '30', '193', '238', '204', '102', '97', '161', '43', '228', '1', '6', '196', '193', '26', '15', '128', '46', '155', '32', '133', '69', '47', '135', '78', '205', '200', '200', '202', '3', '95', '191', '78', '26', '248', '258', '191', '240', '49', '110', '38', '193', '200', '200', '96', '200', '204', '135', '9', '200', '258', '15', '46', '15', '149', '155', '197', '2', '166', '151', '72', '135', '97', '211', '238', '135', '163', '161', '26', '163', '129', '241', '239', '241', '16', '26', '18', '263', '260', '166', '3', '264', '195', '79', '132', '251', '50', '133', '15', '25', '15', '191', '15', '78', '50', '78', '71', '227', '69', '200', '69', '109', '64', '133', '197', '196', '3', '45', '238', '26', '132', '104', '196', '193', '136', '80', '26', '75', '117', '14', '78', '75', '98', '15', '2', '15', '50', '255', '136', '3', '112', '173', '69', '191', '19', '239', '235', '109', '70', '135', '78', '165', '238', '262', '199', '35', '203', '224', '193', '15', '18', '150', '238', '197', '238', '193', '135', '3', '150', '187', '199', '239', '109', '152', '27', '257', '6', '235', '104', '191', '133', '166', '155', '26', '16', '136', '198', '98', '154', '235', '27', '102', '30', '191', '15', '132', '132', '114', '107', '241', '241', '156', '199', '241', '49', '62', '117', '258', '30', '163', '133', '238', '200', '3', '15', '109', '2', '196', '165', '166', '159', '210', '35', '15', '17', '200', '241', '106', '203', '197', '197', '79', '98', '98', '23', '204', '48', '129', '57', '258', '202', '215', '238', '70', '64', '193', '189', '57', '200', '0', '98', '135', '204', '25', '30', '204', '27', '26', '78', '135', '109', '3', '197', '3', '167', '35', '196', '225', '2', '194', '200', '15', '161', '198', '23', '133', '238', '195', '35', '50', '135', '152', '163', '21', '15', '94', '26', '94', '53', '260', '196', '154', '277', '200', '118', '6', '238', '50', '209', '50', '3', '238', '3', '15', '107', '135', '199', '15', '135', '26', '258', '251', '203', '19', '259', '195', '199', '238', '15', '254', '112', '211', '200', '129', '84', '196', '3', '208', '259', '135', '104', '29', '136', '31', '78', '13', '3', '64', '75', '55', '114', '160', '96', '14', '18', '207', '75', '48', '15', '49', '97', '117', '196', '99', '258', '195', '264', '264', '78', '197', '30', '238', '109', '158', '32', '195', '149', '227', '65', '167', '162', '204', '250', '151', '151', '102', '26', '29', '26', '193', '74', '50', '193', '186', '15', '74', '259', '57', '188', '277', '277', '195', '195', '212', '78', '197', '64', '129', '259', '200', '199', '129', '4', '235', '197', '196', '161', '3', '255', '199', '102', '3', '13', '94', '53', '98', '15', '129', '15', '155', '71', '179', '50', '29', '136', '80', '254', '198', '162', '237', '3', '18', '260', '238', '117', '179', '156', '19', '45', '197', '78', '199', '102', '102', '237', '155', '69', '69', '163', '239', '15', '196', '80', '191', '30', '71', '158', '259', '2', '106', '148', '15', '135', '135', '135', '196', '19', '123', '15', '104', '19', '128', '230', '69', '98', '197', '241', '161', '195', '200', '3', '98', '78', '165', '163', '19', '196', '31', '173', '203', '55', '69', '209', '239', '79', '158', '254', '135', '51', '94', '162', '255', '95', '18', '133', '109', '50', '94', '199', '15', '197', '200', '133', '116', '50', '196', '161', '69', '133', '3', '258', '3', '197', '29', '3', '157', '197', '193', '4', '94', '195', '17', '48', '152', '8', '15', '155', '133', '195', '136', '150', '102', '237', '203', '164', '164', '250', '154', '13', '191', '58', '94', '70', '93', '264', '15', '30', '133', '237', '267', '199', '193', '76', '241', '157', '26', '200', '109', '179', '55', '98', '200', '129', '238', '235', '208', '109', '239', '258', '196', '193', '18', '107', '210', '70', '40', '129', '259', '193', '30', '158', '252', '40', '210', '78', '15', '78', '96', '208', '113', '258', '64', '80', '15', '133', '241', '15', '15', '258', '135', '109', '251', '251', '109', '74', '196', '170', '240', '254', '258', '204', '199', '26', '109', '197', '57', '45', '161', '57', '114', '3', '15', '264', '106', '57', '27', '18', '258', '29', '84', '161', '135', '114', '24', '30', '107', '107', '172', '209', '170', '18', '19', '133', '191', '195', '135', '156', '196', '94', '3', '84', '225', '199', '238', '106', '104', '70', '50', '12', '155', '114', '176', '237', '200', '108', '70', '240', '196', '19', '22', '276', '199', '238', '205', '104', '193', '108', '115', '259', '102', '19', '258', '18', '106', '161', '191', '114', '64', '15', '204', '35', '155', '40', '238', '136', '264', '133', '186', '15', '258', '166', '26', '6', '114', '114', '133', '114', '114', '13', '13', '13', '237', '250', '245', '136', '78', '13', '48', '18', '13', '25', '26', '93', '125', '50', '94', '148', '78', '194', '200', '25', '93', '274', '266', '70', '22', '3', '208', '240', '211', '135', '31', '18', '153', '18', '2', '15', '15', '15', '156', '15', '70', '73', '173', '13', '112', '69', '204', '70', '248', '135', '70', '3', '193', '193', '22', '162', '238', '57', '98', '15', '199', '199', '52', '151', '73', '5', '108', '23', '69', '95', '220', '106', '163', '152', '163', '108', '15', '58', '250', '19', '177', '245', '264', '93', '204', '25', '78', '155', '69', '155', '239', '241', '155', '200', '18', '3', '13', '196', '197', '204', '272', '95', '135', '165', '22', '210', '35', '69', '18', '170', '3', '25', '156', '78', '135', '102', '107', '255', '19', '70', '70', '109', '198', '156', '64', '170', '138', '78', '203', '166', '108', '114', '179', '228', '74', '49', '14', '264', '26', '195', '78', '107', '75', '18', '106', '151', '70', '58', '15', '196', '238', '109', '104', '104', '109', '136', '148', '210', '3', '161', '15', '104', '238', '208', '133', '108', '241', '18', '209', '74', '156', '114', '199', '174', '35', '18', '170', '135', '238', '244', '155', '162', '197', '3', '98', '227', '267', '238', '193', '50', '238', '98', '112', '124', '25', '114', '241', '159', '154', '78', '161', '136', '114', '153', '225', '154', '13', '50', '11', '25', '15', '209', '11', '166', '203', '259', '237', '22', '114', '276', '98', '278', '150', '198', '18', '15', '78', '15', '162', '26', '3', '196', '3', '102', '189', '71', '69', '16', '18', '152', '133', '135', '162', '241', '15', '104', '15', '259', '202', '2', '238', '207', '15', '204', '30', '215', '112', '210', '42', '96', '193', '15', '110', '163', '211', '78', '108', '15', '31', '27', '15', '31', '196', '195', '96', '15', '161', '94', '193', '195', '198', '98', '261', '258', '76', '15', '104', '258', '135', '18', '15', '202', '258', '108', '156', '13', '22', '251', '199', '155', '258', '211', '3', '207', '202', '259', '107', '202', '259', '135', '135', '148', '158', '154', '99', '99', '3', '207', '94', '25', '162', '31', '241', '25', '15', '114', '18', '114', '148', '19', '15', '203', '191', '19', '13', '193', '204', '25', '166', '155', '196', '98', '162', '197', '29', '152', '191', '133', '199', '230', '40', '25', '204', '79', '3', '19', '165', '204', '108', '208', '3', '251', '274', '15', '166', '264', '35', '13', '193', '114', '258', '13', '19', '133', '23', '17', '199', '204', '153', '250', '204', '16', '12', '107', '191', '18', '3', '15', '165', '15', '69', '17', '165', '17', '15', '15', '156', '23', '23', '15', '197', '104', '197', '26', '208', '135', '211', '250', '18', '204', '25', '196', '53', '69', '104', '104', '19', '78', '241', '230', '30', '195', '96', '26', '109', '196', '69', '238', '15', '18', '133', '26', '106', '109', '98', '15', '258', '274', '193', '203', '58', '57', '274', '2', '202', '161', '136', '135', '155', '241', '35', '151', '137', '5', '114', '96', '16', '209', '158', '15', '266', '161', '15', '25', '274', '202', '3', '161', '237', '267', '204', '125', '15', '151', '155', '259', '151', '210', '238', '15', '26', '132', '50', '173', '197', '114', '241', '15', '258', '109', '166', '274', '13', '191', '15', '26', '164', '258', '163', '15', '264', '11', '260', '148', '150', '207', '196', '98', '272', '133', '196', '107', '26', '191', '163', '191', '135', '3', '18', '18', '119', '69', '196', '15', '156', '135', '112', '258', '155', '256', '199', '204', '212', '18', '135', '254', '72', '258', '254', '15', '133', '18', '259', '193', '258', '204', '40', '18', '29', '155', '104', '107', '274', '241', '25', '26', '69', '148', '166', '46', '94', '135', '15', '155', '104', '241', '32', '274', '249', '15', '197', '104', '44', '259', '69', '191', '15', '198', '193', '133', '17', '258', '248', '166', '197', '200', '250', '204', '3', '225', '238', '163', '69', '15', '15', '26', '202', '19', '209', '78', '212', '15', '78', '243', '110', '191', '15', '18', '107', '38', '208', '166', '239', '237', '199', '15', '204', '258', '173', '172', '200', '274', '243', '19', '191', '135', '203', '69', '149', '148', '196', '256', '204', '97', '191', '26', '114', '274', '200', '69', '69', '64', '69', '239', '25', '274', '203', '78', '135', '196', '202', '17', '2', '2', '3', '98', '98', '62', '2', '95', '96', '2', '207', '78', '200', '26', '64', '18', '93', '49', '57', '154', '200', '69', '94', '98', '98', '56', '56', '166', '98', '212', '226', '226', '15', '69', '15', '273', '272', '104', '193', '3', '98', '199', '98', '25', '125', '259', '249', '108', '40', '26', '198', '218', '73', '148', '69', '204', '241', '35', '3', '274', '70', '18', '71', '35', '172', '133', '106', '69', '107', '199', '219', '19', '133', '69', '135', '109', '57', '191', '135', '252', '104', '3', '94', '3', '72', '126', '16', '36', '84', '241', '3', '218', '98', '108', '46', '19', '196', '118', '104', '29', '238', '69', '10', '204', '264', '259', '124', '242', '107', '133', '25', '64', '30', '194', '259', '159', '264', '274', '52', '200', '104', '29', '30', '193', '98', '94', '114', '204', '40', '18', '198', '97', '3', '99', '274', '239', '177', '15', '18', '126', '126', '208', '148', '15', '104', '29', '36', '218', '70', '15', '23', '278', '23', '36', '191', '210', '51', '245', '167', '72', '237', '25', '203', '36', '196', '193', '196', '15', '151', '207', '203', '103', '166', '135', '110', '27', '13', '27', '191', '98', '57', '166', '50', '21', '15', '197', '23', '2', '70', '69', '193', '274', '104', '166', '258', '126', '238', '161', '18', '193', '69', '140', '170', '197', '204', '165', '203', '170', '15', '58', '35', '83', '212', '150', '73', '25', '15', '64', '133', '210', '108', '79', '193', '15', '126', '165', '15', '208', '98', '3', '200', '126', '3', '40', '165', '98', '69', '15', '165', '133', '96', '173', '203', '133', '238', '248', '15', '7', '38', '207', '108', '165', '199', '18', '162', '27', '3', '72', '65', '250', '209', '166', '64', '209', '237', '212', '64', '225', '120', '108', '203', '135', '248', '69', '251', '135', '204', '160', '160', '135', '148', '15', '15', '23', '207', '274', '29', '28', '194', '150', '40', '104', '208', '125', '70', '18', '3', '19', '102', '104', '23', '29', '207', '133', '204', '111', '2', '125', '126', '7', '194', '209', '203', '83', '126', '126', '258', '119', '162', '241', '133', '18', '208', '197', '209', '15', '78', '80', '110', '237', '104', '30', '208', '103', '15', '200', '15', '238', '254', '108', '173', '40', '69', '175', '17', '155', '136', '26', '3', '135', '13', '241', '161', '166', '170', '162', '133', '53', '211', '204', '15', '161', '238', '110', '18', '191', '18', '50', '71', '30', '17', '208', '248', '203', '203', '196', '210', '258', '204', '3', '18', '196', '125', '173', '199', '249', '18', '104', '165', '241', '2', '193', '238', '78', '15', '166', '15', '126', '3', '237', '161', '52', '207', '3', '3', '50', '56', '22', '56', '69', '93', '135', '207', '69', '99', '3', '133', '68', '133', '161', '108', '126', '250', '60', '155', '98', '18', '16', '78', '78', '240', '235', '176', '22', '199', '160', '235', '274', '209', '64', '173', '137', '128', '25', '18', '160', '203', '70', '165', '193', '110', '207', '64', '85', '252', '78', '51', '2', '99', '19', '22', '198', '204', '166', '166', '151', '56', '133', '210', '26', '149', '109', '30', '209', '209', '51', '3', '193', '119', '208', '93', '165', '248', '55', '203', '204', '58', '18', '199', '211', '83', '32', '135', '212', '156', '19', '235', '173', '191', '209', '203', '146', '208', '58', '158', '102', '199', '200', '200', '173', '70', '52', '107', '211', '213', '237', '196', '205', '104', '135', '258', '255', '96', '19', '124', '128', '57', '199', '98', '166', '102', '40', '15', '238', '3', '148', '225', '126', '135', '114', '193', '204', '15', '209', '209', '193', '259', '64', '161', '26', '274', '126', '18', '196', '0', '60', '207', '17', '71', '218', '239', '248', '38', '154', '61', '128', '199', '196', '173', '25', '46', '98', '258', '108', '3', '108', '196', '93', '93', '99', '172', '69', '241', '196', '30', '135', '278', '109', '18', '5', '209', '57', '159', '70', '193', '163', '249', '196', '196', '72', '103', '133', '17', '204', '199', '166', '202', '29', '57', '193', '150', '98', '119', '25', '205', '18', '173', '166', '209', '57', '25', '58', '258', '26', '15', '15', '101', '26', '74', '1', '4', '74', '263', '']\n",
            "['209', '63', '216', '108', '196', '200', '203', '246', '188', '193', '25', '106', '56', '53', '223', '161', '200', '208', '78', '147', '147', '179', '147', '80', '56', '209', '209', '200', '214', '202', '203', '194', '26', '237', '165', '258', '238', '238', '76', '193', '196', '213', '161', '213', '209', '194', '196', '25', '151', '260', '135', '173', '30', '108', '272', '259', '176', '213', '213', '21', '5', '171', '200', '195', '240', '194', '176', '240', '240', '80', '173', '98', '214', '21', '76', '76', '73', '173', '173', '78', '61', '258', '191', '3', '78', '173', '193', '118', '200', '264', '260', '49', '211', '258', '18', '173', '78', '3', '3', '1', '213', '135', '182', '239', '208', '278', '50', '194', '200', '124', '189', '117', '264', '244', '76', '193', '221', '78', '78', '173', '197', '78', '161', '231', '135', '22', '78', '25', '25', '197', '200', '147', '203', '191', '196', '193', '263', '32', '197', '191', '4', '56', '196', '104', '191', '196', '173', '147', '194', '96', '161', '161', '238', '200', '3', '78', '78', '53', '41', '237', '53', '18', '10', '194', '97', '96', '208', '25', '10', '117', '104', '54', '96', '194', '21', '245', '238', '202', '209', '161', '97', '258', '240', '213', '136', '155', '152', '21', '191', '191', '118', '68', '258', '189', '197', '208', '61', '78', '135', '202', '96', '155', '108', '272', '80', '25', '155', '35', '213', '191', '194', '96', '96', '191', '264', '191', '191', '191', '2', '258', '191', '173', '259', '15', '47', '258', '156', '118', '160', '115', '105', '26', '173', '192', '76', '76', '135', '240', '118', '161', '200', '230', '18', '200', '25', '10', '161', '244', '24', '24', '200', '205', '104', '15', '193', '156', '95', '209', '240', '191', '194', '128', '208', '208', '218', '173', '10', '25', '18', '107', '46', '32', '197', '172', '209', '213', '79', '121', '78', '78', '193', '2', '78', '194', '149', '136', '80', '3', '21', '200', '78', '21', '197', '194', '118', '202', '161', '180', '209', '173', '2', '193', '6', '260', '197', '78', '78', '42', '191', '213', '135', '193', '196', '202', '222', '217', '42', '259', '213', '76', '46', '30', '15', '260', '202', '18', '259', '104', '135', '76', '196', '240', '240', '240', '108', '24', '200', '197', '262', '176', '15', '156', '264', '156', '200', '191', '161', '196', '78', '238', '161', '221', '118', '14', '14', '14', '200', '278', '238', '200', '196', '78', '173', '25', '197', '197', '196', '161', '18', '10', '10', '10', '18', '66', '203', '19', '246', '196', '244', '240', '161', '200', '78', '240', '209', '208', '118', '215', '194', '193', '168', '229', '191', '135', '172', '196', '197', '161', '78', '196', '197', '258', '78', '78', '238', '221', '121', '264', '104', '146', '50', '78', '217', '11', '238', '238', '238', '3', '3', '154', '215', '79', '18', '158', '104', '18', '13', '197', '117', '197', '161', '213', '242', '32', '238', '230', '2', '2', '32', '161', '251', '96', '156', '242', '22', '22', '140', '3', '149', '135', '191', '173', '135', '200', '238', '238', '78', '156', '209', '78', '97', '97', '160', '25', '258', '240', '161', '15', '161', '96', '209', '80', '132', '140', '4', '1', '240', '196', '274', '272', '62', '80', '196', '196', '191', '79', '156', '15', '196', '209', '22', '251', '202', '202', '121', '73', '191', '238', '191', '150', '150', '135', '46', '203', '202', '238', '22', '3', '258', '239', '238', '238', '165', '238', '238', '194', '200', '200', '197', '200', '196', '191', '194', '76', '78', '208', '200', '71', '208', '240', '76', '117', '120', '108', '96', '144', '30', '133', '133', '202', '117', '161', '251', '29', '104', '242', '197', '118', '216', '262', '236', '141', '11', '235', '235', '133', '191', '238', '202', '80', '80', '22', '140', '191', '197', '202', '133', '184', '184', '184', '184', '143', '274', '50', '237', '196', '274', '265', '10', '18', '10', '121', '238', '245', '228', '76', '25', '191', '197', '268', '10', '240', '195', '196', '195', '15', '48', '200', '183', '70', '59', '197', '196', '235', '161', '161', '215', '104', '97', '191', '194', '191', '200', '200', '197', '76', '200', '93', '256', '160', '238', '146', '238', '239', '15', '140', '258', '140', '21', '79', '202', '258', '79', '62', '205', '161', '104', '10', '161', '196', '137', '152', '191', '117', '213', '242', '176', '218', '195', '191', '162', '191', '135', '241', '213', '213', '84', '79', '68', '18', '98', '197', '155', '41', '246', '269', '78', '209', '238', '80', '238', '62', '237', '273', '273', '230', '150', '121', '260', '244', '161', '202', '78', '109', '155', '264', '145', '50', '274', '173', '210', '258', '238', '211', '156', '215', '195', '177', '244', '3', '141', '196', '191', '194', '29', '30', '10', '16', '200', '200', '156', '53', '3', '62', '165', '173', '173', '22', '205', '274', '150', '183', '116', '188', '188', '98', '161', '204', '80', '172', '156', '104', '229', '259', '78', '169', '3', '200', '118', '161', '80', '260', '240', '150', '197', '196', '62', '93', '6', '272', '272', '197', '160', '78', '96', '70', '195', '191', '191', '80', '3', '3', '240', '147', '196', '191', '238', '96', '203', '3', '15', '10', '58', '55', '96', '41', '15', '26', '50', '238', '194', '79', '240', '161', '191', '195', '263', '50', '240', '147', '235', '195', '203', '59', '178', '176', '2', '15', '203', '191', '18', '278', '196', '14', '153', '58', '175', '18', '217', '217', '80', '191', '200', '18', '200', '173', '24', '171', '171', '210', '124', '34', '200', '218', '104', '4', '223', '193', '194', '200', '200', '13', '62', '194', '197', '80', '197', '146', '146', '96', '97', '97', '2', '196', '191', '191', '104', '238', '10', '200', '239', '135', '104', '213', '2', '221', '169', '25', '21', '194', '161', '21', '278', '3', '195', '46', '176', '25', '117', '191', '62', '62', '62', '78', '50', '264', '79', '197', '80', '80', '80', '191', '191', '78', '161', '194', '195', '197', '203', '259', '278', '49', '147', '147', '147', '147', '22', '78', '78', '258', '2', '202', '221', '55', '258', '104', '104', '84', '240', '221', '274', '104', '22', '241', '63', '197', '197', '196', '159', '200', '22', '22', '22', '78', '78', '173', '104', '97', '104', '191', '146', '223', '268', '96', '14', '173', '204', '241', '97', '15', '118', '78', '78', '196', '194', '258', '96', '96', '259', '75', '10', '197', '30', '2', '238', '121', '50', '161', '173', '173', '186', '185', '186', '78', '194', '221', '218', '35', '33', '13', '34', '78', '21', '215', '97', '60', '24', '78', '193', '173', '235', '78', '200', '258', '140', '259', '194', '197', '195', '200', '145', '76', '108', '130', '238', '215', '213', '2', '217', '121', '178', '140', '62', '156', '176', '196', '196', '41', '223', '147', '172', '79', '200', '2', '244', '244', '191', '213', '104', '191', '237', '237', '33', '117', '195', '195', '264', '195', '140', '210', '41', '197', '14', '257', '196', '238', '238', '79', '161', '156', '196', '238', '238', '213', '104', '3', '10', '118', '204', '235', '33', '259', '274', '259', '176', '78', '78', '196', '34', '52', '25', '30', '194', '153', '21', '70', '191', '18', '191', '191', '172', '194', '194', '194', '194', '148', '104', '125', '2', '2', '2', '185', '194', '29', '104', '78', '189', '272', '204', '183', '213', '78', '104', '197', '240', '97', '168', '168', '182', '238', '197', '18', '194', '229', '221', '62', '78', '53', '104', '214', '189', '189', '189', '213', '117', '117', '191', '29', '104', '104', '276', '276', '22', '195', '195', '155', '139', '139', '59', '168', '135', '140', '79', '146', '200', '245', '70', '160', '153', '50', '202', '154', '263', '130', '194', '117', '194', '194', '191', '33', '29', '29', '238', '239', '194', '21', '274', '215', '18', '18', '140', '2', '215', '25', '46', '137', '238', '30', '160', '156', '46', '196', '263', '242', '104', '0', '0', '73', '213', '135', '22', '191', '191', '241', '202', '221', '15', '15', '191', '238', '213', '194', '197', '179', '238', '200', '200', '200', '5', '238', '194', '55', '117', '166', '49', '244', '239', '191', '78', '78', '78', '194', '200', '117', '210', '22', '121', '121', '121', '21', '237', '194', '121', '78', '104', '194', '79', '191', '251', '195', '197', '274', '176', '21', '43', '43', '10', '61', '215', '215', '30', '214', '194', '10', '30', '139', '96', '226', '226', '265', '2', '2', '121', '121', '41', '237', '104', '238', '238', '161', '78', '78', '78', '11', '184', '140', '140', '161', '104', '104', '237', '130', '195', '160', '118', '161', '18', '11', '241', '184', '196', '178', '22', '30', '146', '146', '10', '10', '141', '141', '135', '18', '18', '10', '26', '208', '97', '141', '18', '18', '97', '194', '194', '79', '213', '205', '205', '205', '75', '241', '50', '226', '238', '97', '222', '215', '215', '139', '238', '51', '238', '96', '173', '109', '200', '117', '192', '2', '195', '10', '3', '18', '49', '238', '238', '191', '195', '52', '197', '200', '200', '79', '191', '200', '200', '30', '195', '197', '195', '25', '260', '73', '30', '193', '21', '200', '242', '203', '120', '78', '260', '241', '0', '30', '202', '195', '30', '191', '78', '194', '194', '2', '205', '191', '196', '173', '197', '30', '196', '172', '139', '2', '18', '191', '230', '56', '194', '215', '195', '2', '71', '13', '146', '264', '50', '191', '235', '93', '191', '139', '139', '227', '235', '186', '235', '24', '18', '261', '128', '33', '71', '25', '176', '39', '10', '195', '202', '29', '29', '221', '29', '197', '139', '78', '227', '146', '137', '195', '205', '205', '195', '135', '62', '153', '146', '29', '75', '0', '194', '194', '180', '213', '78', '18', '194', '239', '196', '239', '197', '194', '153', '192', '221', '10', '46', '50', '241', '117', '117', '117', '195', '173', '78', '195', '194', '239', '2', '12', '173', '197', '30', '269', '197', '176', '135', '30', '146', '191', '93', '93', '135', '260', '218', '156', '155', '200', '241', '125', '18', '18', '183', '203', '239', '259', '173', '213', '180', '180', '210', '120', '104', '194', '156', '258', '172', '9', '0', '241', '192', '143', '189', '204', '50', '63', '197', '243', '241', '93', '125', '2', '21', '260', '4', '4', '3', '191', '150', '128', '78', '238', '245', '8', '80', '238', '79', '79', '240', '58', '104', '155', '80', '80', '188', '191', '235', '170', '170', '170', '146', '258', '195', '276', '25', '78', '215', '240', '161', '53', '147', '78', '78', '78', '78', '221', '221', '96', '18', '19', '130', '18', '18', '18', '80', '80', '194', '79', '93', '121', '78', '139', '139', '78', '197', '121', '194', '200', '80', '101', '22', '12', '135', '97', '205', '181', '60', '193', '15', '140', '172', '191', '241', '222', '135', '29', '25', '179', '191', '191', '259', '194', '133', '240', '121', '121', '241', '195', '29', '191', '209', '18', '139', '139', '50', '18', '12', '18', '133', '200', '25', '10', '70', '25', '242', '18', '161', '49', '172', '172', '172', '172', '128', '96', '195', '200', '78', '10', '239', '22', '173', '142', '142', '22', '194', '192', '3', '200', '241', '96', '241', '62', '139', '213', '202', '135', '79', '120', '192', '78', '118', '241', '46', '151', '150', '151', '150', '118', '221', '183', '237', '196', '140', '140', '39', '179', '213', '140', '41', '73', '78', '217', '258', '30', '118', '140', '219', '50', '10', '139', '150', '251', '24', '10', '18', '78', '18', '172', '135', '146', '71', '196', '202', '197', '25', '106', '160', '215', '15', '6', '101', '165', '140', '145', '80', '3', '238', '239', '78', '268', '0', '109', '191', '215', '238', '238', '238', '203', '50', '146', '15', '25', '156', '191', '16', '2', '195', '189', '96', '193', '109', '204', '161', '185', '210', '50', '235', '30', '242', '193', '102', '194', '121', '177', '241', '102', '132', '240', '71', '192', '79', '25', '197', '39', '161', '22', '258', '237', '15', '7', '78', '156', '197', '70', '176', '139', '240', '132', '78', '177', '191', '194', '117', '10', '79', '238', '195', '24', '101', '118', '196', '2', '102', '21', '180', '185', '170', '79', '139', '135', '191', '151', '200', '191', '154', '262', '200', '237', '56', '27', '191', '180', '258', '139', '139', '192', '30', '0', '242', '239', '242', '25', '191', '242', '240', '3', '56', '215', '171', '192', '193', '222', '238', '237', '215', '272', '2', '156', '237', '151', '189', '80', '15', '260', '260', '217', '235', '22', '228', '135', '18', '191', '157', '78', '2', '240', '213', '179', '2', '191', '79', '79', '153', '3', '194', '78', '121', '71', '121', '149', '257', '194', '191', '188', '15', '25', '125', '102', '15', '130', '130', '177', '10', '135', '265', '240', '149', '196', '200', '204', '204', '213', '177', '197', '194', '13', '202', '30', '213', '125', '193', '125', '125', '125', '125', '125', '15', '180', '78', '1', '25', '96', '200', '29', '156', '172', '202', '22', '78', '197', '188', '241', '101', '101', '101', '221', '50', '191', '15', '15', '0', '101', '101', '168', '56', '197', '130', '62', '204', '29', '10', '96', '97', '139', '97', '97', '2', '2', '262', '39', '176', '172', '194', '194', '18', '78', '13', '80', '139', '196', '207', '191', '151', '192', '150', '13', '104', '104', '101', '15', '125', '101', '15', '0', '50', '194', '264', '50', '25', '25', '125', '125', '125', '125', '125', '25', '29', '25', '25', '78', '240', '261', '19', '2', '74', '78', '238', '50', '50', '203', '197', '71', '104', '80', '194', '149', '170', '130', '2', '237', '170', '202', '48', '97', '93', '179', '235', '78', '102', '102', '70', '208', '238', '104', '146', '118', '118', '96', '237', '274', '218', '241', '102', '146', '209', '209', '240', '170', '137', '7', '197', '191', '264', '29', '46', '96', '193', '19', '12', '213', '215', '275', '179', '18', '183', '78', '221', '18', '170', '119', '21', '140', '240', '96', '192', '192', '21', '96', '149', '239', '135', '191', '240', '184', '183', '24', '2', '96', '197', '194', '215', '18', '192', '121', '202', '196', '25', '97', '214', '3', '97', '16', '0', '192', '135', '215', '5', '12', '80', '143', '202', '242', '151', '13', '209', '203', '200', '30', '209', '194', '102', '102', '102', '102', '102', '102', '22', '24', '6', '18', '5', '191', '101', '50', '101', '101', '192', '153', '153', '101', '101', '10', '192', '25', '140', '50', '241', '102', '102', '32', '150', '101', '102', '153', '102', '101', '153', '102', '25', '10', '125', '18', '125', '177', '209', '102', '10', '78', '78', '137', '18', '237', '241', '175', '195', '175', '96', '176', '237', '96', '96', '101', '193', '238', '170', '238', '102', '222', '268', '238', '102', '244', '34', '18', '244', '202', '149', '149', '193', '102', '170', '194', '75', '101', '222', '238', '50', '195', '135', '22', '93', '18', '102', '102', '93', '93', '191', '147', '244', '96', '2', '241', '241', '93', '189', '189', '177', '35', '137', '197', '191', '40', '40', '195', '176', '35', '200', '200', '237', '135', '140', '140', '93', '156', '200', '78', '191', '22', '81', '238', '101', '213', '101', '78', '109', '19', '140', '196', '78', '78', '76', '218', '14', '265', '101', '241', '155', '4', '50', '78', '78', '238', '135', '191', '265', '81', '238', '238', '102', '102', '102', '140', '162', '32', '17', '17', '171', '171', '191', '101', '102', '192', '101', '17', '171', '135', '102', '242', '195', '237', '21', '18', '238', '191', '278', '140', '149', '97', '260', '155', '276', '192', '196', '21', '191', '2', '97', '0', '150', '221', '213', '209', '238', '80', '260', '194', '104', '96', '193', '14', '240', '193', '117', '117', '155', '17', '264', '260', '209', '209', '2', '137', '71', '72', '101', '205', '173', '191', '118', '24', '41', '197', '78', '94', '237', '156', '193', '153', '154', '153', '209', '110', '78', '78', '213', '238', '78', '191', '93', '79', '177', '267', '133', '133', '46', '139', '40', '143', '74', '241', '202', '50', '3', '102', '78', '149', '191', '156', '18', '93', '195', '192', '137', '240', '135', '204', '189', '22', '56', '154', '25', '192', '130', '277', '191', '80', '39', '21', '21', '238', '238', '23', '192', '191', '192', '94', '94', '80', '139', '17', '10', '22', '227', '227', '140', '102', '21', '17', '227', '227', '23', '243', '264', '18', '274', '149', '238', '30', '221', '22', '238', '193', '135', '205', '195', '238', '238', '197', '130', '195', '18', '104', '102', '14', '59', '104', '18', '18', '258', '34', '241', '138', '188', '176', '102', '1', '140', '140', '22', '22', '278', '10', '96', '35', '145', '120', '33', '33', '22', '33', '22', '35', '22', '19', '33', '102', '192', '192', '175', '22', '72', '191', '222', '125', '22', '79', '18', '194', '18', '250', '192', '192', '96', '48', '274', '273', '132', '133', '192', '40', '171', '171', '246', '191', '10', '166', '197', '195', '125', '125', '200', '153', '192', '149', '110', '139', '194', '48', '102', '102', '104', '203', '238', '238', '238', '78', '18', '238', '121', '18', '238', '96', '238', '238', '13', '191', '18', '18', '240', '96', '40', '188', '18', '137', '175', '28', '28', '28', '149', '191', '191', '207', '239', '239', '153', '153', '153', '153', '153', '153', '154', '153', '154', '153', '153', '153', '18', '0', '27', '18', '25', '96', '96', '137', '33', '10', '130', '153', '153', '154', '180', '137', '44', '241', '153', '268', '25', '161', '25', '39', '242', '4', '238', '195', '102', '135', '13', '229', '140', '1', '79', '192', '229', '191', '118', '21', '260', '203', '18', '177', '191', '10', '238', '10', '133', '14', '175', '175', '192', '18', '153', '153', '254', '140', '149', '22', '22', '22', '237', '40', '135', '191', '192', '192', '62', '205', '19', '238', '176', '214', '200', '96', '15', '153', '191', '108', '29', '166', '28', '193', '197', '96', '195', '108', '102', '241', '195', '10', '33', '166', '197', '168', '2', '96', '26', '33', '204', '258', '149', '153', '153', '40', '137', '241', '166', '221', '23', '221', '153', '15', '113', '113', '78', '171', '40', '71', '21', '71', '80', '15', '191', '191', '135', '110', '10', '259', '102', '149', '149', '109', '109', '175', '97', '97', '97', '153', '15', '238', '193', '175', '38', '38', '137', '18', '235', '155', '137', '265', '110', '18', '18', '256', '191', '224', '173', '104', '10', '152', '24', '24', '24', '17', '40', '40', '40', '40', '40', '40', '112', '6', '18', '18', '210', '96', '191', '27', '213', '17', '153', '26', '133', '18', '18', '153', '38', '96', '40', '38', '40', '40', '10', '17', '17', '95', '71', '192', '17', '259', '213', '200', '15', '11', '207', '194', '18', '135', '238', '191', '241', '191', '116', '35', '147', '156', '67', '78', '17', '227', '191', '117', '188', '130', '72', '96', '235', '97', '18', '150', '203', '25', '126', '149', '200', '94', '4', '4', '1', '244', '228', '252', '149', '191', '149', '15', '17', '17', '29', '18', '18', '116', '23', '237', '94', '184', '268', '197', '175', '243', '28', '16', '10', '50', '191', '102', '102', '153', '149', '111', '78', '191', '135', '18', '35', '78', '238', '208', '93', '95', '258', '17', '111', '111', '262', '22', '22', '104', '161', '209', '21', '195', '12', '116', '241', '242', '239', '17', '97', '16', '40', '117', '94', '96', '78', '15', '103', '195', '135', '235', '247', '135', '130', '260', '239', '176', '93', '191', '209', '239', '93', '264', '30', '18', '30', '18', '18', '103', '191', '97', '17', '95', '115', '209', '140', '22', '22', '241', '10', '153', '3', '188', '238', '226', '195', '97', '96', '238', '97', '130', '97', '116', '96', '96', '242', '45', '149', '166', '155', '96', '116', '264', '10', '237', '192', '71', '22', '71', '175', '149', '116', '237', '274', '18', '235', '176', '116', '202', '250', '250', '78', '2', '196', '2', '97', '143', '18', '111', '111', '80', '78', '153', '80', '40', '110', '30', '112', '30', '111', '215', '112', '149', '78', '78', '209', '110', '110', '238', '109', '239', '112', '151', '154', '16', '16', '16', '78', '137', '179', '72', '149', '10', '111', '268', '96', '170', '111', '38', '192', '153', '183', '195', '30', '30', '147', '195', '147', '93', '264', '153', '25', '18', '19', '22', '74', '40', '93', '166', '15', '127', '147', '192', '260', '238', '103', '269', '127', '19', '19', '153', '18', '78', '238', '30', '30', '241', '16', '235', '250', '204', '238', '218', '40', '93', '184', '18', '0', '241', '195', '94', '154', '109', '156', '10', '21', '30', '197', '39', '78', '24', '107', '142', '142', '228', '229', '229', '229', '232', '179', '239', '142', '232', '23', '175', '147', '137', '137', '137', '171', '191', '117', '128', '127', '204', '98', '147', '235', '239', '153', '15', '204', '255', '80', '204', '204', '149', '113', '19', '167', '22', '250', '17', '17', '17', '18', '17', '67', '121', '125', '26', '97', '97', '97', '97', '247', '124', '163', '147', '112', '111', '208', '108', '38', '209', '154', '154', '194', '135', '22', '147', '241', '120', '218', '260', '156', '155', '241', '192', '215', '180', '179', '149', '40', '193', '192', '150', '102', '93', '1', '149', '137', '133', '102', '102', '102', '78', '145', '21', '96', '175', '175', '78', '123', '30', '95', '19', '18', '110', '18', '152', '152', '147', '26', '135', '260', '158', '203', '195', '158', '112', '156', '125', '213', '230', '30', '15', '125', '106', '111', '195', '181', '130', '195', '130', '239', '15', '194', '16', '15', '111', '61', '72', '57', '128', '239', '239', '203', '25', '218', '10', '155', '191', '46', '98', '175', '260', '102', '21', '191', '260', '191', '239', '15', '30', '93', '147', '103', '78', '177', '30', '95', '155', '155', '155', '103', '103', '112', '241', '191', '191', '17', '78', '153', '150', '113', '115', '147', '78', '40', '163', '33', '15', '149', '103', '192', '191', '96', '106', '115', '143', '235', '200', '204', '238', '204', '204', '74', '196', '113', '246', '102', '150', '16', '176', '20', '195', '128', '197', '15', '71', '71', '193', '24', '24', '24', '170', '204', '209', '111', '144', '112', '22', '222', '224', '28', '209', '155', '213', '275', '196', '239', '195', '25', '103', '149', '128', '19', '96', '102', '135', '17', '232', '155', '191', '20', '18', '241', '30', '25', '205', '97', '97', '96', '72', '182', '135', '135', '20', '149', '149', '78', '135', '40', '18', '153', '10', '26', '158', '205', '157', '157', '135', '135', '140', '155', '192', '26', '268', '263', '245', '155', '15', '177', '159', '212', '161', '239', '200', '40', '35', '153', '197', '259', '15', '254', '175', '10', '30', '239', '67', '195', '200', '191', '22', '66', '191', '112', '133', '128', '235', '161', '155', '17', '93', '36', '113', '239', '18', '18', '191', '165', '163', '163', '238', '79', '158', '40', '102', '40', '200', '191', '60', '241', '245', '96', '78', '30', '15', '173', '192', '192', '158', '64', '113', '200', '74', '128', '135', '115', '115', '10', '74', '102', '102', '115', '111', '238', '244', '109', '109', '238', '152', '10', '237', '18', '50', '16', '173', '204', '15', '208', '276', '43', '109', '258', '222', '15', '94', '235', '203', '79', '128', '113', '3', '204', '106', '133', '214', '218', '25', '98', '166', '71', '30', '175', '175', '97', '135', '108', '19', '18', '196', '35', '12', '196', '78', '166', '155', '72', '155', '22', '109', '258', '196', '263', '161', '204', '210', '14', '208', '208', '208', '208', '204', '102', '102', '213', '155', '251', '153', '153', '153', '154', '154', '237', '97', '19', '28', '94', '136', '112', '19', '97', '152', '153', '153', '111', '111', '135', '135', '105', '17', '105', '152', '152', '152', '153', '79', '266', '135', '120', '125', '149', '50', '162', '72', '17', '147', '109', '115', '24', '105', '244', '149', '238', '203', '35', '202', '177', '173', '239', '19', '19', '179', '237', '14', '18', '185', '58', '158', '144', '213', '237', '155', '250', '239', '93', '237', '97', '195', '157', '107', '109', '246', '165', '98', '49', '2', '266', '149', '20', '18', '71', '108', '22', '115', '239', '40', '178', '232', '238', '218', '118', '118', '210', '235', '46', '228', '192', '112', '149', '96', '111', '21', '95', '95', '95', '18', '57', '206', '18', '239', '157', '80', '196', '112', '176', '99', '106', '71', '235', '135', '18', '209', '78', '20', '20', '103', '33', '3', '103', '166', '79', '79', '156', '229', '18', '238', '2', '16', '128', '200', '117', '22', '197', '166', '78', '237', '241', '107', '53', '189', '204', '243', '163', '221', '15', '200', '221', '152', '23', '193', '107', '195', '99', '97', '19', '125', '276', '113', '193', '64', '78', '197', '117', '175', '135', '135', '78', '93', '154', '153', '256', '256', '135', '137', '135', '137', '149', '110', '15', '159', '21', '158', '192', '192', '117', '29', '97', '258', '78', '46', '5', '80', '15', '105', '260', '161', '191', '2', '138', '117', '215', '176', '156', '194', '265', '115', '181', '21', '14', '106', '106', '108', '200', '156', '103', '102', '29', '17', '204', '175', '227', '118', '18', '239', '204', '258', '78', '155', '213', '109', '192', '186', '186', '15', '241', '133', '30', '30', '239', '30', '218', '109', '138', '241', '241', '45', '135', '112', '232', '104', '106', '200', '53', '23', '112', '17', '117', '59', '260', '195', '106', '117', '159', '204', '200', '228', '165', '165', '261', '200', '227', '166', '93', '21', '80', '112', '152', '196', '203', '228', '98', '102', '15', '139', '110', '13', '238', '6', '171', '94', '208', '155', '112', '203', '239', '191', '39', '136', '203', '195', '46', '203', '148', '192', '210', '23', '4', '153', '29', '165', '192', '204', '232', '94', '112', '239', '98', '71', '248', '17', '155', '10', '78', '191', '191', '111', '25', '25', '19', '19', '17', '235', '102', '265', '195', '235', '163', '238', '71', '197', '107', '155', '30', '72', '19', '259', '80', '155', '23', '64', '239', '197', '156', '57', '203', '258', '200', '205', '246', '238', '258', '227', '238', '238', '19', '148', '161', '111', '105', '202', '109', '262', '133', '137', '221', '135', '52', '170', '111', '36', '30', '156', '102', '268', '115', '22', '153', '133', '259', '98', '235', '135', '191', '25', '235', '25', '29', '94', '195', '203', '212', '112', '135', '245', '260', '200', '96', '158', '23', '165', '22', '18', '249', '240', '109', '43', '37', '15', '241', '241', '23', '135', '240', '108', '98', '268', '275', '191', '136', '263', '259', '196', '98', '98', '204', '248', '123', '105', '204', '240', '9', '9', '98', '218', '189', '238', '246', '155', '130', '157', '74', '109', '29', '189', '203', '213', '195', '241', '159', '238', '204', '218', '105', '23', '16', '263', '206', '150', '152', '204', '202', '173', '267', '18', '153', '155', '196', '2', '97', '158', '158', '204', '98', '202', '98', '204', '258', '98', '22', '113', '113', '135', '71', '237', '155', '163', '250', '233', '26', '26', '26', '26', '26', '19', '170', '18', '15', '19', '191', '30', '30', '48', '35', '30', '237', '46', '119', '16', '78', '239', '150', '245', '235', '158', '27', '241', '30', '38', '107', '75', '241', '135', '106', '106', '98', '159', '96', '135', '111', '176', '175', '200', '18', '14', '242', '163', '187', '20', '12', '191', '23', '106', '171', '25', '78', '162', '191', '157', '94', '237', '246', '203', '108', '238', '204', '204', '1', '102', '154', '106', '170', '102', '200', '2', '135', '106', '258', '258', '227', '26', '15', '25', '71', '203', '96', '3', '241', '200', '30', '108', '98', '108', '27', '106', '26', '150', '259', '78', '113', '113', '158', '202', '16', '136', '239', '156', '239', '35', '276', '252', '115', '22', '22', '258', '172', '3', '154', '3', '98', '237', '102', '235', '112', '206', '98', '57', '118', '153', '18', '26', '30', '196', '163', '191', '153', '72', '24', '156', '117', '122', '30', '135', '192', '98', '28', '30', '30', '95', '241', '155', '17', '195', '31', '103', '202', '98', '237', '94', '112', '163', '163', '94', '163', '112', '19', '195', '191', '15', '135', '78', '109', '160', '152', '97', '98', '107', '167', '97', '138', '138', '26', '191', '97', '97', '30', '30', '206', '202', '13', '30', '240', '111', '261', '79', '0', '111', '19', '152', '107', '107', '18', '98', '28', '258', '244', '106', '237', '204', '191', '15', '183', '202', '16', '111', '166', '268', '202', '12', '30', '78', '110', '133', '71', '197', '111', '52', '79', '171', '156', '94', '13', '118', '111', '71', '133', '18', '105', '30', '22', '133', '60', '237', '191', '6', '126', '3', '238', '276', '78', '162', '150', '115', '191', '107', '200', '148', '135', '72', '155', '241', '4', '108', '160', '203', '20', '133', '2', '80', '80', '154', '18', '15', '115', '239', '167', '167', '35', '268', '18', '221', '18', '195', '204', '195', '216', '133', '112', '104', '223', '191', '115', '30', '30', '30', '30', '30', '202', '204', '165', '200', '200', '29', '191', '237', '267', '18', '175', '81', '264', '264', '178', '115', '102', '178', '212', '273', '202', '109', '259', '166', '135', '155', '219', '161', '18', '94', '241', '191', '196', '191', '98', '26', '26', '204', '204', '135', '27', '238', '260', '182', '26', '98', '94', '93', '238', '156', '19', '111', '79', '16', '192', '113', '19', '241', '18', '202', '260', '272', '106', '163', '156', '106', '17', '109', '221', '78', '203', '7', '136', '197', '193', '191', '98', '112', '200', '95', '221', '98', '161', '191', '3', '106', '189', '111', '25', '196', '196', '241', '163', '200', '24', '78', '176', '3', '98', '102', '98', '19', '161', '224', '80', '18', '191', '112', '105', '254', '71', '200', '137', '29', '20', '165', '8', '106', '23', '106', '155', '200', '265', '135', '126', '203', '188', '161', '158', '258', '255', '98', '203', '50', '35', '102', '158', '175', '40', '239', '26', '66', '57', '193', '155', '15', '135', '10', '107', '106', '229', '18', '191', '204', '107', '99', '235', '238', '18', '205', '205', '35', '238', '208', '203', '110', '110', '109', '112', '23', '18', '19', '182', '156', '18', '18', '156', '111', '135', '137', '133', '137', '240', '99', '197', '113', '15', '165', '39', '97', '97', '15', '150', '163', '109', '268', '15', '15', '238', '203', '67', '161', '197', '18', '196', '97', '161', '197', '161', '185', '238', '191', '30', '121', '133', '160', '160', '74', '135', '75', '54', '135', '196', '23', '241', '166', '194', '239', '135', '240', '208', '238', '98', '135', '152', '176', '22', '174', '113', '241', '237', '57', '195', '191', '24', '165', '165', '204', '204', '56', '109', '27', '153', '252', '228', '165', '252', '200', '28', '158', '135', '196', '22', '18', '133', '30', '204', '237', '196', '93', '106', '94', '25', '93', '159', '4', '195', '23', '237', '25', '199', '57', '26', '276', '274', '204', '75', '18', '204', '108', '22', '133', '191', '111', '204', '203', '133', '237', '166', '4', '62', '202', '166', '161', '165', '74', '109', '118', '200', '22', '206', '25', '209', '17', '155', '204', '64', '23', '30', '202', '149', '171', '135', '136', '184', '26', '18', '159', '195', '18', '166', '156', '98', '133', '38', '40', '18', '18', '166', '109', '109', '209', '22', '195', '163', '163', '108', '18', '15', '99', '135', '174', '26', '19', '166', '133', '259', '151', '235', '204', '228', '133', '30', '161', '111', '242', '84', '238', '98', '195', '78', '160', '28', '25', '264', '165', '41', '19', '55', '46', '202', '121', '15', '32', '98', '238', '0', '78', '31', '30', '203', '264', '203', '135', '104', '111', '78', '212', '111', '98', '126', '22', '2', '30', '237', '212', '265', '163', '106', '106', '235', '109', '223', '161', '202', '150', '170', '255', '112', '114', '170', '98', '96', '197', '36', '18', '228', '14', '255', '204', '102', '239', '0', '185', '30', '22', '23', '246', '199', '133', '191', '78', '195', '153', '112', '191', '200', '170', '171', '200', '78', '128', '25', '153', '237', '99', '94', '208', '107', '208', '208', '78', '204', '204', '27', '191', '272', '134', '158', '158', '174', '118', '64', '18', '258', '150', '189', '18', '153', '241', '204', '112', '106', '135', '71', '148', '7', '133', '140', '187', '23', '223', '19', '19', '2', '136', '2', '200', '161', '15', '98', '158', '148', '3', '250', '153', '235', '165', '220', '176', '110', '240', '151', '148', '133', '74', '193', '46', '27', '173', '94', '176', '105', '256', '161', '226', '18', '78', '191', '171', '243', '26', '0', '108', '167', '159', '239', '150', '112', '155', '79', '241', '202', '165', '254', '107', '202', '156', '166', '193', '239', '135', '15', '203', '187', '148', '25', '15', '22', '194', '239', '56', '191', '176', '262', '200', '110', '72', '99', '102', '135', '98', '113', '15', '15', '159', '262', '171', '151', '98', '204', '213', '255', '26', '243', '175', '156', '98', '107', '191', '165', '241', '26', '165', '171', '30', '160', '166', '266', '11', '19', '211', '93', '254', '30', '239', '115', '273', '205', '112', '237', '263', '74', '98', '79', '122', '94', '265', '155', '191', '16', '235', '229', '98', '191', '227', '17', '111', '267', '135', '202', '195', '10', '180', '196', '196', '133', '135', '133', '95', '111', '122', '258', '191', '3', '114', '107', '19', '264', '74', '56', '200', '200', '133', '161', '97', '15', '97', '197', '196', '253', '93', '202', '258', '106', '153', '26', '109', '40', '149', '8', '196', '15', '119', '155', '23', '237', '193', '264', '148', '151', '238', '200', '94', '75', '148', '175', '24', '195', '171', '30', '203', '200', '202', '16', '158', '223', '191', '22', '264', '135', '35', '57', '94', '191', '196', '202', '98', '18', '196', '3', '133', '78', '191', '208', '258', '18', '200', '29', '135', '109', '110', '2', '185', '98', '133', '19', '22', '204', '95', '238', '25', '133', '250', '135', '135', '3', '47', '158', '187', '2', '78', '115', '155', '3', '218', '166', '112', '266', '22', '195', '135', '72', '97', '197', '209', '127', '80', '2', '98', '14', '239', '15', '113', '165', '204', '78', '204', '191', '175', '200', '193', '18', '196', '133', '109', '237', '56', '196', '3', '163', '163', '163', '195', '15', '263', '18', '84', '18', '133', '107', '18', '102', '156', '238', '102', '26', '244', '82', '75', '23', '258', '238', '136', '3', '204', '184', '200', '196', '158', '106', '27', '238', '155', '152', '50', '50', '2', '235', '98', '203', '166', '109', '25', '18', '151', '133', '200', '26', '26', '26', '19', '107', '156', '23', '26', '3', '70', '225', '41', '26', '239', '26', '186', '26', '34', '272', '272', '104', '78', '260', '18', '204', '26', '78', '18', '210', '135', '203', '71', '207', '26', '133', '13', '25', '26', '115', '94', '264', '135', '244', '79', '30', '263', '22', '109', '205', '176', '106', '22', '17', '151', '106', '191', '160', '160', '165', '78', '177', '196', '260', '166', '35', '191', '260', '98', '18', '237', '18', '161', '18', '197', '133', '30', '35', '200', '170', '155', '245', '19', '200', '160', '96', '172', '165', '55', '137', '241', '133', '191', '112', '25', '126', '238', '18', '18', '108', '135', '161', '135', '73', '112', '107', '8', '112', '109', '195', '25', '30', '105', '161', '241', '110', '75', '238', '215', '238', '104', '25', '3', '155', '30', '16', '3', '175', '99', '3', '200', '23', '3', '199', '203', '191', '196', '196', '204', '98', '105', '133', '197', '135', '161', '39', '15', '30', '133', '55', '191', '203', '133', '203', '140', '200', '149', '30', '96', '135', '23', '26', '3', '18', '155', '158', '135', '135', '18', '250', '18', '22', '114', '18', '18', '166', '18', '30', '18', '239', '18', '161', '50', '50', '49', '51', '23', '55', '98', '78', '210', '211', '18', '239', '78', '13', '193', '114', '78', '18', '25', '133', '19', '176', '4', '173', '50', '108', '17', '4', '204', '94', '112', '207', '109', '115', '23', '235', '197', '194', '197', '213', '148', '40', '157', '107', '200', '147', '14', '14', '18', '238', '18', '238', '18', '238', '57', '200', '135', '30', '18', '191', '191', '200', '166', '151', '161', '73', '19', '73', '239', '239', '165', '108', '2', '93', '166', '133', '133', '173', '35', '193', '114', '74', '19', '40', '58', '104', '16', '260', '160', '158', '112', '210', '158', '167', '203', '203', '3', '3', '109', '30', '133', '258', '70', '204', '200', '152', '155', '192', '204', '108', '191', '204', '26', '98', '26', '98', '264', '158', '18', '150', '137', '153', '196', '199', '191', '115', '168', '157', '149', '151', '78', '137', '160', '259', '166', '196', '171', '175', '199', '166', '161', '70', '13', '166', '152', '152', '206', '58', '18', '207', '58', '74', '207', '29', '46', '109', '135', '74', '174', '109', '40', '10', '30', '258', '16', '111', '30', '15', '239', '204', '48', '25', '196', '0', '156', '72', '72', '96', '22', '72', '133', '133', '238', '26', '3', '50', '152', '193', '235', '136', '153', '50', '1', '19', '239', '30', '25', '179', '161', '30', '191', '3', '27', '133', '3', '40', '23', '133', '150', '151', '265', '32', '191', '3', '25', '170', '238', '40', '259', '166', '137', '241', '133', '209', '159', '78', '78', '153', '26', '52', '241', '36', '250', '96', '30', '30', '38', '18', '18', '238', '99', '72', '111', '3', '244', '191', '226', '3', '191', '114', '135', '109', '40', '79', '202', '78', '114', '238', '10', '150', '108', '163', '108', '113', '197', '109', '115', '161', '3', '238', '3', '237', '161', '107', '155', '78', '202', '112', '38', '195', '137', '250', '133', '237', '161', '25', '26', '25', '15', '15', '239', '166', '114', '229', '111', '40', '135', '94', '102', '162', '204', '237', '17', '166', '136', '22', '98', '99', '99', '166', '35', '196', '3', '203', '135', '135', '84', '98', '266', '238', '13', '166', '78', '193', '98', '241', '104', '153', '192', '200', '10', '95', '95', '148', '107', '14', '17', '191', '239', '19', '203', '255', '197', '213', '94', '3', '107', '23', '35', '10', '237', '235', '78', '29', '18', '160', '200', '150', '99', '109', '155', '161', '133', '114', '18', '18', '18', '264', '133', '104', '161', '70', '17', '204', '195', '204', '241', '99', '82', '241', '241', '161', '18', '235', '175', '2', '2', '202', '50', '230', '26', '40', '40', '200', '78', '156', '26', '112', '26', '30', '71', '184', '200', '135', '153', '23', '106', '125', '175', '133', '133', '7', '38', '196', '161', '0', '49', '241', '158', '15', '21', '104', '2', '3', '176', '259', '135', '78', '75', '112', '200', '1', '207', '238', '161', '150', '158', '2', '45', '31', '237', '149', '3', '26', '165', '135', '152', '115', '133', '18', '40', '166', '30', '173', '40', '26', '78', '50', '48', '50', '13', '45', '200', '200', '18', '151', '205', '135', '38', '117', '196', '135', '155', '111', '196', '196', '166', '23', '25', '161', '18', '124', '259', '165', '58', '170', '204', '250', '103', '160', '238', '194', '163', '241', '255', '235', '17', '133', '26', '135', '196', '237', '114', '258', '193', '248', '40', '195', '84', '238', '146', '199', '40', '161', '22', '137', '43', '166', '156', '4', '104', '133', '177', '211', '193', '78', '203', '246', '237', '264', '3', '149', '78', '78', '111', '239', '13', '238', '176', '112', '150', '156', '3', '6', '118', '178', '178', '21', '186', '150', '241', '50', '243', '191', '121', '278', '29', '200', '18', '3', '197', '191', '98', '173', '195', '25', '195', '264', '221', '15', '156', '25', '200', '211', '195', '26', '35', '2', '135', '126', '113', '167', '191', '264', '196', '133', '18', '26', '146', '203', '219', '23', '160', '160', '135', '259', '3', '159', '120', '160', '106', '240', '133', '115', '258', '38', '22', '3', '26', '22', '107', '49', '48', '25', '237', '72', '219', '196', '196', '111', '135', '196', '18', '166', '172', '135', '26', '258', '70', '98', '40', '166', '23', '96', '3', '30', '155', '126', '146', '114', '242', '237', '157', '238', '241', '40', '133', '200', '161', '175', '200', '175', '84', '266', '18', '99', '30', '242', '151', '153', '166', '199', '228', '3', '237', '132', '110', '135', '80', '15', '15', '15', '84', '133', '15', '177', '3', '202', '255', '71', '115', '235', '19', '18', '18', '102', '58', '78', '165', '166', '264', '12', '150', '15', '2', '18', '135', '109', '200', '258', '128', '3', '199', '197', '239', '109', '222', '239', '107', '125', '132', '204', '204', '133', '135', '30', '158', '203', '15', '264', '237', '133', '196', '3', '252', '26', '98', '79', '199', '210', '0', '99', '30', '26', '259', '6', '49', '166', '98', '108', '9', '133', '102', '155', '98', '114', '78', '35', '166', '188', '202', '235', '30', '149', '165', '176', '138', '29', '153', '25', '25', '137', '133', '166', '135', '125', '71', '193', '154', '165', '98', '23', '56', '198', '26', '26', '0', '109', '136', '163', '151', '25', '15', '241', '195', '204', '191', '195', '161', '175', '133', '237', '78', '165', '258', '135', '78', '135', '258', '18', '235', '30', '187', '150', '275', '149', '2', '36', '112', '22', '161', '238', '203', '112', '265', '75', '133', '205', '258', '158', '18', '26', '18', '25', '26', '15', '26', '239', '15', '240', '18', '71', '71', '78', '135', '3', '148', '166', '26', '176', '2', '197', '150', '176', '19', '18', '97', '166', '18', '153', '250', '133', '210', '20', '192', '196', '26', '191', '2', '81', '70', '5', '79', '3', '30', '193', '193', '69', '160', '176', '246', '108', '54', '250', '135', '200', '80', '3', '167', '18', '166', '200', '152', '23', '149', '19', '14', '13', '237', '196', '19', '112', '254', '204', '191', '23', '264', '199', '209', '137', '31', '135', '238', '78', '18', '191', '15', '164', '195', '74', '163', '163', '41', '99', '26', '135', '26', '40', '40', '250', '200', '200', '26', '237', '4', '195', '158', '35', '71', '191', '3', '218', '274', '58', '3', '23', '177', '132', '49', '62', '238', '166', '97', '264', '193', '29', '26', '49', '241', '197', '200', '64', '227', '258', '44', '15', '133', '175', '238', '109', '254', '237', '198', '175', '166', '213', '174', '196', '107', '259', '98', '10', '196', '210', '191', '96', '166', '135', '15', '197', '78', '78', '195', '176', '18', '98', '129', '170', '18', '247', '203', '26', '158', '40', '64', '200', '163', '196', '19', '205', '197', '193', '191', '69', '96', '74', '35', '36', '64', '133', '172', '19', '266', '96', '94', '103', '258', '193', '264', '98', '254', '50', '193', '25', '170', '75', '84', '27', '252', '203', '94', '94', '19', '98', '137', '12', '153', '3', '155', '135', '238', '15', '196', '15', '26', '84', '49', '27', '135', '203', '48', '133', '129', '133', '258', '155', '35', '195', '71', '241', '135', '166', '193', '165', '104', '196', '117', '196', '26', '237', '109', '95', '196', '60', '40', '158', '193', '250', '104', '94', '178', '27', '193', '78', '106', '79', '204', '133', '237', '235', '135', '202', '70', '15', '264', '241', '254', '110', '199', '49', '255', '30', '193', '238', '204', '102', '97', '161', '43', '228', '1', '6', '196', '193', '26', '15', '128', '46', '155', '32', '133', '69', '47', '135', '78', '205', '200', '200', '202', '3', '95', '191', '78', '26', '248', '258', '191', '240', '49', '110', '38', '193', '200', '200', '96', '200', '204', '135', '9', '200', '258', '15', '46', '15', '149', '155', '197', '2', '166', '151', '72', '135', '97', '211', '238', '135', '163', '161', '26', '163', '129', '241', '239', '241', '16', '26', '18', '263', '260', '166', '3', '264', '195', '79', '132', '251', '50', '133', '15', '25', '15', '191', '15', '78', '50', '78', '71', '227', '69', '200', '69', '109', '64', '133', '197', '196', '3', '45', '238', '26', '132', '104', '196', '193', '136', '80', '26', '75', '117', '14', '78', '75', '98', '15', '2', '15', '50', '255', '136', '3', '112', '173', '69', '191', '19', '239', '235', '109', '70', '135', '78', '165', '238', '262', '199', '35', '203', '224', '193', '15', '18', '150', '238', '197', '238', '193', '135', '3', '150', '187', '199', '239', '109', '152', '27', '257', '6', '235', '104', '191', '133', '166', '155', '26', '16', '136', '198', '98', '154', '235', '27', '102', '30', '191', '15', '132', '132', '114', '107', '241', '241', '156', '199', '241', '49', '62', '117', '258', '30', '163', '133', '238', '200', '3', '15', '109', '2', '196', '165', '166', '159', '210', '35', '15', '17', '200', '241', '106', '203', '197', '197', '79', '98', '98', '23', '204', '48', '129', '57', '258', '202', '215', '238', '70', '64', '193', '189', '57', '200', '0', '98', '135', '204', '25', '30', '204', '27', '26', '78', '135', '109', '3', '197', '3', '167', '35', '196', '225', '2', '194', '200', '15', '161', '198', '23', '133', '238', '195', '35', '50', '135', '152', '163', '21', '15', '94', '26', '94', '53', '260', '196', '154', '277', '200', '118', '6', '238', '50', '209', '50', '3', '238', '3', '15', '107', '135', '199', '15', '135', '26', '258', '251', '203', '19', '259', '195', '199', '238', '15', '254', '112', '211', '200', '129', '84', '196', '3', '208', '259', '135', '104', '29', '136', '31', '78', '13', '3', '64', '75', '55', '114', '160', '96', '14', '18', '207', '75', '48', '15', '49', '97', '117', '196', '99', '258', '195', '264', '264', '78', '197', '30', '238', '109', '158', '32', '195', '149', '227', '65', '167', '162', '204', '250', '151', '151', '102', '26', '29', '26', '193', '74', '50', '193', '186', '15', '74', '259', '57', '188', '277', '277', '195', '195', '212', '78', '197', '64', '129', '259', '200', '199', '129', '4', '235', '197', '196', '161', '3', '255', '199', '102', '3', '13', '94', '53', '98', '15', '129', '15', '155', '71', '179', '50', '29', '136', '80', '254', '198', '162', '237', '3', '18', '260', '238', '117', '179', '156', '19', '45', '197', '78', '199', '102', '102', '237', '155', '69', '69', '163', '239', '15', '196', '80', '191', '30', '71', '158', '259', '2', '106', '148', '15', '135', '135', '135', '196', '19', '123', '15', '104', '19', '128', '230', '69', '98', '197', '241', '161', '195', '200', '3', '98', '78', '165', '163', '19', '196', '31', '173', '203', '55', '69', '209', '239', '79', '158', '254', '135', '51', '94', '162', '255', '95', '18', '133', '109', '50', '94', '199', '15', '197', '200', '133', '116', '50', '196', '161', '69', '133', '3', '258', '3', '197', '29', '3', '157', '197', '193', '4', '94', '195', '17', '48', '152', '8', '15', '155', '133', '195', '136', '150', '102', '237', '203', '164', '164', '250', '154', '13', '191', '58', '94', '70', '93', '264', '15', '30', '133', '237', '267', '199', '193', '76', '241', '157', '26', '200', '109', '179', '55', '98', '200', '129', '238', '235', '208', '109', '239', '258', '196', '193', '18', '107', '210', '70', '40', '129', '259', '193', '30', '158', '252', '40', '210', '78', '15', '78', '96', '208', '113', '258', '64', '80', '15', '133', '241', '15', '15', '258', '135', '109', '251', '251', '109', '74', '196', '170', '240', '254', '258', '204', '199', '26', '109', '197', '57', '45', '161', '57', '114', '3', '15', '264', '106', '57', '27', '18', '258', '29', '84', '161', '135', '114', '24', '30', '107', '107', '172', '209', '170', '18', '19', '133', '191', '195', '135', '156', '196', '94', '3', '84', '225', '199', '238', '106', '104', '70', '50', '12', '155', '114', '176', '237', '200', '108', '70', '240', '196', '19', '22', '276', '199', '238', '205', '104', '193', '108', '115', '259', '102', '19', '258', '18', '106', '161', '191', '114', '64', '15', '204', '35', '155', '40', '238', '136', '264', '133', '186', '15', '258', '166', '26', '6', '114', '114', '133', '114', '114', '13', '13', '13', '237', '250', '245', '136', '78', '13', '48', '18', '13', '25', '26', '93', '125', '50', '94', '148', '78', '194', '200', '25', '93', '274', '266', '70', '22', '3', '208', '240', '211', '135', '31', '18', '153', '18', '2', '15', '15', '15', '156', '15', '70', '73', '173', '13', '112', '69', '204', '70', '248', '135', '70', '3', '193', '193', '22', '162', '238', '57', '98', '15', '199', '199', '52', '151', '73', '5', '108', '23', '69', '95', '220', '106', '163', '152', '163', '108', '15', '58', '250', '19', '177', '245', '264', '93', '204', '25', '78', '155', '69', '155', '239', '241', '155', '200', '18', '3', '13', '196', '197', '204', '272', '95', '135', '165', '22', '210', '35', '69', '18', '170', '3', '25', '156', '78', '135', '102', '107', '255', '19', '70', '70', '109', '198', '156', '64', '170', '138', '78', '203', '166', '108', '114', '179', '228', '74', '49', '14', '264', '26', '195', '78', '107', '75', '18', '106', '151', '70', '58', '15', '196', '238', '109', '104', '104', '109', '136', '148', '210', '3', '161', '15', '104', '238', '208', '133', '108', '241', '18', '209', '74', '156', '114', '199', '174', '35', '18', '170', '135', '238', '244', '155', '162', '197', '3', '98', '227', '267', '238', '193', '50', '238', '98', '112', '124', '25', '114', '241', '159', '154', '78', '161', '136', '114', '153', '225', '154', '13', '50', '11', '25', '15', '209', '11', '166', '203', '259', '237', '22', '114', '276', '98', '278', '150', '198', '18', '15', '78', '15', '162', '26', '3', '196', '3', '102', '189', '71', '69', '16', '18', '152', '133', '135', '162', '241', '15', '104', '15', '259', '202', '2', '238', '207', '15', '204', '30', '215', '112', '210', '42', '96', '193', '15', '110', '163', '211', '78', '108', '15', '31', '27', '15', '31', '196', '195', '96', '15', '161', '94', '193', '195', '198', '98', '261', '258', '76', '15', '104', '258', '135', '18', '15', '202', '258', '108', '156', '13', '22', '251', '199', '155', '258', '211', '3', '207', '202', '259', '107', '202', '259', '135', '135', '148', '158', '154', '99', '99', '3', '207', '94', '25', '162', '31', '241', '25', '15', '114', '18', '114', '148', '19', '15', '203', '191', '19', '13', '193', '204', '25', '166', '155', '196', '98', '162', '197', '29', '152', '191', '133', '199', '230', '40', '25', '204', '79', '3', '19', '165', '204', '108', '208', '3', '251', '274', '15', '166', '264', '35', '13', '193', '114', '258', '13', '19', '133', '23', '17', '199', '204', '153', '250', '204', '16', '12', '107', '191', '18', '3', '15', '165', '15', '69', '17', '165', '17', '15', '15', '156', '23', '23', '15', '197', '104', '197', '26', '208', '135', '211', '250', '18', '204', '25', '196', '53', '69', '104', '104', '19', '78', '241', '230', '30', '195', '96', '26', '109', '196', '69', '238', '15', '18', '133', '26', '106', '109', '98', '15', '258', '274', '193', '203', '58', '57', '274', '2', '202', '161', '136', '135', '155', '241', '35', '151', '137', '5', '114', '96', '16', '209', '158', '15', '266', '161', '15', '25', '274', '202', '3', '161', '237', '267', '204', '125', '15', '151', '155', '259', '151', '210', '238', '15', '26', '132', '50', '173', '197', '114', '241', '15', '258', '109', '166', '274', '13', '191', '15', '26', '164', '258', '163', '15', '264', '11', '260', '148', '150', '207', '196', '98', '272', '133', '196', '107', '26', '191', '163', '191', '135', '3', '18', '18', '119', '69', '196', '15', '156', '135', '112', '258', '155', '256', '199', '204', '212', '18', '135', '254', '72', '258', '254', '15', '133', '18', '259', '193', '258', '204', '40', '18', '29', '155', '104', '107', '274', '241', '25', '26', '69', '148', '166', '46', '94', '135', '15', '155', '104', '241', '32', '274', '249', '15', '197', '104', '44', '259', '69', '191', '15', '198', '193', '133', '17', '258', '248', '166', '197', '200', '250', '204', '3', '225', '238', '163', '69', '15', '15', '26', '202', '19', '209', '78', '212', '15', '78', '243', '110', '191', '15', '18', '107', '38', '208', '166', '239', '237', '199', '15', '204', '258', '173', '172', '200', '274', '243', '19', '191', '135', '203', '69', '149', '148', '196', '256', '204', '97', '191', '26', '114', '274', '200', '69', '69', '64', '69', '239', '25', '274', '203', '78', '135', '196', '202', '17', '2', '2', '3', '98', '98', '62', '2', '95', '96', '2', '207', '78', '200', '26', '64', '18', '93', '49', '57', '154', '200', '69', '94', '98', '98', '56', '56', '166', '98', '212', '226', '226', '15', '69', '15', '273', '272', '104', '193', '3', '98', '199', '98', '25', '125', '259', '249', '108', '40', '26', '198', '218', '73', '148', '69', '204', '241', '35', '3', '274', '70', '18', '71', '35', '172', '133', '106', '69', '107', '199', '219', '19', '133', '69', '135', '109', '57', '191', '135', '252', '104', '3', '94', '3', '72', '126', '16', '36', '84', '241', '3', '218', '98', '108', '46', '19', '196', '118', '104', '29', '238', '69', '10', '204', '264', '259', '124', '242', '107', '133', '25', '64', '30', '194', '259', '159', '264', '274', '52', '200', '104', '29', '30', '193', '98', '94', '114', '204', '40', '18', '198', '97', '3', '99', '274', '239', '177', '15', '18', '126', '126', '208', '148', '15', '104', '29', '36', '218', '70', '15', '23', '278', '23', '36', '191', '210', '51', '245', '167', '72', '237', '25', '203', '36', '196', '193', '196', '15', '151', '207', '203', '103', '166', '135', '110', '27', '13', '27', '191', '98', '57', '166', '50', '21', '15', '197', '23', '2', '70', '69', '193', '274', '104', '166', '258', '126', '238', '161', '18', '193', '69', '140', '170', '197', '204', '165', '203', '170', '15', '58', '35', '83', '212', '150', '73', '25', '15', '64', '133', '210', '108', '79', '193', '15', '126', '165', '15', '208', '98', '3', '200', '126', '3', '40', '165', '98', '69', '15', '165', '133', '96', '173', '203', '133', '238', '248', '15', '7', '38', '207', '108', '165', '199', '18', '162', '27', '3', '72', '65', '250', '209', '166', '64', '209', '237', '212', '64', '225', '120', '108', '203', '135', '248', '69', '251', '135', '204', '160', '160', '135', '148', '15', '15', '23', '207', '274', '29', '28', '194', '150', '40', '104', '208', '125', '70', '18', '3', '19', '102', '104', '23', '29', '207', '133', '204', '111', '2', '125', '126', '7', '194', '209', '203', '83', '126', '126', '258', '119', '162', '241', '133', '18', '208', '197', '209', '15', '78', '80', '110', '237', '104', '30', '208', '103', '15', '200', '15', '238', '254', '108', '173', '40', '69', '175', '17', '155', '136', '26', '3', '135', '13', '241', '161', '166', '170', '162', '133', '53', '211', '204', '15', '161', '238', '110', '18', '191', '18', '50', '71', '30', '17', '208', '248', '203', '203', '196', '210', '258', '204', '3', '18', '196', '125', '173', '199', '249', '18', '104', '165', '241', '2', '193', '238', '78', '15', '166', '15', '126', '3', '237', '161', '52', '207', '3', '3', '50', '56', '22', '56', '69', '93', '135', '207', '69', '99', '3', '133', '68', '133', '161', '108', '126', '250', '60', '155', '98', '18', '16', '78', '78', '240', '235', '176', '22', '199', '160', '235', '274', '209', '64', '173', '137', '128', '25', '18', '160', '203', '70', '165', '193', '110', '207', '64', '85', '252', '78', '51', '2', '99', '19', '22', '198', '204', '166', '166', '151', '56', '133', '210', '26', '149', '109', '30', '209', '209', '51', '3', '193', '119', '208', '93', '165', '248', '55', '203', '204', '58', '18', '199', '211', '83', '32', '135', '212', '156', '19', '235', '173', '191', '209', '203', '146', '208', '58', '158', '102', '199', '200', '200', '173', '70', '52', '107', '211', '213', '237', '196', '205', '104', '135', '258', '255', '96', '19', '124', '128', '57', '199', '98', '166', '102', '40', '15', '238', '3', '148', '225', '126', '135', '114', '193', '204', '15', '209', '209', '193', '259', '64', '161', '26', '274', '126', '18', '196', '0', '60', '207', '17', '71', '218', '239', '248', '38', '154', '61', '128', '199', '196', '173', '25', '46', '98', '258', '108', '3', '108', '196', '93', '93', '99', '172', '69', '241', '196', '30', '135', '278', '109', '18', '5', '209', '57', '159', '70', '193', '163', '249', '196', '196', '72', '103', '133', '17', '204', '199', '166', '202', '29', '57', '193', '150', '98', '119', '25', '205', '18', '173', '166', '209', '57', '25', '58', '258', '26', '15', '15', '101', '26', '74', '1', '4', '74', '263']\n",
            "     filenum                                               text  label\n",
            "0          1  novelty.' General Electric Co. v. Wabash Appli...    209\n",
            "1          2  Court properly to shift to Congress the respon...     63\n",
            "2          3  would sell' at the carrier's price. In the Val...    216\n",
            "3          4  not to coerce the surrender of lands without c...    108\n",
            "4          6  for the privilege of doing it, which place an ...    200\n",
            "...      ...                                                ...    ...\n",
            "6000    8411  too here, \"[w]e see no reason to reject Califo...     15\n",
            "6001    8412  the courts of the State.\" U. S. C. If the stat...     15\n",
            "6002    8413  abandoned. A mere recital in briefs of the exi...    101\n",
            "6003    8414  sentence less than death.\" ' \" U. S., at (quot...     26\n",
            "6004    8417  \"As we pointed out in United States v. Califor...      4\n",
            "\n",
            "[6005 rows x 3 columns]\n"
          ]
        }
      ],
      "source": [
        "fh = open('sc_model_3.txt')\n",
        "filenums=[]\n",
        "records=[]\n",
        "for record in fh:\n",
        "    record=record.split(\"---\")\n",
        "    filenum=record[0]\n",
        "    modelnum=record[1]\n",
        "    text=record[2]\n",
        "    filenums.append(filenum)\n",
        "    records.append(text)\n",
        "    \n",
        "print(len(filenums))\n",
        "print(len(records))\n",
        "\n",
        "summarized_data = pd.DataFrame(list(zip(filenums, records)),columns =['filenum','text'])\n",
        "len_list = [len(ele.split()) for ele in records]\n",
        "res = 0 if len(len_list) == 0 else (float(sum(len_list)) / len(len_list))\n",
        "print(\"Average Length %s\" % res)\n",
        "\n",
        "#temp_file = open(\"labels_sc.txt\", \"r\")\n",
        "temp_file = open(\"labels_sc_279.txt\", \"r\")\n",
        "\n",
        "data = temp_file.read()\n",
        "\n",
        "label_list = data.split(\"\\n\")\n",
        "print(label_list)\n",
        "label_list = label_list[0:-1]\n",
        "print(label_list)\n",
        "label_list = [int(i) for i in label_list]\n",
        "temp_file.close()\n",
        "\n",
        "\n",
        "new_label_list=[]\n",
        "for num in filenums:\n",
        "  new_label_list.append(label_list[int(num)-1])\n",
        "summarized_data['label'] = new_label_list\n",
        "\n",
        "print(summarized_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBg7k7sMwIiY",
        "outputId": "6cf86cb5-be58-4bbe-8c3c-e431c8058ad8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_1[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_roberta_model[0][0]']       \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 279)          143127      ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[209  63 216 ... 101  26   4]\n",
            "[   1    2    3 ... 8413 8414 8417]\n",
            "[6622   35 6713 4989 8233 7444 5061 3942 8344 5464  317 4515 3001 6991\n",
            " 2875 1125 3885 6647 4857 4504  214 8340 2969 3847 4609 1346 5993 7358\n",
            " 2752 7651 8188 7073 4782 6069 2346 6483 1663 4952 5072 1967 4071 7590\n",
            "  860 6418 5030 7297  626 5463 6029 6302 7256 5667 1235 3993 1323 4263\n",
            " 3835 2624 5801 4293 8129 5866 5777 6059 4837 3843 1418 5409 5274 6205\n",
            " 2356 5997 6685 3444 3310 7721 5928 2360 7197 4252 8343  909 4354 4752\n",
            "  649 1916 3488 2496 5765 6612 4004 4052 1403 6697 5268 6293 6380 4602\n",
            " 6057 1851 4577 4236 7978 5083 6552  361 7706 4186 4452 4396 8351 3834\n",
            "  812 5108  463 6600 8264 8064 7121 8279 5556 5119 7947 3350 7152 5199\n",
            " 4282 1092 5025 5051  933 1927 3615 1221 8062 1489 7442 5459 2031 7351\n",
            "  234 8015 7416 7921 6574 6257 8396  941 4520 3300 5640  240 7810 7425\n",
            " 1321 6580  928 1170 5404 8273 6742   94    9 7861 7975 1018 2140 1415\n",
            " 7061 7767 4990 8124 5927 6051 3269 6916 5959 2846 6818 7052 7775 7582\n",
            " 8116 4240 4767 5248 1812 2175 6055 8331  998 1564 7326 1559  292 3065\n",
            " 6764 6854 4915 5552 3583 5902 4229 2144 7565 3712 7224 4502 1406 1651\n",
            " 6256 3827 6525 6318 2686 3616 5369 8235 1704 1011 3351 8089 8021  102\n",
            "  224 5671 8294 7707 2985 6624 5146 7381 3709 6824 6308 2281 6837 5436\n",
            " 1741 3520 2649 4886  966 5476 1199 8387 5449 1028 2226 6166 4387  350\n",
            " 6123 5236 5041  734 3412 6085  587  489 5760 4982 5115 5575 3839 6743\n",
            " 7173 5724 7689 7787 8302 3150 2247 8128 6660 3505 4610 2866 4161 6905\n",
            " 7096  768 1062 4411 6087 2096 4903 7546 6630 2928 4640 7125 1792 2063\n",
            " 1700 7928 8095  737 5849  922 2359 6015 4489 4361 6081 6346 5014 5167\n",
            " 1023 8009 3335  268 4777 5221 4323 6168  711 5702 6129 7181 1860 7192\n",
            " 1347 5467 6579   67 2693 6134 4841 4270 4611 2943 6746 7696 3466 3950\n",
            " 6125 8370 6142 6131  995 5548 1721 5067 4015 7092 3156   25 1749 5554\n",
            " 4878 8248 6836 5310 2698 6960 2339 1650 6227 4104 5130 7089 6993 2400\n",
            " 1289 4305 5585 7933 6284 7057 3807 5790 8347 5280 2841 7220 1384 7693\n",
            " 8090 4171 5260  133 7451 3957 1719 7379 4743 4746 5435 6216 4858 4417\n",
            " 8355 4492 2648 5104 5278  743 7308  158 7543 2168 5913 7111 3414 7592\n",
            "   39 7157 5647 4555 3956 3617 4630 4384 6596   96 4539 6749 7202 2413\n",
            " 4749 5122 5052 5929  252 5565  336 6008 4564 5580 7237 1466 1568 5958\n",
            " 7657 4327 5091 6918 6487 4082 2989 7448 4715 3382  467 3784 4624 4419\n",
            " 1117 7954 7122 6392 3174 1534 4164 3659 7526  498 5944 4572 2307 2929\n",
            " 7480 4412  731 4563 7692 1869 2472 6966 4238 6549 1594 2276 7269 2545\n",
            " 2216  605 6587 6315 5297 5282 1619 5504 6399 3917 6342 5754 5721 7385\n",
            " 7909 6126 7382 8382 4383 5502 7130  485 4936 7674 5372 3893 7109 4605\n",
            " 5211 5462 8065   50 4080 3334 8167 8378 5595 1873 8207 7076 4106 3719\n",
            " 4627  395  231 2405 6973 3480 3246 5414 5931 8208 4825 6203 3019 5822\n",
            "   63 5854 6543 5787 4764 6701 6562 3822 4861 3051 1315 8318 1089 8169\n",
            " 8218 7549  707 1284 1076 2465 3815 4636   80 7025 7127 4026 2777 4961\n",
            " 7531  744 6640 6684 4297 7939 7384  446 3287 6675 7752 5279 8196 3020\n",
            " 1203 4192 4253 2887 2556 5729 1898 3208 5973 7033 1431 5915 8312 5799\n",
            " 7105  511 6404 2852 1561 4150 1178]\n"
          ]
        }
      ],
      "source": [
        "gc.collect()\n",
        "tf.keras.backend.clear_session()\n",
        "dbert_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "dbert_model = TFRobertaModel.from_pretrained('roberta-base')\n",
        "max_len=512\n",
        "sentences=summarized_data['text']\n",
        "labels=summarized_data['label']\n",
        "filenumbers=summarized_data['filenum']\n",
        "len(sentences),len(labels),len(filenumbers)\n",
        "\n",
        "model_3=create_model()\n",
        "input_ids=[]\n",
        "attention_masks=[]\n",
        "\n",
        "for sent in sentences:\n",
        "  dbert_inps=dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n",
        "  input_ids.append(dbert_inps['input_ids'])\n",
        "  attention_masks.append(dbert_inps['attention_mask'])\n",
        "input_ids=np.asarray(input_ids)\n",
        "\n",
        "attention_masks=np.array(attention_masks)\n",
        "labels=np.array(labels)\n",
        "filenumbers=[int(i) for i in filenumbers]\n",
        "filenumbers=np.array(filenumbers)\n",
        "print(labels)\n",
        "print(filenumbers)\n",
        "\n",
        "#train_inp,val_inp,train_label,val_label,train_mask,val_mask,train_fnum,val_fnum=train_test_split(input_ids,labels,attention_masks,filenumbers,test_size=0.1,random_state=42)\n",
        "\n",
        "new_train_inp_3=[]\n",
        "new_val_inp_3=[]\n",
        "new_train_label_3=[]\n",
        "new_val_label_3=[]\n",
        "new_train_mask_3=[]\n",
        "new_train_fnum_3=[]\n",
        "new_val_fnum_3=[]\n",
        "new_val_mask_3=[]\n",
        "for i in range(len(train_fnum)):\n",
        "  if train_fnum[i] in filenumbers:\n",
        "    new_train_inp_3.append(train_inp[i])\n",
        "    #new_val_inp.append(val_inp[i])\n",
        "    new_train_label_3.append(train_label[i])\n",
        "    #new_val_label.append(val_label[i])\n",
        "    new_train_mask_3.append(train_mask[i])\n",
        "    #new_val_mask.append(val_mask[i])\n",
        "    new_train_fnum_3.append(train_fnum[i])\n",
        "\n",
        "for i in range(len(val_fnum)):\n",
        "  if val_fnum[i] in filenumbers:\n",
        "    #new_train_inp.append(train_inp[i])\n",
        "    new_val_inp_3.append(val_inp[i])\n",
        "    #new_train_label.append(train_label[i])\n",
        "    new_val_label_3.append(val_label[i])\n",
        "    #new_train_mask.append(train_mask[i])\n",
        "    new_val_mask_3.append(val_mask[i])\n",
        "    new_val_fnum_3.append(val_fnum[i])\n",
        "\n",
        "# print(new_train_label)\n",
        "# print(len(new_train_label))\n",
        "# print(new_val_label)\n",
        "# print(len(new_val_label))\n",
        "new_train_inp_3=np.array(new_train_inp_3)\n",
        "new_val_inp_3=np.array(new_val_inp_3)\n",
        "new_train_label_3=np.array(new_train_label_3)\n",
        "new_val_label_3=np.array(new_val_label_3)\n",
        "new_train_mask_3=np.array(new_train_mask_3)\n",
        "new_train_fnum_3=np.array(new_train_fnum_3)\n",
        "new_val_fnum_3=np.array(new_val_fnum_3)\n",
        "new_val_mask_3=np.array(new_val_mask_3)\n",
        "\n",
        "print(new_val_fnum_3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYIS7zLEOS8u",
        "outputId": "cf6ac4de-8da0-49b4-d776-7a0f636b86d0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_1[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_roberta_model[0][0]']       \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 279)          143127      ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[209  63 216 ... 101  26   4]\n",
            "[   1    2    3 ... 8413 8414 8417]\n",
            "[6622   35 6713 4989 8233 7444 5061 3942 8344 5464  317 4515 3001 6991\n",
            " 2875 1125 3885 6647 4857 4504  214 8340 2969 3847 4609 1346 5993 7358\n",
            " 2752 7651 8188 7073 4782 6069 2346 6483 1663 4952 5072 1967 4071 7590\n",
            "  860 6418 5030 7297  626 5463 6029 6302 7256 5667 1235 3993 1323 4263\n",
            " 3835 2624 5801 4293 8129 5866 5777 6059 4837 3843 1418 5409 5274 6205\n",
            " 2356 5997 6685 3444 3310 7721 5928 2360 7197 4252 8343  909 4354 4752\n",
            "  649 1916 3488 2496 5765 6612 4004 4052 1403 6697 5268 6293 6380 4602\n",
            " 6057 1851 4577 4236 7978 5083 6552  361 7706 4186 4452 4396 8351 3834\n",
            "  812 5108  463 6600 8264 8064 7121 8279 5556 5119 7947 3350 7152 5199\n",
            " 4282 1092 5025 5051  933 1927 3615 1221 8062 1489 7442 5459 2031 7351\n",
            "  234 8015 7416 7921 6574 6257 8396  941 4520 3300 5640  240 7810 7425\n",
            " 1321 6580  928 1170 5404 8273 6742   94    9 7861 7975 1018 2140 1415\n",
            " 7061 7767 4990 8124 5927 6051 3269 6916 5959 2846 6818 7052 7775 7582\n",
            " 8116 4240 4767 5248 1812 2175 6055 8331  998 1564 7326 1559  292 3065\n",
            " 6764 6854 4915 5552 3583 5902 4229 2144 7565 3712 7224 4502 1406 1651\n",
            " 6256 3827 6525 6318 2686 3616 5369 8235 1704 1011 3351 8089 8021  102\n",
            "  224 5671 8294 7707 2985 6624 5146 7381 3709 6824 6308 2281 6837 5436\n",
            " 1741 3520 2649 4886  966 5476 1199 8387 5449 1028 2226 6166 4387  350\n",
            " 6123 5236 5041  734 3412 6085  587  489 5760 4982 5115 5575 3839 6743\n",
            " 7173 5724 7689 7787 8302 3150 2247 8128 6660 3505 4610 2866 4161 6905\n",
            " 7096  768 1062 4411 6087 2096 4903 7546 6630 2928 4640 7125 1792 2063\n",
            " 1700 7928 8095  737 5849  922 2359 6015 4489 4361 6081 6346 5014 5167\n",
            " 1023 8009 3335  268 4777 5221 4323 6168  711 5702 6129 7181 1860 7192\n",
            " 1347 5467 6579   67 2693 6134 4841 4270 4611 2943 6746 7696 3466 3950\n",
            " 6125 8370 6142 6131  995 5548 1721 5067 4015 7092 3156   25 1749 5554\n",
            " 4878 8248 6836 5310 2698 6960 2339 1650 6227 4104 5130 7089 6993 2400\n",
            " 1289 4305 5585 7933 6284 7057 3807 5790 8347 5280 2841 7220 1384 7693\n",
            " 8090 4171 5260  133 7451 3957 1719 7379 4743 4746 5435 6216 4858 4417\n",
            " 8355 4492 2648 5104 5278  743 7308  158 7543 2168 5913 7111 3414 7592\n",
            "   39 7157 5647 4555 3956 3617 4630 4384 6596   96 4539 6749 7202 2413\n",
            " 4749 5122 5052 5929  252 5565  336 6008 4564 5580 7237 1466 1568 5958\n",
            " 7657 4327 5091 6918 6487 4082 2989 7448 4715 3382  467 3784 4624 4419\n",
            " 1117 7954 7122 6392 3174 1534 4164 3659 7526  498 5944 4572 2307 2929\n",
            " 7480 4412  731 4563 7692 1869 2472 6966 4238 6549 1594 2276 7269 2545\n",
            " 2216  605 6587 6315 5297 5282 1619 5504 6399 3917 6342 5754 5721 7385\n",
            " 7909 6126 7382 8382 4383 5502 7130  485 4936 7674 5372 3893 7109 4605\n",
            " 5211 5462 8065   50 4080 3334 8167 8378 5595 1873 8207 7076 4106 3719\n",
            " 4627  395  231 2405 6973 3480 3246 5414 5931 8208 4825 6203 3019 5822\n",
            "   63 5854 6543 5787 4764 6701 6562 3822 4861 3051 1315 8318 1089 8169\n",
            " 8218 7549  707 1284 1076 2465 3815 4636   80 7025 7127 4026 2777 4961\n",
            " 7531  744 6640 6684 4297 7939 7384  446 3287 6675 7752 5279 8196 3020\n",
            " 1203 4192 4253 2887 2556 5729 1898 3208 5973 7033 1431 5915 8312 5799\n",
            " 7105  511 6404 2852 1561 4150 1178]\n",
            "Mon Jun  6 15:09:09 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   42C    P0    40W / 300W |   4723MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "677/677 [==============================] - 248s 344ms/step - loss: 14.1324 - accuracy: 0.1231 - val_loss: 12.6147 - val_accuracy: 0.2958\n",
            "Epoch 2/5\n",
            "677/677 [==============================] - 233s 344ms/step - loss: 12.0843 - accuracy: 0.3468 - val_loss: 11.3792 - val_accuracy: 0.4084\n",
            "Epoch 3/5\n",
            "677/677 [==============================] - 233s 344ms/step - loss: 10.9601 - accuracy: 0.4686 - val_loss: 10.6720 - val_accuracy: 0.4840\n",
            "Epoch 4/5\n",
            "677/677 [==============================] - 233s 344ms/step - loss: 10.1512 - accuracy: 0.5468 - val_loss: 10.1765 - val_accuracy: 0.5294\n",
            "Epoch 5/5\n",
            "677/677 [==============================] - 229s 338ms/step - loss: 9.5725 - accuracy: 0.6059 - val_loss: 9.8934 - val_accuracy: 0.5008\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_3[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_roberta_model[1][0]']       \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 279)          143127      ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.48812351543942994\n",
            "Weighted F1: 0.4199760433339229\n",
            "Micro F1: 0.48812351543942994\n",
            "Weighted Precision: 0.4000268489564647\n",
            "Micro Precision: 0.48812351543942994\n",
            "Weighted Recall: 0.48812351543942994\n",
            "Micro Recall: 0.48812351543942994\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_1[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_roberta_model[0][0]']       \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 279)          143127      ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[209  63 216 ... 101  26   4]\n",
            "[   1    2    3 ... 8413 8414 8417]\n",
            "[6622   35 6713 4989 8233 7444 5061 3942 8344 5464  317 4515 3001 6991\n",
            " 2875 1125 3885 6647 4857 4504  214 8340 2969 3847 4609 1346 5993 7358\n",
            " 2752 7651 8188 7073 4782 6069 2346 6483 1663 4952 5072 1967 4071 7590\n",
            "  860 6418 5030 7297  626 5463 6029 6302 7256 5667 1235 3993 1323 4263\n",
            " 3835 2624 5801 4293 8129 5866 5777 6059 4837 3843 1418 5409 5274 6205\n",
            " 2356 5997 6685 3444 3310 7721 5928 2360 7197 4252 8343  909 4354 4752\n",
            "  649 1916 3488 2496 5765 6612 4004 4052 1403 6697 5268 6293 6380 4602\n",
            " 6057 1851 4577 4236 7978 5083 6552  361 7706 4186 4452 4396 8351 3834\n",
            "  812 5108  463 6600 8264 8064 7121 8279 5556 5119 7947 3350 7152 5199\n",
            " 4282 1092 5025 5051  933 1927 3615 1221 8062 1489 7442 5459 2031 7351\n",
            "  234 8015 7416 7921 6574 6257 8396  941 4520 3300 5640  240 7810 7425\n",
            " 1321 6580  928 1170 5404 8273 6742   94    9 7861 7975 1018 2140 1415\n",
            " 7061 7767 4990 8124 5927 6051 3269 6916 5959 2846 6818 7052 7775 7582\n",
            " 8116 4240 4767 5248 1812 2175 6055 8331  998 1564 7326 1559  292 3065\n",
            " 6764 6854 4915 5552 3583 5902 4229 2144 7565 3712 7224 4502 1406 1651\n",
            " 6256 3827 6525 6318 2686 3616 5369 8235 1704 1011 3351 8089 8021  102\n",
            "  224 5671 8294 7707 2985 6624 5146 7381 3709 6824 6308 2281 6837 5436\n",
            " 1741 3520 2649 4886  966 5476 1199 8387 5449 1028 2226 6166 4387  350\n",
            " 6123 5236 5041  734 3412 6085  587  489 5760 4982 5115 5575 3839 6743\n",
            " 7173 5724 7689 7787 8302 3150 2247 8128 6660 3505 4610 2866 4161 6905\n",
            " 7096  768 1062 4411 6087 2096 4903 7546 6630 2928 4640 7125 1792 2063\n",
            " 1700 7928 8095  737 5849  922 2359 6015 4489 4361 6081 6346 5014 5167\n",
            " 1023 8009 3335  268 4777 5221 4323 6168  711 5702 6129 7181 1860 7192\n",
            " 1347 5467 6579   67 2693 6134 4841 4270 4611 2943 6746 7696 3466 3950\n",
            " 6125 8370 6142 6131  995 5548 1721 5067 4015 7092 3156   25 1749 5554\n",
            " 4878 8248 6836 5310 2698 6960 2339 1650 6227 4104 5130 7089 6993 2400\n",
            " 1289 4305 5585 7933 6284 7057 3807 5790 8347 5280 2841 7220 1384 7693\n",
            " 8090 4171 5260  133 7451 3957 1719 7379 4743 4746 5435 6216 4858 4417\n",
            " 8355 4492 2648 5104 5278  743 7308  158 7543 2168 5913 7111 3414 7592\n",
            "   39 7157 5647 4555 3956 3617 4630 4384 6596   96 4539 6749 7202 2413\n",
            " 4749 5122 5052 5929  252 5565  336 6008 4564 5580 7237 1466 1568 5958\n",
            " 7657 4327 5091 6918 6487 4082 2989 7448 4715 3382  467 3784 4624 4419\n",
            " 1117 7954 7122 6392 3174 1534 4164 3659 7526  498 5944 4572 2307 2929\n",
            " 7480 4412  731 4563 7692 1869 2472 6966 4238 6549 1594 2276 7269 2545\n",
            " 2216  605 6587 6315 5297 5282 1619 5504 6399 3917 6342 5754 5721 7385\n",
            " 7909 6126 7382 8382 4383 5502 7130  485 4936 7674 5372 3893 7109 4605\n",
            " 5211 5462 8065   50 4080 3334 8167 8378 5595 1873 8207 7076 4106 3719\n",
            " 4627  395  231 2405 6973 3480 3246 5414 5931 8208 4825 6203 3019 5822\n",
            "   63 5854 6543 5787 4764 6701 6562 3822 4861 3051 1315 8318 1089 8169\n",
            " 8218 7549  707 1284 1076 2465 3815 4636   80 7025 7127 4026 2777 4961\n",
            " 7531  744 6640 6684 4297 7939 7384  446 3287 6675 7752 5279 8196 3020\n",
            " 1203 4192 4253 2887 2556 5729 1898 3208 5973 7033 1431 5915 8312 5799\n",
            " 7105  511 6404 2852 1561 4150 1178]\n",
            "Mon Jun  6 15:30:31 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   44C    P0    41W / 300W |  15237MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "677/677 [==============================] - 249s 345ms/step - loss: 14.3486 - accuracy: 0.0525 - val_loss: 12.8784 - val_accuracy: 0.2185\n",
            "Epoch 2/5\n",
            "677/677 [==============================] - 233s 344ms/step - loss: 12.1144 - accuracy: 0.2982 - val_loss: 11.2150 - val_accuracy: 0.3983\n",
            "Epoch 3/5\n",
            "677/677 [==============================] - 234s 345ms/step - loss: 10.8231 - accuracy: 0.4318 - val_loss: 10.4172 - val_accuracy: 0.4756\n",
            "Epoch 4/5\n",
            "677/677 [==============================] - 233s 344ms/step - loss: 9.9876 - accuracy: 0.5238 - val_loss: 9.8946 - val_accuracy: 0.5076\n",
            "Epoch 5/5\n",
            "677/677 [==============================] - 234s 346ms/step - loss: 9.3466 - accuracy: 0.5917 - val_loss: 9.5501 - val_accuracy: 0.5143\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_3[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_roberta_model[1][0]']       \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 279)          143127      ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.4845605700712589\n",
            "Weighted F1: 0.4240655264054055\n",
            "Micro F1: 0.4845605700712589\n",
            "Weighted Precision: 0.40587272369646327\n",
            "Micro Precision: 0.4845605700712589\n",
            "Weighted Recall: 0.4845605700712589\n",
            "Micro Recall: 0.4845605700712589\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_1[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_roberta_model[0][0]']       \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 279)          143127      ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[209  63 216 ... 101  26   4]\n",
            "[   1    2    3 ... 8413 8414 8417]\n",
            "[6622   35 6713 4989 8233 7444 5061 3942 8344 5464  317 4515 3001 6991\n",
            " 2875 1125 3885 6647 4857 4504  214 8340 2969 3847 4609 1346 5993 7358\n",
            " 2752 7651 8188 7073 4782 6069 2346 6483 1663 4952 5072 1967 4071 7590\n",
            "  860 6418 5030 7297  626 5463 6029 6302 7256 5667 1235 3993 1323 4263\n",
            " 3835 2624 5801 4293 8129 5866 5777 6059 4837 3843 1418 5409 5274 6205\n",
            " 2356 5997 6685 3444 3310 7721 5928 2360 7197 4252 8343  909 4354 4752\n",
            "  649 1916 3488 2496 5765 6612 4004 4052 1403 6697 5268 6293 6380 4602\n",
            " 6057 1851 4577 4236 7978 5083 6552  361 7706 4186 4452 4396 8351 3834\n",
            "  812 5108  463 6600 8264 8064 7121 8279 5556 5119 7947 3350 7152 5199\n",
            " 4282 1092 5025 5051  933 1927 3615 1221 8062 1489 7442 5459 2031 7351\n",
            "  234 8015 7416 7921 6574 6257 8396  941 4520 3300 5640  240 7810 7425\n",
            " 1321 6580  928 1170 5404 8273 6742   94    9 7861 7975 1018 2140 1415\n",
            " 7061 7767 4990 8124 5927 6051 3269 6916 5959 2846 6818 7052 7775 7582\n",
            " 8116 4240 4767 5248 1812 2175 6055 8331  998 1564 7326 1559  292 3065\n",
            " 6764 6854 4915 5552 3583 5902 4229 2144 7565 3712 7224 4502 1406 1651\n",
            " 6256 3827 6525 6318 2686 3616 5369 8235 1704 1011 3351 8089 8021  102\n",
            "  224 5671 8294 7707 2985 6624 5146 7381 3709 6824 6308 2281 6837 5436\n",
            " 1741 3520 2649 4886  966 5476 1199 8387 5449 1028 2226 6166 4387  350\n",
            " 6123 5236 5041  734 3412 6085  587  489 5760 4982 5115 5575 3839 6743\n",
            " 7173 5724 7689 7787 8302 3150 2247 8128 6660 3505 4610 2866 4161 6905\n",
            " 7096  768 1062 4411 6087 2096 4903 7546 6630 2928 4640 7125 1792 2063\n",
            " 1700 7928 8095  737 5849  922 2359 6015 4489 4361 6081 6346 5014 5167\n",
            " 1023 8009 3335  268 4777 5221 4323 6168  711 5702 6129 7181 1860 7192\n",
            " 1347 5467 6579   67 2693 6134 4841 4270 4611 2943 6746 7696 3466 3950\n",
            " 6125 8370 6142 6131  995 5548 1721 5067 4015 7092 3156   25 1749 5554\n",
            " 4878 8248 6836 5310 2698 6960 2339 1650 6227 4104 5130 7089 6993 2400\n",
            " 1289 4305 5585 7933 6284 7057 3807 5790 8347 5280 2841 7220 1384 7693\n",
            " 8090 4171 5260  133 7451 3957 1719 7379 4743 4746 5435 6216 4858 4417\n",
            " 8355 4492 2648 5104 5278  743 7308  158 7543 2168 5913 7111 3414 7592\n",
            "   39 7157 5647 4555 3956 3617 4630 4384 6596   96 4539 6749 7202 2413\n",
            " 4749 5122 5052 5929  252 5565  336 6008 4564 5580 7237 1466 1568 5958\n",
            " 7657 4327 5091 6918 6487 4082 2989 7448 4715 3382  467 3784 4624 4419\n",
            " 1117 7954 7122 6392 3174 1534 4164 3659 7526  498 5944 4572 2307 2929\n",
            " 7480 4412  731 4563 7692 1869 2472 6966 4238 6549 1594 2276 7269 2545\n",
            " 2216  605 6587 6315 5297 5282 1619 5504 6399 3917 6342 5754 5721 7385\n",
            " 7909 6126 7382 8382 4383 5502 7130  485 4936 7674 5372 3893 7109 4605\n",
            " 5211 5462 8065   50 4080 3334 8167 8378 5595 1873 8207 7076 4106 3719\n",
            " 4627  395  231 2405 6973 3480 3246 5414 5931 8208 4825 6203 3019 5822\n",
            "   63 5854 6543 5787 4764 6701 6562 3822 4861 3051 1315 8318 1089 8169\n",
            " 8218 7549  707 1284 1076 2465 3815 4636   80 7025 7127 4026 2777 4961\n",
            " 7531  744 6640 6684 4297 7939 7384  446 3287 6675 7752 5279 8196 3020\n",
            " 1203 4192 4253 2887 2556 5729 1898 3208 5973 7033 1431 5915 8312 5799\n",
            " 7105  511 6404 2852 1561 4150 1178]\n",
            "Mon Jun  6 15:52:52 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P0    39W / 300W |  15237MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "677/677 [==============================] - 249s 345ms/step - loss: 14.1857 - accuracy: 0.1018 - val_loss: 12.6743 - val_accuracy: 0.2924\n",
            "Epoch 2/5\n",
            "677/677 [==============================] - 237s 350ms/step - loss: 12.0791 - accuracy: 0.3327 - val_loss: 11.3665 - val_accuracy: 0.4151\n",
            "Epoch 3/5\n",
            "677/677 [==============================] - 234s 345ms/step - loss: 10.9622 - accuracy: 0.4549 - val_loss: 10.6672 - val_accuracy: 0.4941\n",
            "Epoch 4/5\n",
            "677/677 [==============================] - 230s 340ms/step - loss: 10.1695 - accuracy: 0.5259 - val_loss: 10.2166 - val_accuracy: 0.4891\n",
            "Epoch 5/5\n",
            "677/677 [==============================] - 234s 345ms/step - loss: 9.5290 - accuracy: 0.5998 - val_loss: 9.8787 - val_accuracy: 0.5025\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_3[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_roberta_model[1][0]']       \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 279)          143127      ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.47624703087885983\n",
            "Weighted F1: 0.4176437911375501\n",
            "Micro F1: 0.47624703087885983\n",
            "Weighted Precision: 0.41143939052903217\n",
            "Micro Precision: 0.47624703087885983\n",
            "Weighted Recall: 0.47624703087885983\n",
            "Micro Recall: 0.47624703087885983\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_1[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_roberta_model[0][0]']       \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 279)          143127      ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[209  63 216 ... 101  26   4]\n",
            "[   1    2    3 ... 8413 8414 8417]\n",
            "[6622   35 6713 4989 8233 7444 5061 3942 8344 5464  317 4515 3001 6991\n",
            " 2875 1125 3885 6647 4857 4504  214 8340 2969 3847 4609 1346 5993 7358\n",
            " 2752 7651 8188 7073 4782 6069 2346 6483 1663 4952 5072 1967 4071 7590\n",
            "  860 6418 5030 7297  626 5463 6029 6302 7256 5667 1235 3993 1323 4263\n",
            " 3835 2624 5801 4293 8129 5866 5777 6059 4837 3843 1418 5409 5274 6205\n",
            " 2356 5997 6685 3444 3310 7721 5928 2360 7197 4252 8343  909 4354 4752\n",
            "  649 1916 3488 2496 5765 6612 4004 4052 1403 6697 5268 6293 6380 4602\n",
            " 6057 1851 4577 4236 7978 5083 6552  361 7706 4186 4452 4396 8351 3834\n",
            "  812 5108  463 6600 8264 8064 7121 8279 5556 5119 7947 3350 7152 5199\n",
            " 4282 1092 5025 5051  933 1927 3615 1221 8062 1489 7442 5459 2031 7351\n",
            "  234 8015 7416 7921 6574 6257 8396  941 4520 3300 5640  240 7810 7425\n",
            " 1321 6580  928 1170 5404 8273 6742   94    9 7861 7975 1018 2140 1415\n",
            " 7061 7767 4990 8124 5927 6051 3269 6916 5959 2846 6818 7052 7775 7582\n",
            " 8116 4240 4767 5248 1812 2175 6055 8331  998 1564 7326 1559  292 3065\n",
            " 6764 6854 4915 5552 3583 5902 4229 2144 7565 3712 7224 4502 1406 1651\n",
            " 6256 3827 6525 6318 2686 3616 5369 8235 1704 1011 3351 8089 8021  102\n",
            "  224 5671 8294 7707 2985 6624 5146 7381 3709 6824 6308 2281 6837 5436\n",
            " 1741 3520 2649 4886  966 5476 1199 8387 5449 1028 2226 6166 4387  350\n",
            " 6123 5236 5041  734 3412 6085  587  489 5760 4982 5115 5575 3839 6743\n",
            " 7173 5724 7689 7787 8302 3150 2247 8128 6660 3505 4610 2866 4161 6905\n",
            " 7096  768 1062 4411 6087 2096 4903 7546 6630 2928 4640 7125 1792 2063\n",
            " 1700 7928 8095  737 5849  922 2359 6015 4489 4361 6081 6346 5014 5167\n",
            " 1023 8009 3335  268 4777 5221 4323 6168  711 5702 6129 7181 1860 7192\n",
            " 1347 5467 6579   67 2693 6134 4841 4270 4611 2943 6746 7696 3466 3950\n",
            " 6125 8370 6142 6131  995 5548 1721 5067 4015 7092 3156   25 1749 5554\n",
            " 4878 8248 6836 5310 2698 6960 2339 1650 6227 4104 5130 7089 6993 2400\n",
            " 1289 4305 5585 7933 6284 7057 3807 5790 8347 5280 2841 7220 1384 7693\n",
            " 8090 4171 5260  133 7451 3957 1719 7379 4743 4746 5435 6216 4858 4417\n",
            " 8355 4492 2648 5104 5278  743 7308  158 7543 2168 5913 7111 3414 7592\n",
            "   39 7157 5647 4555 3956 3617 4630 4384 6596   96 4539 6749 7202 2413\n",
            " 4749 5122 5052 5929  252 5565  336 6008 4564 5580 7237 1466 1568 5958\n",
            " 7657 4327 5091 6918 6487 4082 2989 7448 4715 3382  467 3784 4624 4419\n",
            " 1117 7954 7122 6392 3174 1534 4164 3659 7526  498 5944 4572 2307 2929\n",
            " 7480 4412  731 4563 7692 1869 2472 6966 4238 6549 1594 2276 7269 2545\n",
            " 2216  605 6587 6315 5297 5282 1619 5504 6399 3917 6342 5754 5721 7385\n",
            " 7909 6126 7382 8382 4383 5502 7130  485 4936 7674 5372 3893 7109 4605\n",
            " 5211 5462 8065   50 4080 3334 8167 8378 5595 1873 8207 7076 4106 3719\n",
            " 4627  395  231 2405 6973 3480 3246 5414 5931 8208 4825 6203 3019 5822\n",
            "   63 5854 6543 5787 4764 6701 6562 3822 4861 3051 1315 8318 1089 8169\n",
            " 8218 7549  707 1284 1076 2465 3815 4636   80 7025 7127 4026 2777 4961\n",
            " 7531  744 6640 6684 4297 7939 7384  446 3287 6675 7752 5279 8196 3020\n",
            " 1203 4192 4253 2887 2556 5729 1898 3208 5973 7033 1431 5915 8312 5799\n",
            " 7105  511 6404 2852 1561 4150 1178]\n",
            "Mon Jun  6 16:14:19 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   45C    P0    41W / 300W |  15237MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "677/677 [==============================] - 250s 345ms/step - loss: 14.3093 - accuracy: 0.0830 - val_loss: 12.7144 - val_accuracy: 0.2756\n",
            "Epoch 2/5\n",
            "677/677 [==============================] - 234s 345ms/step - loss: 12.0190 - accuracy: 0.3410 - val_loss: 11.2585 - val_accuracy: 0.4403\n",
            "Epoch 3/5\n",
            "677/677 [==============================] - 234s 346ms/step - loss: 10.8248 - accuracy: 0.4616 - val_loss: 10.5048 - val_accuracy: 0.4689\n",
            "Epoch 4/5\n",
            "677/677 [==============================] - 234s 345ms/step - loss: 9.9979 - accuracy: 0.5381 - val_loss: 10.0823 - val_accuracy: 0.4958\n",
            "Epoch 5/5\n",
            "677/677 [==============================] - 234s 345ms/step - loss: 9.3913 - accuracy: 0.6096 - val_loss: 9.6972 - val_accuracy: 0.5126\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_3[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_roberta_model[1][0]']       \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 279)          143127      ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.4833729216152019\n",
            "Weighted F1: 0.42229699513377117\n",
            "Micro F1: 0.4833729216152019\n",
            "Weighted Precision: 0.40418431707303454\n",
            "Micro Precision: 0.4833729216152019\n",
            "Weighted Recall: 0.4833729216152019\n",
            "Micro Recall: 0.4833729216152019\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_1[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_roberta_model[0][0]']       \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 279)          143127      ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[209  63 216 ... 101  26   4]\n",
            "[   1    2    3 ... 8413 8414 8417]\n",
            "[6622   35 6713 4989 8233 7444 5061 3942 8344 5464  317 4515 3001 6991\n",
            " 2875 1125 3885 6647 4857 4504  214 8340 2969 3847 4609 1346 5993 7358\n",
            " 2752 7651 8188 7073 4782 6069 2346 6483 1663 4952 5072 1967 4071 7590\n",
            "  860 6418 5030 7297  626 5463 6029 6302 7256 5667 1235 3993 1323 4263\n",
            " 3835 2624 5801 4293 8129 5866 5777 6059 4837 3843 1418 5409 5274 6205\n",
            " 2356 5997 6685 3444 3310 7721 5928 2360 7197 4252 8343  909 4354 4752\n",
            "  649 1916 3488 2496 5765 6612 4004 4052 1403 6697 5268 6293 6380 4602\n",
            " 6057 1851 4577 4236 7978 5083 6552  361 7706 4186 4452 4396 8351 3834\n",
            "  812 5108  463 6600 8264 8064 7121 8279 5556 5119 7947 3350 7152 5199\n",
            " 4282 1092 5025 5051  933 1927 3615 1221 8062 1489 7442 5459 2031 7351\n",
            "  234 8015 7416 7921 6574 6257 8396  941 4520 3300 5640  240 7810 7425\n",
            " 1321 6580  928 1170 5404 8273 6742   94    9 7861 7975 1018 2140 1415\n",
            " 7061 7767 4990 8124 5927 6051 3269 6916 5959 2846 6818 7052 7775 7582\n",
            " 8116 4240 4767 5248 1812 2175 6055 8331  998 1564 7326 1559  292 3065\n",
            " 6764 6854 4915 5552 3583 5902 4229 2144 7565 3712 7224 4502 1406 1651\n",
            " 6256 3827 6525 6318 2686 3616 5369 8235 1704 1011 3351 8089 8021  102\n",
            "  224 5671 8294 7707 2985 6624 5146 7381 3709 6824 6308 2281 6837 5436\n",
            " 1741 3520 2649 4886  966 5476 1199 8387 5449 1028 2226 6166 4387  350\n",
            " 6123 5236 5041  734 3412 6085  587  489 5760 4982 5115 5575 3839 6743\n",
            " 7173 5724 7689 7787 8302 3150 2247 8128 6660 3505 4610 2866 4161 6905\n",
            " 7096  768 1062 4411 6087 2096 4903 7546 6630 2928 4640 7125 1792 2063\n",
            " 1700 7928 8095  737 5849  922 2359 6015 4489 4361 6081 6346 5014 5167\n",
            " 1023 8009 3335  268 4777 5221 4323 6168  711 5702 6129 7181 1860 7192\n",
            " 1347 5467 6579   67 2693 6134 4841 4270 4611 2943 6746 7696 3466 3950\n",
            " 6125 8370 6142 6131  995 5548 1721 5067 4015 7092 3156   25 1749 5554\n",
            " 4878 8248 6836 5310 2698 6960 2339 1650 6227 4104 5130 7089 6993 2400\n",
            " 1289 4305 5585 7933 6284 7057 3807 5790 8347 5280 2841 7220 1384 7693\n",
            " 8090 4171 5260  133 7451 3957 1719 7379 4743 4746 5435 6216 4858 4417\n",
            " 8355 4492 2648 5104 5278  743 7308  158 7543 2168 5913 7111 3414 7592\n",
            "   39 7157 5647 4555 3956 3617 4630 4384 6596   96 4539 6749 7202 2413\n",
            " 4749 5122 5052 5929  252 5565  336 6008 4564 5580 7237 1466 1568 5958\n",
            " 7657 4327 5091 6918 6487 4082 2989 7448 4715 3382  467 3784 4624 4419\n",
            " 1117 7954 7122 6392 3174 1534 4164 3659 7526  498 5944 4572 2307 2929\n",
            " 7480 4412  731 4563 7692 1869 2472 6966 4238 6549 1594 2276 7269 2545\n",
            " 2216  605 6587 6315 5297 5282 1619 5504 6399 3917 6342 5754 5721 7385\n",
            " 7909 6126 7382 8382 4383 5502 7130  485 4936 7674 5372 3893 7109 4605\n",
            " 5211 5462 8065   50 4080 3334 8167 8378 5595 1873 8207 7076 4106 3719\n",
            " 4627  395  231 2405 6973 3480 3246 5414 5931 8208 4825 6203 3019 5822\n",
            "   63 5854 6543 5787 4764 6701 6562 3822 4861 3051 1315 8318 1089 8169\n",
            " 8218 7549  707 1284 1076 2465 3815 4636   80 7025 7127 4026 2777 4961\n",
            " 7531  744 6640 6684 4297 7939 7384  446 3287 6675 7752 5279 8196 3020\n",
            " 1203 4192 4253 2887 2556 5729 1898 3208 5973 7033 1431 5915 8312 5799\n",
            " 7105  511 6404 2852 1561 4150 1178]\n",
            "Mon Jun  6 16:35:47 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   44C    P0    40W / 300W |  15237MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "677/677 [==============================] - 250s 345ms/step - loss: 14.3047 - accuracy: 0.0664 - val_loss: 12.7799 - val_accuracy: 0.2353\n",
            "Epoch 2/5\n",
            "677/677 [==============================] - 234s 345ms/step - loss: 12.1027 - accuracy: 0.3144 - val_loss: 11.3219 - val_accuracy: 0.4067\n",
            "Epoch 3/5\n",
            "677/677 [==============================] - 233s 344ms/step - loss: 10.8518 - accuracy: 0.4519 - val_loss: 10.5352 - val_accuracy: 0.4555\n",
            "Epoch 4/5\n",
            "677/677 [==============================] - 234s 345ms/step - loss: 10.0549 - accuracy: 0.5226 - val_loss: 10.0527 - val_accuracy: 0.5092\n",
            "Epoch 5/5\n",
            "677/677 [==============================] - 233s 344ms/step - loss: 9.4154 - accuracy: 0.5880 - val_loss: 9.6879 - val_accuracy: 0.5176\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_3[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_roberta_model[1][0]']       \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 279)          143127      ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.4809976247030879\n",
            "Weighted F1: 0.42582654598635117\n",
            "Micro F1: 0.4809976247030879\n",
            "Weighted Precision: 0.4085574355911851\n",
            "Micro Precision: 0.4809976247030879\n",
            "Weighted Recall: 0.4809976247030879\n",
            "Micro Recall: 0.4809976247030879\n",
            "Average Accuracy: 0.4826603325415677\n",
            "Average Weighted F1: 0.42196178039940013\n",
            "Average Micro F1: 0.4826603325415677\n",
            "Average Weighted Precision: 0.4060161431692359\n",
            "Average Micro Precision: 0.4826603325415677\n",
            "Average Weighted Recall: 0.4826603325415677\n",
            "Average Micro Recall: 0.4826603325415677\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "total_accuracy=0\n",
        "total_weighted_f1=0\n",
        "total_micro_f1=0\n",
        "total_weighted_precision=0\n",
        "total_micro_precision=0\n",
        "total_weighted_recall=0\n",
        "total_micro_recall=0\n",
        "\n",
        "for f in range(5):\n",
        "  gc.collect()\n",
        "  tf.keras.backend.clear_session()\n",
        "  dbert_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "  dbert_model = TFRobertaModel.from_pretrained('roberta-base')\n",
        "  max_len=512\n",
        "  sentences=summarized_data['text']\n",
        "  labels=summarized_data['label']\n",
        "  filenumbers=summarized_data['filenum']\n",
        "  len(sentences),len(labels),len(filenumbers)\n",
        "  \n",
        "  model_3=create_model()\n",
        "  input_ids=[]\n",
        "  attention_masks=[]\n",
        "\n",
        "  for sent in sentences:\n",
        "    dbert_inps=dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n",
        "    input_ids.append(dbert_inps['input_ids'])\n",
        "    attention_masks.append(dbert_inps['attention_mask'])\n",
        "  input_ids=np.asarray(input_ids)\n",
        "\n",
        "  attention_masks=np.array(attention_masks)\n",
        "  labels=np.array(labels)\n",
        "  filenumbers=[int(i) for i in filenumbers]\n",
        "  filenumbers=np.array(filenumbers)\n",
        "  print(labels)\n",
        "  print(filenumbers)\n",
        "\n",
        "  #train_inp,val_inp,train_label,val_label,train_mask,val_mask,train_fnum,val_fnum=train_test_split(input_ids,labels,attention_masks,filenumbers,test_size=0.1,random_state=42)\n",
        "\n",
        "  new_train_inp_3=[]\n",
        "  new_val_inp_3=[]\n",
        "  new_train_label_3=[]\n",
        "  new_val_label_3=[]\n",
        "  new_train_mask_3=[]\n",
        "  new_train_fnum_3=[]\n",
        "  new_val_fnum_3=[]\n",
        "  new_val_mask_3=[]\n",
        "  for i in range(len(train_fnum)):\n",
        "    if train_fnum[i] in filenumbers:\n",
        "      new_train_inp_3.append(train_inp[i])\n",
        "      #new_val_inp.append(val_inp[i])\n",
        "      new_train_label_3.append(train_label[i])\n",
        "      #new_val_label.append(val_label[i])\n",
        "      new_train_mask_3.append(train_mask[i])\n",
        "      #new_val_mask.append(val_mask[i])\n",
        "      new_train_fnum_3.append(train_fnum[i])\n",
        "\n",
        "  for i in range(len(val_fnum)):\n",
        "    if val_fnum[i] in filenumbers:\n",
        "      #new_train_inp.append(train_inp[i])\n",
        "      new_val_inp_3.append(val_inp[i])\n",
        "      #new_train_label.append(train_label[i])\n",
        "      new_val_label_3.append(val_label[i])\n",
        "      #new_train_mask.append(train_mask[i])\n",
        "      new_val_mask_3.append(val_mask[i])\n",
        "      new_val_fnum_3.append(val_fnum[i])\n",
        "\n",
        "  # print(new_train_label)\n",
        "  # print(len(new_train_label))\n",
        "  # print(new_val_label)\n",
        "  # print(len(new_val_label))\n",
        "  new_train_inp_3=np.array(new_train_inp_3)\n",
        "  new_val_inp_3=np.array(new_val_inp_3)\n",
        "  new_train_label_3=np.array(new_train_label_3)\n",
        "  new_val_label_3=np.array(new_val_label_3)\n",
        "  new_train_mask_3=np.array(new_train_mask_3)\n",
        "  new_train_fnum_3=np.array(new_train_fnum_3)\n",
        "  new_val_fnum_3=np.array(new_val_fnum_3)\n",
        "  new_val_mask_3=np.array(new_val_mask_3)\n",
        "  \n",
        "  print(new_val_fnum_3)\n",
        "  \n",
        "  log_dir='dbert_model'\n",
        "\n",
        "  model_save_path='./drive/MyDrive/Ensemble/roberta-ensemble-512-model_3-'+str(f)+'-279labels.h5'\n",
        "\n",
        "  loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "  accuracy = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
        "  callbacks= [tf.keras.callbacks.ModelCheckpoint(filepath=model_save_path,monitor='val_accuracy',mode='max',save_best_only=True,save_weights_only=True),keras.callbacks.TensorBoard(log_dir=log_dir)]\n",
        "  model_3.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])\n",
        "  gpu_info = !nvidia-smi\n",
        "  gpu_info = '\\n'.join(gpu_info)\n",
        "  if gpu_info.find('failed') >= 0:\n",
        "    print('Not connected to a GPU')\n",
        "  else:\n",
        "    print(gpu_info)\n",
        "  \n",
        "  history=model_3.fit([new_train_inp_3,new_train_mask_3],new_train_label_3,batch_size=8,epochs=5,validation_data=([new_val_inp_3,new_val_mask_3],new_val_label_3),callbacks=callbacks)\n",
        "\n",
        "  pred_labels=[]\n",
        "\n",
        "  model_saved_3= create_model()\n",
        "  model_saved_3.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])\n",
        "  model_saved_3.load_weights('./drive/MyDrive/Ensemble/roberta-ensemble-512-model_3-'+str(f)+'-279labels.h5')\n",
        "\n",
        "  for i in range(0,len(val_inp)):\n",
        "    pred=model_saved_3.predict([val_inp[i].reshape(1,512),val_mask[i].reshape(1,512)])\n",
        "    pred_label = pred.argmax(axis=1)\n",
        "    pred_labels.append(pred_label)\n",
        "  accuracy=accuracy_score(val_label, pred_labels)\n",
        "  print(\"Accuracy: \"+str(accuracy))\n",
        "  total_accuracy=total_accuracy+accuracy\n",
        "  \n",
        "  weighted_f1=f1_score(val_label,pred_labels, average='weighted')\n",
        "  print(\"Weighted F1: \"+ str(weighted_f1))\n",
        "  total_weighted_f1=total_weighted_f1+weighted_f1\n",
        "  micro_f1=f1_score(val_label,pred_labels, average='micro')\n",
        "  print(\"Micro F1: \"+ str(micro_f1))\n",
        "  total_micro_f1=total_micro_f1+micro_f1\n",
        "\n",
        "  weighted_precision=precision_score(val_label, pred_labels, average='weighted')\n",
        "  print(\"Weighted Precision: \" + str(weighted_precision))\n",
        "  total_weighted_precision=total_weighted_precision+weighted_precision\n",
        "  micro_precision=precision_score(val_label, pred_labels, average='micro')\n",
        "  print(\"Micro Precision: \" + str(micro_precision))\n",
        "  total_micro_precision=total_micro_precision+micro_precision\n",
        "\n",
        "  weighted_recall=recall_score(val_label, pred_labels, average='weighted')\n",
        "  print(\"Weighted Recall: \" + str(weighted_recall))\n",
        "  total_weighted_recall=total_weighted_recall+weighted_recall\n",
        "  micro_recall=recall_score(val_label, pred_labels, average='micro')\n",
        "  print(\"Micro Recall: \" + str(micro_recall))\n",
        "  total_micro_recall=total_micro_recall+micro_recall\n",
        "\n",
        "\n",
        "print(\"Average Accuracy: \"+str(total_accuracy/5))\n",
        "print(\"Average Weighted F1: \"+str(total_weighted_f1/5))\n",
        "print(\"Average Micro F1: \"+str(total_micro_f1/5))\n",
        "print(\"Average Weighted Precision: \"+str(total_weighted_precision/5))\n",
        "print(\"Average Micro Precision: \"+str(total_micro_precision/5))\n",
        "print(\"Average Weighted Recall: \"+str(total_weighted_recall/5))\n",
        "print(\"Average Micro Recall: \"+str(total_micro_recall/5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2gkm41KOvo3",
        "outputId": "4135745e-5e27-4367-891e-b3f7049b538d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5275\n",
            "5275\n",
            "Average Length 511.7759241706161\n",
            "['209', '63', '216', '108', '196', '200', '203', '246', '188', '193', '25', '106', '56', '53', '223', '161', '200', '208', '78', '147', '147', '179', '147', '80', '56', '209', '209', '200', '214', '202', '203', '194', '26', '237', '165', '258', '238', '238', '76', '193', '196', '213', '161', '213', '209', '194', '196', '25', '151', '260', '135', '173', '30', '108', '272', '259', '176', '213', '213', '21', '5', '171', '200', '195', '240', '194', '176', '240', '240', '80', '173', '98', '214', '21', '76', '76', '73', '173', '173', '78', '61', '258', '191', '3', '78', '173', '193', '118', '200', '264', '260', '49', '211', '258', '18', '173', '78', '3', '3', '1', '213', '135', '182', '239', '208', '278', '50', '194', '200', '124', '189', '117', '264', '244', '76', '193', '221', '78', '78', '173', '197', '78', '161', '231', '135', '22', '78', '25', '25', '197', '200', '147', '203', '191', '196', '193', '263', '32', '197', '191', '4', '56', '196', '104', '191', '196', '173', '147', '194', '96', '161', '161', '238', '200', '3', '78', '78', '53', '41', '237', '53', '18', '10', '194', '97', '96', '208', '25', '10', '117', '104', '54', '96', '194', '21', '245', '238', '202', '209', '161', '97', '258', '240', '213', '136', '155', '152', '21', '191', '191', '118', '68', '258', '189', '197', '208', '61', '78', '135', '202', '96', '155', '108', '272', '80', '25', '155', '35', '213', '191', '194', '96', '96', '191', '264', '191', '191', '191', '2', '258', '191', '173', '259', '15', '47', '258', '156', '118', '160', '115', '105', '26', '173', '192', '76', '76', '135', '240', '118', '161', '200', '230', '18', '200', '25', '10', '161', '244', '24', '24', '200', '205', '104', '15', '193', '156', '95', '209', '240', '191', '194', '128', '208', '208', '218', '173', '10', '25', '18', '107', '46', '32', '197', '172', '209', '213', '79', '121', '78', '78', '193', '2', '78', '194', '149', '136', '80', '3', '21', '200', '78', '21', '197', '194', '118', '202', '161', '180', '209', '173', '2', '193', '6', '260', '197', '78', '78', '42', '191', '213', '135', '193', '196', '202', '222', '217', '42', '259', '213', '76', '46', '30', '15', '260', '202', '18', '259', '104', '135', '76', '196', '240', '240', '240', '108', '24', '200', '197', '262', '176', '15', '156', '264', '156', '200', '191', '161', '196', '78', '238', '161', '221', '118', '14', '14', '14', '200', '278', '238', '200', '196', '78', '173', '25', '197', '197', '196', '161', '18', '10', '10', '10', '18', '66', '203', '19', '246', '196', '244', '240', '161', '200', '78', '240', '209', '208', '118', '215', '194', '193', '168', '229', '191', '135', '172', '196', '197', '161', '78', '196', '197', '258', '78', '78', '238', '221', '121', '264', '104', '146', '50', '78', '217', '11', '238', '238', '238', '3', '3', '154', '215', '79', '18', '158', '104', '18', '13', '197', '117', '197', '161', '213', '242', '32', '238', '230', '2', '2', '32', '161', '251', '96', '156', '242', '22', '22', '140', '3', '149', '135', '191', '173', '135', '200', '238', '238', '78', '156', '209', '78', '97', '97', '160', '25', '258', '240', '161', '15', '161', '96', '209', '80', '132', '140', '4', '1', '240', '196', '274', '272', '62', '80', '196', '196', '191', '79', '156', '15', '196', '209', '22', '251', '202', '202', '121', '73', '191', '238', '191', '150', '150', '135', '46', '203', '202', '238', '22', '3', '258', '239', '238', '238', '165', '238', '238', '194', '200', '200', '197', '200', '196', '191', '194', '76', '78', '208', '200', '71', '208', '240', '76', '117', '120', '108', '96', '144', '30', '133', '133', '202', '117', '161', '251', '29', '104', '242', '197', '118', '216', '262', '236', '141', '11', '235', '235', '133', '191', '238', '202', '80', '80', '22', '140', '191', '197', '202', '133', '184', '184', '184', '184', '143', '274', '50', '237', '196', '274', '265', '10', '18', '10', '121', '238', '245', '228', '76', '25', '191', '197', '268', '10', '240', '195', '196', '195', '15', '48', '200', '183', '70', '59', '197', '196', '235', '161', '161', '215', '104', '97', '191', '194', '191', '200', '200', '197', '76', '200', '93', '256', '160', '238', '146', '238', '239', '15', '140', '258', '140', '21', '79', '202', '258', '79', '62', '205', '161', '104', '10', '161', '196', '137', '152', '191', '117', '213', '242', '176', '218', '195', '191', '162', '191', '135', '241', '213', '213', '84', '79', '68', '18', '98', '197', '155', '41', '246', '269', '78', '209', '238', '80', '238', '62', '237', '273', '273', '230', '150', '121', '260', '244', '161', '202', '78', '109', '155', '264', '145', '50', '274', '173', '210', '258', '238', '211', '156', '215', '195', '177', '244', '3', '141', '196', '191', '194', '29', '30', '10', '16', '200', '200', '156', '53', '3', '62', '165', '173', '173', '22', '205', '274', '150', '183', '116', '188', '188', '98', '161', '204', '80', '172', '156', '104', '229', '259', '78', '169', '3', '200', '118', '161', '80', '260', '240', '150', '197', '196', '62', '93', '6', '272', '272', '197', '160', '78', '96', '70', '195', '191', '191', '80', '3', '3', '240', '147', '196', '191', '238', '96', '203', '3', '15', '10', '58', '55', '96', '41', '15', '26', '50', '238', '194', '79', '240', '161', '191', '195', '263', '50', '240', '147', '235', '195', '203', '59', '178', '176', '2', '15', '203', '191', '18', '278', '196', '14', '153', '58', '175', '18', '217', '217', '80', '191', '200', '18', '200', '173', '24', '171', '171', '210', '124', '34', '200', '218', '104', '4', '223', '193', '194', '200', '200', '13', '62', '194', '197', '80', '197', '146', '146', '96', '97', '97', '2', '196', '191', '191', '104', '238', '10', '200', '239', '135', '104', '213', '2', '221', '169', '25', '21', '194', '161', '21', '278', '3', '195', '46', '176', '25', '117', '191', '62', '62', '62', '78', '50', '264', '79', '197', '80', '80', '80', '191', '191', '78', '161', '194', '195', '197', '203', '259', '278', '49', '147', '147', '147', '147', '22', '78', '78', '258', '2', '202', '221', '55', '258', '104', '104', '84', '240', '221', '274', '104', '22', '241', '63', '197', '197', '196', '159', '200', '22', '22', '22', '78', '78', '173', '104', '97', '104', '191', '146', '223', '268', '96', '14', '173', '204', '241', '97', '15', '118', '78', '78', '196', '194', '258', '96', '96', '259', '75', '10', '197', '30', '2', '238', '121', '50', '161', '173', '173', '186', '185', '186', '78', '194', '221', '218', '35', '33', '13', '34', '78', '21', '215', '97', '60', '24', '78', '193', '173', '235', '78', '200', '258', '140', '259', '194', '197', '195', '200', '145', '76', '108', '130', '238', '215', '213', '2', '217', '121', '178', '140', '62', '156', '176', '196', '196', '41', '223', '147', '172', '79', '200', '2', '244', '244', '191', '213', '104', '191', '237', '237', '33', '117', '195', '195', '264', '195', '140', '210', '41', '197', '14', '257', '196', '238', '238', '79', '161', '156', '196', '238', '238', '213', '104', '3', '10', '118', '204', '235', '33', '259', '274', '259', '176', '78', '78', '196', '34', '52', '25', '30', '194', '153', '21', '70', '191', '18', '191', '191', '172', '194', '194', '194', '194', '148', '104', '125', '2', '2', '2', '185', '194', '29', '104', '78', '189', '272', '204', '183', '213', '78', '104', '197', '240', '97', '168', '168', '182', '238', '197', '18', '194', '229', '221', '62', '78', '53', '104', '214', '189', '189', '189', '213', '117', '117', '191', '29', '104', '104', '276', '276', '22', '195', '195', '155', '139', '139', '59', '168', '135', '140', '79', '146', '200', '245', '70', '160', '153', '50', '202', '154', '263', '130', '194', '117', '194', '194', '191', '33', '29', '29', '238', '239', '194', '21', '274', '215', '18', '18', '140', '2', '215', '25', '46', '137', '238', '30', '160', '156', '46', '196', '263', '242', '104', '0', '0', '73', '213', '135', '22', '191', '191', '241', '202', '221', '15', '15', '191', '238', '213', '194', '197', '179', '238', '200', '200', '200', '5', '238', '194', '55', '117', '166', '49', '244', '239', '191', '78', '78', '78', '194', '200', '117', '210', '22', '121', '121', '121', '21', '237', '194', '121', '78', '104', '194', '79', '191', '251', '195', '197', '274', '176', '21', '43', '43', '10', '61', '215', '215', '30', '214', '194', '10', '30', '139', '96', '226', '226', '265', '2', '2', '121', '121', '41', '237', '104', '238', '238', '161', '78', '78', '78', '11', '184', '140', '140', '161', '104', '104', '237', '130', '195', '160', '118', '161', '18', '11', '241', '184', '196', '178', '22', '30', '146', '146', '10', '10', '141', '141', '135', '18', '18', '10', '26', '208', '97', '141', '18', '18', '97', '194', '194', '79', '213', '205', '205', '205', '75', '241', '50', '226', '238', '97', '222', '215', '215', '139', '238', '51', '238', '96', '173', '109', '200', '117', '192', '2', '195', '10', '3', '18', '49', '238', '238', '191', '195', '52', '197', '200', '200', '79', '191', '200', '200', '30', '195', '197', '195', '25', '260', '73', '30', '193', '21', '200', '242', '203', '120', '78', '260', '241', '0', '30', '202', '195', '30', '191', '78', '194', '194', '2', '205', '191', '196', '173', '197', '30', '196', '172', '139', '2', '18', '191', '230', '56', '194', '215', '195', '2', '71', '13', '146', '264', '50', '191', '235', '93', '191', '139', '139', '227', '235', '186', '235', '24', '18', '261', '128', '33', '71', '25', '176', '39', '10', '195', '202', '29', '29', '221', '29', '197', '139', '78', '227', '146', '137', '195', '205', '205', '195', '135', '62', '153', '146', '29', '75', '0', '194', '194', '180', '213', '78', '18', '194', '239', '196', '239', '197', '194', '153', '192', '221', '10', '46', '50', '241', '117', '117', '117', '195', '173', '78', '195', '194', '239', '2', '12', '173', '197', '30', '269', '197', '176', '135', '30', '146', '191', '93', '93', '135', '260', '218', '156', '155', '200', '241', '125', '18', '18', '183', '203', '239', '259', '173', '213', '180', '180', '210', '120', '104', '194', '156', '258', '172', '9', '0', '241', '192', '143', '189', '204', '50', '63', '197', '243', '241', '93', '125', '2', '21', '260', '4', '4', '3', '191', '150', '128', '78', '238', '245', '8', '80', '238', '79', '79', '240', '58', '104', '155', '80', '80', '188', '191', '235', '170', '170', '170', '146', '258', '195', '276', '25', '78', '215', '240', '161', '53', '147', '78', '78', '78', '78', '221', '221', '96', '18', '19', '130', '18', '18', '18', '80', '80', '194', '79', '93', '121', '78', '139', '139', '78', '197', '121', '194', '200', '80', '101', '22', '12', '135', '97', '205', '181', '60', '193', '15', '140', '172', '191', '241', '222', '135', '29', '25', '179', '191', '191', '259', '194', '133', '240', '121', '121', '241', '195', '29', '191', '209', '18', '139', '139', '50', '18', '12', '18', '133', '200', '25', '10', '70', '25', '242', '18', '161', '49', '172', '172', '172', '172', '128', '96', '195', '200', '78', '10', '239', '22', '173', '142', '142', '22', '194', '192', '3', '200', '241', '96', '241', '62', '139', '213', '202', '135', '79', '120', '192', '78', '118', '241', '46', '151', '150', '151', '150', '118', '221', '183', '237', '196', '140', '140', '39', '179', '213', '140', '41', '73', '78', '217', '258', '30', '118', '140', '219', '50', '10', '139', '150', '251', '24', '10', '18', '78', '18', '172', '135', '146', '71', '196', '202', '197', '25', '106', '160', '215', '15', '6', '101', '165', '140', '145', '80', '3', '238', '239', '78', '268', '0', '109', '191', '215', '238', '238', '238', '203', '50', '146', '15', '25', '156', '191', '16', '2', '195', '189', '96', '193', '109', '204', '161', '185', '210', '50', '235', '30', '242', '193', '102', '194', '121', '177', '241', '102', '132', '240', '71', '192', '79', '25', '197', '39', '161', '22', '258', '237', '15', '7', '78', '156', '197', '70', '176', '139', '240', '132', '78', '177', '191', '194', '117', '10', '79', '238', '195', '24', '101', '118', '196', '2', '102', '21', '180', '185', '170', '79', '139', '135', '191', '151', '200', '191', '154', '262', '200', '237', '56', '27', '191', '180', '258', '139', '139', '192', '30', '0', '242', '239', '242', '25', '191', '242', '240', '3', '56', '215', '171', '192', '193', '222', '238', '237', '215', '272', '2', '156', '237', '151', '189', '80', '15', '260', '260', '217', '235', '22', '228', '135', '18', '191', '157', '78', '2', '240', '213', '179', '2', '191', '79', '79', '153', '3', '194', '78', '121', '71', '121', '149', '257', '194', '191', '188', '15', '25', '125', '102', '15', '130', '130', '177', '10', '135', '265', '240', '149', '196', '200', '204', '204', '213', '177', '197', '194', '13', '202', '30', '213', '125', '193', '125', '125', '125', '125', '125', '15', '180', '78', '1', '25', '96', '200', '29', '156', '172', '202', '22', '78', '197', '188', '241', '101', '101', '101', '221', '50', '191', '15', '15', '0', '101', '101', '168', '56', '197', '130', '62', '204', '29', '10', '96', '97', '139', '97', '97', '2', '2', '262', '39', '176', '172', '194', '194', '18', '78', '13', '80', '139', '196', '207', '191', '151', '192', '150', '13', '104', '104', '101', '15', '125', '101', '15', '0', '50', '194', '264', '50', '25', '25', '125', '125', '125', '125', '125', '25', '29', '25', '25', '78', '240', '261', '19', '2', '74', '78', '238', '50', '50', '203', '197', '71', '104', '80', '194', '149', '170', '130', '2', '237', '170', '202', '48', '97', '93', '179', '235', '78', '102', '102', '70', '208', '238', '104', '146', '118', '118', '96', '237', '274', '218', '241', '102', '146', '209', '209', '240', '170', '137', '7', '197', '191', '264', '29', '46', '96', '193', '19', '12', '213', '215', '275', '179', '18', '183', '78', '221', '18', '170', '119', '21', '140', '240', '96', '192', '192', '21', '96', '149', '239', '135', '191', '240', '184', '183', '24', '2', '96', '197', '194', '215', '18', '192', '121', '202', '196', '25', '97', '214', '3', '97', '16', '0', '192', '135', '215', '5', '12', '80', '143', '202', '242', '151', '13', '209', '203', '200', '30', '209', '194', '102', '102', '102', '102', '102', '102', '22', '24', '6', '18', '5', '191', '101', '50', '101', '101', '192', '153', '153', '101', '101', '10', '192', '25', '140', '50', '241', '102', '102', '32', '150', '101', '102', '153', '102', '101', '153', '102', '25', '10', '125', '18', '125', '177', '209', '102', '10', '78', '78', '137', '18', '237', '241', '175', '195', '175', '96', '176', '237', '96', '96', '101', '193', '238', '170', '238', '102', '222', '268', '238', '102', '244', '34', '18', '244', '202', '149', '149', '193', '102', '170', '194', '75', '101', '222', '238', '50', '195', '135', '22', '93', '18', '102', '102', '93', '93', '191', '147', '244', '96', '2', '241', '241', '93', '189', '189', '177', '35', '137', '197', '191', '40', '40', '195', '176', '35', '200', '200', '237', '135', '140', '140', '93', '156', '200', '78', '191', '22', '81', '238', '101', '213', '101', '78', '109', '19', '140', '196', '78', '78', '76', '218', '14', '265', '101', '241', '155', '4', '50', '78', '78', '238', '135', '191', '265', '81', '238', '238', '102', '102', '102', '140', '162', '32', '17', '17', '171', '171', '191', '101', '102', '192', '101', '17', '171', '135', '102', '242', '195', '237', '21', '18', '238', '191', '278', '140', '149', '97', '260', '155', '276', '192', '196', '21', '191', '2', '97', '0', '150', '221', '213', '209', '238', '80', '260', '194', '104', '96', '193', '14', '240', '193', '117', '117', '155', '17', '264', '260', '209', '209', '2', '137', '71', '72', '101', '205', '173', '191', '118', '24', '41', '197', '78', '94', '237', '156', '193', '153', '154', '153', '209', '110', '78', '78', '213', '238', '78', '191', '93', '79', '177', '267', '133', '133', '46', '139', '40', '143', '74', '241', '202', '50', '3', '102', '78', '149', '191', '156', '18', '93', '195', '192', '137', '240', '135', '204', '189', '22', '56', '154', '25', '192', '130', '277', '191', '80', '39', '21', '21', '238', '238', '23', '192', '191', '192', '94', '94', '80', '139', '17', '10', '22', '227', '227', '140', '102', '21', '17', '227', '227', '23', '243', '264', '18', '274', '149', '238', '30', '221', '22', '238', '193', '135', '205', '195', '238', '238', '197', '130', '195', '18', '104', '102', '14', '59', '104', '18', '18', '258', '34', '241', '138', '188', '176', '102', '1', '140', '140', '22', '22', '278', '10', '96', '35', '145', '120', '33', '33', '22', '33', '22', '35', '22', '19', '33', '102', '192', '192', '175', '22', '72', '191', '222', '125', '22', '79', '18', '194', '18', '250', '192', '192', '96', '48', '274', '273', '132', '133', '192', '40', '171', '171', '246', '191', '10', '166', '197', '195', '125', '125', '200', '153', '192', '149', '110', '139', '194', '48', '102', '102', '104', '203', '238', '238', '238', '78', '18', '238', '121', '18', '238', '96', '238', '238', '13', '191', '18', '18', '240', '96', '40', '188', '18', '137', '175', '28', '28', '28', '149', '191', '191', '207', '239', '239', '153', '153', '153', '153', '153', '153', '154', '153', '154', '153', '153', '153', '18', '0', '27', '18', '25', '96', '96', '137', '33', '10', '130', '153', '153', '154', '180', '137', '44', '241', '153', '268', '25', '161', '25', '39', '242', '4', '238', '195', '102', '135', '13', '229', '140', '1', '79', '192', '229', '191', '118', '21', '260', '203', '18', '177', '191', '10', '238', '10', '133', '14', '175', '175', '192', '18', '153', '153', '254', '140', '149', '22', '22', '22', '237', '40', '135', '191', '192', '192', '62', '205', '19', '238', '176', '214', '200', '96', '15', '153', '191', '108', '29', '166', '28', '193', '197', '96', '195', '108', '102', '241', '195', '10', '33', '166', '197', '168', '2', '96', '26', '33', '204', '258', '149', '153', '153', '40', '137', '241', '166', '221', '23', '221', '153', '15', '113', '113', '78', '171', '40', '71', '21', '71', '80', '15', '191', '191', '135', '110', '10', '259', '102', '149', '149', '109', '109', '175', '97', '97', '97', '153', '15', '238', '193', '175', '38', '38', '137', '18', '235', '155', '137', '265', '110', '18', '18', '256', '191', '224', '173', '104', '10', '152', '24', '24', '24', '17', '40', '40', '40', '40', '40', '40', '112', '6', '18', '18', '210', '96', '191', '27', '213', '17', '153', '26', '133', '18', '18', '153', '38', '96', '40', '38', '40', '40', '10', '17', '17', '95', '71', '192', '17', '259', '213', '200', '15', '11', '207', '194', '18', '135', '238', '191', '241', '191', '116', '35', '147', '156', '67', '78', '17', '227', '191', '117', '188', '130', '72', '96', '235', '97', '18', '150', '203', '25', '126', '149', '200', '94', '4', '4', '1', '244', '228', '252', '149', '191', '149', '15', '17', '17', '29', '18', '18', '116', '23', '237', '94', '184', '268', '197', '175', '243', '28', '16', '10', '50', '191', '102', '102', '153', '149', '111', '78', '191', '135', '18', '35', '78', '238', '208', '93', '95', '258', '17', '111', '111', '262', '22', '22', '104', '161', '209', '21', '195', '12', '116', '241', '242', '239', '17', '97', '16', '40', '117', '94', '96', '78', '15', '103', '195', '135', '235', '247', '135', '130', '260', '239', '176', '93', '191', '209', '239', '93', '264', '30', '18', '30', '18', '18', '103', '191', '97', '17', '95', '115', '209', '140', '22', '22', '241', '10', '153', '3', '188', '238', '226', '195', '97', '96', '238', '97', '130', '97', '116', '96', '96', '242', '45', '149', '166', '155', '96', '116', '264', '10', '237', '192', '71', '22', '71', '175', '149', '116', '237', '274', '18', '235', '176', '116', '202', '250', '250', '78', '2', '196', '2', '97', '143', '18', '111', '111', '80', '78', '153', '80', '40', '110', '30', '112', '30', '111', '215', '112', '149', '78', '78', '209', '110', '110', '238', '109', '239', '112', '151', '154', '16', '16', '16', '78', '137', '179', '72', '149', '10', '111', '268', '96', '170', '111', '38', '192', '153', '183', '195', '30', '30', '147', '195', '147', '93', '264', '153', '25', '18', '19', '22', '74', '40', '93', '166', '15', '127', '147', '192', '260', '238', '103', '269', '127', '19', '19', '153', '18', '78', '238', '30', '30', '241', '16', '235', '250', '204', '238', '218', '40', '93', '184', '18', '0', '241', '195', '94', '154', '109', '156', '10', '21', '30', '197', '39', '78', '24', '107', '142', '142', '228', '229', '229', '229', '232', '179', '239', '142', '232', '23', '175', '147', '137', '137', '137', '171', '191', '117', '128', '127', '204', '98', '147', '235', '239', '153', '15', '204', '255', '80', '204', '204', '149', '113', '19', '167', '22', '250', '17', '17', '17', '18', '17', '67', '121', '125', '26', '97', '97', '97', '97', '247', '124', '163', '147', '112', '111', '208', '108', '38', '209', '154', '154', '194', '135', '22', '147', '241', '120', '218', '260', '156', '155', '241', '192', '215', '180', '179', '149', '40', '193', '192', '150', '102', '93', '1', '149', '137', '133', '102', '102', '102', '78', '145', '21', '96', '175', '175', '78', '123', '30', '95', '19', '18', '110', '18', '152', '152', '147', '26', '135', '260', '158', '203', '195', '158', '112', '156', '125', '213', '230', '30', '15', '125', '106', '111', '195', '181', '130', '195', '130', '239', '15', '194', '16', '15', '111', '61', '72', '57', '128', '239', '239', '203', '25', '218', '10', '155', '191', '46', '98', '175', '260', '102', '21', '191', '260', '191', '239', '15', '30', '93', '147', '103', '78', '177', '30', '95', '155', '155', '155', '103', '103', '112', '241', '191', '191', '17', '78', '153', '150', '113', '115', '147', '78', '40', '163', '33', '15', '149', '103', '192', '191', '96', '106', '115', '143', '235', '200', '204', '238', '204', '204', '74', '196', '113', '246', '102', '150', '16', '176', '20', '195', '128', '197', '15', '71', '71', '193', '24', '24', '24', '170', '204', '209', '111', '144', '112', '22', '222', '224', '28', '209', '155', '213', '275', '196', '239', '195', '25', '103', '149', '128', '19', '96', '102', '135', '17', '232', '155', '191', '20', '18', '241', '30', '25', '205', '97', '97', '96', '72', '182', '135', '135', '20', '149', '149', '78', '135', '40', '18', '153', '10', '26', '158', '205', '157', '157', '135', '135', '140', '155', '192', '26', '268', '263', '245', '155', '15', '177', '159', '212', '161', '239', '200', '40', '35', '153', '197', '259', '15', '254', '175', '10', '30', '239', '67', '195', '200', '191', '22', '66', '191', '112', '133', '128', '235', '161', '155', '17', '93', '36', '113', '239', '18', '18', '191', '165', '163', '163', '238', '79', '158', '40', '102', '40', '200', '191', '60', '241', '245', '96', '78', '30', '15', '173', '192', '192', '158', '64', '113', '200', '74', '128', '135', '115', '115', '10', '74', '102', '102', '115', '111', '238', '244', '109', '109', '238', '152', '10', '237', '18', '50', '16', '173', '204', '15', '208', '276', '43', '109', '258', '222', '15', '94', '235', '203', '79', '128', '113', '3', '204', '106', '133', '214', '218', '25', '98', '166', '71', '30', '175', '175', '97', '135', '108', '19', '18', '196', '35', '12', '196', '78', '166', '155', '72', '155', '22', '109', '258', '196', '263', '161', '204', '210', '14', '208', '208', '208', '208', '204', '102', '102', '213', '155', '251', '153', '153', '153', '154', '154', '237', '97', '19', '28', '94', '136', '112', '19', '97', '152', '153', '153', '111', '111', '135', '135', '105', '17', '105', '152', '152', '152', '153', '79', '266', '135', '120', '125', '149', '50', '162', '72', '17', '147', '109', '115', '24', '105', '244', '149', '238', '203', '35', '202', '177', '173', '239', '19', '19', '179', '237', '14', '18', '185', '58', '158', '144', '213', '237', '155', '250', '239', '93', '237', '97', '195', '157', '107', '109', '246', '165', '98', '49', '2', '266', '149', '20', '18', '71', '108', '22', '115', '239', '40', '178', '232', '238', '218', '118', '118', '210', '235', '46', '228', '192', '112', '149', '96', '111', '21', '95', '95', '95', '18', '57', '206', '18', '239', '157', '80', '196', '112', '176', '99', '106', '71', '235', '135', '18', '209', '78', '20', '20', '103', '33', '3', '103', '166', '79', '79', '156', '229', '18', '238', '2', '16', '128', '200', '117', '22', '197', '166', '78', '237', '241', '107', '53', '189', '204', '243', '163', '221', '15', '200', '221', '152', '23', '193', '107', '195', '99', '97', '19', '125', '276', '113', '193', '64', '78', '197', '117', '175', '135', '135', '78', '93', '154', '153', '256', '256', '135', '137', '135', '137', '149', '110', '15', '159', '21', '158', '192', '192', '117', '29', '97', '258', '78', '46', '5', '80', '15', '105', '260', '161', '191', '2', '138', '117', '215', '176', '156', '194', '265', '115', '181', '21', '14', '106', '106', '108', '200', '156', '103', '102', '29', '17', '204', '175', '227', '118', '18', '239', '204', '258', '78', '155', '213', '109', '192', '186', '186', '15', '241', '133', '30', '30', '239', '30', '218', '109', '138', '241', '241', '45', '135', '112', '232', '104', '106', '200', '53', '23', '112', '17', '117', '59', '260', '195', '106', '117', '159', '204', '200', '228', '165', '165', '261', '200', '227', '166', '93', '21', '80', '112', '152', '196', '203', '228', '98', '102', '15', '139', '110', '13', '238', '6', '171', '94', '208', '155', '112', '203', '239', '191', '39', '136', '203', '195', '46', '203', '148', '192', '210', '23', '4', '153', '29', '165', '192', '204', '232', '94', '112', '239', '98', '71', '248', '17', '155', '10', '78', '191', '191', '111', '25', '25', '19', '19', '17', '235', '102', '265', '195', '235', '163', '238', '71', '197', '107', '155', '30', '72', '19', '259', '80', '155', '23', '64', '239', '197', '156', '57', '203', '258', '200', '205', '246', '238', '258', '227', '238', '238', '19', '148', '161', '111', '105', '202', '109', '262', '133', '137', '221', '135', '52', '170', '111', '36', '30', '156', '102', '268', '115', '22', '153', '133', '259', '98', '235', '135', '191', '25', '235', '25', '29', '94', '195', '203', '212', '112', '135', '245', '260', '200', '96', '158', '23', '165', '22', '18', '249', '240', '109', '43', '37', '15', '241', '241', '23', '135', '240', '108', '98', '268', '275', '191', '136', '263', '259', '196', '98', '98', '204', '248', '123', '105', '204', '240', '9', '9', '98', '218', '189', '238', '246', '155', '130', '157', '74', '109', '29', '189', '203', '213', '195', '241', '159', '238', '204', '218', '105', '23', '16', '263', '206', '150', '152', '204', '202', '173', '267', '18', '153', '155', '196', '2', '97', '158', '158', '204', '98', '202', '98', '204', '258', '98', '22', '113', '113', '135', '71', '237', '155', '163', '250', '233', '26', '26', '26', '26', '26', '19', '170', '18', '15', '19', '191', '30', '30', '48', '35', '30', '237', '46', '119', '16', '78', '239', '150', '245', '235', '158', '27', '241', '30', '38', '107', '75', '241', '135', '106', '106', '98', '159', '96', '135', '111', '176', '175', '200', '18', '14', '242', '163', '187', '20', '12', '191', '23', '106', '171', '25', '78', '162', '191', '157', '94', '237', '246', '203', '108', '238', '204', '204', '1', '102', '154', '106', '170', '102', '200', '2', '135', '106', '258', '258', '227', '26', '15', '25', '71', '203', '96', '3', '241', '200', '30', '108', '98', '108', '27', '106', '26', '150', '259', '78', '113', '113', '158', '202', '16', '136', '239', '156', '239', '35', '276', '252', '115', '22', '22', '258', '172', '3', '154', '3', '98', '237', '102', '235', '112', '206', '98', '57', '118', '153', '18', '26', '30', '196', '163', '191', '153', '72', '24', '156', '117', '122', '30', '135', '192', '98', '28', '30', '30', '95', '241', '155', '17', '195', '31', '103', '202', '98', '237', '94', '112', '163', '163', '94', '163', '112', '19', '195', '191', '15', '135', '78', '109', '160', '152', '97', '98', '107', '167', '97', '138', '138', '26', '191', '97', '97', '30', '30', '206', '202', '13', '30', '240', '111', '261', '79', '0', '111', '19', '152', '107', '107', '18', '98', '28', '258', '244', '106', '237', '204', '191', '15', '183', '202', '16', '111', '166', '268', '202', '12', '30', '78', '110', '133', '71', '197', '111', '52', '79', '171', '156', '94', '13', '118', '111', '71', '133', '18', '105', '30', '22', '133', '60', '237', '191', '6', '126', '3', '238', '276', '78', '162', '150', '115', '191', '107', '200', '148', '135', '72', '155', '241', '4', '108', '160', '203', '20', '133', '2', '80', '80', '154', '18', '15', '115', '239', '167', '167', '35', '268', '18', '221', '18', '195', '204', '195', '216', '133', '112', '104', '223', '191', '115', '30', '30', '30', '30', '30', '202', '204', '165', '200', '200', '29', '191', '237', '267', '18', '175', '81', '264', '264', '178', '115', '102', '178', '212', '273', '202', '109', '259', '166', '135', '155', '219', '161', '18', '94', '241', '191', '196', '191', '98', '26', '26', '204', '204', '135', '27', '238', '260', '182', '26', '98', '94', '93', '238', '156', '19', '111', '79', '16', '192', '113', '19', '241', '18', '202', '260', '272', '106', '163', '156', '106', '17', '109', '221', '78', '203', '7', '136', '197', '193', '191', '98', '112', '200', '95', '221', '98', '161', '191', '3', '106', '189', '111', '25', '196', '196', '241', '163', '200', '24', '78', '176', '3', '98', '102', '98', '19', '161', '224', '80', '18', '191', '112', '105', '254', '71', '200', '137', '29', '20', '165', '8', '106', '23', '106', '155', '200', '265', '135', '126', '203', '188', '161', '158', '258', '255', '98', '203', '50', '35', '102', '158', '175', '40', '239', '26', '66', '57', '193', '155', '15', '135', '10', '107', '106', '229', '18', '191', '204', '107', '99', '235', '238', '18', '205', '205', '35', '238', '208', '203', '110', '110', '109', '112', '23', '18', '19', '182', '156', '18', '18', '156', '111', '135', '137', '133', '137', '240', '99', '197', '113', '15', '165', '39', '97', '97', '15', '150', '163', '109', '268', '15', '15', '238', '203', '67', '161', '197', '18', '196', '97', '161', '197', '161', '185', '238', '191', '30', '121', '133', '160', '160', '74', '135', '75', '54', '135', '196', '23', '241', '166', '194', '239', '135', '240', '208', '238', '98', '135', '152', '176', '22', '174', '113', '241', '237', '57', '195', '191', '24', '165', '165', '204', '204', '56', '109', '27', '153', '252', '228', '165', '252', '200', '28', '158', '135', '196', '22', '18', '133', '30', '204', '237', '196', '93', '106', '94', '25', '93', '159', '4', '195', '23', '237', '25', '199', '57', '26', '276', '274', '204', '75', '18', '204', '108', '22', '133', '191', '111', '204', '203', '133', '237', '166', '4', '62', '202', '166', '161', '165', '74', '109', '118', '200', '22', '206', '25', '209', '17', '155', '204', '64', '23', '30', '202', '149', '171', '135', '136', '184', '26', '18', '159', '195', '18', '166', '156', '98', '133', '38', '40', '18', '18', '166', '109', '109', '209', '22', '195', '163', '163', '108', '18', '15', '99', '135', '174', '26', '19', '166', '133', '259', '151', '235', '204', '228', '133', '30', '161', '111', '242', '84', '238', '98', '195', '78', '160', '28', '25', '264', '165', '41', '19', '55', '46', '202', '121', '15', '32', '98', '238', '0', '78', '31', '30', '203', '264', '203', '135', '104', '111', '78', '212', '111', '98', '126', '22', '2', '30', '237', '212', '265', '163', '106', '106', '235', '109', '223', '161', '202', '150', '170', '255', '112', '114', '170', '98', '96', '197', '36', '18', '228', '14', '255', '204', '102', '239', '0', '185', '30', '22', '23', '246', '199', '133', '191', '78', '195', '153', '112', '191', '200', '170', '171', '200', '78', '128', '25', '153', '237', '99', '94', '208', '107', '208', '208', '78', '204', '204', '27', '191', '272', '134', '158', '158', '174', '118', '64', '18', '258', '150', '189', '18', '153', '241', '204', '112', '106', '135', '71', '148', '7', '133', '140', '187', '23', '223', '19', '19', '2', '136', '2', '200', '161', '15', '98', '158', '148', '3', '250', '153', '235', '165', '220', '176', '110', '240', '151', '148', '133', '74', '193', '46', '27', '173', '94', '176', '105', '256', '161', '226', '18', '78', '191', '171', '243', '26', '0', '108', '167', '159', '239', '150', '112', '155', '79', '241', '202', '165', '254', '107', '202', '156', '166', '193', '239', '135', '15', '203', '187', '148', '25', '15', '22', '194', '239', '56', '191', '176', '262', '200', '110', '72', '99', '102', '135', '98', '113', '15', '15', '159', '262', '171', '151', '98', '204', '213', '255', '26', '243', '175', '156', '98', '107', '191', '165', '241', '26', '165', '171', '30', '160', '166', '266', '11', '19', '211', '93', '254', '30', '239', '115', '273', '205', '112', '237', '263', '74', '98', '79', '122', '94', '265', '155', '191', '16', '235', '229', '98', '191', '227', '17', '111', '267', '135', '202', '195', '10', '180', '196', '196', '133', '135', '133', '95', '111', '122', '258', '191', '3', '114', '107', '19', '264', '74', '56', '200', '200', '133', '161', '97', '15', '97', '197', '196', '253', '93', '202', '258', '106', '153', '26', '109', '40', '149', '8', '196', '15', '119', '155', '23', '237', '193', '264', '148', '151', '238', '200', '94', '75', '148', '175', '24', '195', '171', '30', '203', '200', '202', '16', '158', '223', '191', '22', '264', '135', '35', '57', '94', '191', '196', '202', '98', '18', '196', '3', '133', '78', '191', '208', '258', '18', '200', '29', '135', '109', '110', '2', '185', '98', '133', '19', '22', '204', '95', '238', '25', '133', '250', '135', '135', '3', '47', '158', '187', '2', '78', '115', '155', '3', '218', '166', '112', '266', '22', '195', '135', '72', '97', '197', '209', '127', '80', '2', '98', '14', '239', '15', '113', '165', '204', '78', '204', '191', '175', '200', '193', '18', '196', '133', '109', '237', '56', '196', '3', '163', '163', '163', '195', '15', '263', '18', '84', '18', '133', '107', '18', '102', '156', '238', '102', '26', '244', '82', '75', '23', '258', '238', '136', '3', '204', '184', '200', '196', '158', '106', '27', '238', '155', '152', '50', '50', '2', '235', '98', '203', '166', '109', '25', '18', '151', '133', '200', '26', '26', '26', '19', '107', '156', '23', '26', '3', '70', '225', '41', '26', '239', '26', '186', '26', '34', '272', '272', '104', '78', '260', '18', '204', '26', '78', '18', '210', '135', '203', '71', '207', '26', '133', '13', '25', '26', '115', '94', '264', '135', '244', '79', '30', '263', '22', '109', '205', '176', '106', '22', '17', '151', '106', '191', '160', '160', '165', '78', '177', '196', '260', '166', '35', '191', '260', '98', '18', '237', '18', '161', '18', '197', '133', '30', '35', '200', '170', '155', '245', '19', '200', '160', '96', '172', '165', '55', '137', '241', '133', '191', '112', '25', '126', '238', '18', '18', '108', '135', '161', '135', '73', '112', '107', '8', '112', '109', '195', '25', '30', '105', '161', '241', '110', '75', '238', '215', '238', '104', '25', '3', '155', '30', '16', '3', '175', '99', '3', '200', '23', '3', '199', '203', '191', '196', '196', '204', '98', '105', '133', '197', '135', '161', '39', '15', '30', '133', '55', '191', '203', '133', '203', '140', '200', '149', '30', '96', '135', '23', '26', '3', '18', '155', '158', '135', '135', '18', '250', '18', '22', '114', '18', '18', '166', '18', '30', '18', '239', '18', '161', '50', '50', '49', '51', '23', '55', '98', '78', '210', '211', '18', '239', '78', '13', '193', '114', '78', '18', '25', '133', '19', '176', '4', '173', '50', '108', '17', '4', '204', '94', '112', '207', '109', '115', '23', '235', '197', '194', '197', '213', '148', '40', '157', '107', '200', '147', '14', '14', '18', '238', '18', '238', '18', '238', '57', '200', '135', '30', '18', '191', '191', '200', '166', '151', '161', '73', '19', '73', '239', '239', '165', '108', '2', '93', '166', '133', '133', '173', '35', '193', '114', '74', '19', '40', '58', '104', '16', '260', '160', '158', '112', '210', '158', '167', '203', '203', '3', '3', '109', '30', '133', '258', '70', '204', '200', '152', '155', '192', '204', '108', '191', '204', '26', '98', '26', '98', '264', '158', '18', '150', '137', '153', '196', '199', '191', '115', '168', '157', '149', '151', '78', '137', '160', '259', '166', '196', '171', '175', '199', '166', '161', '70', '13', '166', '152', '152', '206', '58', '18', '207', '58', '74', '207', '29', '46', '109', '135', '74', '174', '109', '40', '10', '30', '258', '16', '111', '30', '15', '239', '204', '48', '25', '196', '0', '156', '72', '72', '96', '22', '72', '133', '133', '238', '26', '3', '50', '152', '193', '235', '136', '153', '50', '1', '19', '239', '30', '25', '179', '161', '30', '191', '3', '27', '133', '3', '40', '23', '133', '150', '151', '265', '32', '191', '3', '25', '170', '238', '40', '259', '166', '137', '241', '133', '209', '159', '78', '78', '153', '26', '52', '241', '36', '250', '96', '30', '30', '38', '18', '18', '238', '99', '72', '111', '3', '244', '191', '226', '3', '191', '114', '135', '109', '40', '79', '202', '78', '114', '238', '10', '150', '108', '163', '108', '113', '197', '109', '115', '161', '3', '238', '3', '237', '161', '107', '155', '78', '202', '112', '38', '195', '137', '250', '133', '237', '161', '25', '26', '25', '15', '15', '239', '166', '114', '229', '111', '40', '135', '94', '102', '162', '204', '237', '17', '166', '136', '22', '98', '99', '99', '166', '35', '196', '3', '203', '135', '135', '84', '98', '266', '238', '13', '166', '78', '193', '98', '241', '104', '153', '192', '200', '10', '95', '95', '148', '107', '14', '17', '191', '239', '19', '203', '255', '197', '213', '94', '3', '107', '23', '35', '10', '237', '235', '78', '29', '18', '160', '200', '150', '99', '109', '155', '161', '133', '114', '18', '18', '18', '264', '133', '104', '161', '70', '17', '204', '195', '204', '241', '99', '82', '241', '241', '161', '18', '235', '175', '2', '2', '202', '50', '230', '26', '40', '40', '200', '78', '156', '26', '112', '26', '30', '71', '184', '200', '135', '153', '23', '106', '125', '175', '133', '133', '7', '38', '196', '161', '0', '49', '241', '158', '15', '21', '104', '2', '3', '176', '259', '135', '78', '75', '112', '200', '1', '207', '238', '161', '150', '158', '2', '45', '31', '237', '149', '3', '26', '165', '135', '152', '115', '133', '18', '40', '166', '30', '173', '40', '26', '78', '50', '48', '50', '13', '45', '200', '200', '18', '151', '205', '135', '38', '117', '196', '135', '155', '111', '196', '196', '166', '23', '25', '161', '18', '124', '259', '165', '58', '170', '204', '250', '103', '160', '238', '194', '163', '241', '255', '235', '17', '133', '26', '135', '196', '237', '114', '258', '193', '248', '40', '195', '84', '238', '146', '199', '40', '161', '22', '137', '43', '166', '156', '4', '104', '133', '177', '211', '193', '78', '203', '246', '237', '264', '3', '149', '78', '78', '111', '239', '13', '238', '176', '112', '150', '156', '3', '6', '118', '178', '178', '21', '186', '150', '241', '50', '243', '191', '121', '278', '29', '200', '18', '3', '197', '191', '98', '173', '195', '25', '195', '264', '221', '15', '156', '25', '200', '211', '195', '26', '35', '2', '135', '126', '113', '167', '191', '264', '196', '133', '18', '26', '146', '203', '219', '23', '160', '160', '135', '259', '3', '159', '120', '160', '106', '240', '133', '115', '258', '38', '22', '3', '26', '22', '107', '49', '48', '25', '237', '72', '219', '196', '196', '111', '135', '196', '18', '166', '172', '135', '26', '258', '70', '98', '40', '166', '23', '96', '3', '30', '155', '126', '146', '114', '242', '237', '157', '238', '241', '40', '133', '200', '161', '175', '200', '175', '84', '266', '18', '99', '30', '242', '151', '153', '166', '199', '228', '3', '237', '132', '110', '135', '80', '15', '15', '15', '84', '133', '15', '177', '3', '202', '255', '71', '115', '235', '19', '18', '18', '102', '58', '78', '165', '166', '264', '12', '150', '15', '2', '18', '135', '109', '200', '258', '128', '3', '199', '197', '239', '109', '222', '239', '107', '125', '132', '204', '204', '133', '135', '30', '158', '203', '15', '264', '237', '133', '196', '3', '252', '26', '98', '79', '199', '210', '0', '99', '30', '26', '259', '6', '49', '166', '98', '108', '9', '133', '102', '155', '98', '114', '78', '35', '166', '188', '202', '235', '30', '149', '165', '176', '138', '29', '153', '25', '25', '137', '133', '166', '135', '125', '71', '193', '154', '165', '98', '23', '56', '198', '26', '26', '0', '109', '136', '163', '151', '25', '15', '241', '195', '204', '191', '195', '161', '175', '133', '237', '78', '165', '258', '135', '78', '135', '258', '18', '235', '30', '187', '150', '275', '149', '2', '36', '112', '22', '161', '238', '203', '112', '265', '75', '133', '205', '258', '158', '18', '26', '18', '25', '26', '15', '26', '239', '15', '240', '18', '71', '71', '78', '135', '3', '148', '166', '26', '176', '2', '197', '150', '176', '19', '18', '97', '166', '18', '153', '250', '133', '210', '20', '192', '196', '26', '191', '2', '81', '70', '5', '79', '3', '30', '193', '193', '69', '160', '176', '246', '108', '54', '250', '135', '200', '80', '3', '167', '18', '166', '200', '152', '23', '149', '19', '14', '13', '237', '196', '19', '112', '254', '204', '191', '23', '264', '199', '209', '137', '31', '135', '238', '78', '18', '191', '15', '164', '195', '74', '163', '163', '41', '99', '26', '135', '26', '40', '40', '250', '200', '200', '26', '237', '4', '195', '158', '35', '71', '191', '3', '218', '274', '58', '3', '23', '177', '132', '49', '62', '238', '166', '97', '264', '193', '29', '26', '49', '241', '197', '200', '64', '227', '258', '44', '15', '133', '175', '238', '109', '254', '237', '198', '175', '166', '213', '174', '196', '107', '259', '98', '10', '196', '210', '191', '96', '166', '135', '15', '197', '78', '78', '195', '176', '18', '98', '129', '170', '18', '247', '203', '26', '158', '40', '64', '200', '163', '196', '19', '205', '197', '193', '191', '69', '96', '74', '35', '36', '64', '133', '172', '19', '266', '96', '94', '103', '258', '193', '264', '98', '254', '50', '193', '25', '170', '75', '84', '27', '252', '203', '94', '94', '19', '98', '137', '12', '153', '3', '155', '135', '238', '15', '196', '15', '26', '84', '49', '27', '135', '203', '48', '133', '129', '133', '258', '155', '35', '195', '71', '241', '135', '166', '193', '165', '104', '196', '117', '196', '26', '237', '109', '95', '196', '60', '40', '158', '193', '250', '104', '94', '178', '27', '193', '78', '106', '79', '204', '133', '237', '235', '135', '202', '70', '15', '264', '241', '254', '110', '199', '49', '255', '30', '193', '238', '204', '102', '97', '161', '43', '228', '1', '6', '196', '193', '26', '15', '128', '46', '155', '32', '133', '69', '47', '135', '78', '205', '200', '200', '202', '3', '95', '191', '78', '26', '248', '258', '191', '240', '49', '110', '38', '193', '200', '200', '96', '200', '204', '135', '9', '200', '258', '15', '46', '15', '149', '155', '197', '2', '166', '151', '72', '135', '97', '211', '238', '135', '163', '161', '26', '163', '129', '241', '239', '241', '16', '26', '18', '263', '260', '166', '3', '264', '195', '79', '132', '251', '50', '133', '15', '25', '15', '191', '15', '78', '50', '78', '71', '227', '69', '200', '69', '109', '64', '133', '197', '196', '3', '45', '238', '26', '132', '104', '196', '193', '136', '80', '26', '75', '117', '14', '78', '75', '98', '15', '2', '15', '50', '255', '136', '3', '112', '173', '69', '191', '19', '239', '235', '109', '70', '135', '78', '165', '238', '262', '199', '35', '203', '224', '193', '15', '18', '150', '238', '197', '238', '193', '135', '3', '150', '187', '199', '239', '109', '152', '27', '257', '6', '235', '104', '191', '133', '166', '155', '26', '16', '136', '198', '98', '154', '235', '27', '102', '30', '191', '15', '132', '132', '114', '107', '241', '241', '156', '199', '241', '49', '62', '117', '258', '30', '163', '133', '238', '200', '3', '15', '109', '2', '196', '165', '166', '159', '210', '35', '15', '17', '200', '241', '106', '203', '197', '197', '79', '98', '98', '23', '204', '48', '129', '57', '258', '202', '215', '238', '70', '64', '193', '189', '57', '200', '0', '98', '135', '204', '25', '30', '204', '27', '26', '78', '135', '109', '3', '197', '3', '167', '35', '196', '225', '2', '194', '200', '15', '161', '198', '23', '133', '238', '195', '35', '50', '135', '152', '163', '21', '15', '94', '26', '94', '53', '260', '196', '154', '277', '200', '118', '6', '238', '50', '209', '50', '3', '238', '3', '15', '107', '135', '199', '15', '135', '26', '258', '251', '203', '19', '259', '195', '199', '238', '15', '254', '112', '211', '200', '129', '84', '196', '3', '208', '259', '135', '104', '29', '136', '31', '78', '13', '3', '64', '75', '55', '114', '160', '96', '14', '18', '207', '75', '48', '15', '49', '97', '117', '196', '99', '258', '195', '264', '264', '78', '197', '30', '238', '109', '158', '32', '195', '149', '227', '65', '167', '162', '204', '250', '151', '151', '102', '26', '29', '26', '193', '74', '50', '193', '186', '15', '74', '259', '57', '188', '277', '277', '195', '195', '212', '78', '197', '64', '129', '259', '200', '199', '129', '4', '235', '197', '196', '161', '3', '255', '199', '102', '3', '13', '94', '53', '98', '15', '129', '15', '155', '71', '179', '50', '29', '136', '80', '254', '198', '162', '237', '3', '18', '260', '238', '117', '179', '156', '19', '45', '197', '78', '199', '102', '102', '237', '155', '69', '69', '163', '239', '15', '196', '80', '191', '30', '71', '158', '259', '2', '106', '148', '15', '135', '135', '135', '196', '19', '123', '15', '104', '19', '128', '230', '69', '98', '197', '241', '161', '195', '200', '3', '98', '78', '165', '163', '19', '196', '31', '173', '203', '55', '69', '209', '239', '79', '158', '254', '135', '51', '94', '162', '255', '95', '18', '133', '109', '50', '94', '199', '15', '197', '200', '133', '116', '50', '196', '161', '69', '133', '3', '258', '3', '197', '29', '3', '157', '197', '193', '4', '94', '195', '17', '48', '152', '8', '15', '155', '133', '195', '136', '150', '102', '237', '203', '164', '164', '250', '154', '13', '191', '58', '94', '70', '93', '264', '15', '30', '133', '237', '267', '199', '193', '76', '241', '157', '26', '200', '109', '179', '55', '98', '200', '129', '238', '235', '208', '109', '239', '258', '196', '193', '18', '107', '210', '70', '40', '129', '259', '193', '30', '158', '252', '40', '210', '78', '15', '78', '96', '208', '113', '258', '64', '80', '15', '133', '241', '15', '15', '258', '135', '109', '251', '251', '109', '74', '196', '170', '240', '254', '258', '204', '199', '26', '109', '197', '57', '45', '161', '57', '114', '3', '15', '264', '106', '57', '27', '18', '258', '29', '84', '161', '135', '114', '24', '30', '107', '107', '172', '209', '170', '18', '19', '133', '191', '195', '135', '156', '196', '94', '3', '84', '225', '199', '238', '106', '104', '70', '50', '12', '155', '114', '176', '237', '200', '108', '70', '240', '196', '19', '22', '276', '199', '238', '205', '104', '193', '108', '115', '259', '102', '19', '258', '18', '106', '161', '191', '114', '64', '15', '204', '35', '155', '40', '238', '136', '264', '133', '186', '15', '258', '166', '26', '6', '114', '114', '133', '114', '114', '13', '13', '13', '237', '250', '245', '136', '78', '13', '48', '18', '13', '25', '26', '93', '125', '50', '94', '148', '78', '194', '200', '25', '93', '274', '266', '70', '22', '3', '208', '240', '211', '135', '31', '18', '153', '18', '2', '15', '15', '15', '156', '15', '70', '73', '173', '13', '112', '69', '204', '70', '248', '135', '70', '3', '193', '193', '22', '162', '238', '57', '98', '15', '199', '199', '52', '151', '73', '5', '108', '23', '69', '95', '220', '106', '163', '152', '163', '108', '15', '58', '250', '19', '177', '245', '264', '93', '204', '25', '78', '155', '69', '155', '239', '241', '155', '200', '18', '3', '13', '196', '197', '204', '272', '95', '135', '165', '22', '210', '35', '69', '18', '170', '3', '25', '156', '78', '135', '102', '107', '255', '19', '70', '70', '109', '198', '156', '64', '170', '138', '78', '203', '166', '108', '114', '179', '228', '74', '49', '14', '264', '26', '195', '78', '107', '75', '18', '106', '151', '70', '58', '15', '196', '238', '109', '104', '104', '109', '136', '148', '210', '3', '161', '15', '104', '238', '208', '133', '108', '241', '18', '209', '74', '156', '114', '199', '174', '35', '18', '170', '135', '238', '244', '155', '162', '197', '3', '98', '227', '267', '238', '193', '50', '238', '98', '112', '124', '25', '114', '241', '159', '154', '78', '161', '136', '114', '153', '225', '154', '13', '50', '11', '25', '15', '209', '11', '166', '203', '259', '237', '22', '114', '276', '98', '278', '150', '198', '18', '15', '78', '15', '162', '26', '3', '196', '3', '102', '189', '71', '69', '16', '18', '152', '133', '135', '162', '241', '15', '104', '15', '259', '202', '2', '238', '207', '15', '204', '30', '215', '112', '210', '42', '96', '193', '15', '110', '163', '211', '78', '108', '15', '31', '27', '15', '31', '196', '195', '96', '15', '161', '94', '193', '195', '198', '98', '261', '258', '76', '15', '104', '258', '135', '18', '15', '202', '258', '108', '156', '13', '22', '251', '199', '155', '258', '211', '3', '207', '202', '259', '107', '202', '259', '135', '135', '148', '158', '154', '99', '99', '3', '207', '94', '25', '162', '31', '241', '25', '15', '114', '18', '114', '148', '19', '15', '203', '191', '19', '13', '193', '204', '25', '166', '155', '196', '98', '162', '197', '29', '152', '191', '133', '199', '230', '40', '25', '204', '79', '3', '19', '165', '204', '108', '208', '3', '251', '274', '15', '166', '264', '35', '13', '193', '114', '258', '13', '19', '133', '23', '17', '199', '204', '153', '250', '204', '16', '12', '107', '191', '18', '3', '15', '165', '15', '69', '17', '165', '17', '15', '15', '156', '23', '23', '15', '197', '104', '197', '26', '208', '135', '211', '250', '18', '204', '25', '196', '53', '69', '104', '104', '19', '78', '241', '230', '30', '195', '96', '26', '109', '196', '69', '238', '15', '18', '133', '26', '106', '109', '98', '15', '258', '274', '193', '203', '58', '57', '274', '2', '202', '161', '136', '135', '155', '241', '35', '151', '137', '5', '114', '96', '16', '209', '158', '15', '266', '161', '15', '25', '274', '202', '3', '161', '237', '267', '204', '125', '15', '151', '155', '259', '151', '210', '238', '15', '26', '132', '50', '173', '197', '114', '241', '15', '258', '109', '166', '274', '13', '191', '15', '26', '164', '258', '163', '15', '264', '11', '260', '148', '150', '207', '196', '98', '272', '133', '196', '107', '26', '191', '163', '191', '135', '3', '18', '18', '119', '69', '196', '15', '156', '135', '112', '258', '155', '256', '199', '204', '212', '18', '135', '254', '72', '258', '254', '15', '133', '18', '259', '193', '258', '204', '40', '18', '29', '155', '104', '107', '274', '241', '25', '26', '69', '148', '166', '46', '94', '135', '15', '155', '104', '241', '32', '274', '249', '15', '197', '104', '44', '259', '69', '191', '15', '198', '193', '133', '17', '258', '248', '166', '197', '200', '250', '204', '3', '225', '238', '163', '69', '15', '15', '26', '202', '19', '209', '78', '212', '15', '78', '243', '110', '191', '15', '18', '107', '38', '208', '166', '239', '237', '199', '15', '204', '258', '173', '172', '200', '274', '243', '19', '191', '135', '203', '69', '149', '148', '196', '256', '204', '97', '191', '26', '114', '274', '200', '69', '69', '64', '69', '239', '25', '274', '203', '78', '135', '196', '202', '17', '2', '2', '3', '98', '98', '62', '2', '95', '96', '2', '207', '78', '200', '26', '64', '18', '93', '49', '57', '154', '200', '69', '94', '98', '98', '56', '56', '166', '98', '212', '226', '226', '15', '69', '15', '273', '272', '104', '193', '3', '98', '199', '98', '25', '125', '259', '249', '108', '40', '26', '198', '218', '73', '148', '69', '204', '241', '35', '3', '274', '70', '18', '71', '35', '172', '133', '106', '69', '107', '199', '219', '19', '133', '69', '135', '109', '57', '191', '135', '252', '104', '3', '94', '3', '72', '126', '16', '36', '84', '241', '3', '218', '98', '108', '46', '19', '196', '118', '104', '29', '238', '69', '10', '204', '264', '259', '124', '242', '107', '133', '25', '64', '30', '194', '259', '159', '264', '274', '52', '200', '104', '29', '30', '193', '98', '94', '114', '204', '40', '18', '198', '97', '3', '99', '274', '239', '177', '15', '18', '126', '126', '208', '148', '15', '104', '29', '36', '218', '70', '15', '23', '278', '23', '36', '191', '210', '51', '245', '167', '72', '237', '25', '203', '36', '196', '193', '196', '15', '151', '207', '203', '103', '166', '135', '110', '27', '13', '27', '191', '98', '57', '166', '50', '21', '15', '197', '23', '2', '70', '69', '193', '274', '104', '166', '258', '126', '238', '161', '18', '193', '69', '140', '170', '197', '204', '165', '203', '170', '15', '58', '35', '83', '212', '150', '73', '25', '15', '64', '133', '210', '108', '79', '193', '15', '126', '165', '15', '208', '98', '3', '200', '126', '3', '40', '165', '98', '69', '15', '165', '133', '96', '173', '203', '133', '238', '248', '15', '7', '38', '207', '108', '165', '199', '18', '162', '27', '3', '72', '65', '250', '209', '166', '64', '209', '237', '212', '64', '225', '120', '108', '203', '135', '248', '69', '251', '135', '204', '160', '160', '135', '148', '15', '15', '23', '207', '274', '29', '28', '194', '150', '40', '104', '208', '125', '70', '18', '3', '19', '102', '104', '23', '29', '207', '133', '204', '111', '2', '125', '126', '7', '194', '209', '203', '83', '126', '126', '258', '119', '162', '241', '133', '18', '208', '197', '209', '15', '78', '80', '110', '237', '104', '30', '208', '103', '15', '200', '15', '238', '254', '108', '173', '40', '69', '175', '17', '155', '136', '26', '3', '135', '13', '241', '161', '166', '170', '162', '133', '53', '211', '204', '15', '161', '238', '110', '18', '191', '18', '50', '71', '30', '17', '208', '248', '203', '203', '196', '210', '258', '204', '3', '18', '196', '125', '173', '199', '249', '18', '104', '165', '241', '2', '193', '238', '78', '15', '166', '15', '126', '3', '237', '161', '52', '207', '3', '3', '50', '56', '22', '56', '69', '93', '135', '207', '69', '99', '3', '133', '68', '133', '161', '108', '126', '250', '60', '155', '98', '18', '16', '78', '78', '240', '235', '176', '22', '199', '160', '235', '274', '209', '64', '173', '137', '128', '25', '18', '160', '203', '70', '165', '193', '110', '207', '64', '85', '252', '78', '51', '2', '99', '19', '22', '198', '204', '166', '166', '151', '56', '133', '210', '26', '149', '109', '30', '209', '209', '51', '3', '193', '119', '208', '93', '165', '248', '55', '203', '204', '58', '18', '199', '211', '83', '32', '135', '212', '156', '19', '235', '173', '191', '209', '203', '146', '208', '58', '158', '102', '199', '200', '200', '173', '70', '52', '107', '211', '213', '237', '196', '205', '104', '135', '258', '255', '96', '19', '124', '128', '57', '199', '98', '166', '102', '40', '15', '238', '3', '148', '225', '126', '135', '114', '193', '204', '15', '209', '209', '193', '259', '64', '161', '26', '274', '126', '18', '196', '0', '60', '207', '17', '71', '218', '239', '248', '38', '154', '61', '128', '199', '196', '173', '25', '46', '98', '258', '108', '3', '108', '196', '93', '93', '99', '172', '69', '241', '196', '30', '135', '278', '109', '18', '5', '209', '57', '159', '70', '193', '163', '249', '196', '196', '72', '103', '133', '17', '204', '199', '166', '202', '29', '57', '193', '150', '98', '119', '25', '205', '18', '173', '166', '209', '57', '25', '58', '258', '26', '15', '15', '101', '26', '74', '1', '4', '74', '263', '']\n",
            "['209', '63', '216', '108', '196', '200', '203', '246', '188', '193', '25', '106', '56', '53', '223', '161', '200', '208', '78', '147', '147', '179', '147', '80', '56', '209', '209', '200', '214', '202', '203', '194', '26', '237', '165', '258', '238', '238', '76', '193', '196', '213', '161', '213', '209', '194', '196', '25', '151', '260', '135', '173', '30', '108', '272', '259', '176', '213', '213', '21', '5', '171', '200', '195', '240', '194', '176', '240', '240', '80', '173', '98', '214', '21', '76', '76', '73', '173', '173', '78', '61', '258', '191', '3', '78', '173', '193', '118', '200', '264', '260', '49', '211', '258', '18', '173', '78', '3', '3', '1', '213', '135', '182', '239', '208', '278', '50', '194', '200', '124', '189', '117', '264', '244', '76', '193', '221', '78', '78', '173', '197', '78', '161', '231', '135', '22', '78', '25', '25', '197', '200', '147', '203', '191', '196', '193', '263', '32', '197', '191', '4', '56', '196', '104', '191', '196', '173', '147', '194', '96', '161', '161', '238', '200', '3', '78', '78', '53', '41', '237', '53', '18', '10', '194', '97', '96', '208', '25', '10', '117', '104', '54', '96', '194', '21', '245', '238', '202', '209', '161', '97', '258', '240', '213', '136', '155', '152', '21', '191', '191', '118', '68', '258', '189', '197', '208', '61', '78', '135', '202', '96', '155', '108', '272', '80', '25', '155', '35', '213', '191', '194', '96', '96', '191', '264', '191', '191', '191', '2', '258', '191', '173', '259', '15', '47', '258', '156', '118', '160', '115', '105', '26', '173', '192', '76', '76', '135', '240', '118', '161', '200', '230', '18', '200', '25', '10', '161', '244', '24', '24', '200', '205', '104', '15', '193', '156', '95', '209', '240', '191', '194', '128', '208', '208', '218', '173', '10', '25', '18', '107', '46', '32', '197', '172', '209', '213', '79', '121', '78', '78', '193', '2', '78', '194', '149', '136', '80', '3', '21', '200', '78', '21', '197', '194', '118', '202', '161', '180', '209', '173', '2', '193', '6', '260', '197', '78', '78', '42', '191', '213', '135', '193', '196', '202', '222', '217', '42', '259', '213', '76', '46', '30', '15', '260', '202', '18', '259', '104', '135', '76', '196', '240', '240', '240', '108', '24', '200', '197', '262', '176', '15', '156', '264', '156', '200', '191', '161', '196', '78', '238', '161', '221', '118', '14', '14', '14', '200', '278', '238', '200', '196', '78', '173', '25', '197', '197', '196', '161', '18', '10', '10', '10', '18', '66', '203', '19', '246', '196', '244', '240', '161', '200', '78', '240', '209', '208', '118', '215', '194', '193', '168', '229', '191', '135', '172', '196', '197', '161', '78', '196', '197', '258', '78', '78', '238', '221', '121', '264', '104', '146', '50', '78', '217', '11', '238', '238', '238', '3', '3', '154', '215', '79', '18', '158', '104', '18', '13', '197', '117', '197', '161', '213', '242', '32', '238', '230', '2', '2', '32', '161', '251', '96', '156', '242', '22', '22', '140', '3', '149', '135', '191', '173', '135', '200', '238', '238', '78', '156', '209', '78', '97', '97', '160', '25', '258', '240', '161', '15', '161', '96', '209', '80', '132', '140', '4', '1', '240', '196', '274', '272', '62', '80', '196', '196', '191', '79', '156', '15', '196', '209', '22', '251', '202', '202', '121', '73', '191', '238', '191', '150', '150', '135', '46', '203', '202', '238', '22', '3', '258', '239', '238', '238', '165', '238', '238', '194', '200', '200', '197', '200', '196', '191', '194', '76', '78', '208', '200', '71', '208', '240', '76', '117', '120', '108', '96', '144', '30', '133', '133', '202', '117', '161', '251', '29', '104', '242', '197', '118', '216', '262', '236', '141', '11', '235', '235', '133', '191', '238', '202', '80', '80', '22', '140', '191', '197', '202', '133', '184', '184', '184', '184', '143', '274', '50', '237', '196', '274', '265', '10', '18', '10', '121', '238', '245', '228', '76', '25', '191', '197', '268', '10', '240', '195', '196', '195', '15', '48', '200', '183', '70', '59', '197', '196', '235', '161', '161', '215', '104', '97', '191', '194', '191', '200', '200', '197', '76', '200', '93', '256', '160', '238', '146', '238', '239', '15', '140', '258', '140', '21', '79', '202', '258', '79', '62', '205', '161', '104', '10', '161', '196', '137', '152', '191', '117', '213', '242', '176', '218', '195', '191', '162', '191', '135', '241', '213', '213', '84', '79', '68', '18', '98', '197', '155', '41', '246', '269', '78', '209', '238', '80', '238', '62', '237', '273', '273', '230', '150', '121', '260', '244', '161', '202', '78', '109', '155', '264', '145', '50', '274', '173', '210', '258', '238', '211', '156', '215', '195', '177', '244', '3', '141', '196', '191', '194', '29', '30', '10', '16', '200', '200', '156', '53', '3', '62', '165', '173', '173', '22', '205', '274', '150', '183', '116', '188', '188', '98', '161', '204', '80', '172', '156', '104', '229', '259', '78', '169', '3', '200', '118', '161', '80', '260', '240', '150', '197', '196', '62', '93', '6', '272', '272', '197', '160', '78', '96', '70', '195', '191', '191', '80', '3', '3', '240', '147', '196', '191', '238', '96', '203', '3', '15', '10', '58', '55', '96', '41', '15', '26', '50', '238', '194', '79', '240', '161', '191', '195', '263', '50', '240', '147', '235', '195', '203', '59', '178', '176', '2', '15', '203', '191', '18', '278', '196', '14', '153', '58', '175', '18', '217', '217', '80', '191', '200', '18', '200', '173', '24', '171', '171', '210', '124', '34', '200', '218', '104', '4', '223', '193', '194', '200', '200', '13', '62', '194', '197', '80', '197', '146', '146', '96', '97', '97', '2', '196', '191', '191', '104', '238', '10', '200', '239', '135', '104', '213', '2', '221', '169', '25', '21', '194', '161', '21', '278', '3', '195', '46', '176', '25', '117', '191', '62', '62', '62', '78', '50', '264', '79', '197', '80', '80', '80', '191', '191', '78', '161', '194', '195', '197', '203', '259', '278', '49', '147', '147', '147', '147', '22', '78', '78', '258', '2', '202', '221', '55', '258', '104', '104', '84', '240', '221', '274', '104', '22', '241', '63', '197', '197', '196', '159', '200', '22', '22', '22', '78', '78', '173', '104', '97', '104', '191', '146', '223', '268', '96', '14', '173', '204', '241', '97', '15', '118', '78', '78', '196', '194', '258', '96', '96', '259', '75', '10', '197', '30', '2', '238', '121', '50', '161', '173', '173', '186', '185', '186', '78', '194', '221', '218', '35', '33', '13', '34', '78', '21', '215', '97', '60', '24', '78', '193', '173', '235', '78', '200', '258', '140', '259', '194', '197', '195', '200', '145', '76', '108', '130', '238', '215', '213', '2', '217', '121', '178', '140', '62', '156', '176', '196', '196', '41', '223', '147', '172', '79', '200', '2', '244', '244', '191', '213', '104', '191', '237', '237', '33', '117', '195', '195', '264', '195', '140', '210', '41', '197', '14', '257', '196', '238', '238', '79', '161', '156', '196', '238', '238', '213', '104', '3', '10', '118', '204', '235', '33', '259', '274', '259', '176', '78', '78', '196', '34', '52', '25', '30', '194', '153', '21', '70', '191', '18', '191', '191', '172', '194', '194', '194', '194', '148', '104', '125', '2', '2', '2', '185', '194', '29', '104', '78', '189', '272', '204', '183', '213', '78', '104', '197', '240', '97', '168', '168', '182', '238', '197', '18', '194', '229', '221', '62', '78', '53', '104', '214', '189', '189', '189', '213', '117', '117', '191', '29', '104', '104', '276', '276', '22', '195', '195', '155', '139', '139', '59', '168', '135', '140', '79', '146', '200', '245', '70', '160', '153', '50', '202', '154', '263', '130', '194', '117', '194', '194', '191', '33', '29', '29', '238', '239', '194', '21', '274', '215', '18', '18', '140', '2', '215', '25', '46', '137', '238', '30', '160', '156', '46', '196', '263', '242', '104', '0', '0', '73', '213', '135', '22', '191', '191', '241', '202', '221', '15', '15', '191', '238', '213', '194', '197', '179', '238', '200', '200', '200', '5', '238', '194', '55', '117', '166', '49', '244', '239', '191', '78', '78', '78', '194', '200', '117', '210', '22', '121', '121', '121', '21', '237', '194', '121', '78', '104', '194', '79', '191', '251', '195', '197', '274', '176', '21', '43', '43', '10', '61', '215', '215', '30', '214', '194', '10', '30', '139', '96', '226', '226', '265', '2', '2', '121', '121', '41', '237', '104', '238', '238', '161', '78', '78', '78', '11', '184', '140', '140', '161', '104', '104', '237', '130', '195', '160', '118', '161', '18', '11', '241', '184', '196', '178', '22', '30', '146', '146', '10', '10', '141', '141', '135', '18', '18', '10', '26', '208', '97', '141', '18', '18', '97', '194', '194', '79', '213', '205', '205', '205', '75', '241', '50', '226', '238', '97', '222', '215', '215', '139', '238', '51', '238', '96', '173', '109', '200', '117', '192', '2', '195', '10', '3', '18', '49', '238', '238', '191', '195', '52', '197', '200', '200', '79', '191', '200', '200', '30', '195', '197', '195', '25', '260', '73', '30', '193', '21', '200', '242', '203', '120', '78', '260', '241', '0', '30', '202', '195', '30', '191', '78', '194', '194', '2', '205', '191', '196', '173', '197', '30', '196', '172', '139', '2', '18', '191', '230', '56', '194', '215', '195', '2', '71', '13', '146', '264', '50', '191', '235', '93', '191', '139', '139', '227', '235', '186', '235', '24', '18', '261', '128', '33', '71', '25', '176', '39', '10', '195', '202', '29', '29', '221', '29', '197', '139', '78', '227', '146', '137', '195', '205', '205', '195', '135', '62', '153', '146', '29', '75', '0', '194', '194', '180', '213', '78', '18', '194', '239', '196', '239', '197', '194', '153', '192', '221', '10', '46', '50', '241', '117', '117', '117', '195', '173', '78', '195', '194', '239', '2', '12', '173', '197', '30', '269', '197', '176', '135', '30', '146', '191', '93', '93', '135', '260', '218', '156', '155', '200', '241', '125', '18', '18', '183', '203', '239', '259', '173', '213', '180', '180', '210', '120', '104', '194', '156', '258', '172', '9', '0', '241', '192', '143', '189', '204', '50', '63', '197', '243', '241', '93', '125', '2', '21', '260', '4', '4', '3', '191', '150', '128', '78', '238', '245', '8', '80', '238', '79', '79', '240', '58', '104', '155', '80', '80', '188', '191', '235', '170', '170', '170', '146', '258', '195', '276', '25', '78', '215', '240', '161', '53', '147', '78', '78', '78', '78', '221', '221', '96', '18', '19', '130', '18', '18', '18', '80', '80', '194', '79', '93', '121', '78', '139', '139', '78', '197', '121', '194', '200', '80', '101', '22', '12', '135', '97', '205', '181', '60', '193', '15', '140', '172', '191', '241', '222', '135', '29', '25', '179', '191', '191', '259', '194', '133', '240', '121', '121', '241', '195', '29', '191', '209', '18', '139', '139', '50', '18', '12', '18', '133', '200', '25', '10', '70', '25', '242', '18', '161', '49', '172', '172', '172', '172', '128', '96', '195', '200', '78', '10', '239', '22', '173', '142', '142', '22', '194', '192', '3', '200', '241', '96', '241', '62', '139', '213', '202', '135', '79', '120', '192', '78', '118', '241', '46', '151', '150', '151', '150', '118', '221', '183', '237', '196', '140', '140', '39', '179', '213', '140', '41', '73', '78', '217', '258', '30', '118', '140', '219', '50', '10', '139', '150', '251', '24', '10', '18', '78', '18', '172', '135', '146', '71', '196', '202', '197', '25', '106', '160', '215', '15', '6', '101', '165', '140', '145', '80', '3', '238', '239', '78', '268', '0', '109', '191', '215', '238', '238', '238', '203', '50', '146', '15', '25', '156', '191', '16', '2', '195', '189', '96', '193', '109', '204', '161', '185', '210', '50', '235', '30', '242', '193', '102', '194', '121', '177', '241', '102', '132', '240', '71', '192', '79', '25', '197', '39', '161', '22', '258', '237', '15', '7', '78', '156', '197', '70', '176', '139', '240', '132', '78', '177', '191', '194', '117', '10', '79', '238', '195', '24', '101', '118', '196', '2', '102', '21', '180', '185', '170', '79', '139', '135', '191', '151', '200', '191', '154', '262', '200', '237', '56', '27', '191', '180', '258', '139', '139', '192', '30', '0', '242', '239', '242', '25', '191', '242', '240', '3', '56', '215', '171', '192', '193', '222', '238', '237', '215', '272', '2', '156', '237', '151', '189', '80', '15', '260', '260', '217', '235', '22', '228', '135', '18', '191', '157', '78', '2', '240', '213', '179', '2', '191', '79', '79', '153', '3', '194', '78', '121', '71', '121', '149', '257', '194', '191', '188', '15', '25', '125', '102', '15', '130', '130', '177', '10', '135', '265', '240', '149', '196', '200', '204', '204', '213', '177', '197', '194', '13', '202', '30', '213', '125', '193', '125', '125', '125', '125', '125', '15', '180', '78', '1', '25', '96', '200', '29', '156', '172', '202', '22', '78', '197', '188', '241', '101', '101', '101', '221', '50', '191', '15', '15', '0', '101', '101', '168', '56', '197', '130', '62', '204', '29', '10', '96', '97', '139', '97', '97', '2', '2', '262', '39', '176', '172', '194', '194', '18', '78', '13', '80', '139', '196', '207', '191', '151', '192', '150', '13', '104', '104', '101', '15', '125', '101', '15', '0', '50', '194', '264', '50', '25', '25', '125', '125', '125', '125', '125', '25', '29', '25', '25', '78', '240', '261', '19', '2', '74', '78', '238', '50', '50', '203', '197', '71', '104', '80', '194', '149', '170', '130', '2', '237', '170', '202', '48', '97', '93', '179', '235', '78', '102', '102', '70', '208', '238', '104', '146', '118', '118', '96', '237', '274', '218', '241', '102', '146', '209', '209', '240', '170', '137', '7', '197', '191', '264', '29', '46', '96', '193', '19', '12', '213', '215', '275', '179', '18', '183', '78', '221', '18', '170', '119', '21', '140', '240', '96', '192', '192', '21', '96', '149', '239', '135', '191', '240', '184', '183', '24', '2', '96', '197', '194', '215', '18', '192', '121', '202', '196', '25', '97', '214', '3', '97', '16', '0', '192', '135', '215', '5', '12', '80', '143', '202', '242', '151', '13', '209', '203', '200', '30', '209', '194', '102', '102', '102', '102', '102', '102', '22', '24', '6', '18', '5', '191', '101', '50', '101', '101', '192', '153', '153', '101', '101', '10', '192', '25', '140', '50', '241', '102', '102', '32', '150', '101', '102', '153', '102', '101', '153', '102', '25', '10', '125', '18', '125', '177', '209', '102', '10', '78', '78', '137', '18', '237', '241', '175', '195', '175', '96', '176', '237', '96', '96', '101', '193', '238', '170', '238', '102', '222', '268', '238', '102', '244', '34', '18', '244', '202', '149', '149', '193', '102', '170', '194', '75', '101', '222', '238', '50', '195', '135', '22', '93', '18', '102', '102', '93', '93', '191', '147', '244', '96', '2', '241', '241', '93', '189', '189', '177', '35', '137', '197', '191', '40', '40', '195', '176', '35', '200', '200', '237', '135', '140', '140', '93', '156', '200', '78', '191', '22', '81', '238', '101', '213', '101', '78', '109', '19', '140', '196', '78', '78', '76', '218', '14', '265', '101', '241', '155', '4', '50', '78', '78', '238', '135', '191', '265', '81', '238', '238', '102', '102', '102', '140', '162', '32', '17', '17', '171', '171', '191', '101', '102', '192', '101', '17', '171', '135', '102', '242', '195', '237', '21', '18', '238', '191', '278', '140', '149', '97', '260', '155', '276', '192', '196', '21', '191', '2', '97', '0', '150', '221', '213', '209', '238', '80', '260', '194', '104', '96', '193', '14', '240', '193', '117', '117', '155', '17', '264', '260', '209', '209', '2', '137', '71', '72', '101', '205', '173', '191', '118', '24', '41', '197', '78', '94', '237', '156', '193', '153', '154', '153', '209', '110', '78', '78', '213', '238', '78', '191', '93', '79', '177', '267', '133', '133', '46', '139', '40', '143', '74', '241', '202', '50', '3', '102', '78', '149', '191', '156', '18', '93', '195', '192', '137', '240', '135', '204', '189', '22', '56', '154', '25', '192', '130', '277', '191', '80', '39', '21', '21', '238', '238', '23', '192', '191', '192', '94', '94', '80', '139', '17', '10', '22', '227', '227', '140', '102', '21', '17', '227', '227', '23', '243', '264', '18', '274', '149', '238', '30', '221', '22', '238', '193', '135', '205', '195', '238', '238', '197', '130', '195', '18', '104', '102', '14', '59', '104', '18', '18', '258', '34', '241', '138', '188', '176', '102', '1', '140', '140', '22', '22', '278', '10', '96', '35', '145', '120', '33', '33', '22', '33', '22', '35', '22', '19', '33', '102', '192', '192', '175', '22', '72', '191', '222', '125', '22', '79', '18', '194', '18', '250', '192', '192', '96', '48', '274', '273', '132', '133', '192', '40', '171', '171', '246', '191', '10', '166', '197', '195', '125', '125', '200', '153', '192', '149', '110', '139', '194', '48', '102', '102', '104', '203', '238', '238', '238', '78', '18', '238', '121', '18', '238', '96', '238', '238', '13', '191', '18', '18', '240', '96', '40', '188', '18', '137', '175', '28', '28', '28', '149', '191', '191', '207', '239', '239', '153', '153', '153', '153', '153', '153', '154', '153', '154', '153', '153', '153', '18', '0', '27', '18', '25', '96', '96', '137', '33', '10', '130', '153', '153', '154', '180', '137', '44', '241', '153', '268', '25', '161', '25', '39', '242', '4', '238', '195', '102', '135', '13', '229', '140', '1', '79', '192', '229', '191', '118', '21', '260', '203', '18', '177', '191', '10', '238', '10', '133', '14', '175', '175', '192', '18', '153', '153', '254', '140', '149', '22', '22', '22', '237', '40', '135', '191', '192', '192', '62', '205', '19', '238', '176', '214', '200', '96', '15', '153', '191', '108', '29', '166', '28', '193', '197', '96', '195', '108', '102', '241', '195', '10', '33', '166', '197', '168', '2', '96', '26', '33', '204', '258', '149', '153', '153', '40', '137', '241', '166', '221', '23', '221', '153', '15', '113', '113', '78', '171', '40', '71', '21', '71', '80', '15', '191', '191', '135', '110', '10', '259', '102', '149', '149', '109', '109', '175', '97', '97', '97', '153', '15', '238', '193', '175', '38', '38', '137', '18', '235', '155', '137', '265', '110', '18', '18', '256', '191', '224', '173', '104', '10', '152', '24', '24', '24', '17', '40', '40', '40', '40', '40', '40', '112', '6', '18', '18', '210', '96', '191', '27', '213', '17', '153', '26', '133', '18', '18', '153', '38', '96', '40', '38', '40', '40', '10', '17', '17', '95', '71', '192', '17', '259', '213', '200', '15', '11', '207', '194', '18', '135', '238', '191', '241', '191', '116', '35', '147', '156', '67', '78', '17', '227', '191', '117', '188', '130', '72', '96', '235', '97', '18', '150', '203', '25', '126', '149', '200', '94', '4', '4', '1', '244', '228', '252', '149', '191', '149', '15', '17', '17', '29', '18', '18', '116', '23', '237', '94', '184', '268', '197', '175', '243', '28', '16', '10', '50', '191', '102', '102', '153', '149', '111', '78', '191', '135', '18', '35', '78', '238', '208', '93', '95', '258', '17', '111', '111', '262', '22', '22', '104', '161', '209', '21', '195', '12', '116', '241', '242', '239', '17', '97', '16', '40', '117', '94', '96', '78', '15', '103', '195', '135', '235', '247', '135', '130', '260', '239', '176', '93', '191', '209', '239', '93', '264', '30', '18', '30', '18', '18', '103', '191', '97', '17', '95', '115', '209', '140', '22', '22', '241', '10', '153', '3', '188', '238', '226', '195', '97', '96', '238', '97', '130', '97', '116', '96', '96', '242', '45', '149', '166', '155', '96', '116', '264', '10', '237', '192', '71', '22', '71', '175', '149', '116', '237', '274', '18', '235', '176', '116', '202', '250', '250', '78', '2', '196', '2', '97', '143', '18', '111', '111', '80', '78', '153', '80', '40', '110', '30', '112', '30', '111', '215', '112', '149', '78', '78', '209', '110', '110', '238', '109', '239', '112', '151', '154', '16', '16', '16', '78', '137', '179', '72', '149', '10', '111', '268', '96', '170', '111', '38', '192', '153', '183', '195', '30', '30', '147', '195', '147', '93', '264', '153', '25', '18', '19', '22', '74', '40', '93', '166', '15', '127', '147', '192', '260', '238', '103', '269', '127', '19', '19', '153', '18', '78', '238', '30', '30', '241', '16', '235', '250', '204', '238', '218', '40', '93', '184', '18', '0', '241', '195', '94', '154', '109', '156', '10', '21', '30', '197', '39', '78', '24', '107', '142', '142', '228', '229', '229', '229', '232', '179', '239', '142', '232', '23', '175', '147', '137', '137', '137', '171', '191', '117', '128', '127', '204', '98', '147', '235', '239', '153', '15', '204', '255', '80', '204', '204', '149', '113', '19', '167', '22', '250', '17', '17', '17', '18', '17', '67', '121', '125', '26', '97', '97', '97', '97', '247', '124', '163', '147', '112', '111', '208', '108', '38', '209', '154', '154', '194', '135', '22', '147', '241', '120', '218', '260', '156', '155', '241', '192', '215', '180', '179', '149', '40', '193', '192', '150', '102', '93', '1', '149', '137', '133', '102', '102', '102', '78', '145', '21', '96', '175', '175', '78', '123', '30', '95', '19', '18', '110', '18', '152', '152', '147', '26', '135', '260', '158', '203', '195', '158', '112', '156', '125', '213', '230', '30', '15', '125', '106', '111', '195', '181', '130', '195', '130', '239', '15', '194', '16', '15', '111', '61', '72', '57', '128', '239', '239', '203', '25', '218', '10', '155', '191', '46', '98', '175', '260', '102', '21', '191', '260', '191', '239', '15', '30', '93', '147', '103', '78', '177', '30', '95', '155', '155', '155', '103', '103', '112', '241', '191', '191', '17', '78', '153', '150', '113', '115', '147', '78', '40', '163', '33', '15', '149', '103', '192', '191', '96', '106', '115', '143', '235', '200', '204', '238', '204', '204', '74', '196', '113', '246', '102', '150', '16', '176', '20', '195', '128', '197', '15', '71', '71', '193', '24', '24', '24', '170', '204', '209', '111', '144', '112', '22', '222', '224', '28', '209', '155', '213', '275', '196', '239', '195', '25', '103', '149', '128', '19', '96', '102', '135', '17', '232', '155', '191', '20', '18', '241', '30', '25', '205', '97', '97', '96', '72', '182', '135', '135', '20', '149', '149', '78', '135', '40', '18', '153', '10', '26', '158', '205', '157', '157', '135', '135', '140', '155', '192', '26', '268', '263', '245', '155', '15', '177', '159', '212', '161', '239', '200', '40', '35', '153', '197', '259', '15', '254', '175', '10', '30', '239', '67', '195', '200', '191', '22', '66', '191', '112', '133', '128', '235', '161', '155', '17', '93', '36', '113', '239', '18', '18', '191', '165', '163', '163', '238', '79', '158', '40', '102', '40', '200', '191', '60', '241', '245', '96', '78', '30', '15', '173', '192', '192', '158', '64', '113', '200', '74', '128', '135', '115', '115', '10', '74', '102', '102', '115', '111', '238', '244', '109', '109', '238', '152', '10', '237', '18', '50', '16', '173', '204', '15', '208', '276', '43', '109', '258', '222', '15', '94', '235', '203', '79', '128', '113', '3', '204', '106', '133', '214', '218', '25', '98', '166', '71', '30', '175', '175', '97', '135', '108', '19', '18', '196', '35', '12', '196', '78', '166', '155', '72', '155', '22', '109', '258', '196', '263', '161', '204', '210', '14', '208', '208', '208', '208', '204', '102', '102', '213', '155', '251', '153', '153', '153', '154', '154', '237', '97', '19', '28', '94', '136', '112', '19', '97', '152', '153', '153', '111', '111', '135', '135', '105', '17', '105', '152', '152', '152', '153', '79', '266', '135', '120', '125', '149', '50', '162', '72', '17', '147', '109', '115', '24', '105', '244', '149', '238', '203', '35', '202', '177', '173', '239', '19', '19', '179', '237', '14', '18', '185', '58', '158', '144', '213', '237', '155', '250', '239', '93', '237', '97', '195', '157', '107', '109', '246', '165', '98', '49', '2', '266', '149', '20', '18', '71', '108', '22', '115', '239', '40', '178', '232', '238', '218', '118', '118', '210', '235', '46', '228', '192', '112', '149', '96', '111', '21', '95', '95', '95', '18', '57', '206', '18', '239', '157', '80', '196', '112', '176', '99', '106', '71', '235', '135', '18', '209', '78', '20', '20', '103', '33', '3', '103', '166', '79', '79', '156', '229', '18', '238', '2', '16', '128', '200', '117', '22', '197', '166', '78', '237', '241', '107', '53', '189', '204', '243', '163', '221', '15', '200', '221', '152', '23', '193', '107', '195', '99', '97', '19', '125', '276', '113', '193', '64', '78', '197', '117', '175', '135', '135', '78', '93', '154', '153', '256', '256', '135', '137', '135', '137', '149', '110', '15', '159', '21', '158', '192', '192', '117', '29', '97', '258', '78', '46', '5', '80', '15', '105', '260', '161', '191', '2', '138', '117', '215', '176', '156', '194', '265', '115', '181', '21', '14', '106', '106', '108', '200', '156', '103', '102', '29', '17', '204', '175', '227', '118', '18', '239', '204', '258', '78', '155', '213', '109', '192', '186', '186', '15', '241', '133', '30', '30', '239', '30', '218', '109', '138', '241', '241', '45', '135', '112', '232', '104', '106', '200', '53', '23', '112', '17', '117', '59', '260', '195', '106', '117', '159', '204', '200', '228', '165', '165', '261', '200', '227', '166', '93', '21', '80', '112', '152', '196', '203', '228', '98', '102', '15', '139', '110', '13', '238', '6', '171', '94', '208', '155', '112', '203', '239', '191', '39', '136', '203', '195', '46', '203', '148', '192', '210', '23', '4', '153', '29', '165', '192', '204', '232', '94', '112', '239', '98', '71', '248', '17', '155', '10', '78', '191', '191', '111', '25', '25', '19', '19', '17', '235', '102', '265', '195', '235', '163', '238', '71', '197', '107', '155', '30', '72', '19', '259', '80', '155', '23', '64', '239', '197', '156', '57', '203', '258', '200', '205', '246', '238', '258', '227', '238', '238', '19', '148', '161', '111', '105', '202', '109', '262', '133', '137', '221', '135', '52', '170', '111', '36', '30', '156', '102', '268', '115', '22', '153', '133', '259', '98', '235', '135', '191', '25', '235', '25', '29', '94', '195', '203', '212', '112', '135', '245', '260', '200', '96', '158', '23', '165', '22', '18', '249', '240', '109', '43', '37', '15', '241', '241', '23', '135', '240', '108', '98', '268', '275', '191', '136', '263', '259', '196', '98', '98', '204', '248', '123', '105', '204', '240', '9', '9', '98', '218', '189', '238', '246', '155', '130', '157', '74', '109', '29', '189', '203', '213', '195', '241', '159', '238', '204', '218', '105', '23', '16', '263', '206', '150', '152', '204', '202', '173', '267', '18', '153', '155', '196', '2', '97', '158', '158', '204', '98', '202', '98', '204', '258', '98', '22', '113', '113', '135', '71', '237', '155', '163', '250', '233', '26', '26', '26', '26', '26', '19', '170', '18', '15', '19', '191', '30', '30', '48', '35', '30', '237', '46', '119', '16', '78', '239', '150', '245', '235', '158', '27', '241', '30', '38', '107', '75', '241', '135', '106', '106', '98', '159', '96', '135', '111', '176', '175', '200', '18', '14', '242', '163', '187', '20', '12', '191', '23', '106', '171', '25', '78', '162', '191', '157', '94', '237', '246', '203', '108', '238', '204', '204', '1', '102', '154', '106', '170', '102', '200', '2', '135', '106', '258', '258', '227', '26', '15', '25', '71', '203', '96', '3', '241', '200', '30', '108', '98', '108', '27', '106', '26', '150', '259', '78', '113', '113', '158', '202', '16', '136', '239', '156', '239', '35', '276', '252', '115', '22', '22', '258', '172', '3', '154', '3', '98', '237', '102', '235', '112', '206', '98', '57', '118', '153', '18', '26', '30', '196', '163', '191', '153', '72', '24', '156', '117', '122', '30', '135', '192', '98', '28', '30', '30', '95', '241', '155', '17', '195', '31', '103', '202', '98', '237', '94', '112', '163', '163', '94', '163', '112', '19', '195', '191', '15', '135', '78', '109', '160', '152', '97', '98', '107', '167', '97', '138', '138', '26', '191', '97', '97', '30', '30', '206', '202', '13', '30', '240', '111', '261', '79', '0', '111', '19', '152', '107', '107', '18', '98', '28', '258', '244', '106', '237', '204', '191', '15', '183', '202', '16', '111', '166', '268', '202', '12', '30', '78', '110', '133', '71', '197', '111', '52', '79', '171', '156', '94', '13', '118', '111', '71', '133', '18', '105', '30', '22', '133', '60', '237', '191', '6', '126', '3', '238', '276', '78', '162', '150', '115', '191', '107', '200', '148', '135', '72', '155', '241', '4', '108', '160', '203', '20', '133', '2', '80', '80', '154', '18', '15', '115', '239', '167', '167', '35', '268', '18', '221', '18', '195', '204', '195', '216', '133', '112', '104', '223', '191', '115', '30', '30', '30', '30', '30', '202', '204', '165', '200', '200', '29', '191', '237', '267', '18', '175', '81', '264', '264', '178', '115', '102', '178', '212', '273', '202', '109', '259', '166', '135', '155', '219', '161', '18', '94', '241', '191', '196', '191', '98', '26', '26', '204', '204', '135', '27', '238', '260', '182', '26', '98', '94', '93', '238', '156', '19', '111', '79', '16', '192', '113', '19', '241', '18', '202', '260', '272', '106', '163', '156', '106', '17', '109', '221', '78', '203', '7', '136', '197', '193', '191', '98', '112', '200', '95', '221', '98', '161', '191', '3', '106', '189', '111', '25', '196', '196', '241', '163', '200', '24', '78', '176', '3', '98', '102', '98', '19', '161', '224', '80', '18', '191', '112', '105', '254', '71', '200', '137', '29', '20', '165', '8', '106', '23', '106', '155', '200', '265', '135', '126', '203', '188', '161', '158', '258', '255', '98', '203', '50', '35', '102', '158', '175', '40', '239', '26', '66', '57', '193', '155', '15', '135', '10', '107', '106', '229', '18', '191', '204', '107', '99', '235', '238', '18', '205', '205', '35', '238', '208', '203', '110', '110', '109', '112', '23', '18', '19', '182', '156', '18', '18', '156', '111', '135', '137', '133', '137', '240', '99', '197', '113', '15', '165', '39', '97', '97', '15', '150', '163', '109', '268', '15', '15', '238', '203', '67', '161', '197', '18', '196', '97', '161', '197', '161', '185', '238', '191', '30', '121', '133', '160', '160', '74', '135', '75', '54', '135', '196', '23', '241', '166', '194', '239', '135', '240', '208', '238', '98', '135', '152', '176', '22', '174', '113', '241', '237', '57', '195', '191', '24', '165', '165', '204', '204', '56', '109', '27', '153', '252', '228', '165', '252', '200', '28', '158', '135', '196', '22', '18', '133', '30', '204', '237', '196', '93', '106', '94', '25', '93', '159', '4', '195', '23', '237', '25', '199', '57', '26', '276', '274', '204', '75', '18', '204', '108', '22', '133', '191', '111', '204', '203', '133', '237', '166', '4', '62', '202', '166', '161', '165', '74', '109', '118', '200', '22', '206', '25', '209', '17', '155', '204', '64', '23', '30', '202', '149', '171', '135', '136', '184', '26', '18', '159', '195', '18', '166', '156', '98', '133', '38', '40', '18', '18', '166', '109', '109', '209', '22', '195', '163', '163', '108', '18', '15', '99', '135', '174', '26', '19', '166', '133', '259', '151', '235', '204', '228', '133', '30', '161', '111', '242', '84', '238', '98', '195', '78', '160', '28', '25', '264', '165', '41', '19', '55', '46', '202', '121', '15', '32', '98', '238', '0', '78', '31', '30', '203', '264', '203', '135', '104', '111', '78', '212', '111', '98', '126', '22', '2', '30', '237', '212', '265', '163', '106', '106', '235', '109', '223', '161', '202', '150', '170', '255', '112', '114', '170', '98', '96', '197', '36', '18', '228', '14', '255', '204', '102', '239', '0', '185', '30', '22', '23', '246', '199', '133', '191', '78', '195', '153', '112', '191', '200', '170', '171', '200', '78', '128', '25', '153', '237', '99', '94', '208', '107', '208', '208', '78', '204', '204', '27', '191', '272', '134', '158', '158', '174', '118', '64', '18', '258', '150', '189', '18', '153', '241', '204', '112', '106', '135', '71', '148', '7', '133', '140', '187', '23', '223', '19', '19', '2', '136', '2', '200', '161', '15', '98', '158', '148', '3', '250', '153', '235', '165', '220', '176', '110', '240', '151', '148', '133', '74', '193', '46', '27', '173', '94', '176', '105', '256', '161', '226', '18', '78', '191', '171', '243', '26', '0', '108', '167', '159', '239', '150', '112', '155', '79', '241', '202', '165', '254', '107', '202', '156', '166', '193', '239', '135', '15', '203', '187', '148', '25', '15', '22', '194', '239', '56', '191', '176', '262', '200', '110', '72', '99', '102', '135', '98', '113', '15', '15', '159', '262', '171', '151', '98', '204', '213', '255', '26', '243', '175', '156', '98', '107', '191', '165', '241', '26', '165', '171', '30', '160', '166', '266', '11', '19', '211', '93', '254', '30', '239', '115', '273', '205', '112', '237', '263', '74', '98', '79', '122', '94', '265', '155', '191', '16', '235', '229', '98', '191', '227', '17', '111', '267', '135', '202', '195', '10', '180', '196', '196', '133', '135', '133', '95', '111', '122', '258', '191', '3', '114', '107', '19', '264', '74', '56', '200', '200', '133', '161', '97', '15', '97', '197', '196', '253', '93', '202', '258', '106', '153', '26', '109', '40', '149', '8', '196', '15', '119', '155', '23', '237', '193', '264', '148', '151', '238', '200', '94', '75', '148', '175', '24', '195', '171', '30', '203', '200', '202', '16', '158', '223', '191', '22', '264', '135', '35', '57', '94', '191', '196', '202', '98', '18', '196', '3', '133', '78', '191', '208', '258', '18', '200', '29', '135', '109', '110', '2', '185', '98', '133', '19', '22', '204', '95', '238', '25', '133', '250', '135', '135', '3', '47', '158', '187', '2', '78', '115', '155', '3', '218', '166', '112', '266', '22', '195', '135', '72', '97', '197', '209', '127', '80', '2', '98', '14', '239', '15', '113', '165', '204', '78', '204', '191', '175', '200', '193', '18', '196', '133', '109', '237', '56', '196', '3', '163', '163', '163', '195', '15', '263', '18', '84', '18', '133', '107', '18', '102', '156', '238', '102', '26', '244', '82', '75', '23', '258', '238', '136', '3', '204', '184', '200', '196', '158', '106', '27', '238', '155', '152', '50', '50', '2', '235', '98', '203', '166', '109', '25', '18', '151', '133', '200', '26', '26', '26', '19', '107', '156', '23', '26', '3', '70', '225', '41', '26', '239', '26', '186', '26', '34', '272', '272', '104', '78', '260', '18', '204', '26', '78', '18', '210', '135', '203', '71', '207', '26', '133', '13', '25', '26', '115', '94', '264', '135', '244', '79', '30', '263', '22', '109', '205', '176', '106', '22', '17', '151', '106', '191', '160', '160', '165', '78', '177', '196', '260', '166', '35', '191', '260', '98', '18', '237', '18', '161', '18', '197', '133', '30', '35', '200', '170', '155', '245', '19', '200', '160', '96', '172', '165', '55', '137', '241', '133', '191', '112', '25', '126', '238', '18', '18', '108', '135', '161', '135', '73', '112', '107', '8', '112', '109', '195', '25', '30', '105', '161', '241', '110', '75', '238', '215', '238', '104', '25', '3', '155', '30', '16', '3', '175', '99', '3', '200', '23', '3', '199', '203', '191', '196', '196', '204', '98', '105', '133', '197', '135', '161', '39', '15', '30', '133', '55', '191', '203', '133', '203', '140', '200', '149', '30', '96', '135', '23', '26', '3', '18', '155', '158', '135', '135', '18', '250', '18', '22', '114', '18', '18', '166', '18', '30', '18', '239', '18', '161', '50', '50', '49', '51', '23', '55', '98', '78', '210', '211', '18', '239', '78', '13', '193', '114', '78', '18', '25', '133', '19', '176', '4', '173', '50', '108', '17', '4', '204', '94', '112', '207', '109', '115', '23', '235', '197', '194', '197', '213', '148', '40', '157', '107', '200', '147', '14', '14', '18', '238', '18', '238', '18', '238', '57', '200', '135', '30', '18', '191', '191', '200', '166', '151', '161', '73', '19', '73', '239', '239', '165', '108', '2', '93', '166', '133', '133', '173', '35', '193', '114', '74', '19', '40', '58', '104', '16', '260', '160', '158', '112', '210', '158', '167', '203', '203', '3', '3', '109', '30', '133', '258', '70', '204', '200', '152', '155', '192', '204', '108', '191', '204', '26', '98', '26', '98', '264', '158', '18', '150', '137', '153', '196', '199', '191', '115', '168', '157', '149', '151', '78', '137', '160', '259', '166', '196', '171', '175', '199', '166', '161', '70', '13', '166', '152', '152', '206', '58', '18', '207', '58', '74', '207', '29', '46', '109', '135', '74', '174', '109', '40', '10', '30', '258', '16', '111', '30', '15', '239', '204', '48', '25', '196', '0', '156', '72', '72', '96', '22', '72', '133', '133', '238', '26', '3', '50', '152', '193', '235', '136', '153', '50', '1', '19', '239', '30', '25', '179', '161', '30', '191', '3', '27', '133', '3', '40', '23', '133', '150', '151', '265', '32', '191', '3', '25', '170', '238', '40', '259', '166', '137', '241', '133', '209', '159', '78', '78', '153', '26', '52', '241', '36', '250', '96', '30', '30', '38', '18', '18', '238', '99', '72', '111', '3', '244', '191', '226', '3', '191', '114', '135', '109', '40', '79', '202', '78', '114', '238', '10', '150', '108', '163', '108', '113', '197', '109', '115', '161', '3', '238', '3', '237', '161', '107', '155', '78', '202', '112', '38', '195', '137', '250', '133', '237', '161', '25', '26', '25', '15', '15', '239', '166', '114', '229', '111', '40', '135', '94', '102', '162', '204', '237', '17', '166', '136', '22', '98', '99', '99', '166', '35', '196', '3', '203', '135', '135', '84', '98', '266', '238', '13', '166', '78', '193', '98', '241', '104', '153', '192', '200', '10', '95', '95', '148', '107', '14', '17', '191', '239', '19', '203', '255', '197', '213', '94', '3', '107', '23', '35', '10', '237', '235', '78', '29', '18', '160', '200', '150', '99', '109', '155', '161', '133', '114', '18', '18', '18', '264', '133', '104', '161', '70', '17', '204', '195', '204', '241', '99', '82', '241', '241', '161', '18', '235', '175', '2', '2', '202', '50', '230', '26', '40', '40', '200', '78', '156', '26', '112', '26', '30', '71', '184', '200', '135', '153', '23', '106', '125', '175', '133', '133', '7', '38', '196', '161', '0', '49', '241', '158', '15', '21', '104', '2', '3', '176', '259', '135', '78', '75', '112', '200', '1', '207', '238', '161', '150', '158', '2', '45', '31', '237', '149', '3', '26', '165', '135', '152', '115', '133', '18', '40', '166', '30', '173', '40', '26', '78', '50', '48', '50', '13', '45', '200', '200', '18', '151', '205', '135', '38', '117', '196', '135', '155', '111', '196', '196', '166', '23', '25', '161', '18', '124', '259', '165', '58', '170', '204', '250', '103', '160', '238', '194', '163', '241', '255', '235', '17', '133', '26', '135', '196', '237', '114', '258', '193', '248', '40', '195', '84', '238', '146', '199', '40', '161', '22', '137', '43', '166', '156', '4', '104', '133', '177', '211', '193', '78', '203', '246', '237', '264', '3', '149', '78', '78', '111', '239', '13', '238', '176', '112', '150', '156', '3', '6', '118', '178', '178', '21', '186', '150', '241', '50', '243', '191', '121', '278', '29', '200', '18', '3', '197', '191', '98', '173', '195', '25', '195', '264', '221', '15', '156', '25', '200', '211', '195', '26', '35', '2', '135', '126', '113', '167', '191', '264', '196', '133', '18', '26', '146', '203', '219', '23', '160', '160', '135', '259', '3', '159', '120', '160', '106', '240', '133', '115', '258', '38', '22', '3', '26', '22', '107', '49', '48', '25', '237', '72', '219', '196', '196', '111', '135', '196', '18', '166', '172', '135', '26', '258', '70', '98', '40', '166', '23', '96', '3', '30', '155', '126', '146', '114', '242', '237', '157', '238', '241', '40', '133', '200', '161', '175', '200', '175', '84', '266', '18', '99', '30', '242', '151', '153', '166', '199', '228', '3', '237', '132', '110', '135', '80', '15', '15', '15', '84', '133', '15', '177', '3', '202', '255', '71', '115', '235', '19', '18', '18', '102', '58', '78', '165', '166', '264', '12', '150', '15', '2', '18', '135', '109', '200', '258', '128', '3', '199', '197', '239', '109', '222', '239', '107', '125', '132', '204', '204', '133', '135', '30', '158', '203', '15', '264', '237', '133', '196', '3', '252', '26', '98', '79', '199', '210', '0', '99', '30', '26', '259', '6', '49', '166', '98', '108', '9', '133', '102', '155', '98', '114', '78', '35', '166', '188', '202', '235', '30', '149', '165', '176', '138', '29', '153', '25', '25', '137', '133', '166', '135', '125', '71', '193', '154', '165', '98', '23', '56', '198', '26', '26', '0', '109', '136', '163', '151', '25', '15', '241', '195', '204', '191', '195', '161', '175', '133', '237', '78', '165', '258', '135', '78', '135', '258', '18', '235', '30', '187', '150', '275', '149', '2', '36', '112', '22', '161', '238', '203', '112', '265', '75', '133', '205', '258', '158', '18', '26', '18', '25', '26', '15', '26', '239', '15', '240', '18', '71', '71', '78', '135', '3', '148', '166', '26', '176', '2', '197', '150', '176', '19', '18', '97', '166', '18', '153', '250', '133', '210', '20', '192', '196', '26', '191', '2', '81', '70', '5', '79', '3', '30', '193', '193', '69', '160', '176', '246', '108', '54', '250', '135', '200', '80', '3', '167', '18', '166', '200', '152', '23', '149', '19', '14', '13', '237', '196', '19', '112', '254', '204', '191', '23', '264', '199', '209', '137', '31', '135', '238', '78', '18', '191', '15', '164', '195', '74', '163', '163', '41', '99', '26', '135', '26', '40', '40', '250', '200', '200', '26', '237', '4', '195', '158', '35', '71', '191', '3', '218', '274', '58', '3', '23', '177', '132', '49', '62', '238', '166', '97', '264', '193', '29', '26', '49', '241', '197', '200', '64', '227', '258', '44', '15', '133', '175', '238', '109', '254', '237', '198', '175', '166', '213', '174', '196', '107', '259', '98', '10', '196', '210', '191', '96', '166', '135', '15', '197', '78', '78', '195', '176', '18', '98', '129', '170', '18', '247', '203', '26', '158', '40', '64', '200', '163', '196', '19', '205', '197', '193', '191', '69', '96', '74', '35', '36', '64', '133', '172', '19', '266', '96', '94', '103', '258', '193', '264', '98', '254', '50', '193', '25', '170', '75', '84', '27', '252', '203', '94', '94', '19', '98', '137', '12', '153', '3', '155', '135', '238', '15', '196', '15', '26', '84', '49', '27', '135', '203', '48', '133', '129', '133', '258', '155', '35', '195', '71', '241', '135', '166', '193', '165', '104', '196', '117', '196', '26', '237', '109', '95', '196', '60', '40', '158', '193', '250', '104', '94', '178', '27', '193', '78', '106', '79', '204', '133', '237', '235', '135', '202', '70', '15', '264', '241', '254', '110', '199', '49', '255', '30', '193', '238', '204', '102', '97', '161', '43', '228', '1', '6', '196', '193', '26', '15', '128', '46', '155', '32', '133', '69', '47', '135', '78', '205', '200', '200', '202', '3', '95', '191', '78', '26', '248', '258', '191', '240', '49', '110', '38', '193', '200', '200', '96', '200', '204', '135', '9', '200', '258', '15', '46', '15', '149', '155', '197', '2', '166', '151', '72', '135', '97', '211', '238', '135', '163', '161', '26', '163', '129', '241', '239', '241', '16', '26', '18', '263', '260', '166', '3', '264', '195', '79', '132', '251', '50', '133', '15', '25', '15', '191', '15', '78', '50', '78', '71', '227', '69', '200', '69', '109', '64', '133', '197', '196', '3', '45', '238', '26', '132', '104', '196', '193', '136', '80', '26', '75', '117', '14', '78', '75', '98', '15', '2', '15', '50', '255', '136', '3', '112', '173', '69', '191', '19', '239', '235', '109', '70', '135', '78', '165', '238', '262', '199', '35', '203', '224', '193', '15', '18', '150', '238', '197', '238', '193', '135', '3', '150', '187', '199', '239', '109', '152', '27', '257', '6', '235', '104', '191', '133', '166', '155', '26', '16', '136', '198', '98', '154', '235', '27', '102', '30', '191', '15', '132', '132', '114', '107', '241', '241', '156', '199', '241', '49', '62', '117', '258', '30', '163', '133', '238', '200', '3', '15', '109', '2', '196', '165', '166', '159', '210', '35', '15', '17', '200', '241', '106', '203', '197', '197', '79', '98', '98', '23', '204', '48', '129', '57', '258', '202', '215', '238', '70', '64', '193', '189', '57', '200', '0', '98', '135', '204', '25', '30', '204', '27', '26', '78', '135', '109', '3', '197', '3', '167', '35', '196', '225', '2', '194', '200', '15', '161', '198', '23', '133', '238', '195', '35', '50', '135', '152', '163', '21', '15', '94', '26', '94', '53', '260', '196', '154', '277', '200', '118', '6', '238', '50', '209', '50', '3', '238', '3', '15', '107', '135', '199', '15', '135', '26', '258', '251', '203', '19', '259', '195', '199', '238', '15', '254', '112', '211', '200', '129', '84', '196', '3', '208', '259', '135', '104', '29', '136', '31', '78', '13', '3', '64', '75', '55', '114', '160', '96', '14', '18', '207', '75', '48', '15', '49', '97', '117', '196', '99', '258', '195', '264', '264', '78', '197', '30', '238', '109', '158', '32', '195', '149', '227', '65', '167', '162', '204', '250', '151', '151', '102', '26', '29', '26', '193', '74', '50', '193', '186', '15', '74', '259', '57', '188', '277', '277', '195', '195', '212', '78', '197', '64', '129', '259', '200', '199', '129', '4', '235', '197', '196', '161', '3', '255', '199', '102', '3', '13', '94', '53', '98', '15', '129', '15', '155', '71', '179', '50', '29', '136', '80', '254', '198', '162', '237', '3', '18', '260', '238', '117', '179', '156', '19', '45', '197', '78', '199', '102', '102', '237', '155', '69', '69', '163', '239', '15', '196', '80', '191', '30', '71', '158', '259', '2', '106', '148', '15', '135', '135', '135', '196', '19', '123', '15', '104', '19', '128', '230', '69', '98', '197', '241', '161', '195', '200', '3', '98', '78', '165', '163', '19', '196', '31', '173', '203', '55', '69', '209', '239', '79', '158', '254', '135', '51', '94', '162', '255', '95', '18', '133', '109', '50', '94', '199', '15', '197', '200', '133', '116', '50', '196', '161', '69', '133', '3', '258', '3', '197', '29', '3', '157', '197', '193', '4', '94', '195', '17', '48', '152', '8', '15', '155', '133', '195', '136', '150', '102', '237', '203', '164', '164', '250', '154', '13', '191', '58', '94', '70', '93', '264', '15', '30', '133', '237', '267', '199', '193', '76', '241', '157', '26', '200', '109', '179', '55', '98', '200', '129', '238', '235', '208', '109', '239', '258', '196', '193', '18', '107', '210', '70', '40', '129', '259', '193', '30', '158', '252', '40', '210', '78', '15', '78', '96', '208', '113', '258', '64', '80', '15', '133', '241', '15', '15', '258', '135', '109', '251', '251', '109', '74', '196', '170', '240', '254', '258', '204', '199', '26', '109', '197', '57', '45', '161', '57', '114', '3', '15', '264', '106', '57', '27', '18', '258', '29', '84', '161', '135', '114', '24', '30', '107', '107', '172', '209', '170', '18', '19', '133', '191', '195', '135', '156', '196', '94', '3', '84', '225', '199', '238', '106', '104', '70', '50', '12', '155', '114', '176', '237', '200', '108', '70', '240', '196', '19', '22', '276', '199', '238', '205', '104', '193', '108', '115', '259', '102', '19', '258', '18', '106', '161', '191', '114', '64', '15', '204', '35', '155', '40', '238', '136', '264', '133', '186', '15', '258', '166', '26', '6', '114', '114', '133', '114', '114', '13', '13', '13', '237', '250', '245', '136', '78', '13', '48', '18', '13', '25', '26', '93', '125', '50', '94', '148', '78', '194', '200', '25', '93', '274', '266', '70', '22', '3', '208', '240', '211', '135', '31', '18', '153', '18', '2', '15', '15', '15', '156', '15', '70', '73', '173', '13', '112', '69', '204', '70', '248', '135', '70', '3', '193', '193', '22', '162', '238', '57', '98', '15', '199', '199', '52', '151', '73', '5', '108', '23', '69', '95', '220', '106', '163', '152', '163', '108', '15', '58', '250', '19', '177', '245', '264', '93', '204', '25', '78', '155', '69', '155', '239', '241', '155', '200', '18', '3', '13', '196', '197', '204', '272', '95', '135', '165', '22', '210', '35', '69', '18', '170', '3', '25', '156', '78', '135', '102', '107', '255', '19', '70', '70', '109', '198', '156', '64', '170', '138', '78', '203', '166', '108', '114', '179', '228', '74', '49', '14', '264', '26', '195', '78', '107', '75', '18', '106', '151', '70', '58', '15', '196', '238', '109', '104', '104', '109', '136', '148', '210', '3', '161', '15', '104', '238', '208', '133', '108', '241', '18', '209', '74', '156', '114', '199', '174', '35', '18', '170', '135', '238', '244', '155', '162', '197', '3', '98', '227', '267', '238', '193', '50', '238', '98', '112', '124', '25', '114', '241', '159', '154', '78', '161', '136', '114', '153', '225', '154', '13', '50', '11', '25', '15', '209', '11', '166', '203', '259', '237', '22', '114', '276', '98', '278', '150', '198', '18', '15', '78', '15', '162', '26', '3', '196', '3', '102', '189', '71', '69', '16', '18', '152', '133', '135', '162', '241', '15', '104', '15', '259', '202', '2', '238', '207', '15', '204', '30', '215', '112', '210', '42', '96', '193', '15', '110', '163', '211', '78', '108', '15', '31', '27', '15', '31', '196', '195', '96', '15', '161', '94', '193', '195', '198', '98', '261', '258', '76', '15', '104', '258', '135', '18', '15', '202', '258', '108', '156', '13', '22', '251', '199', '155', '258', '211', '3', '207', '202', '259', '107', '202', '259', '135', '135', '148', '158', '154', '99', '99', '3', '207', '94', '25', '162', '31', '241', '25', '15', '114', '18', '114', '148', '19', '15', '203', '191', '19', '13', '193', '204', '25', '166', '155', '196', '98', '162', '197', '29', '152', '191', '133', '199', '230', '40', '25', '204', '79', '3', '19', '165', '204', '108', '208', '3', '251', '274', '15', '166', '264', '35', '13', '193', '114', '258', '13', '19', '133', '23', '17', '199', '204', '153', '250', '204', '16', '12', '107', '191', '18', '3', '15', '165', '15', '69', '17', '165', '17', '15', '15', '156', '23', '23', '15', '197', '104', '197', '26', '208', '135', '211', '250', '18', '204', '25', '196', '53', '69', '104', '104', '19', '78', '241', '230', '30', '195', '96', '26', '109', '196', '69', '238', '15', '18', '133', '26', '106', '109', '98', '15', '258', '274', '193', '203', '58', '57', '274', '2', '202', '161', '136', '135', '155', '241', '35', '151', '137', '5', '114', '96', '16', '209', '158', '15', '266', '161', '15', '25', '274', '202', '3', '161', '237', '267', '204', '125', '15', '151', '155', '259', '151', '210', '238', '15', '26', '132', '50', '173', '197', '114', '241', '15', '258', '109', '166', '274', '13', '191', '15', '26', '164', '258', '163', '15', '264', '11', '260', '148', '150', '207', '196', '98', '272', '133', '196', '107', '26', '191', '163', '191', '135', '3', '18', '18', '119', '69', '196', '15', '156', '135', '112', '258', '155', '256', '199', '204', '212', '18', '135', '254', '72', '258', '254', '15', '133', '18', '259', '193', '258', '204', '40', '18', '29', '155', '104', '107', '274', '241', '25', '26', '69', '148', '166', '46', '94', '135', '15', '155', '104', '241', '32', '274', '249', '15', '197', '104', '44', '259', '69', '191', '15', '198', '193', '133', '17', '258', '248', '166', '197', '200', '250', '204', '3', '225', '238', '163', '69', '15', '15', '26', '202', '19', '209', '78', '212', '15', '78', '243', '110', '191', '15', '18', '107', '38', '208', '166', '239', '237', '199', '15', '204', '258', '173', '172', '200', '274', '243', '19', '191', '135', '203', '69', '149', '148', '196', '256', '204', '97', '191', '26', '114', '274', '200', '69', '69', '64', '69', '239', '25', '274', '203', '78', '135', '196', '202', '17', '2', '2', '3', '98', '98', '62', '2', '95', '96', '2', '207', '78', '200', '26', '64', '18', '93', '49', '57', '154', '200', '69', '94', '98', '98', '56', '56', '166', '98', '212', '226', '226', '15', '69', '15', '273', '272', '104', '193', '3', '98', '199', '98', '25', '125', '259', '249', '108', '40', '26', '198', '218', '73', '148', '69', '204', '241', '35', '3', '274', '70', '18', '71', '35', '172', '133', '106', '69', '107', '199', '219', '19', '133', '69', '135', '109', '57', '191', '135', '252', '104', '3', '94', '3', '72', '126', '16', '36', '84', '241', '3', '218', '98', '108', '46', '19', '196', '118', '104', '29', '238', '69', '10', '204', '264', '259', '124', '242', '107', '133', '25', '64', '30', '194', '259', '159', '264', '274', '52', '200', '104', '29', '30', '193', '98', '94', '114', '204', '40', '18', '198', '97', '3', '99', '274', '239', '177', '15', '18', '126', '126', '208', '148', '15', '104', '29', '36', '218', '70', '15', '23', '278', '23', '36', '191', '210', '51', '245', '167', '72', '237', '25', '203', '36', '196', '193', '196', '15', '151', '207', '203', '103', '166', '135', '110', '27', '13', '27', '191', '98', '57', '166', '50', '21', '15', '197', '23', '2', '70', '69', '193', '274', '104', '166', '258', '126', '238', '161', '18', '193', '69', '140', '170', '197', '204', '165', '203', '170', '15', '58', '35', '83', '212', '150', '73', '25', '15', '64', '133', '210', '108', '79', '193', '15', '126', '165', '15', '208', '98', '3', '200', '126', '3', '40', '165', '98', '69', '15', '165', '133', '96', '173', '203', '133', '238', '248', '15', '7', '38', '207', '108', '165', '199', '18', '162', '27', '3', '72', '65', '250', '209', '166', '64', '209', '237', '212', '64', '225', '120', '108', '203', '135', '248', '69', '251', '135', '204', '160', '160', '135', '148', '15', '15', '23', '207', '274', '29', '28', '194', '150', '40', '104', '208', '125', '70', '18', '3', '19', '102', '104', '23', '29', '207', '133', '204', '111', '2', '125', '126', '7', '194', '209', '203', '83', '126', '126', '258', '119', '162', '241', '133', '18', '208', '197', '209', '15', '78', '80', '110', '237', '104', '30', '208', '103', '15', '200', '15', '238', '254', '108', '173', '40', '69', '175', '17', '155', '136', '26', '3', '135', '13', '241', '161', '166', '170', '162', '133', '53', '211', '204', '15', '161', '238', '110', '18', '191', '18', '50', '71', '30', '17', '208', '248', '203', '203', '196', '210', '258', '204', '3', '18', '196', '125', '173', '199', '249', '18', '104', '165', '241', '2', '193', '238', '78', '15', '166', '15', '126', '3', '237', '161', '52', '207', '3', '3', '50', '56', '22', '56', '69', '93', '135', '207', '69', '99', '3', '133', '68', '133', '161', '108', '126', '250', '60', '155', '98', '18', '16', '78', '78', '240', '235', '176', '22', '199', '160', '235', '274', '209', '64', '173', '137', '128', '25', '18', '160', '203', '70', '165', '193', '110', '207', '64', '85', '252', '78', '51', '2', '99', '19', '22', '198', '204', '166', '166', '151', '56', '133', '210', '26', '149', '109', '30', '209', '209', '51', '3', '193', '119', '208', '93', '165', '248', '55', '203', '204', '58', '18', '199', '211', '83', '32', '135', '212', '156', '19', '235', '173', '191', '209', '203', '146', '208', '58', '158', '102', '199', '200', '200', '173', '70', '52', '107', '211', '213', '237', '196', '205', '104', '135', '258', '255', '96', '19', '124', '128', '57', '199', '98', '166', '102', '40', '15', '238', '3', '148', '225', '126', '135', '114', '193', '204', '15', '209', '209', '193', '259', '64', '161', '26', '274', '126', '18', '196', '0', '60', '207', '17', '71', '218', '239', '248', '38', '154', '61', '128', '199', '196', '173', '25', '46', '98', '258', '108', '3', '108', '196', '93', '93', '99', '172', '69', '241', '196', '30', '135', '278', '109', '18', '5', '209', '57', '159', '70', '193', '163', '249', '196', '196', '72', '103', '133', '17', '204', '199', '166', '202', '29', '57', '193', '150', '98', '119', '25', '205', '18', '173', '166', '209', '57', '25', '58', '258', '26', '15', '15', '101', '26', '74', '1', '4', '74', '263']\n",
            "     filenum                                               text  label\n",
            "0          1  scope of what is meant by the equivalent of an...    209\n",
            "1          2  time during which the silence has endured, can...     63\n",
            "2          4  on original Indian title were held to be outsi...    108\n",
            "3          6  who framed it. Every word appears to have been...    200\n",
            "4          7  in of the Act, the general policy declarations...    203\n",
            "...      ...                                                ...    ...\n",
            "5270    8411  of life without the possibility of parole. In ...     15\n",
            "5271    8412  habeas relief. We reverse. In Ylst, we said th...     15\n",
            "5272    8413  existence. Novelty in procedural requirements ...    101\n",
            "5273    8414  circumstances marked 'yes' in Section II outwe...     26\n",
            "5274    8417  it, the States' claims of ownership prior to t...      4\n",
            "\n",
            "[5275 rows x 3 columns]\n"
          ]
        }
      ],
      "source": [
        "fh = open('sc_model_4.txt')\n",
        "filenums=[]\n",
        "records=[]\n",
        "for record in fh:\n",
        "    record=record.split(\"---\")\n",
        "    filenum=record[0]\n",
        "    modelnum=record[1]\n",
        "    text=record[2]\n",
        "    filenums.append(filenum)\n",
        "    records.append(text)\n",
        "    \n",
        "print(len(filenums))\n",
        "print(len(records))\n",
        "\n",
        "summarized_data = pd.DataFrame(list(zip(filenums, records)),columns =['filenum','text'])\n",
        "len_list = [len(ele.split()) for ele in records]\n",
        "res = 0 if len(len_list) == 0 else (float(sum(len_list)) / len(len_list))\n",
        "print(\"Average Length %s\" % res)\n",
        "\n",
        "#temp_file = open(\"labels_sc.txt\", \"r\")\n",
        "temp_file = open(\"labels_sc_279.txt\", \"r\")\n",
        "\n",
        "data = temp_file.read()\n",
        "\n",
        "label_list = data.split(\"\\n\")\n",
        "print(label_list)\n",
        "label_list = label_list[0:-1]\n",
        "print(label_list)\n",
        "label_list = [int(i) for i in label_list]\n",
        "temp_file.close()\n",
        "\n",
        "\n",
        "new_label_list=[]\n",
        "for num in filenums:\n",
        "  new_label_list.append(label_list[int(num)-1])\n",
        "summarized_data['label'] = new_label_list\n",
        "\n",
        "print(summarized_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kenM-hJwR4q",
        "outputId": "b90476ed-38db-4133-d79c-375d7076dda4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_1[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_roberta_model[0][0]']       \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 279)          143127      ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[209  63 108 ... 101  26   4]\n",
            "[   1    2    4 ... 8413 8414 8417]\n",
            "[6622   35 6713 4989 8233 7444 5061 8344 5464  317 4515 3001 6991 2875\n",
            " 3885 6647 4857 4504  214 8340 2969 3847 4609 1346 5993 7358 2752 7651\n",
            " 8188 7073 6069 2346 6483 4952 5072 1967 4071 7590 5030 7297  626 5463\n",
            " 6029 6302 7256 3993 4263 3835 5801 4293 8129 5866 5777 4837 3843 1418\n",
            " 5274 6205 2356 5997 6685 3310 7721 5928 2360 7197 4252 8343  909 4354\n",
            " 4752  649 3488 2496 5765 6612 4004 4052 1403 6697 5268 6293 6380 4602\n",
            " 6057 4577 4236 7978 5083 6552  361 7706 4186 8351  812 5108 6600 8264\n",
            " 8064 7121 8279 5556 5119 7947 3350 7152 5199 4282 5025 5051 1927 3615\n",
            " 1221 8062 1489 2031 7351  234 8015 7416 7921 6574 6257 8396  941 4520\n",
            " 3300 5640  240 7810 7425 1321 6580 5404 8273 6742   94    9 7861 7975\n",
            " 1018 2140 1415 7061 7767 4990 8124 5927 6051 3269 6916 2846 6818 7052\n",
            " 7775 7582 8116 4767 5248 1812 6055 8331  998 1564 7326  292 3065 6764\n",
            " 6854 4915 3583 5902 4229 2144 7565 3712 7224 4502 1406 1651 3827 6525\n",
            " 6318 2686 5369 8235 1704 1011 3351 8021  102  224 8294 7707 2985 5146\n",
            " 3709 6824 2281 6837 5436 2649 4886  966 5476 1199 8387 5449 1028 2226\n",
            " 6166 4387  350 6123 5236 5041  734 3412 6085  587  489 4982 5115 5575\n",
            " 3839 6743 7173 5724 7689 7787 8302 3150 2247 8128 6660 3505 4610 2866\n",
            " 4161 6905 7096  768 1062 4411 6087 2096 4903 7546 6630 2928 4640 7125\n",
            " 1792 2063 1700 7928 8095  737 5849  922 2359 6015 4489 4361 6081 6346\n",
            " 5014 5167 1023 8009  268 4777 5221 4323 6168 5702 6129 7181 1860 7192\n",
            " 5467 6579   67 2693 6134 4270 4611 6746 7696 3466 3950 6125 8370 6142\n",
            " 6131  995 5548 1721 5067 4015 7092 3156   25 1749 4878 8248 6836 5310\n",
            " 6960 2339 1650 6227 4104 5130 7089 6993 1289 4305 5585 7933 6284 7057\n",
            " 3807 5790 8347 5280 2841 7693 8090 4171 5260  133 7451 3957 1719 4743\n",
            " 4746 5435 6216 4858 4417 8355 4492 2648 5104 5278 7308  158 7543 2168\n",
            " 5913 7111 3414 7592   39 7157 5647 4555 3956 3617 4630 4384 6596 4539\n",
            " 6749 7202 2413 4749 5122 5929  252 5565  336 6008 4564 5580 7237 5958\n",
            " 7657 4327 5091 6918 6487 2989 7448 4715 3382  467 3784 4419 1117 7954\n",
            " 7122 3174 1534 4164 7526  498 5944 4572 2307 2929 7480 4412 4563 7692\n",
            " 1869 2472 6966 6549 1594 2276 7269 2545 2216  605 6587 6315 5297 5282\n",
            " 1619 5504 6399 3917 6342 5721 7909 6126 8382 4383 5502 7130  485 4936\n",
            " 7674 5372 7109 4605 5211 5462 8065   50 4080 3334 8167 8378 5595 1873\n",
            " 8207 7076 4106 3719 4627  231 2405 6973 3480 3246 5931 8208 4825 6203\n",
            " 3019 5822   63 5854 6543 5787 4764 6701 6562 3822 4861 1315 8318 8169\n",
            " 8218 7549  707 1076 2465 4636   80 7025 7127 4026 4961 7531  744 6684\n",
            " 4297 7939 7384  446 6675 7752 5279 8196 3020 4192 4253 2887 2556 5729\n",
            " 1898 3208 5973 7033 1431 5915 8312 5799 7105  511 6404 2852 1561]\n"
          ]
        }
      ],
      "source": [
        "gc.collect()\n",
        "tf.keras.backend.clear_session()\n",
        "dbert_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "dbert_model = TFRobertaModel.from_pretrained('roberta-base')\n",
        "max_len=512\n",
        "sentences=summarized_data['text']\n",
        "labels=summarized_data['label']\n",
        "filenumbers=summarized_data['filenum']\n",
        "len(sentences),len(labels),len(filenumbers)\n",
        "\n",
        "model_4=create_model()\n",
        "input_ids=[]\n",
        "attention_masks=[]\n",
        "\n",
        "for sent in sentences:\n",
        "  dbert_inps=dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n",
        "  input_ids.append(dbert_inps['input_ids'])\n",
        "  attention_masks.append(dbert_inps['attention_mask'])\n",
        "input_ids=np.asarray(input_ids)\n",
        "\n",
        "attention_masks=np.array(attention_masks)\n",
        "labels=np.array(labels)\n",
        "filenumbers=[int(i) for i in filenumbers]\n",
        "filenumbers=np.array(filenumbers)\n",
        "print(labels)\n",
        "print(filenumbers)\n",
        "\n",
        "#train_inp,val_inp,train_label,val_label,train_mask,val_mask,train_fnum,val_fnum=train_test_split(input_ids,labels,attention_masks,filenumbers,test_size=0.1,random_state=42)\n",
        "\n",
        "new_train_inp_4=[]\n",
        "new_val_inp_4=[]\n",
        "new_train_label_4=[]\n",
        "new_val_label_4=[]\n",
        "new_train_mask_4=[]\n",
        "new_train_fnum_4=[]\n",
        "new_val_fnum_4=[]\n",
        "new_val_mask_4=[]\n",
        "for i in range(len(train_fnum)):\n",
        "  if train_fnum[i] in filenumbers:\n",
        "    new_train_inp_4.append(train_inp[i])\n",
        "    #new_val_inp.append(val_inp[i])\n",
        "    new_train_label_4.append(train_label[i])\n",
        "    #new_val_label.append(val_label[i])\n",
        "    new_train_mask_4.append(train_mask[i])\n",
        "    #new_val_mask.append(val_mask[i])\n",
        "    new_train_fnum_4.append(train_fnum[i])\n",
        "\n",
        "for i in range(len(val_fnum)):\n",
        "  if val_fnum[i] in filenumbers:\n",
        "    #new_train_inp.append(train_inp[i])\n",
        "    new_val_inp_4.append(val_inp[i])\n",
        "    #new_train_label.append(train_label[i])\n",
        "    new_val_label_4.append(val_label[i])\n",
        "    #new_train_mask.append(train_mask[i])\n",
        "    new_val_mask_4.append(val_mask[i])\n",
        "    new_val_fnum_4.append(val_fnum[i])\n",
        "\n",
        "# print(new_train_label)\n",
        "# print(len(new_train_label))\n",
        "# print(new_val_label)\n",
        "# print(len(new_val_label))\n",
        "new_train_inp_4=np.array(new_train_inp_4)\n",
        "new_val_inp_4=np.array(new_val_inp_4)\n",
        "new_train_label_4=np.array(new_train_label_4)\n",
        "new_val_label_4=np.array(new_val_label_4)\n",
        "new_train_mask_4=np.array(new_train_mask_4)\n",
        "new_train_fnum_4=np.array(new_train_fnum_4)\n",
        "new_val_fnum_4=np.array(new_val_fnum_4)\n",
        "new_val_mask_4=np.array(new_val_mask_4)\n",
        "\n",
        "print(new_val_fnum_4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qAa4P7OtO0QK",
        "outputId": "e7ad8fc3-50a5-4930-d6c1-be3968fa12a7"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_1[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_roberta_model[0][0]']       \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 279)          143127      ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[209  63 108 ... 101  26   4]\n",
            "[   1    2    4 ... 8413 8414 8417]\n",
            "[6622   35 6713 4989 8233 7444 5061 8344 5464  317 4515 3001 6991 2875\n",
            " 3885 6647 4857 4504  214 8340 2969 3847 4609 1346 5993 7358 2752 7651\n",
            " 8188 7073 6069 2346 6483 4952 5072 1967 4071 7590 5030 7297  626 5463\n",
            " 6029 6302 7256 3993 4263 3835 5801 4293 8129 5866 5777 4837 3843 1418\n",
            " 5274 6205 2356 5997 6685 3310 7721 5928 2360 7197 4252 8343  909 4354\n",
            " 4752  649 3488 2496 5765 6612 4004 4052 1403 6697 5268 6293 6380 4602\n",
            " 6057 4577 4236 7978 5083 6552  361 7706 4186 8351  812 5108 6600 8264\n",
            " 8064 7121 8279 5556 5119 7947 3350 7152 5199 4282 5025 5051 1927 3615\n",
            " 1221 8062 1489 2031 7351  234 8015 7416 7921 6574 6257 8396  941 4520\n",
            " 3300 5640  240 7810 7425 1321 6580 5404 8273 6742   94    9 7861 7975\n",
            " 1018 2140 1415 7061 7767 4990 8124 5927 6051 3269 6916 2846 6818 7052\n",
            " 7775 7582 8116 4767 5248 1812 6055 8331  998 1564 7326  292 3065 6764\n",
            " 6854 4915 3583 5902 4229 2144 7565 3712 7224 4502 1406 1651 3827 6525\n",
            " 6318 2686 5369 8235 1704 1011 3351 8021  102  224 8294 7707 2985 5146\n",
            " 3709 6824 2281 6837 5436 2649 4886  966 5476 1199 8387 5449 1028 2226\n",
            " 6166 4387  350 6123 5236 5041  734 3412 6085  587  489 4982 5115 5575\n",
            " 3839 6743 7173 5724 7689 7787 8302 3150 2247 8128 6660 3505 4610 2866\n",
            " 4161 6905 7096  768 1062 4411 6087 2096 4903 7546 6630 2928 4640 7125\n",
            " 1792 2063 1700 7928 8095  737 5849  922 2359 6015 4489 4361 6081 6346\n",
            " 5014 5167 1023 8009  268 4777 5221 4323 6168 5702 6129 7181 1860 7192\n",
            " 5467 6579   67 2693 6134 4270 4611 6746 7696 3466 3950 6125 8370 6142\n",
            " 6131  995 5548 1721 5067 4015 7092 3156   25 1749 4878 8248 6836 5310\n",
            " 6960 2339 1650 6227 4104 5130 7089 6993 1289 4305 5585 7933 6284 7057\n",
            " 3807 5790 8347 5280 2841 7693 8090 4171 5260  133 7451 3957 1719 4743\n",
            " 4746 5435 6216 4858 4417 8355 4492 2648 5104 5278 7308  158 7543 2168\n",
            " 5913 7111 3414 7592   39 7157 5647 4555 3956 3617 4630 4384 6596 4539\n",
            " 6749 7202 2413 4749 5122 5929  252 5565  336 6008 4564 5580 7237 5958\n",
            " 7657 4327 5091 6918 6487 2989 7448 4715 3382  467 3784 4419 1117 7954\n",
            " 7122 3174 1534 4164 7526  498 5944 4572 2307 2929 7480 4412 4563 7692\n",
            " 1869 2472 6966 6549 1594 2276 7269 2545 2216  605 6587 6315 5297 5282\n",
            " 1619 5504 6399 3917 6342 5721 7909 6126 8382 4383 5502 7130  485 4936\n",
            " 7674 5372 7109 4605 5211 5462 8065   50 4080 3334 8167 8378 5595 1873\n",
            " 8207 7076 4106 3719 4627  231 2405 6973 3480 3246 5931 8208 4825 6203\n",
            " 3019 5822   63 5854 6543 5787 4764 6701 6562 3822 4861 1315 8318 8169\n",
            " 8218 7549  707 1076 2465 4636   80 7025 7127 4026 4961 7531  744 6684\n",
            " 4297 7939 7384  446 6675 7752 5279 8196 3020 4192 4253 2887 2556 5729\n",
            " 1898 3208 5973 7033 1431 5915 8312 5799 7105  511 6404 2852 1561]\n",
            "Mon Jun  6 16:57:10 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   44C    P0    40W / 300W |  15237MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "595/595 [==============================] - 221s 346ms/step - loss: 14.4949 - accuracy: 0.0507 - val_loss: 13.0811 - val_accuracy: 0.2186\n",
            "Epoch 2/5\n",
            "595/595 [==============================] - 206s 346ms/step - loss: 12.3842 - accuracy: 0.3005 - val_loss: 11.5223 - val_accuracy: 0.4159\n",
            "Epoch 3/5\n",
            "595/595 [==============================] - 206s 345ms/step - loss: 11.1594 - accuracy: 0.4342 - val_loss: 10.8604 - val_accuracy: 0.4545\n",
            "Epoch 4/5\n",
            "595/595 [==============================] - 206s 346ms/step - loss: 10.3683 - accuracy: 0.5185 - val_loss: 10.4145 - val_accuracy: 0.4642\n",
            "Epoch 5/5\n",
            "595/595 [==============================] - 205s 345ms/step - loss: 9.7331 - accuracy: 0.5841 - val_loss: 10.0206 - val_accuracy: 0.4932\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_3[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_roberta_model[1][0]']       \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 279)          143127      ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.45724465558194777\n",
            "Weighted F1: 0.3902413356389285\n",
            "Micro F1: 0.45724465558194777\n",
            "Weighted Precision: 0.3672576709734887\n",
            "Micro Precision: 0.45724465558194777\n",
            "Weighted Recall: 0.45724465558194777\n",
            "Micro Recall: 0.45724465558194777\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_1[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_roberta_model[0][0]']       \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 279)          143127      ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[209  63 108 ... 101  26   4]\n",
            "[   1    2    4 ... 8413 8414 8417]\n",
            "[6622   35 6713 4989 8233 7444 5061 8344 5464  317 4515 3001 6991 2875\n",
            " 3885 6647 4857 4504  214 8340 2969 3847 4609 1346 5993 7358 2752 7651\n",
            " 8188 7073 6069 2346 6483 4952 5072 1967 4071 7590 5030 7297  626 5463\n",
            " 6029 6302 7256 3993 4263 3835 5801 4293 8129 5866 5777 4837 3843 1418\n",
            " 5274 6205 2356 5997 6685 3310 7721 5928 2360 7197 4252 8343  909 4354\n",
            " 4752  649 3488 2496 5765 6612 4004 4052 1403 6697 5268 6293 6380 4602\n",
            " 6057 4577 4236 7978 5083 6552  361 7706 4186 8351  812 5108 6600 8264\n",
            " 8064 7121 8279 5556 5119 7947 3350 7152 5199 4282 5025 5051 1927 3615\n",
            " 1221 8062 1489 2031 7351  234 8015 7416 7921 6574 6257 8396  941 4520\n",
            " 3300 5640  240 7810 7425 1321 6580 5404 8273 6742   94    9 7861 7975\n",
            " 1018 2140 1415 7061 7767 4990 8124 5927 6051 3269 6916 2846 6818 7052\n",
            " 7775 7582 8116 4767 5248 1812 6055 8331  998 1564 7326  292 3065 6764\n",
            " 6854 4915 3583 5902 4229 2144 7565 3712 7224 4502 1406 1651 3827 6525\n",
            " 6318 2686 5369 8235 1704 1011 3351 8021  102  224 8294 7707 2985 5146\n",
            " 3709 6824 2281 6837 5436 2649 4886  966 5476 1199 8387 5449 1028 2226\n",
            " 6166 4387  350 6123 5236 5041  734 3412 6085  587  489 4982 5115 5575\n",
            " 3839 6743 7173 5724 7689 7787 8302 3150 2247 8128 6660 3505 4610 2866\n",
            " 4161 6905 7096  768 1062 4411 6087 2096 4903 7546 6630 2928 4640 7125\n",
            " 1792 2063 1700 7928 8095  737 5849  922 2359 6015 4489 4361 6081 6346\n",
            " 5014 5167 1023 8009  268 4777 5221 4323 6168 5702 6129 7181 1860 7192\n",
            " 5467 6579   67 2693 6134 4270 4611 6746 7696 3466 3950 6125 8370 6142\n",
            " 6131  995 5548 1721 5067 4015 7092 3156   25 1749 4878 8248 6836 5310\n",
            " 6960 2339 1650 6227 4104 5130 7089 6993 1289 4305 5585 7933 6284 7057\n",
            " 3807 5790 8347 5280 2841 7693 8090 4171 5260  133 7451 3957 1719 4743\n",
            " 4746 5435 6216 4858 4417 8355 4492 2648 5104 5278 7308  158 7543 2168\n",
            " 5913 7111 3414 7592   39 7157 5647 4555 3956 3617 4630 4384 6596 4539\n",
            " 6749 7202 2413 4749 5122 5929  252 5565  336 6008 4564 5580 7237 5958\n",
            " 7657 4327 5091 6918 6487 2989 7448 4715 3382  467 3784 4419 1117 7954\n",
            " 7122 3174 1534 4164 7526  498 5944 4572 2307 2929 7480 4412 4563 7692\n",
            " 1869 2472 6966 6549 1594 2276 7269 2545 2216  605 6587 6315 5297 5282\n",
            " 1619 5504 6399 3917 6342 5721 7909 6126 8382 4383 5502 7130  485 4936\n",
            " 7674 5372 7109 4605 5211 5462 8065   50 4080 3334 8167 8378 5595 1873\n",
            " 8207 7076 4106 3719 4627  231 2405 6973 3480 3246 5931 8208 4825 6203\n",
            " 3019 5822   63 5854 6543 5787 4764 6701 6562 3822 4861 1315 8318 8169\n",
            " 8218 7549  707 1076 2465 4636   80 7025 7127 4026 4961 7531  744 6684\n",
            " 4297 7939 7384  446 6675 7752 5279 8196 3020 4192 4253 2887 2556 5729\n",
            " 1898 3208 5973 7033 1431 5915 8312 5799 7105  511 6404 2852 1561]\n",
            "Mon Jun  6 17:16:11 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   44C    P0    40W / 300W |  15237MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "595/595 [==============================] - 221s 347ms/step - loss: 14.4762 - accuracy: 0.0513 - val_loss: 13.1046 - val_accuracy: 0.2031\n",
            "Epoch 2/5\n",
            "595/595 [==============================] - 206s 346ms/step - loss: 12.3752 - accuracy: 0.2997 - val_loss: 11.5324 - val_accuracy: 0.4159\n",
            "Epoch 3/5\n",
            "595/595 [==============================] - 206s 346ms/step - loss: 11.1129 - accuracy: 0.4380 - val_loss: 10.7956 - val_accuracy: 0.4584\n",
            "Epoch 4/5\n",
            "595/595 [==============================] - 206s 346ms/step - loss: 10.2797 - accuracy: 0.5301 - val_loss: 10.3276 - val_accuracy: 0.4855\n",
            "Epoch 5/5\n",
            "595/595 [==============================] - 206s 347ms/step - loss: 9.6518 - accuracy: 0.5950 - val_loss: 9.9620 - val_accuracy: 0.5184\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_3[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_roberta_model[1][0]']       \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 279)          143127      ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.47743467933491684\n",
            "Weighted F1: 0.40827144344514116\n",
            "Micro F1: 0.47743467933491684\n",
            "Weighted Precision: 0.3803192231603759\n",
            "Micro Precision: 0.47743467933491684\n",
            "Weighted Recall: 0.47743467933491684\n",
            "Micro Recall: 0.47743467933491684\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_1[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_roberta_model[0][0]']       \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 279)          143127      ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[209  63 108 ... 101  26   4]\n",
            "[   1    2    4 ... 8413 8414 8417]\n",
            "[6622   35 6713 4989 8233 7444 5061 8344 5464  317 4515 3001 6991 2875\n",
            " 3885 6647 4857 4504  214 8340 2969 3847 4609 1346 5993 7358 2752 7651\n",
            " 8188 7073 6069 2346 6483 4952 5072 1967 4071 7590 5030 7297  626 5463\n",
            " 6029 6302 7256 3993 4263 3835 5801 4293 8129 5866 5777 4837 3843 1418\n",
            " 5274 6205 2356 5997 6685 3310 7721 5928 2360 7197 4252 8343  909 4354\n",
            " 4752  649 3488 2496 5765 6612 4004 4052 1403 6697 5268 6293 6380 4602\n",
            " 6057 4577 4236 7978 5083 6552  361 7706 4186 8351  812 5108 6600 8264\n",
            " 8064 7121 8279 5556 5119 7947 3350 7152 5199 4282 5025 5051 1927 3615\n",
            " 1221 8062 1489 2031 7351  234 8015 7416 7921 6574 6257 8396  941 4520\n",
            " 3300 5640  240 7810 7425 1321 6580 5404 8273 6742   94    9 7861 7975\n",
            " 1018 2140 1415 7061 7767 4990 8124 5927 6051 3269 6916 2846 6818 7052\n",
            " 7775 7582 8116 4767 5248 1812 6055 8331  998 1564 7326  292 3065 6764\n",
            " 6854 4915 3583 5902 4229 2144 7565 3712 7224 4502 1406 1651 3827 6525\n",
            " 6318 2686 5369 8235 1704 1011 3351 8021  102  224 8294 7707 2985 5146\n",
            " 3709 6824 2281 6837 5436 2649 4886  966 5476 1199 8387 5449 1028 2226\n",
            " 6166 4387  350 6123 5236 5041  734 3412 6085  587  489 4982 5115 5575\n",
            " 3839 6743 7173 5724 7689 7787 8302 3150 2247 8128 6660 3505 4610 2866\n",
            " 4161 6905 7096  768 1062 4411 6087 2096 4903 7546 6630 2928 4640 7125\n",
            " 1792 2063 1700 7928 8095  737 5849  922 2359 6015 4489 4361 6081 6346\n",
            " 5014 5167 1023 8009  268 4777 5221 4323 6168 5702 6129 7181 1860 7192\n",
            " 5467 6579   67 2693 6134 4270 4611 6746 7696 3466 3950 6125 8370 6142\n",
            " 6131  995 5548 1721 5067 4015 7092 3156   25 1749 4878 8248 6836 5310\n",
            " 6960 2339 1650 6227 4104 5130 7089 6993 1289 4305 5585 7933 6284 7057\n",
            " 3807 5790 8347 5280 2841 7693 8090 4171 5260  133 7451 3957 1719 4743\n",
            " 4746 5435 6216 4858 4417 8355 4492 2648 5104 5278 7308  158 7543 2168\n",
            " 5913 7111 3414 7592   39 7157 5647 4555 3956 3617 4630 4384 6596 4539\n",
            " 6749 7202 2413 4749 5122 5929  252 5565  336 6008 4564 5580 7237 5958\n",
            " 7657 4327 5091 6918 6487 2989 7448 4715 3382  467 3784 4419 1117 7954\n",
            " 7122 3174 1534 4164 7526  498 5944 4572 2307 2929 7480 4412 4563 7692\n",
            " 1869 2472 6966 6549 1594 2276 7269 2545 2216  605 6587 6315 5297 5282\n",
            " 1619 5504 6399 3917 6342 5721 7909 6126 8382 4383 5502 7130  485 4936\n",
            " 7674 5372 7109 4605 5211 5462 8065   50 4080 3334 8167 8378 5595 1873\n",
            " 8207 7076 4106 3719 4627  231 2405 6973 3480 3246 5931 8208 4825 6203\n",
            " 3019 5822   63 5854 6543 5787 4764 6701 6562 3822 4861 1315 8318 8169\n",
            " 8218 7549  707 1076 2465 4636   80 7025 7127 4026 4961 7531  744 6684\n",
            " 4297 7939 7384  446 6675 7752 5279 8196 3020 4192 4253 2887 2556 5729\n",
            " 1898 3208 5973 7033 1431 5915 8312 5799 7105  511 6404 2852 1561]\n",
            "Mon Jun  6 17:35:15 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   45C    P0    41W / 300W |  15237MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "595/595 [==============================] - 222s 347ms/step - loss: 14.5902 - accuracy: 0.0328 - val_loss: 13.3839 - val_accuracy: 0.1393\n",
            "Epoch 2/5\n",
            "595/595 [==============================] - 206s 346ms/step - loss: 12.5310 - accuracy: 0.2533 - val_loss: 11.6033 - val_accuracy: 0.3772\n",
            "Epoch 3/5\n",
            "595/595 [==============================] - 206s 345ms/step - loss: 11.1482 - accuracy: 0.4140 - val_loss: 10.8445 - val_accuracy: 0.4352\n",
            "Epoch 4/5\n",
            "595/595 [==============================] - 206s 346ms/step - loss: 10.3095 - accuracy: 0.5095 - val_loss: 10.3383 - val_accuracy: 0.4855\n",
            "Epoch 5/5\n",
            "595/595 [==============================] - 205s 345ms/step - loss: 9.6579 - accuracy: 0.5811 - val_loss: 9.9669 - val_accuracy: 0.5068\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_3[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_roberta_model[1][0]']       \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 279)          143127      ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.4643705463182898\n",
            "Weighted F1: 0.40098170347809964\n",
            "Micro F1: 0.4643705463182898\n",
            "Weighted Precision: 0.4012626588413182\n",
            "Micro Precision: 0.4643705463182898\n",
            "Weighted Recall: 0.4643705463182898\n",
            "Micro Recall: 0.4643705463182898\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_1[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_roberta_model[0][0]']       \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 279)          143127      ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[209  63 108 ... 101  26   4]\n",
            "[   1    2    4 ... 8413 8414 8417]\n",
            "[6622   35 6713 4989 8233 7444 5061 8344 5464  317 4515 3001 6991 2875\n",
            " 3885 6647 4857 4504  214 8340 2969 3847 4609 1346 5993 7358 2752 7651\n",
            " 8188 7073 6069 2346 6483 4952 5072 1967 4071 7590 5030 7297  626 5463\n",
            " 6029 6302 7256 3993 4263 3835 5801 4293 8129 5866 5777 4837 3843 1418\n",
            " 5274 6205 2356 5997 6685 3310 7721 5928 2360 7197 4252 8343  909 4354\n",
            " 4752  649 3488 2496 5765 6612 4004 4052 1403 6697 5268 6293 6380 4602\n",
            " 6057 4577 4236 7978 5083 6552  361 7706 4186 8351  812 5108 6600 8264\n",
            " 8064 7121 8279 5556 5119 7947 3350 7152 5199 4282 5025 5051 1927 3615\n",
            " 1221 8062 1489 2031 7351  234 8015 7416 7921 6574 6257 8396  941 4520\n",
            " 3300 5640  240 7810 7425 1321 6580 5404 8273 6742   94    9 7861 7975\n",
            " 1018 2140 1415 7061 7767 4990 8124 5927 6051 3269 6916 2846 6818 7052\n",
            " 7775 7582 8116 4767 5248 1812 6055 8331  998 1564 7326  292 3065 6764\n",
            " 6854 4915 3583 5902 4229 2144 7565 3712 7224 4502 1406 1651 3827 6525\n",
            " 6318 2686 5369 8235 1704 1011 3351 8021  102  224 8294 7707 2985 5146\n",
            " 3709 6824 2281 6837 5436 2649 4886  966 5476 1199 8387 5449 1028 2226\n",
            " 6166 4387  350 6123 5236 5041  734 3412 6085  587  489 4982 5115 5575\n",
            " 3839 6743 7173 5724 7689 7787 8302 3150 2247 8128 6660 3505 4610 2866\n",
            " 4161 6905 7096  768 1062 4411 6087 2096 4903 7546 6630 2928 4640 7125\n",
            " 1792 2063 1700 7928 8095  737 5849  922 2359 6015 4489 4361 6081 6346\n",
            " 5014 5167 1023 8009  268 4777 5221 4323 6168 5702 6129 7181 1860 7192\n",
            " 5467 6579   67 2693 6134 4270 4611 6746 7696 3466 3950 6125 8370 6142\n",
            " 6131  995 5548 1721 5067 4015 7092 3156   25 1749 4878 8248 6836 5310\n",
            " 6960 2339 1650 6227 4104 5130 7089 6993 1289 4305 5585 7933 6284 7057\n",
            " 3807 5790 8347 5280 2841 7693 8090 4171 5260  133 7451 3957 1719 4743\n",
            " 4746 5435 6216 4858 4417 8355 4492 2648 5104 5278 7308  158 7543 2168\n",
            " 5913 7111 3414 7592   39 7157 5647 4555 3956 3617 4630 4384 6596 4539\n",
            " 6749 7202 2413 4749 5122 5929  252 5565  336 6008 4564 5580 7237 5958\n",
            " 7657 4327 5091 6918 6487 2989 7448 4715 3382  467 3784 4419 1117 7954\n",
            " 7122 3174 1534 4164 7526  498 5944 4572 2307 2929 7480 4412 4563 7692\n",
            " 1869 2472 6966 6549 1594 2276 7269 2545 2216  605 6587 6315 5297 5282\n",
            " 1619 5504 6399 3917 6342 5721 7909 6126 8382 4383 5502 7130  485 4936\n",
            " 7674 5372 7109 4605 5211 5462 8065   50 4080 3334 8167 8378 5595 1873\n",
            " 8207 7076 4106 3719 4627  231 2405 6973 3480 3246 5931 8208 4825 6203\n",
            " 3019 5822   63 5854 6543 5787 4764 6701 6562 3822 4861 1315 8318 8169\n",
            " 8218 7549  707 1076 2465 4636   80 7025 7127 4026 4961 7531  744 6684\n",
            " 4297 7939 7384  446 6675 7752 5279 8196 3020 4192 4253 2887 2556 5729\n",
            " 1898 3208 5973 7033 1431 5915 8312 5799 7105  511 6404 2852 1561]\n",
            "Mon Jun  6 17:54:18 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   44C    P0    40W / 300W |  15237MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "595/595 [==============================] - 222s 347ms/step - loss: 14.3980 - accuracy: 0.0694 - val_loss: 12.9480 - val_accuracy: 0.2224\n",
            "Epoch 2/5\n",
            "595/595 [==============================] - 207s 348ms/step - loss: 12.3169 - accuracy: 0.3098 - val_loss: 11.5478 - val_accuracy: 0.4081\n",
            "Epoch 3/5\n",
            "595/595 [==============================] - 206s 346ms/step - loss: 11.1151 - accuracy: 0.4409 - val_loss: 10.8848 - val_accuracy: 0.4545\n",
            "Epoch 4/5\n",
            "595/595 [==============================] - 206s 346ms/step - loss: 10.3005 - accuracy: 0.5254 - val_loss: 10.3781 - val_accuracy: 0.4681\n",
            "Epoch 5/5\n",
            "595/595 [==============================] - 206s 346ms/step - loss: 9.6793 - accuracy: 0.5906 - val_loss: 9.9819 - val_accuracy: 0.4932\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_3[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_roberta_model[1][0]']       \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 279)          143127      ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.46318289786223277\n",
            "Weighted F1: 0.3994013542599501\n",
            "Micro F1: 0.46318289786223277\n",
            "Weighted Precision: 0.3831667058934412\n",
            "Micro Precision: 0.46318289786223277\n",
            "Weighted Recall: 0.46318289786223277\n",
            "Micro Recall: 0.46318289786223277\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_1[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_roberta_model[0][0]']       \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 279)          143127      ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[209  63 108 ... 101  26   4]\n",
            "[   1    2    4 ... 8413 8414 8417]\n",
            "[6622   35 6713 4989 8233 7444 5061 8344 5464  317 4515 3001 6991 2875\n",
            " 3885 6647 4857 4504  214 8340 2969 3847 4609 1346 5993 7358 2752 7651\n",
            " 8188 7073 6069 2346 6483 4952 5072 1967 4071 7590 5030 7297  626 5463\n",
            " 6029 6302 7256 3993 4263 3835 5801 4293 8129 5866 5777 4837 3843 1418\n",
            " 5274 6205 2356 5997 6685 3310 7721 5928 2360 7197 4252 8343  909 4354\n",
            " 4752  649 3488 2496 5765 6612 4004 4052 1403 6697 5268 6293 6380 4602\n",
            " 6057 4577 4236 7978 5083 6552  361 7706 4186 8351  812 5108 6600 8264\n",
            " 8064 7121 8279 5556 5119 7947 3350 7152 5199 4282 5025 5051 1927 3615\n",
            " 1221 8062 1489 2031 7351  234 8015 7416 7921 6574 6257 8396  941 4520\n",
            " 3300 5640  240 7810 7425 1321 6580 5404 8273 6742   94    9 7861 7975\n",
            " 1018 2140 1415 7061 7767 4990 8124 5927 6051 3269 6916 2846 6818 7052\n",
            " 7775 7582 8116 4767 5248 1812 6055 8331  998 1564 7326  292 3065 6764\n",
            " 6854 4915 3583 5902 4229 2144 7565 3712 7224 4502 1406 1651 3827 6525\n",
            " 6318 2686 5369 8235 1704 1011 3351 8021  102  224 8294 7707 2985 5146\n",
            " 3709 6824 2281 6837 5436 2649 4886  966 5476 1199 8387 5449 1028 2226\n",
            " 6166 4387  350 6123 5236 5041  734 3412 6085  587  489 4982 5115 5575\n",
            " 3839 6743 7173 5724 7689 7787 8302 3150 2247 8128 6660 3505 4610 2866\n",
            " 4161 6905 7096  768 1062 4411 6087 2096 4903 7546 6630 2928 4640 7125\n",
            " 1792 2063 1700 7928 8095  737 5849  922 2359 6015 4489 4361 6081 6346\n",
            " 5014 5167 1023 8009  268 4777 5221 4323 6168 5702 6129 7181 1860 7192\n",
            " 5467 6579   67 2693 6134 4270 4611 6746 7696 3466 3950 6125 8370 6142\n",
            " 6131  995 5548 1721 5067 4015 7092 3156   25 1749 4878 8248 6836 5310\n",
            " 6960 2339 1650 6227 4104 5130 7089 6993 1289 4305 5585 7933 6284 7057\n",
            " 3807 5790 8347 5280 2841 7693 8090 4171 5260  133 7451 3957 1719 4743\n",
            " 4746 5435 6216 4858 4417 8355 4492 2648 5104 5278 7308  158 7543 2168\n",
            " 5913 7111 3414 7592   39 7157 5647 4555 3956 3617 4630 4384 6596 4539\n",
            " 6749 7202 2413 4749 5122 5929  252 5565  336 6008 4564 5580 7237 5958\n",
            " 7657 4327 5091 6918 6487 2989 7448 4715 3382  467 3784 4419 1117 7954\n",
            " 7122 3174 1534 4164 7526  498 5944 4572 2307 2929 7480 4412 4563 7692\n",
            " 1869 2472 6966 6549 1594 2276 7269 2545 2216  605 6587 6315 5297 5282\n",
            " 1619 5504 6399 3917 6342 5721 7909 6126 8382 4383 5502 7130  485 4936\n",
            " 7674 5372 7109 4605 5211 5462 8065   50 4080 3334 8167 8378 5595 1873\n",
            " 8207 7076 4106 3719 4627  231 2405 6973 3480 3246 5931 8208 4825 6203\n",
            " 3019 5822   63 5854 6543 5787 4764 6701 6562 3822 4861 1315 8318 8169\n",
            " 8218 7549  707 1076 2465 4636   80 7025 7127 4026 4961 7531  744 6684\n",
            " 4297 7939 7384  446 6675 7752 5279 8196 3020 4192 4253 2887 2556 5729\n",
            " 1898 3208 5973 7033 1431 5915 8312 5799 7105  511 6404 2852 1561]\n",
            "Mon Jun  6 18:13:23 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   45C    P0    41W / 300W |  15237MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "595/595 [==============================] - 222s 347ms/step - loss: 14.3895 - accuracy: 0.0742 - val_loss: 12.9122 - val_accuracy: 0.2360\n",
            "Epoch 2/5\n",
            "595/595 [==============================] - 206s 346ms/step - loss: 12.3449 - accuracy: 0.3205 - val_loss: 11.6488 - val_accuracy: 0.3810\n",
            "Epoch 3/5\n",
            "595/595 [==============================] - 206s 346ms/step - loss: 11.2357 - accuracy: 0.4262 - val_loss: 10.9292 - val_accuracy: 0.4410\n",
            "Epoch 4/5\n",
            "595/595 [==============================] - 206s 347ms/step - loss: 10.4342 - accuracy: 0.5088 - val_loss: 10.4846 - val_accuracy: 0.4603\n",
            "Epoch 5/5\n",
            "595/595 [==============================] - 205s 345ms/step - loss: 9.7770 - accuracy: 0.5786 - val_loss: 10.0745 - val_accuracy: 0.4913\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_3[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_roberta_model[1][0]']       \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 279)          143127      ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.4643705463182898\n",
            "Weighted F1: 0.39685929386852076\n",
            "Micro F1: 0.4643705463182898\n",
            "Weighted Precision: 0.37912778029351246\n",
            "Micro Precision: 0.4643705463182898\n",
            "Weighted Recall: 0.4643705463182898\n",
            "Micro Recall: 0.4643705463182898\n",
            "Average Accuracy: 0.4653206650831354\n",
            "Average Weighted F1: 0.39915102613812803\n",
            "Average Micro F1: 0.4653206650831354\n",
            "Average Weighted Precision: 0.38222680783242724\n",
            "Average Micro Precision: 0.4653206650831354\n",
            "Average Weighted Recall: 0.4653206650831354\n",
            "Average Micro Recall: 0.4653206650831354\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "total_accuracy=0\n",
        "total_weighted_f1=0\n",
        "total_micro_f1=0\n",
        "total_weighted_precision=0\n",
        "total_micro_precision=0\n",
        "total_weighted_recall=0\n",
        "total_micro_recall=0\n",
        "\n",
        "for f in range(5):\n",
        "  gc.collect()\n",
        "  tf.keras.backend.clear_session()\n",
        "  dbert_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "  dbert_model = TFRobertaModel.from_pretrained('roberta-base')\n",
        "  max_len=512\n",
        "  sentences=summarized_data['text']\n",
        "  labels=summarized_data['label']\n",
        "  filenumbers=summarized_data['filenum']\n",
        "  len(sentences),len(labels),len(filenumbers)\n",
        "  \n",
        "  model_4=create_model()\n",
        "  input_ids=[]\n",
        "  attention_masks=[]\n",
        "\n",
        "  for sent in sentences:\n",
        "    dbert_inps=dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n",
        "    input_ids.append(dbert_inps['input_ids'])\n",
        "    attention_masks.append(dbert_inps['attention_mask'])\n",
        "  input_ids=np.asarray(input_ids)\n",
        "\n",
        "  attention_masks=np.array(attention_masks)\n",
        "  labels=np.array(labels)\n",
        "  filenumbers=[int(i) for i in filenumbers]\n",
        "  filenumbers=np.array(filenumbers)\n",
        "  print(labels)\n",
        "  print(filenumbers)\n",
        "\n",
        "  #train_inp,val_inp,train_label,val_label,train_mask,val_mask,train_fnum,val_fnum=train_test_split(input_ids,labels,attention_masks,filenumbers,test_size=0.1,random_state=42)\n",
        "\n",
        "  new_train_inp_4=[]\n",
        "  new_val_inp_4=[]\n",
        "  new_train_label_4=[]\n",
        "  new_val_label_4=[]\n",
        "  new_train_mask_4=[]\n",
        "  new_train_fnum_4=[]\n",
        "  new_val_fnum_4=[]\n",
        "  new_val_mask_4=[]\n",
        "  for i in range(len(train_fnum)):\n",
        "    if train_fnum[i] in filenumbers:\n",
        "      new_train_inp_4.append(train_inp[i])\n",
        "      #new_val_inp.append(val_inp[i])\n",
        "      new_train_label_4.append(train_label[i])\n",
        "      #new_val_label.append(val_label[i])\n",
        "      new_train_mask_4.append(train_mask[i])\n",
        "      #new_val_mask.append(val_mask[i])\n",
        "      new_train_fnum_4.append(train_fnum[i])\n",
        "\n",
        "  for i in range(len(val_fnum)):\n",
        "    if val_fnum[i] in filenumbers:\n",
        "      #new_train_inp.append(train_inp[i])\n",
        "      new_val_inp_4.append(val_inp[i])\n",
        "      #new_train_label.append(train_label[i])\n",
        "      new_val_label_4.append(val_label[i])\n",
        "      #new_train_mask.append(train_mask[i])\n",
        "      new_val_mask_4.append(val_mask[i])\n",
        "      new_val_fnum_4.append(val_fnum[i])\n",
        "\n",
        "  # print(new_train_label)\n",
        "  # print(len(new_train_label))\n",
        "  # print(new_val_label)\n",
        "  # print(len(new_val_label))\n",
        "  new_train_inp_4=np.array(new_train_inp_4)\n",
        "  new_val_inp_4=np.array(new_val_inp_4)\n",
        "  new_train_label_4=np.array(new_train_label_4)\n",
        "  new_val_label_4=np.array(new_val_label_4)\n",
        "  new_train_mask_4=np.array(new_train_mask_4)\n",
        "  new_train_fnum_4=np.array(new_train_fnum_4)\n",
        "  new_val_fnum_4=np.array(new_val_fnum_4)\n",
        "  new_val_mask_4=np.array(new_val_mask_4)\n",
        "\n",
        "  print(new_val_fnum_4)\n",
        "  \n",
        "  log_dir='dbert_model'\n",
        "\n",
        "  model_save_path='./drive/MyDrive/Ensemble/roberta-ensemble-512-model_4-'+str(f)+'-279labels.h5'\n",
        "\n",
        "  loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "  accuracy = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
        "  callbacks= [tf.keras.callbacks.ModelCheckpoint(filepath=model_save_path,monitor='val_accuracy',mode='max',save_best_only=True,save_weights_only=True),keras.callbacks.TensorBoard(log_dir=log_dir)]\n",
        "  model_4.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])\n",
        "  gpu_info = !nvidia-smi\n",
        "  gpu_info = '\\n'.join(gpu_info)\n",
        "  if gpu_info.find('failed') >= 0:\n",
        "    print('Not connected to a GPU')\n",
        "  else:\n",
        "    print(gpu_info)\n",
        "  \n",
        "  history=model_4.fit([new_train_inp_4,new_train_mask_4],new_train_label_4,batch_size=8,epochs=5,validation_data=([new_val_inp_4,new_val_mask_4],new_val_label_4),callbacks=callbacks)\n",
        "\n",
        "  pred_labels=[]\n",
        "\n",
        "  model_saved_4= create_model()\n",
        "  model_saved_4.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])\n",
        "  model_saved_4.load_weights('./drive/MyDrive/Ensemble/roberta-ensemble-512-model_4-'+str(f)+'-279labels.h5')\n",
        "\n",
        "  for i in range(0,len(val_inp)):\n",
        "    pred=model_saved_4.predict([val_inp[i].reshape(1,512),val_mask[i].reshape(1,512)])\n",
        "    pred_label = pred.argmax(axis=1)\n",
        "    pred_labels.append(pred_label)\n",
        "  accuracy=accuracy_score(val_label, pred_labels)\n",
        "  print(\"Accuracy: \"+str(accuracy))\n",
        "  total_accuracy=total_accuracy+accuracy\n",
        "  \n",
        "  weighted_f1=f1_score(val_label,pred_labels, average='weighted')\n",
        "  print(\"Weighted F1: \"+ str(weighted_f1))\n",
        "  total_weighted_f1=total_weighted_f1+weighted_f1\n",
        "  micro_f1=f1_score(val_label,pred_labels, average='micro')\n",
        "  print(\"Micro F1: \"+ str(micro_f1))\n",
        "  total_micro_f1=total_micro_f1+micro_f1\n",
        "\n",
        "  weighted_precision=precision_score(val_label, pred_labels, average='weighted')\n",
        "  print(\"Weighted Precision: \" + str(weighted_precision))\n",
        "  total_weighted_precision=total_weighted_precision+weighted_precision\n",
        "  micro_precision=precision_score(val_label, pred_labels, average='micro')\n",
        "  print(\"Micro Precision: \" + str(micro_precision))\n",
        "  total_micro_precision=total_micro_precision+micro_precision\n",
        "\n",
        "  weighted_recall=recall_score(val_label, pred_labels, average='weighted')\n",
        "  print(\"Weighted Recall: \" + str(weighted_recall))\n",
        "  total_weighted_recall=total_weighted_recall+weighted_recall\n",
        "  micro_recall=recall_score(val_label, pred_labels, average='micro')\n",
        "  print(\"Micro Recall: \" + str(micro_recall))\n",
        "  total_micro_recall=total_micro_recall+micro_recall\n",
        "\n",
        "\n",
        "print(\"Average Accuracy: \"+str(total_accuracy/5))\n",
        "print(\"Average Weighted F1: \"+str(total_weighted_f1/5))\n",
        "print(\"Average Micro F1: \"+str(total_micro_f1/5))\n",
        "print(\"Average Weighted Precision: \"+str(total_weighted_precision/5))\n",
        "print(\"Average Micro Precision: \"+str(total_micro_precision/5))\n",
        "print(\"Average Weighted Recall: \"+str(total_weighted_recall/5))\n",
        "print(\"Average Micro Recall: \"+str(total_micro_recall/5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "maHyXaSvPmP9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb2b2d0c-de0e-4c58-fcb3-50cc1c052268"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4504\n",
            "4504\n",
            "Average Length 511.5863676731794\n",
            "['209', '63', '216', '108', '196', '200', '203', '246', '188', '193', '25', '106', '56', '53', '223', '161', '200', '208', '78', '147', '147', '179', '147', '80', '56', '209', '209', '200', '214', '202', '203', '194', '26', '237', '165', '258', '238', '238', '76', '193', '196', '213', '161', '213', '209', '194', '196', '25', '151', '260', '135', '173', '30', '108', '272', '259', '176', '213', '213', '21', '5', '171', '200', '195', '240', '194', '176', '240', '240', '80', '173', '98', '214', '21', '76', '76', '73', '173', '173', '78', '61', '258', '191', '3', '78', '173', '193', '118', '200', '264', '260', '49', '211', '258', '18', '173', '78', '3', '3', '1', '213', '135', '182', '239', '208', '278', '50', '194', '200', '124', '189', '117', '264', '244', '76', '193', '221', '78', '78', '173', '197', '78', '161', '231', '135', '22', '78', '25', '25', '197', '200', '147', '203', '191', '196', '193', '263', '32', '197', '191', '4', '56', '196', '104', '191', '196', '173', '147', '194', '96', '161', '161', '238', '200', '3', '78', '78', '53', '41', '237', '53', '18', '10', '194', '97', '96', '208', '25', '10', '117', '104', '54', '96', '194', '21', '245', '238', '202', '209', '161', '97', '258', '240', '213', '136', '155', '152', '21', '191', '191', '118', '68', '258', '189', '197', '208', '61', '78', '135', '202', '96', '155', '108', '272', '80', '25', '155', '35', '213', '191', '194', '96', '96', '191', '264', '191', '191', '191', '2', '258', '191', '173', '259', '15', '47', '258', '156', '118', '160', '115', '105', '26', '173', '192', '76', '76', '135', '240', '118', '161', '200', '230', '18', '200', '25', '10', '161', '244', '24', '24', '200', '205', '104', '15', '193', '156', '95', '209', '240', '191', '194', '128', '208', '208', '218', '173', '10', '25', '18', '107', '46', '32', '197', '172', '209', '213', '79', '121', '78', '78', '193', '2', '78', '194', '149', '136', '80', '3', '21', '200', '78', '21', '197', '194', '118', '202', '161', '180', '209', '173', '2', '193', '6', '260', '197', '78', '78', '42', '191', '213', '135', '193', '196', '202', '222', '217', '42', '259', '213', '76', '46', '30', '15', '260', '202', '18', '259', '104', '135', '76', '196', '240', '240', '240', '108', '24', '200', '197', '262', '176', '15', '156', '264', '156', '200', '191', '161', '196', '78', '238', '161', '221', '118', '14', '14', '14', '200', '278', '238', '200', '196', '78', '173', '25', '197', '197', '196', '161', '18', '10', '10', '10', '18', '66', '203', '19', '246', '196', '244', '240', '161', '200', '78', '240', '209', '208', '118', '215', '194', '193', '168', '229', '191', '135', '172', '196', '197', '161', '78', '196', '197', '258', '78', '78', '238', '221', '121', '264', '104', '146', '50', '78', '217', '11', '238', '238', '238', '3', '3', '154', '215', '79', '18', '158', '104', '18', '13', '197', '117', '197', '161', '213', '242', '32', '238', '230', '2', '2', '32', '161', '251', '96', '156', '242', '22', '22', '140', '3', '149', '135', '191', '173', '135', '200', '238', '238', '78', '156', '209', '78', '97', '97', '160', '25', '258', '240', '161', '15', '161', '96', '209', '80', '132', '140', '4', '1', '240', '196', '274', '272', '62', '80', '196', '196', '191', '79', '156', '15', '196', '209', '22', '251', '202', '202', '121', '73', '191', '238', '191', '150', '150', '135', '46', '203', '202', '238', '22', '3', '258', '239', '238', '238', '165', '238', '238', '194', '200', '200', '197', '200', '196', '191', '194', '76', '78', '208', '200', '71', '208', '240', '76', '117', '120', '108', '96', '144', '30', '133', '133', '202', '117', '161', '251', '29', '104', '242', '197', '118', '216', '262', '236', '141', '11', '235', '235', '133', '191', '238', '202', '80', '80', '22', '140', '191', '197', '202', '133', '184', '184', '184', '184', '143', '274', '50', '237', '196', '274', '265', '10', '18', '10', '121', '238', '245', '228', '76', '25', '191', '197', '268', '10', '240', '195', '196', '195', '15', '48', '200', '183', '70', '59', '197', '196', '235', '161', '161', '215', '104', '97', '191', '194', '191', '200', '200', '197', '76', '200', '93', '256', '160', '238', '146', '238', '239', '15', '140', '258', '140', '21', '79', '202', '258', '79', '62', '205', '161', '104', '10', '161', '196', '137', '152', '191', '117', '213', '242', '176', '218', '195', '191', '162', '191', '135', '241', '213', '213', '84', '79', '68', '18', '98', '197', '155', '41', '246', '269', '78', '209', '238', '80', '238', '62', '237', '273', '273', '230', '150', '121', '260', '244', '161', '202', '78', '109', '155', '264', '145', '50', '274', '173', '210', '258', '238', '211', '156', '215', '195', '177', '244', '3', '141', '196', '191', '194', '29', '30', '10', '16', '200', '200', '156', '53', '3', '62', '165', '173', '173', '22', '205', '274', '150', '183', '116', '188', '188', '98', '161', '204', '80', '172', '156', '104', '229', '259', '78', '169', '3', '200', '118', '161', '80', '260', '240', '150', '197', '196', '62', '93', '6', '272', '272', '197', '160', '78', '96', '70', '195', '191', '191', '80', '3', '3', '240', '147', '196', '191', '238', '96', '203', '3', '15', '10', '58', '55', '96', '41', '15', '26', '50', '238', '194', '79', '240', '161', '191', '195', '263', '50', '240', '147', '235', '195', '203', '59', '178', '176', '2', '15', '203', '191', '18', '278', '196', '14', '153', '58', '175', '18', '217', '217', '80', '191', '200', '18', '200', '173', '24', '171', '171', '210', '124', '34', '200', '218', '104', '4', '223', '193', '194', '200', '200', '13', '62', '194', '197', '80', '197', '146', '146', '96', '97', '97', '2', '196', '191', '191', '104', '238', '10', '200', '239', '135', '104', '213', '2', '221', '169', '25', '21', '194', '161', '21', '278', '3', '195', '46', '176', '25', '117', '191', '62', '62', '62', '78', '50', '264', '79', '197', '80', '80', '80', '191', '191', '78', '161', '194', '195', '197', '203', '259', '278', '49', '147', '147', '147', '147', '22', '78', '78', '258', '2', '202', '221', '55', '258', '104', '104', '84', '240', '221', '274', '104', '22', '241', '63', '197', '197', '196', '159', '200', '22', '22', '22', '78', '78', '173', '104', '97', '104', '191', '146', '223', '268', '96', '14', '173', '204', '241', '97', '15', '118', '78', '78', '196', '194', '258', '96', '96', '259', '75', '10', '197', '30', '2', '238', '121', '50', '161', '173', '173', '186', '185', '186', '78', '194', '221', '218', '35', '33', '13', '34', '78', '21', '215', '97', '60', '24', '78', '193', '173', '235', '78', '200', '258', '140', '259', '194', '197', '195', '200', '145', '76', '108', '130', '238', '215', '213', '2', '217', '121', '178', '140', '62', '156', '176', '196', '196', '41', '223', '147', '172', '79', '200', '2', '244', '244', '191', '213', '104', '191', '237', '237', '33', '117', '195', '195', '264', '195', '140', '210', '41', '197', '14', '257', '196', '238', '238', '79', '161', '156', '196', '238', '238', '213', '104', '3', '10', '118', '204', '235', '33', '259', '274', '259', '176', '78', '78', '196', '34', '52', '25', '30', '194', '153', '21', '70', '191', '18', '191', '191', '172', '194', '194', '194', '194', '148', '104', '125', '2', '2', '2', '185', '194', '29', '104', '78', '189', '272', '204', '183', '213', '78', '104', '197', '240', '97', '168', '168', '182', '238', '197', '18', '194', '229', '221', '62', '78', '53', '104', '214', '189', '189', '189', '213', '117', '117', '191', '29', '104', '104', '276', '276', '22', '195', '195', '155', '139', '139', '59', '168', '135', '140', '79', '146', '200', '245', '70', '160', '153', '50', '202', '154', '263', '130', '194', '117', '194', '194', '191', '33', '29', '29', '238', '239', '194', '21', '274', '215', '18', '18', '140', '2', '215', '25', '46', '137', '238', '30', '160', '156', '46', '196', '263', '242', '104', '0', '0', '73', '213', '135', '22', '191', '191', '241', '202', '221', '15', '15', '191', '238', '213', '194', '197', '179', '238', '200', '200', '200', '5', '238', '194', '55', '117', '166', '49', '244', '239', '191', '78', '78', '78', '194', '200', '117', '210', '22', '121', '121', '121', '21', '237', '194', '121', '78', '104', '194', '79', '191', '251', '195', '197', '274', '176', '21', '43', '43', '10', '61', '215', '215', '30', '214', '194', '10', '30', '139', '96', '226', '226', '265', '2', '2', '121', '121', '41', '237', '104', '238', '238', '161', '78', '78', '78', '11', '184', '140', '140', '161', '104', '104', '237', '130', '195', '160', '118', '161', '18', '11', '241', '184', '196', '178', '22', '30', '146', '146', '10', '10', '141', '141', '135', '18', '18', '10', '26', '208', '97', '141', '18', '18', '97', '194', '194', '79', '213', '205', '205', '205', '75', '241', '50', '226', '238', '97', '222', '215', '215', '139', '238', '51', '238', '96', '173', '109', '200', '117', '192', '2', '195', '10', '3', '18', '49', '238', '238', '191', '195', '52', '197', '200', '200', '79', '191', '200', '200', '30', '195', '197', '195', '25', '260', '73', '30', '193', '21', '200', '242', '203', '120', '78', '260', '241', '0', '30', '202', '195', '30', '191', '78', '194', '194', '2', '205', '191', '196', '173', '197', '30', '196', '172', '139', '2', '18', '191', '230', '56', '194', '215', '195', '2', '71', '13', '146', '264', '50', '191', '235', '93', '191', '139', '139', '227', '235', '186', '235', '24', '18', '261', '128', '33', '71', '25', '176', '39', '10', '195', '202', '29', '29', '221', '29', '197', '139', '78', '227', '146', '137', '195', '205', '205', '195', '135', '62', '153', '146', '29', '75', '0', '194', '194', '180', '213', '78', '18', '194', '239', '196', '239', '197', '194', '153', '192', '221', '10', '46', '50', '241', '117', '117', '117', '195', '173', '78', '195', '194', '239', '2', '12', '173', '197', '30', '269', '197', '176', '135', '30', '146', '191', '93', '93', '135', '260', '218', '156', '155', '200', '241', '125', '18', '18', '183', '203', '239', '259', '173', '213', '180', '180', '210', '120', '104', '194', '156', '258', '172', '9', '0', '241', '192', '143', '189', '204', '50', '63', '197', '243', '241', '93', '125', '2', '21', '260', '4', '4', '3', '191', '150', '128', '78', '238', '245', '8', '80', '238', '79', '79', '240', '58', '104', '155', '80', '80', '188', '191', '235', '170', '170', '170', '146', '258', '195', '276', '25', '78', '215', '240', '161', '53', '147', '78', '78', '78', '78', '221', '221', '96', '18', '19', '130', '18', '18', '18', '80', '80', '194', '79', '93', '121', '78', '139', '139', '78', '197', '121', '194', '200', '80', '101', '22', '12', '135', '97', '205', '181', '60', '193', '15', '140', '172', '191', '241', '222', '135', '29', '25', '179', '191', '191', '259', '194', '133', '240', '121', '121', '241', '195', '29', '191', '209', '18', '139', '139', '50', '18', '12', '18', '133', '200', '25', '10', '70', '25', '242', '18', '161', '49', '172', '172', '172', '172', '128', '96', '195', '200', '78', '10', '239', '22', '173', '142', '142', '22', '194', '192', '3', '200', '241', '96', '241', '62', '139', '213', '202', '135', '79', '120', '192', '78', '118', '241', '46', '151', '150', '151', '150', '118', '221', '183', '237', '196', '140', '140', '39', '179', '213', '140', '41', '73', '78', '217', '258', '30', '118', '140', '219', '50', '10', '139', '150', '251', '24', '10', '18', '78', '18', '172', '135', '146', '71', '196', '202', '197', '25', '106', '160', '215', '15', '6', '101', '165', '140', '145', '80', '3', '238', '239', '78', '268', '0', '109', '191', '215', '238', '238', '238', '203', '50', '146', '15', '25', '156', '191', '16', '2', '195', '189', '96', '193', '109', '204', '161', '185', '210', '50', '235', '30', '242', '193', '102', '194', '121', '177', '241', '102', '132', '240', '71', '192', '79', '25', '197', '39', '161', '22', '258', '237', '15', '7', '78', '156', '197', '70', '176', '139', '240', '132', '78', '177', '191', '194', '117', '10', '79', '238', '195', '24', '101', '118', '196', '2', '102', '21', '180', '185', '170', '79', '139', '135', '191', '151', '200', '191', '154', '262', '200', '237', '56', '27', '191', '180', '258', '139', '139', '192', '30', '0', '242', '239', '242', '25', '191', '242', '240', '3', '56', '215', '171', '192', '193', '222', '238', '237', '215', '272', '2', '156', '237', '151', '189', '80', '15', '260', '260', '217', '235', '22', '228', '135', '18', '191', '157', '78', '2', '240', '213', '179', '2', '191', '79', '79', '153', '3', '194', '78', '121', '71', '121', '149', '257', '194', '191', '188', '15', '25', '125', '102', '15', '130', '130', '177', '10', '135', '265', '240', '149', '196', '200', '204', '204', '213', '177', '197', '194', '13', '202', '30', '213', '125', '193', '125', '125', '125', '125', '125', '15', '180', '78', '1', '25', '96', '200', '29', '156', '172', '202', '22', '78', '197', '188', '241', '101', '101', '101', '221', '50', '191', '15', '15', '0', '101', '101', '168', '56', '197', '130', '62', '204', '29', '10', '96', '97', '139', '97', '97', '2', '2', '262', '39', '176', '172', '194', '194', '18', '78', '13', '80', '139', '196', '207', '191', '151', '192', '150', '13', '104', '104', '101', '15', '125', '101', '15', '0', '50', '194', '264', '50', '25', '25', '125', '125', '125', '125', '125', '25', '29', '25', '25', '78', '240', '261', '19', '2', '74', '78', '238', '50', '50', '203', '197', '71', '104', '80', '194', '149', '170', '130', '2', '237', '170', '202', '48', '97', '93', '179', '235', '78', '102', '102', '70', '208', '238', '104', '146', '118', '118', '96', '237', '274', '218', '241', '102', '146', '209', '209', '240', '170', '137', '7', '197', '191', '264', '29', '46', '96', '193', '19', '12', '213', '215', '275', '179', '18', '183', '78', '221', '18', '170', '119', '21', '140', '240', '96', '192', '192', '21', '96', '149', '239', '135', '191', '240', '184', '183', '24', '2', '96', '197', '194', '215', '18', '192', '121', '202', '196', '25', '97', '214', '3', '97', '16', '0', '192', '135', '215', '5', '12', '80', '143', '202', '242', '151', '13', '209', '203', '200', '30', '209', '194', '102', '102', '102', '102', '102', '102', '22', '24', '6', '18', '5', '191', '101', '50', '101', '101', '192', '153', '153', '101', '101', '10', '192', '25', '140', '50', '241', '102', '102', '32', '150', '101', '102', '153', '102', '101', '153', '102', '25', '10', '125', '18', '125', '177', '209', '102', '10', '78', '78', '137', '18', '237', '241', '175', '195', '175', '96', '176', '237', '96', '96', '101', '193', '238', '170', '238', '102', '222', '268', '238', '102', '244', '34', '18', '244', '202', '149', '149', '193', '102', '170', '194', '75', '101', '222', '238', '50', '195', '135', '22', '93', '18', '102', '102', '93', '93', '191', '147', '244', '96', '2', '241', '241', '93', '189', '189', '177', '35', '137', '197', '191', '40', '40', '195', '176', '35', '200', '200', '237', '135', '140', '140', '93', '156', '200', '78', '191', '22', '81', '238', '101', '213', '101', '78', '109', '19', '140', '196', '78', '78', '76', '218', '14', '265', '101', '241', '155', '4', '50', '78', '78', '238', '135', '191', '265', '81', '238', '238', '102', '102', '102', '140', '162', '32', '17', '17', '171', '171', '191', '101', '102', '192', '101', '17', '171', '135', '102', '242', '195', '237', '21', '18', '238', '191', '278', '140', '149', '97', '260', '155', '276', '192', '196', '21', '191', '2', '97', '0', '150', '221', '213', '209', '238', '80', '260', '194', '104', '96', '193', '14', '240', '193', '117', '117', '155', '17', '264', '260', '209', '209', '2', '137', '71', '72', '101', '205', '173', '191', '118', '24', '41', '197', '78', '94', '237', '156', '193', '153', '154', '153', '209', '110', '78', '78', '213', '238', '78', '191', '93', '79', '177', '267', '133', '133', '46', '139', '40', '143', '74', '241', '202', '50', '3', '102', '78', '149', '191', '156', '18', '93', '195', '192', '137', '240', '135', '204', '189', '22', '56', '154', '25', '192', '130', '277', '191', '80', '39', '21', '21', '238', '238', '23', '192', '191', '192', '94', '94', '80', '139', '17', '10', '22', '227', '227', '140', '102', '21', '17', '227', '227', '23', '243', '264', '18', '274', '149', '238', '30', '221', '22', '238', '193', '135', '205', '195', '238', '238', '197', '130', '195', '18', '104', '102', '14', '59', '104', '18', '18', '258', '34', '241', '138', '188', '176', '102', '1', '140', '140', '22', '22', '278', '10', '96', '35', '145', '120', '33', '33', '22', '33', '22', '35', '22', '19', '33', '102', '192', '192', '175', '22', '72', '191', '222', '125', '22', '79', '18', '194', '18', '250', '192', '192', '96', '48', '274', '273', '132', '133', '192', '40', '171', '171', '246', '191', '10', '166', '197', '195', '125', '125', '200', '153', '192', '149', '110', '139', '194', '48', '102', '102', '104', '203', '238', '238', '238', '78', '18', '238', '121', '18', '238', '96', '238', '238', '13', '191', '18', '18', '240', '96', '40', '188', '18', '137', '175', '28', '28', '28', '149', '191', '191', '207', '239', '239', '153', '153', '153', '153', '153', '153', '154', '153', '154', '153', '153', '153', '18', '0', '27', '18', '25', '96', '96', '137', '33', '10', '130', '153', '153', '154', '180', '137', '44', '241', '153', '268', '25', '161', '25', '39', '242', '4', '238', '195', '102', '135', '13', '229', '140', '1', '79', '192', '229', '191', '118', '21', '260', '203', '18', '177', '191', '10', '238', '10', '133', '14', '175', '175', '192', '18', '153', '153', '254', '140', '149', '22', '22', '22', '237', '40', '135', '191', '192', '192', '62', '205', '19', '238', '176', '214', '200', '96', '15', '153', '191', '108', '29', '166', '28', '193', '197', '96', '195', '108', '102', '241', '195', '10', '33', '166', '197', '168', '2', '96', '26', '33', '204', '258', '149', '153', '153', '40', '137', '241', '166', '221', '23', '221', '153', '15', '113', '113', '78', '171', '40', '71', '21', '71', '80', '15', '191', '191', '135', '110', '10', '259', '102', '149', '149', '109', '109', '175', '97', '97', '97', '153', '15', '238', '193', '175', '38', '38', '137', '18', '235', '155', '137', '265', '110', '18', '18', '256', '191', '224', '173', '104', '10', '152', '24', '24', '24', '17', '40', '40', '40', '40', '40', '40', '112', '6', '18', '18', '210', '96', '191', '27', '213', '17', '153', '26', '133', '18', '18', '153', '38', '96', '40', '38', '40', '40', '10', '17', '17', '95', '71', '192', '17', '259', '213', '200', '15', '11', '207', '194', '18', '135', '238', '191', '241', '191', '116', '35', '147', '156', '67', '78', '17', '227', '191', '117', '188', '130', '72', '96', '235', '97', '18', '150', '203', '25', '126', '149', '200', '94', '4', '4', '1', '244', '228', '252', '149', '191', '149', '15', '17', '17', '29', '18', '18', '116', '23', '237', '94', '184', '268', '197', '175', '243', '28', '16', '10', '50', '191', '102', '102', '153', '149', '111', '78', '191', '135', '18', '35', '78', '238', '208', '93', '95', '258', '17', '111', '111', '262', '22', '22', '104', '161', '209', '21', '195', '12', '116', '241', '242', '239', '17', '97', '16', '40', '117', '94', '96', '78', '15', '103', '195', '135', '235', '247', '135', '130', '260', '239', '176', '93', '191', '209', '239', '93', '264', '30', '18', '30', '18', '18', '103', '191', '97', '17', '95', '115', '209', '140', '22', '22', '241', '10', '153', '3', '188', '238', '226', '195', '97', '96', '238', '97', '130', '97', '116', '96', '96', '242', '45', '149', '166', '155', '96', '116', '264', '10', '237', '192', '71', '22', '71', '175', '149', '116', '237', '274', '18', '235', '176', '116', '202', '250', '250', '78', '2', '196', '2', '97', '143', '18', '111', '111', '80', '78', '153', '80', '40', '110', '30', '112', '30', '111', '215', '112', '149', '78', '78', '209', '110', '110', '238', '109', '239', '112', '151', '154', '16', '16', '16', '78', '137', '179', '72', '149', '10', '111', '268', '96', '170', '111', '38', '192', '153', '183', '195', '30', '30', '147', '195', '147', '93', '264', '153', '25', '18', '19', '22', '74', '40', '93', '166', '15', '127', '147', '192', '260', '238', '103', '269', '127', '19', '19', '153', '18', '78', '238', '30', '30', '241', '16', '235', '250', '204', '238', '218', '40', '93', '184', '18', '0', '241', '195', '94', '154', '109', '156', '10', '21', '30', '197', '39', '78', '24', '107', '142', '142', '228', '229', '229', '229', '232', '179', '239', '142', '232', '23', '175', '147', '137', '137', '137', '171', '191', '117', '128', '127', '204', '98', '147', '235', '239', '153', '15', '204', '255', '80', '204', '204', '149', '113', '19', '167', '22', '250', '17', '17', '17', '18', '17', '67', '121', '125', '26', '97', '97', '97', '97', '247', '124', '163', '147', '112', '111', '208', '108', '38', '209', '154', '154', '194', '135', '22', '147', '241', '120', '218', '260', '156', '155', '241', '192', '215', '180', '179', '149', '40', '193', '192', '150', '102', '93', '1', '149', '137', '133', '102', '102', '102', '78', '145', '21', '96', '175', '175', '78', '123', '30', '95', '19', '18', '110', '18', '152', '152', '147', '26', '135', '260', '158', '203', '195', '158', '112', '156', '125', '213', '230', '30', '15', '125', '106', '111', '195', '181', '130', '195', '130', '239', '15', '194', '16', '15', '111', '61', '72', '57', '128', '239', '239', '203', '25', '218', '10', '155', '191', '46', '98', '175', '260', '102', '21', '191', '260', '191', '239', '15', '30', '93', '147', '103', '78', '177', '30', '95', '155', '155', '155', '103', '103', '112', '241', '191', '191', '17', '78', '153', '150', '113', '115', '147', '78', '40', '163', '33', '15', '149', '103', '192', '191', '96', '106', '115', '143', '235', '200', '204', '238', '204', '204', '74', '196', '113', '246', '102', '150', '16', '176', '20', '195', '128', '197', '15', '71', '71', '193', '24', '24', '24', '170', '204', '209', '111', '144', '112', '22', '222', '224', '28', '209', '155', '213', '275', '196', '239', '195', '25', '103', '149', '128', '19', '96', '102', '135', '17', '232', '155', '191', '20', '18', '241', '30', '25', '205', '97', '97', '96', '72', '182', '135', '135', '20', '149', '149', '78', '135', '40', '18', '153', '10', '26', '158', '205', '157', '157', '135', '135', '140', '155', '192', '26', '268', '263', '245', '155', '15', '177', '159', '212', '161', '239', '200', '40', '35', '153', '197', '259', '15', '254', '175', '10', '30', '239', '67', '195', '200', '191', '22', '66', '191', '112', '133', '128', '235', '161', '155', '17', '93', '36', '113', '239', '18', '18', '191', '165', '163', '163', '238', '79', '158', '40', '102', '40', '200', '191', '60', '241', '245', '96', '78', '30', '15', '173', '192', '192', '158', '64', '113', '200', '74', '128', '135', '115', '115', '10', '74', '102', '102', '115', '111', '238', '244', '109', '109', '238', '152', '10', '237', '18', '50', '16', '173', '204', '15', '208', '276', '43', '109', '258', '222', '15', '94', '235', '203', '79', '128', '113', '3', '204', '106', '133', '214', '218', '25', '98', '166', '71', '30', '175', '175', '97', '135', '108', '19', '18', '196', '35', '12', '196', '78', '166', '155', '72', '155', '22', '109', '258', '196', '263', '161', '204', '210', '14', '208', '208', '208', '208', '204', '102', '102', '213', '155', '251', '153', '153', '153', '154', '154', '237', '97', '19', '28', '94', '136', '112', '19', '97', '152', '153', '153', '111', '111', '135', '135', '105', '17', '105', '152', '152', '152', '153', '79', '266', '135', '120', '125', '149', '50', '162', '72', '17', '147', '109', '115', '24', '105', '244', '149', '238', '203', '35', '202', '177', '173', '239', '19', '19', '179', '237', '14', '18', '185', '58', '158', '144', '213', '237', '155', '250', '239', '93', '237', '97', '195', '157', '107', '109', '246', '165', '98', '49', '2', '266', '149', '20', '18', '71', '108', '22', '115', '239', '40', '178', '232', '238', '218', '118', '118', '210', '235', '46', '228', '192', '112', '149', '96', '111', '21', '95', '95', '95', '18', '57', '206', '18', '239', '157', '80', '196', '112', '176', '99', '106', '71', '235', '135', '18', '209', '78', '20', '20', '103', '33', '3', '103', '166', '79', '79', '156', '229', '18', '238', '2', '16', '128', '200', '117', '22', '197', '166', '78', '237', '241', '107', '53', '189', '204', '243', '163', '221', '15', '200', '221', '152', '23', '193', '107', '195', '99', '97', '19', '125', '276', '113', '193', '64', '78', '197', '117', '175', '135', '135', '78', '93', '154', '153', '256', '256', '135', '137', '135', '137', '149', '110', '15', '159', '21', '158', '192', '192', '117', '29', '97', '258', '78', '46', '5', '80', '15', '105', '260', '161', '191', '2', '138', '117', '215', '176', '156', '194', '265', '115', '181', '21', '14', '106', '106', '108', '200', '156', '103', '102', '29', '17', '204', '175', '227', '118', '18', '239', '204', '258', '78', '155', '213', '109', '192', '186', '186', '15', '241', '133', '30', '30', '239', '30', '218', '109', '138', '241', '241', '45', '135', '112', '232', '104', '106', '200', '53', '23', '112', '17', '117', '59', '260', '195', '106', '117', '159', '204', '200', '228', '165', '165', '261', '200', '227', '166', '93', '21', '80', '112', '152', '196', '203', '228', '98', '102', '15', '139', '110', '13', '238', '6', '171', '94', '208', '155', '112', '203', '239', '191', '39', '136', '203', '195', '46', '203', '148', '192', '210', '23', '4', '153', '29', '165', '192', '204', '232', '94', '112', '239', '98', '71', '248', '17', '155', '10', '78', '191', '191', '111', '25', '25', '19', '19', '17', '235', '102', '265', '195', '235', '163', '238', '71', '197', '107', '155', '30', '72', '19', '259', '80', '155', '23', '64', '239', '197', '156', '57', '203', '258', '200', '205', '246', '238', '258', '227', '238', '238', '19', '148', '161', '111', '105', '202', '109', '262', '133', '137', '221', '135', '52', '170', '111', '36', '30', '156', '102', '268', '115', '22', '153', '133', '259', '98', '235', '135', '191', '25', '235', '25', '29', '94', '195', '203', '212', '112', '135', '245', '260', '200', '96', '158', '23', '165', '22', '18', '249', '240', '109', '43', '37', '15', '241', '241', '23', '135', '240', '108', '98', '268', '275', '191', '136', '263', '259', '196', '98', '98', '204', '248', '123', '105', '204', '240', '9', '9', '98', '218', '189', '238', '246', '155', '130', '157', '74', '109', '29', '189', '203', '213', '195', '241', '159', '238', '204', '218', '105', '23', '16', '263', '206', '150', '152', '204', '202', '173', '267', '18', '153', '155', '196', '2', '97', '158', '158', '204', '98', '202', '98', '204', '258', '98', '22', '113', '113', '135', '71', '237', '155', '163', '250', '233', '26', '26', '26', '26', '26', '19', '170', '18', '15', '19', '191', '30', '30', '48', '35', '30', '237', '46', '119', '16', '78', '239', '150', '245', '235', '158', '27', '241', '30', '38', '107', '75', '241', '135', '106', '106', '98', '159', '96', '135', '111', '176', '175', '200', '18', '14', '242', '163', '187', '20', '12', '191', '23', '106', '171', '25', '78', '162', '191', '157', '94', '237', '246', '203', '108', '238', '204', '204', '1', '102', '154', '106', '170', '102', '200', '2', '135', '106', '258', '258', '227', '26', '15', '25', '71', '203', '96', '3', '241', '200', '30', '108', '98', '108', '27', '106', '26', '150', '259', '78', '113', '113', '158', '202', '16', '136', '239', '156', '239', '35', '276', '252', '115', '22', '22', '258', '172', '3', '154', '3', '98', '237', '102', '235', '112', '206', '98', '57', '118', '153', '18', '26', '30', '196', '163', '191', '153', '72', '24', '156', '117', '122', '30', '135', '192', '98', '28', '30', '30', '95', '241', '155', '17', '195', '31', '103', '202', '98', '237', '94', '112', '163', '163', '94', '163', '112', '19', '195', '191', '15', '135', '78', '109', '160', '152', '97', '98', '107', '167', '97', '138', '138', '26', '191', '97', '97', '30', '30', '206', '202', '13', '30', '240', '111', '261', '79', '0', '111', '19', '152', '107', '107', '18', '98', '28', '258', '244', '106', '237', '204', '191', '15', '183', '202', '16', '111', '166', '268', '202', '12', '30', '78', '110', '133', '71', '197', '111', '52', '79', '171', '156', '94', '13', '118', '111', '71', '133', '18', '105', '30', '22', '133', '60', '237', '191', '6', '126', '3', '238', '276', '78', '162', '150', '115', '191', '107', '200', '148', '135', '72', '155', '241', '4', '108', '160', '203', '20', '133', '2', '80', '80', '154', '18', '15', '115', '239', '167', '167', '35', '268', '18', '221', '18', '195', '204', '195', '216', '133', '112', '104', '223', '191', '115', '30', '30', '30', '30', '30', '202', '204', '165', '200', '200', '29', '191', '237', '267', '18', '175', '81', '264', '264', '178', '115', '102', '178', '212', '273', '202', '109', '259', '166', '135', '155', '219', '161', '18', '94', '241', '191', '196', '191', '98', '26', '26', '204', '204', '135', '27', '238', '260', '182', '26', '98', '94', '93', '238', '156', '19', '111', '79', '16', '192', '113', '19', '241', '18', '202', '260', '272', '106', '163', '156', '106', '17', '109', '221', '78', '203', '7', '136', '197', '193', '191', '98', '112', '200', '95', '221', '98', '161', '191', '3', '106', '189', '111', '25', '196', '196', '241', '163', '200', '24', '78', '176', '3', '98', '102', '98', '19', '161', '224', '80', '18', '191', '112', '105', '254', '71', '200', '137', '29', '20', '165', '8', '106', '23', '106', '155', '200', '265', '135', '126', '203', '188', '161', '158', '258', '255', '98', '203', '50', '35', '102', '158', '175', '40', '239', '26', '66', '57', '193', '155', '15', '135', '10', '107', '106', '229', '18', '191', '204', '107', '99', '235', '238', '18', '205', '205', '35', '238', '208', '203', '110', '110', '109', '112', '23', '18', '19', '182', '156', '18', '18', '156', '111', '135', '137', '133', '137', '240', '99', '197', '113', '15', '165', '39', '97', '97', '15', '150', '163', '109', '268', '15', '15', '238', '203', '67', '161', '197', '18', '196', '97', '161', '197', '161', '185', '238', '191', '30', '121', '133', '160', '160', '74', '135', '75', '54', '135', '196', '23', '241', '166', '194', '239', '135', '240', '208', '238', '98', '135', '152', '176', '22', '174', '113', '241', '237', '57', '195', '191', '24', '165', '165', '204', '204', '56', '109', '27', '153', '252', '228', '165', '252', '200', '28', '158', '135', '196', '22', '18', '133', '30', '204', '237', '196', '93', '106', '94', '25', '93', '159', '4', '195', '23', '237', '25', '199', '57', '26', '276', '274', '204', '75', '18', '204', '108', '22', '133', '191', '111', '204', '203', '133', '237', '166', '4', '62', '202', '166', '161', '165', '74', '109', '118', '200', '22', '206', '25', '209', '17', '155', '204', '64', '23', '30', '202', '149', '171', '135', '136', '184', '26', '18', '159', '195', '18', '166', '156', '98', '133', '38', '40', '18', '18', '166', '109', '109', '209', '22', '195', '163', '163', '108', '18', '15', '99', '135', '174', '26', '19', '166', '133', '259', '151', '235', '204', '228', '133', '30', '161', '111', '242', '84', '238', '98', '195', '78', '160', '28', '25', '264', '165', '41', '19', '55', '46', '202', '121', '15', '32', '98', '238', '0', '78', '31', '30', '203', '264', '203', '135', '104', '111', '78', '212', '111', '98', '126', '22', '2', '30', '237', '212', '265', '163', '106', '106', '235', '109', '223', '161', '202', '150', '170', '255', '112', '114', '170', '98', '96', '197', '36', '18', '228', '14', '255', '204', '102', '239', '0', '185', '30', '22', '23', '246', '199', '133', '191', '78', '195', '153', '112', '191', '200', '170', '171', '200', '78', '128', '25', '153', '237', '99', '94', '208', '107', '208', '208', '78', '204', '204', '27', '191', '272', '134', '158', '158', '174', '118', '64', '18', '258', '150', '189', '18', '153', '241', '204', '112', '106', '135', '71', '148', '7', '133', '140', '187', '23', '223', '19', '19', '2', '136', '2', '200', '161', '15', '98', '158', '148', '3', '250', '153', '235', '165', '220', '176', '110', '240', '151', '148', '133', '74', '193', '46', '27', '173', '94', '176', '105', '256', '161', '226', '18', '78', '191', '171', '243', '26', '0', '108', '167', '159', '239', '150', '112', '155', '79', '241', '202', '165', '254', '107', '202', '156', '166', '193', '239', '135', '15', '203', '187', '148', '25', '15', '22', '194', '239', '56', '191', '176', '262', '200', '110', '72', '99', '102', '135', '98', '113', '15', '15', '159', '262', '171', '151', '98', '204', '213', '255', '26', '243', '175', '156', '98', '107', '191', '165', '241', '26', '165', '171', '30', '160', '166', '266', '11', '19', '211', '93', '254', '30', '239', '115', '273', '205', '112', '237', '263', '74', '98', '79', '122', '94', '265', '155', '191', '16', '235', '229', '98', '191', '227', '17', '111', '267', '135', '202', '195', '10', '180', '196', '196', '133', '135', '133', '95', '111', '122', '258', '191', '3', '114', '107', '19', '264', '74', '56', '200', '200', '133', '161', '97', '15', '97', '197', '196', '253', '93', '202', '258', '106', '153', '26', '109', '40', '149', '8', '196', '15', '119', '155', '23', '237', '193', '264', '148', '151', '238', '200', '94', '75', '148', '175', '24', '195', '171', '30', '203', '200', '202', '16', '158', '223', '191', '22', '264', '135', '35', '57', '94', '191', '196', '202', '98', '18', '196', '3', '133', '78', '191', '208', '258', '18', '200', '29', '135', '109', '110', '2', '185', '98', '133', '19', '22', '204', '95', '238', '25', '133', '250', '135', '135', '3', '47', '158', '187', '2', '78', '115', '155', '3', '218', '166', '112', '266', '22', '195', '135', '72', '97', '197', '209', '127', '80', '2', '98', '14', '239', '15', '113', '165', '204', '78', '204', '191', '175', '200', '193', '18', '196', '133', '109', '237', '56', '196', '3', '163', '163', '163', '195', '15', '263', '18', '84', '18', '133', '107', '18', '102', '156', '238', '102', '26', '244', '82', '75', '23', '258', '238', '136', '3', '204', '184', '200', '196', '158', '106', '27', '238', '155', '152', '50', '50', '2', '235', '98', '203', '166', '109', '25', '18', '151', '133', '200', '26', '26', '26', '19', '107', '156', '23', '26', '3', '70', '225', '41', '26', '239', '26', '186', '26', '34', '272', '272', '104', '78', '260', '18', '204', '26', '78', '18', '210', '135', '203', '71', '207', '26', '133', '13', '25', '26', '115', '94', '264', '135', '244', '79', '30', '263', '22', '109', '205', '176', '106', '22', '17', '151', '106', '191', '160', '160', '165', '78', '177', '196', '260', '166', '35', '191', '260', '98', '18', '237', '18', '161', '18', '197', '133', '30', '35', '200', '170', '155', '245', '19', '200', '160', '96', '172', '165', '55', '137', '241', '133', '191', '112', '25', '126', '238', '18', '18', '108', '135', '161', '135', '73', '112', '107', '8', '112', '109', '195', '25', '30', '105', '161', '241', '110', '75', '238', '215', '238', '104', '25', '3', '155', '30', '16', '3', '175', '99', '3', '200', '23', '3', '199', '203', '191', '196', '196', '204', '98', '105', '133', '197', '135', '161', '39', '15', '30', '133', '55', '191', '203', '133', '203', '140', '200', '149', '30', '96', '135', '23', '26', '3', '18', '155', '158', '135', '135', '18', '250', '18', '22', '114', '18', '18', '166', '18', '30', '18', '239', '18', '161', '50', '50', '49', '51', '23', '55', '98', '78', '210', '211', '18', '239', '78', '13', '193', '114', '78', '18', '25', '133', '19', '176', '4', '173', '50', '108', '17', '4', '204', '94', '112', '207', '109', '115', '23', '235', '197', '194', '197', '213', '148', '40', '157', '107', '200', '147', '14', '14', '18', '238', '18', '238', '18', '238', '57', '200', '135', '30', '18', '191', '191', '200', '166', '151', '161', '73', '19', '73', '239', '239', '165', '108', '2', '93', '166', '133', '133', '173', '35', '193', '114', '74', '19', '40', '58', '104', '16', '260', '160', '158', '112', '210', '158', '167', '203', '203', '3', '3', '109', '30', '133', '258', '70', '204', '200', '152', '155', '192', '204', '108', '191', '204', '26', '98', '26', '98', '264', '158', '18', '150', '137', '153', '196', '199', '191', '115', '168', '157', '149', '151', '78', '137', '160', '259', '166', '196', '171', '175', '199', '166', '161', '70', '13', '166', '152', '152', '206', '58', '18', '207', '58', '74', '207', '29', '46', '109', '135', '74', '174', '109', '40', '10', '30', '258', '16', '111', '30', '15', '239', '204', '48', '25', '196', '0', '156', '72', '72', '96', '22', '72', '133', '133', '238', '26', '3', '50', '152', '193', '235', '136', '153', '50', '1', '19', '239', '30', '25', '179', '161', '30', '191', '3', '27', '133', '3', '40', '23', '133', '150', '151', '265', '32', '191', '3', '25', '170', '238', '40', '259', '166', '137', '241', '133', '209', '159', '78', '78', '153', '26', '52', '241', '36', '250', '96', '30', '30', '38', '18', '18', '238', '99', '72', '111', '3', '244', '191', '226', '3', '191', '114', '135', '109', '40', '79', '202', '78', '114', '238', '10', '150', '108', '163', '108', '113', '197', '109', '115', '161', '3', '238', '3', '237', '161', '107', '155', '78', '202', '112', '38', '195', '137', '250', '133', '237', '161', '25', '26', '25', '15', '15', '239', '166', '114', '229', '111', '40', '135', '94', '102', '162', '204', '237', '17', '166', '136', '22', '98', '99', '99', '166', '35', '196', '3', '203', '135', '135', '84', '98', '266', '238', '13', '166', '78', '193', '98', '241', '104', '153', '192', '200', '10', '95', '95', '148', '107', '14', '17', '191', '239', '19', '203', '255', '197', '213', '94', '3', '107', '23', '35', '10', '237', '235', '78', '29', '18', '160', '200', '150', '99', '109', '155', '161', '133', '114', '18', '18', '18', '264', '133', '104', '161', '70', '17', '204', '195', '204', '241', '99', '82', '241', '241', '161', '18', '235', '175', '2', '2', '202', '50', '230', '26', '40', '40', '200', '78', '156', '26', '112', '26', '30', '71', '184', '200', '135', '153', '23', '106', '125', '175', '133', '133', '7', '38', '196', '161', '0', '49', '241', '158', '15', '21', '104', '2', '3', '176', '259', '135', '78', '75', '112', '200', '1', '207', '238', '161', '150', '158', '2', '45', '31', '237', '149', '3', '26', '165', '135', '152', '115', '133', '18', '40', '166', '30', '173', '40', '26', '78', '50', '48', '50', '13', '45', '200', '200', '18', '151', '205', '135', '38', '117', '196', '135', '155', '111', '196', '196', '166', '23', '25', '161', '18', '124', '259', '165', '58', '170', '204', '250', '103', '160', '238', '194', '163', '241', '255', '235', '17', '133', '26', '135', '196', '237', '114', '258', '193', '248', '40', '195', '84', '238', '146', '199', '40', '161', '22', '137', '43', '166', '156', '4', '104', '133', '177', '211', '193', '78', '203', '246', '237', '264', '3', '149', '78', '78', '111', '239', '13', '238', '176', '112', '150', '156', '3', '6', '118', '178', '178', '21', '186', '150', '241', '50', '243', '191', '121', '278', '29', '200', '18', '3', '197', '191', '98', '173', '195', '25', '195', '264', '221', '15', '156', '25', '200', '211', '195', '26', '35', '2', '135', '126', '113', '167', '191', '264', '196', '133', '18', '26', '146', '203', '219', '23', '160', '160', '135', '259', '3', '159', '120', '160', '106', '240', '133', '115', '258', '38', '22', '3', '26', '22', '107', '49', '48', '25', '237', '72', '219', '196', '196', '111', '135', '196', '18', '166', '172', '135', '26', '258', '70', '98', '40', '166', '23', '96', '3', '30', '155', '126', '146', '114', '242', '237', '157', '238', '241', '40', '133', '200', '161', '175', '200', '175', '84', '266', '18', '99', '30', '242', '151', '153', '166', '199', '228', '3', '237', '132', '110', '135', '80', '15', '15', '15', '84', '133', '15', '177', '3', '202', '255', '71', '115', '235', '19', '18', '18', '102', '58', '78', '165', '166', '264', '12', '150', '15', '2', '18', '135', '109', '200', '258', '128', '3', '199', '197', '239', '109', '222', '239', '107', '125', '132', '204', '204', '133', '135', '30', '158', '203', '15', '264', '237', '133', '196', '3', '252', '26', '98', '79', '199', '210', '0', '99', '30', '26', '259', '6', '49', '166', '98', '108', '9', '133', '102', '155', '98', '114', '78', '35', '166', '188', '202', '235', '30', '149', '165', '176', '138', '29', '153', '25', '25', '137', '133', '166', '135', '125', '71', '193', '154', '165', '98', '23', '56', '198', '26', '26', '0', '109', '136', '163', '151', '25', '15', '241', '195', '204', '191', '195', '161', '175', '133', '237', '78', '165', '258', '135', '78', '135', '258', '18', '235', '30', '187', '150', '275', '149', '2', '36', '112', '22', '161', '238', '203', '112', '265', '75', '133', '205', '258', '158', '18', '26', '18', '25', '26', '15', '26', '239', '15', '240', '18', '71', '71', '78', '135', '3', '148', '166', '26', '176', '2', '197', '150', '176', '19', '18', '97', '166', '18', '153', '250', '133', '210', '20', '192', '196', '26', '191', '2', '81', '70', '5', '79', '3', '30', '193', '193', '69', '160', '176', '246', '108', '54', '250', '135', '200', '80', '3', '167', '18', '166', '200', '152', '23', '149', '19', '14', '13', '237', '196', '19', '112', '254', '204', '191', '23', '264', '199', '209', '137', '31', '135', '238', '78', '18', '191', '15', '164', '195', '74', '163', '163', '41', '99', '26', '135', '26', '40', '40', '250', '200', '200', '26', '237', '4', '195', '158', '35', '71', '191', '3', '218', '274', '58', '3', '23', '177', '132', '49', '62', '238', '166', '97', '264', '193', '29', '26', '49', '241', '197', '200', '64', '227', '258', '44', '15', '133', '175', '238', '109', '254', '237', '198', '175', '166', '213', '174', '196', '107', '259', '98', '10', '196', '210', '191', '96', '166', '135', '15', '197', '78', '78', '195', '176', '18', '98', '129', '170', '18', '247', '203', '26', '158', '40', '64', '200', '163', '196', '19', '205', '197', '193', '191', '69', '96', '74', '35', '36', '64', '133', '172', '19', '266', '96', '94', '103', '258', '193', '264', '98', '254', '50', '193', '25', '170', '75', '84', '27', '252', '203', '94', '94', '19', '98', '137', '12', '153', '3', '155', '135', '238', '15', '196', '15', '26', '84', '49', '27', '135', '203', '48', '133', '129', '133', '258', '155', '35', '195', '71', '241', '135', '166', '193', '165', '104', '196', '117', '196', '26', '237', '109', '95', '196', '60', '40', '158', '193', '250', '104', '94', '178', '27', '193', '78', '106', '79', '204', '133', '237', '235', '135', '202', '70', '15', '264', '241', '254', '110', '199', '49', '255', '30', '193', '238', '204', '102', '97', '161', '43', '228', '1', '6', '196', '193', '26', '15', '128', '46', '155', '32', '133', '69', '47', '135', '78', '205', '200', '200', '202', '3', '95', '191', '78', '26', '248', '258', '191', '240', '49', '110', '38', '193', '200', '200', '96', '200', '204', '135', '9', '200', '258', '15', '46', '15', '149', '155', '197', '2', '166', '151', '72', '135', '97', '211', '238', '135', '163', '161', '26', '163', '129', '241', '239', '241', '16', '26', '18', '263', '260', '166', '3', '264', '195', '79', '132', '251', '50', '133', '15', '25', '15', '191', '15', '78', '50', '78', '71', '227', '69', '200', '69', '109', '64', '133', '197', '196', '3', '45', '238', '26', '132', '104', '196', '193', '136', '80', '26', '75', '117', '14', '78', '75', '98', '15', '2', '15', '50', '255', '136', '3', '112', '173', '69', '191', '19', '239', '235', '109', '70', '135', '78', '165', '238', '262', '199', '35', '203', '224', '193', '15', '18', '150', '238', '197', '238', '193', '135', '3', '150', '187', '199', '239', '109', '152', '27', '257', '6', '235', '104', '191', '133', '166', '155', '26', '16', '136', '198', '98', '154', '235', '27', '102', '30', '191', '15', '132', '132', '114', '107', '241', '241', '156', '199', '241', '49', '62', '117', '258', '30', '163', '133', '238', '200', '3', '15', '109', '2', '196', '165', '166', '159', '210', '35', '15', '17', '200', '241', '106', '203', '197', '197', '79', '98', '98', '23', '204', '48', '129', '57', '258', '202', '215', '238', '70', '64', '193', '189', '57', '200', '0', '98', '135', '204', '25', '30', '204', '27', '26', '78', '135', '109', '3', '197', '3', '167', '35', '196', '225', '2', '194', '200', '15', '161', '198', '23', '133', '238', '195', '35', '50', '135', '152', '163', '21', '15', '94', '26', '94', '53', '260', '196', '154', '277', '200', '118', '6', '238', '50', '209', '50', '3', '238', '3', '15', '107', '135', '199', '15', '135', '26', '258', '251', '203', '19', '259', '195', '199', '238', '15', '254', '112', '211', '200', '129', '84', '196', '3', '208', '259', '135', '104', '29', '136', '31', '78', '13', '3', '64', '75', '55', '114', '160', '96', '14', '18', '207', '75', '48', '15', '49', '97', '117', '196', '99', '258', '195', '264', '264', '78', '197', '30', '238', '109', '158', '32', '195', '149', '227', '65', '167', '162', '204', '250', '151', '151', '102', '26', '29', '26', '193', '74', '50', '193', '186', '15', '74', '259', '57', '188', '277', '277', '195', '195', '212', '78', '197', '64', '129', '259', '200', '199', '129', '4', '235', '197', '196', '161', '3', '255', '199', '102', '3', '13', '94', '53', '98', '15', '129', '15', '155', '71', '179', '50', '29', '136', '80', '254', '198', '162', '237', '3', '18', '260', '238', '117', '179', '156', '19', '45', '197', '78', '199', '102', '102', '237', '155', '69', '69', '163', '239', '15', '196', '80', '191', '30', '71', '158', '259', '2', '106', '148', '15', '135', '135', '135', '196', '19', '123', '15', '104', '19', '128', '230', '69', '98', '197', '241', '161', '195', '200', '3', '98', '78', '165', '163', '19', '196', '31', '173', '203', '55', '69', '209', '239', '79', '158', '254', '135', '51', '94', '162', '255', '95', '18', '133', '109', '50', '94', '199', '15', '197', '200', '133', '116', '50', '196', '161', '69', '133', '3', '258', '3', '197', '29', '3', '157', '197', '193', '4', '94', '195', '17', '48', '152', '8', '15', '155', '133', '195', '136', '150', '102', '237', '203', '164', '164', '250', '154', '13', '191', '58', '94', '70', '93', '264', '15', '30', '133', '237', '267', '199', '193', '76', '241', '157', '26', '200', '109', '179', '55', '98', '200', '129', '238', '235', '208', '109', '239', '258', '196', '193', '18', '107', '210', '70', '40', '129', '259', '193', '30', '158', '252', '40', '210', '78', '15', '78', '96', '208', '113', '258', '64', '80', '15', '133', '241', '15', '15', '258', '135', '109', '251', '251', '109', '74', '196', '170', '240', '254', '258', '204', '199', '26', '109', '197', '57', '45', '161', '57', '114', '3', '15', '264', '106', '57', '27', '18', '258', '29', '84', '161', '135', '114', '24', '30', '107', '107', '172', '209', '170', '18', '19', '133', '191', '195', '135', '156', '196', '94', '3', '84', '225', '199', '238', '106', '104', '70', '50', '12', '155', '114', '176', '237', '200', '108', '70', '240', '196', '19', '22', '276', '199', '238', '205', '104', '193', '108', '115', '259', '102', '19', '258', '18', '106', '161', '191', '114', '64', '15', '204', '35', '155', '40', '238', '136', '264', '133', '186', '15', '258', '166', '26', '6', '114', '114', '133', '114', '114', '13', '13', '13', '237', '250', '245', '136', '78', '13', '48', '18', '13', '25', '26', '93', '125', '50', '94', '148', '78', '194', '200', '25', '93', '274', '266', '70', '22', '3', '208', '240', '211', '135', '31', '18', '153', '18', '2', '15', '15', '15', '156', '15', '70', '73', '173', '13', '112', '69', '204', '70', '248', '135', '70', '3', '193', '193', '22', '162', '238', '57', '98', '15', '199', '199', '52', '151', '73', '5', '108', '23', '69', '95', '220', '106', '163', '152', '163', '108', '15', '58', '250', '19', '177', '245', '264', '93', '204', '25', '78', '155', '69', '155', '239', '241', '155', '200', '18', '3', '13', '196', '197', '204', '272', '95', '135', '165', '22', '210', '35', '69', '18', '170', '3', '25', '156', '78', '135', '102', '107', '255', '19', '70', '70', '109', '198', '156', '64', '170', '138', '78', '203', '166', '108', '114', '179', '228', '74', '49', '14', '264', '26', '195', '78', '107', '75', '18', '106', '151', '70', '58', '15', '196', '238', '109', '104', '104', '109', '136', '148', '210', '3', '161', '15', '104', '238', '208', '133', '108', '241', '18', '209', '74', '156', '114', '199', '174', '35', '18', '170', '135', '238', '244', '155', '162', '197', '3', '98', '227', '267', '238', '193', '50', '238', '98', '112', '124', '25', '114', '241', '159', '154', '78', '161', '136', '114', '153', '225', '154', '13', '50', '11', '25', '15', '209', '11', '166', '203', '259', '237', '22', '114', '276', '98', '278', '150', '198', '18', '15', '78', '15', '162', '26', '3', '196', '3', '102', '189', '71', '69', '16', '18', '152', '133', '135', '162', '241', '15', '104', '15', '259', '202', '2', '238', '207', '15', '204', '30', '215', '112', '210', '42', '96', '193', '15', '110', '163', '211', '78', '108', '15', '31', '27', '15', '31', '196', '195', '96', '15', '161', '94', '193', '195', '198', '98', '261', '258', '76', '15', '104', '258', '135', '18', '15', '202', '258', '108', '156', '13', '22', '251', '199', '155', '258', '211', '3', '207', '202', '259', '107', '202', '259', '135', '135', '148', '158', '154', '99', '99', '3', '207', '94', '25', '162', '31', '241', '25', '15', '114', '18', '114', '148', '19', '15', '203', '191', '19', '13', '193', '204', '25', '166', '155', '196', '98', '162', '197', '29', '152', '191', '133', '199', '230', '40', '25', '204', '79', '3', '19', '165', '204', '108', '208', '3', '251', '274', '15', '166', '264', '35', '13', '193', '114', '258', '13', '19', '133', '23', '17', '199', '204', '153', '250', '204', '16', '12', '107', '191', '18', '3', '15', '165', '15', '69', '17', '165', '17', '15', '15', '156', '23', '23', '15', '197', '104', '197', '26', '208', '135', '211', '250', '18', '204', '25', '196', '53', '69', '104', '104', '19', '78', '241', '230', '30', '195', '96', '26', '109', '196', '69', '238', '15', '18', '133', '26', '106', '109', '98', '15', '258', '274', '193', '203', '58', '57', '274', '2', '202', '161', '136', '135', '155', '241', '35', '151', '137', '5', '114', '96', '16', '209', '158', '15', '266', '161', '15', '25', '274', '202', '3', '161', '237', '267', '204', '125', '15', '151', '155', '259', '151', '210', '238', '15', '26', '132', '50', '173', '197', '114', '241', '15', '258', '109', '166', '274', '13', '191', '15', '26', '164', '258', '163', '15', '264', '11', '260', '148', '150', '207', '196', '98', '272', '133', '196', '107', '26', '191', '163', '191', '135', '3', '18', '18', '119', '69', '196', '15', '156', '135', '112', '258', '155', '256', '199', '204', '212', '18', '135', '254', '72', '258', '254', '15', '133', '18', '259', '193', '258', '204', '40', '18', '29', '155', '104', '107', '274', '241', '25', '26', '69', '148', '166', '46', '94', '135', '15', '155', '104', '241', '32', '274', '249', '15', '197', '104', '44', '259', '69', '191', '15', '198', '193', '133', '17', '258', '248', '166', '197', '200', '250', '204', '3', '225', '238', '163', '69', '15', '15', '26', '202', '19', '209', '78', '212', '15', '78', '243', '110', '191', '15', '18', '107', '38', '208', '166', '239', '237', '199', '15', '204', '258', '173', '172', '200', '274', '243', '19', '191', '135', '203', '69', '149', '148', '196', '256', '204', '97', '191', '26', '114', '274', '200', '69', '69', '64', '69', '239', '25', '274', '203', '78', '135', '196', '202', '17', '2', '2', '3', '98', '98', '62', '2', '95', '96', '2', '207', '78', '200', '26', '64', '18', '93', '49', '57', '154', '200', '69', '94', '98', '98', '56', '56', '166', '98', '212', '226', '226', '15', '69', '15', '273', '272', '104', '193', '3', '98', '199', '98', '25', '125', '259', '249', '108', '40', '26', '198', '218', '73', '148', '69', '204', '241', '35', '3', '274', '70', '18', '71', '35', '172', '133', '106', '69', '107', '199', '219', '19', '133', '69', '135', '109', '57', '191', '135', '252', '104', '3', '94', '3', '72', '126', '16', '36', '84', '241', '3', '218', '98', '108', '46', '19', '196', '118', '104', '29', '238', '69', '10', '204', '264', '259', '124', '242', '107', '133', '25', '64', '30', '194', '259', '159', '264', '274', '52', '200', '104', '29', '30', '193', '98', '94', '114', '204', '40', '18', '198', '97', '3', '99', '274', '239', '177', '15', '18', '126', '126', '208', '148', '15', '104', '29', '36', '218', '70', '15', '23', '278', '23', '36', '191', '210', '51', '245', '167', '72', '237', '25', '203', '36', '196', '193', '196', '15', '151', '207', '203', '103', '166', '135', '110', '27', '13', '27', '191', '98', '57', '166', '50', '21', '15', '197', '23', '2', '70', '69', '193', '274', '104', '166', '258', '126', '238', '161', '18', '193', '69', '140', '170', '197', '204', '165', '203', '170', '15', '58', '35', '83', '212', '150', '73', '25', '15', '64', '133', '210', '108', '79', '193', '15', '126', '165', '15', '208', '98', '3', '200', '126', '3', '40', '165', '98', '69', '15', '165', '133', '96', '173', '203', '133', '238', '248', '15', '7', '38', '207', '108', '165', '199', '18', '162', '27', '3', '72', '65', '250', '209', '166', '64', '209', '237', '212', '64', '225', '120', '108', '203', '135', '248', '69', '251', '135', '204', '160', '160', '135', '148', '15', '15', '23', '207', '274', '29', '28', '194', '150', '40', '104', '208', '125', '70', '18', '3', '19', '102', '104', '23', '29', '207', '133', '204', '111', '2', '125', '126', '7', '194', '209', '203', '83', '126', '126', '258', '119', '162', '241', '133', '18', '208', '197', '209', '15', '78', '80', '110', '237', '104', '30', '208', '103', '15', '200', '15', '238', '254', '108', '173', '40', '69', '175', '17', '155', '136', '26', '3', '135', '13', '241', '161', '166', '170', '162', '133', '53', '211', '204', '15', '161', '238', '110', '18', '191', '18', '50', '71', '30', '17', '208', '248', '203', '203', '196', '210', '258', '204', '3', '18', '196', '125', '173', '199', '249', '18', '104', '165', '241', '2', '193', '238', '78', '15', '166', '15', '126', '3', '237', '161', '52', '207', '3', '3', '50', '56', '22', '56', '69', '93', '135', '207', '69', '99', '3', '133', '68', '133', '161', '108', '126', '250', '60', '155', '98', '18', '16', '78', '78', '240', '235', '176', '22', '199', '160', '235', '274', '209', '64', '173', '137', '128', '25', '18', '160', '203', '70', '165', '193', '110', '207', '64', '85', '252', '78', '51', '2', '99', '19', '22', '198', '204', '166', '166', '151', '56', '133', '210', '26', '149', '109', '30', '209', '209', '51', '3', '193', '119', '208', '93', '165', '248', '55', '203', '204', '58', '18', '199', '211', '83', '32', '135', '212', '156', '19', '235', '173', '191', '209', '203', '146', '208', '58', '158', '102', '199', '200', '200', '173', '70', '52', '107', '211', '213', '237', '196', '205', '104', '135', '258', '255', '96', '19', '124', '128', '57', '199', '98', '166', '102', '40', '15', '238', '3', '148', '225', '126', '135', '114', '193', '204', '15', '209', '209', '193', '259', '64', '161', '26', '274', '126', '18', '196', '0', '60', '207', '17', '71', '218', '239', '248', '38', '154', '61', '128', '199', '196', '173', '25', '46', '98', '258', '108', '3', '108', '196', '93', '93', '99', '172', '69', '241', '196', '30', '135', '278', '109', '18', '5', '209', '57', '159', '70', '193', '163', '249', '196', '196', '72', '103', '133', '17', '204', '199', '166', '202', '29', '57', '193', '150', '98', '119', '25', '205', '18', '173', '166', '209', '57', '25', '58', '258', '26', '15', '15', '101', '26', '74', '1', '4', '74', '263', '']\n",
            "['209', '63', '216', '108', '196', '200', '203', '246', '188', '193', '25', '106', '56', '53', '223', '161', '200', '208', '78', '147', '147', '179', '147', '80', '56', '209', '209', '200', '214', '202', '203', '194', '26', '237', '165', '258', '238', '238', '76', '193', '196', '213', '161', '213', '209', '194', '196', '25', '151', '260', '135', '173', '30', '108', '272', '259', '176', '213', '213', '21', '5', '171', '200', '195', '240', '194', '176', '240', '240', '80', '173', '98', '214', '21', '76', '76', '73', '173', '173', '78', '61', '258', '191', '3', '78', '173', '193', '118', '200', '264', '260', '49', '211', '258', '18', '173', '78', '3', '3', '1', '213', '135', '182', '239', '208', '278', '50', '194', '200', '124', '189', '117', '264', '244', '76', '193', '221', '78', '78', '173', '197', '78', '161', '231', '135', '22', '78', '25', '25', '197', '200', '147', '203', '191', '196', '193', '263', '32', '197', '191', '4', '56', '196', '104', '191', '196', '173', '147', '194', '96', '161', '161', '238', '200', '3', '78', '78', '53', '41', '237', '53', '18', '10', '194', '97', '96', '208', '25', '10', '117', '104', '54', '96', '194', '21', '245', '238', '202', '209', '161', '97', '258', '240', '213', '136', '155', '152', '21', '191', '191', '118', '68', '258', '189', '197', '208', '61', '78', '135', '202', '96', '155', '108', '272', '80', '25', '155', '35', '213', '191', '194', '96', '96', '191', '264', '191', '191', '191', '2', '258', '191', '173', '259', '15', '47', '258', '156', '118', '160', '115', '105', '26', '173', '192', '76', '76', '135', '240', '118', '161', '200', '230', '18', '200', '25', '10', '161', '244', '24', '24', '200', '205', '104', '15', '193', '156', '95', '209', '240', '191', '194', '128', '208', '208', '218', '173', '10', '25', '18', '107', '46', '32', '197', '172', '209', '213', '79', '121', '78', '78', '193', '2', '78', '194', '149', '136', '80', '3', '21', '200', '78', '21', '197', '194', '118', '202', '161', '180', '209', '173', '2', '193', '6', '260', '197', '78', '78', '42', '191', '213', '135', '193', '196', '202', '222', '217', '42', '259', '213', '76', '46', '30', '15', '260', '202', '18', '259', '104', '135', '76', '196', '240', '240', '240', '108', '24', '200', '197', '262', '176', '15', '156', '264', '156', '200', '191', '161', '196', '78', '238', '161', '221', '118', '14', '14', '14', '200', '278', '238', '200', '196', '78', '173', '25', '197', '197', '196', '161', '18', '10', '10', '10', '18', '66', '203', '19', '246', '196', '244', '240', '161', '200', '78', '240', '209', '208', '118', '215', '194', '193', '168', '229', '191', '135', '172', '196', '197', '161', '78', '196', '197', '258', '78', '78', '238', '221', '121', '264', '104', '146', '50', '78', '217', '11', '238', '238', '238', '3', '3', '154', '215', '79', '18', '158', '104', '18', '13', '197', '117', '197', '161', '213', '242', '32', '238', '230', '2', '2', '32', '161', '251', '96', '156', '242', '22', '22', '140', '3', '149', '135', '191', '173', '135', '200', '238', '238', '78', '156', '209', '78', '97', '97', '160', '25', '258', '240', '161', '15', '161', '96', '209', '80', '132', '140', '4', '1', '240', '196', '274', '272', '62', '80', '196', '196', '191', '79', '156', '15', '196', '209', '22', '251', '202', '202', '121', '73', '191', '238', '191', '150', '150', '135', '46', '203', '202', '238', '22', '3', '258', '239', '238', '238', '165', '238', '238', '194', '200', '200', '197', '200', '196', '191', '194', '76', '78', '208', '200', '71', '208', '240', '76', '117', '120', '108', '96', '144', '30', '133', '133', '202', '117', '161', '251', '29', '104', '242', '197', '118', '216', '262', '236', '141', '11', '235', '235', '133', '191', '238', '202', '80', '80', '22', '140', '191', '197', '202', '133', '184', '184', '184', '184', '143', '274', '50', '237', '196', '274', '265', '10', '18', '10', '121', '238', '245', '228', '76', '25', '191', '197', '268', '10', '240', '195', '196', '195', '15', '48', '200', '183', '70', '59', '197', '196', '235', '161', '161', '215', '104', '97', '191', '194', '191', '200', '200', '197', '76', '200', '93', '256', '160', '238', '146', '238', '239', '15', '140', '258', '140', '21', '79', '202', '258', '79', '62', '205', '161', '104', '10', '161', '196', '137', '152', '191', '117', '213', '242', '176', '218', '195', '191', '162', '191', '135', '241', '213', '213', '84', '79', '68', '18', '98', '197', '155', '41', '246', '269', '78', '209', '238', '80', '238', '62', '237', '273', '273', '230', '150', '121', '260', '244', '161', '202', '78', '109', '155', '264', '145', '50', '274', '173', '210', '258', '238', '211', '156', '215', '195', '177', '244', '3', '141', '196', '191', '194', '29', '30', '10', '16', '200', '200', '156', '53', '3', '62', '165', '173', '173', '22', '205', '274', '150', '183', '116', '188', '188', '98', '161', '204', '80', '172', '156', '104', '229', '259', '78', '169', '3', '200', '118', '161', '80', '260', '240', '150', '197', '196', '62', '93', '6', '272', '272', '197', '160', '78', '96', '70', '195', '191', '191', '80', '3', '3', '240', '147', '196', '191', '238', '96', '203', '3', '15', '10', '58', '55', '96', '41', '15', '26', '50', '238', '194', '79', '240', '161', '191', '195', '263', '50', '240', '147', '235', '195', '203', '59', '178', '176', '2', '15', '203', '191', '18', '278', '196', '14', '153', '58', '175', '18', '217', '217', '80', '191', '200', '18', '200', '173', '24', '171', '171', '210', '124', '34', '200', '218', '104', '4', '223', '193', '194', '200', '200', '13', '62', '194', '197', '80', '197', '146', '146', '96', '97', '97', '2', '196', '191', '191', '104', '238', '10', '200', '239', '135', '104', '213', '2', '221', '169', '25', '21', '194', '161', '21', '278', '3', '195', '46', '176', '25', '117', '191', '62', '62', '62', '78', '50', '264', '79', '197', '80', '80', '80', '191', '191', '78', '161', '194', '195', '197', '203', '259', '278', '49', '147', '147', '147', '147', '22', '78', '78', '258', '2', '202', '221', '55', '258', '104', '104', '84', '240', '221', '274', '104', '22', '241', '63', '197', '197', '196', '159', '200', '22', '22', '22', '78', '78', '173', '104', '97', '104', '191', '146', '223', '268', '96', '14', '173', '204', '241', '97', '15', '118', '78', '78', '196', '194', '258', '96', '96', '259', '75', '10', '197', '30', '2', '238', '121', '50', '161', '173', '173', '186', '185', '186', '78', '194', '221', '218', '35', '33', '13', '34', '78', '21', '215', '97', '60', '24', '78', '193', '173', '235', '78', '200', '258', '140', '259', '194', '197', '195', '200', '145', '76', '108', '130', '238', '215', '213', '2', '217', '121', '178', '140', '62', '156', '176', '196', '196', '41', '223', '147', '172', '79', '200', '2', '244', '244', '191', '213', '104', '191', '237', '237', '33', '117', '195', '195', '264', '195', '140', '210', '41', '197', '14', '257', '196', '238', '238', '79', '161', '156', '196', '238', '238', '213', '104', '3', '10', '118', '204', '235', '33', '259', '274', '259', '176', '78', '78', '196', '34', '52', '25', '30', '194', '153', '21', '70', '191', '18', '191', '191', '172', '194', '194', '194', '194', '148', '104', '125', '2', '2', '2', '185', '194', '29', '104', '78', '189', '272', '204', '183', '213', '78', '104', '197', '240', '97', '168', '168', '182', '238', '197', '18', '194', '229', '221', '62', '78', '53', '104', '214', '189', '189', '189', '213', '117', '117', '191', '29', '104', '104', '276', '276', '22', '195', '195', '155', '139', '139', '59', '168', '135', '140', '79', '146', '200', '245', '70', '160', '153', '50', '202', '154', '263', '130', '194', '117', '194', '194', '191', '33', '29', '29', '238', '239', '194', '21', '274', '215', '18', '18', '140', '2', '215', '25', '46', '137', '238', '30', '160', '156', '46', '196', '263', '242', '104', '0', '0', '73', '213', '135', '22', '191', '191', '241', '202', '221', '15', '15', '191', '238', '213', '194', '197', '179', '238', '200', '200', '200', '5', '238', '194', '55', '117', '166', '49', '244', '239', '191', '78', '78', '78', '194', '200', '117', '210', '22', '121', '121', '121', '21', '237', '194', '121', '78', '104', '194', '79', '191', '251', '195', '197', '274', '176', '21', '43', '43', '10', '61', '215', '215', '30', '214', '194', '10', '30', '139', '96', '226', '226', '265', '2', '2', '121', '121', '41', '237', '104', '238', '238', '161', '78', '78', '78', '11', '184', '140', '140', '161', '104', '104', '237', '130', '195', '160', '118', '161', '18', '11', '241', '184', '196', '178', '22', '30', '146', '146', '10', '10', '141', '141', '135', '18', '18', '10', '26', '208', '97', '141', '18', '18', '97', '194', '194', '79', '213', '205', '205', '205', '75', '241', '50', '226', '238', '97', '222', '215', '215', '139', '238', '51', '238', '96', '173', '109', '200', '117', '192', '2', '195', '10', '3', '18', '49', '238', '238', '191', '195', '52', '197', '200', '200', '79', '191', '200', '200', '30', '195', '197', '195', '25', '260', '73', '30', '193', '21', '200', '242', '203', '120', '78', '260', '241', '0', '30', '202', '195', '30', '191', '78', '194', '194', '2', '205', '191', '196', '173', '197', '30', '196', '172', '139', '2', '18', '191', '230', '56', '194', '215', '195', '2', '71', '13', '146', '264', '50', '191', '235', '93', '191', '139', '139', '227', '235', '186', '235', '24', '18', '261', '128', '33', '71', '25', '176', '39', '10', '195', '202', '29', '29', '221', '29', '197', '139', '78', '227', '146', '137', '195', '205', '205', '195', '135', '62', '153', '146', '29', '75', '0', '194', '194', '180', '213', '78', '18', '194', '239', '196', '239', '197', '194', '153', '192', '221', '10', '46', '50', '241', '117', '117', '117', '195', '173', '78', '195', '194', '239', '2', '12', '173', '197', '30', '269', '197', '176', '135', '30', '146', '191', '93', '93', '135', '260', '218', '156', '155', '200', '241', '125', '18', '18', '183', '203', '239', '259', '173', '213', '180', '180', '210', '120', '104', '194', '156', '258', '172', '9', '0', '241', '192', '143', '189', '204', '50', '63', '197', '243', '241', '93', '125', '2', '21', '260', '4', '4', '3', '191', '150', '128', '78', '238', '245', '8', '80', '238', '79', '79', '240', '58', '104', '155', '80', '80', '188', '191', '235', '170', '170', '170', '146', '258', '195', '276', '25', '78', '215', '240', '161', '53', '147', '78', '78', '78', '78', '221', '221', '96', '18', '19', '130', '18', '18', '18', '80', '80', '194', '79', '93', '121', '78', '139', '139', '78', '197', '121', '194', '200', '80', '101', '22', '12', '135', '97', '205', '181', '60', '193', '15', '140', '172', '191', '241', '222', '135', '29', '25', '179', '191', '191', '259', '194', '133', '240', '121', '121', '241', '195', '29', '191', '209', '18', '139', '139', '50', '18', '12', '18', '133', '200', '25', '10', '70', '25', '242', '18', '161', '49', '172', '172', '172', '172', '128', '96', '195', '200', '78', '10', '239', '22', '173', '142', '142', '22', '194', '192', '3', '200', '241', '96', '241', '62', '139', '213', '202', '135', '79', '120', '192', '78', '118', '241', '46', '151', '150', '151', '150', '118', '221', '183', '237', '196', '140', '140', '39', '179', '213', '140', '41', '73', '78', '217', '258', '30', '118', '140', '219', '50', '10', '139', '150', '251', '24', '10', '18', '78', '18', '172', '135', '146', '71', '196', '202', '197', '25', '106', '160', '215', '15', '6', '101', '165', '140', '145', '80', '3', '238', '239', '78', '268', '0', '109', '191', '215', '238', '238', '238', '203', '50', '146', '15', '25', '156', '191', '16', '2', '195', '189', '96', '193', '109', '204', '161', '185', '210', '50', '235', '30', '242', '193', '102', '194', '121', '177', '241', '102', '132', '240', '71', '192', '79', '25', '197', '39', '161', '22', '258', '237', '15', '7', '78', '156', '197', '70', '176', '139', '240', '132', '78', '177', '191', '194', '117', '10', '79', '238', '195', '24', '101', '118', '196', '2', '102', '21', '180', '185', '170', '79', '139', '135', '191', '151', '200', '191', '154', '262', '200', '237', '56', '27', '191', '180', '258', '139', '139', '192', '30', '0', '242', '239', '242', '25', '191', '242', '240', '3', '56', '215', '171', '192', '193', '222', '238', '237', '215', '272', '2', '156', '237', '151', '189', '80', '15', '260', '260', '217', '235', '22', '228', '135', '18', '191', '157', '78', '2', '240', '213', '179', '2', '191', '79', '79', '153', '3', '194', '78', '121', '71', '121', '149', '257', '194', '191', '188', '15', '25', '125', '102', '15', '130', '130', '177', '10', '135', '265', '240', '149', '196', '200', '204', '204', '213', '177', '197', '194', '13', '202', '30', '213', '125', '193', '125', '125', '125', '125', '125', '15', '180', '78', '1', '25', '96', '200', '29', '156', '172', '202', '22', '78', '197', '188', '241', '101', '101', '101', '221', '50', '191', '15', '15', '0', '101', '101', '168', '56', '197', '130', '62', '204', '29', '10', '96', '97', '139', '97', '97', '2', '2', '262', '39', '176', '172', '194', '194', '18', '78', '13', '80', '139', '196', '207', '191', '151', '192', '150', '13', '104', '104', '101', '15', '125', '101', '15', '0', '50', '194', '264', '50', '25', '25', '125', '125', '125', '125', '125', '25', '29', '25', '25', '78', '240', '261', '19', '2', '74', '78', '238', '50', '50', '203', '197', '71', '104', '80', '194', '149', '170', '130', '2', '237', '170', '202', '48', '97', '93', '179', '235', '78', '102', '102', '70', '208', '238', '104', '146', '118', '118', '96', '237', '274', '218', '241', '102', '146', '209', '209', '240', '170', '137', '7', '197', '191', '264', '29', '46', '96', '193', '19', '12', '213', '215', '275', '179', '18', '183', '78', '221', '18', '170', '119', '21', '140', '240', '96', '192', '192', '21', '96', '149', '239', '135', '191', '240', '184', '183', '24', '2', '96', '197', '194', '215', '18', '192', '121', '202', '196', '25', '97', '214', '3', '97', '16', '0', '192', '135', '215', '5', '12', '80', '143', '202', '242', '151', '13', '209', '203', '200', '30', '209', '194', '102', '102', '102', '102', '102', '102', '22', '24', '6', '18', '5', '191', '101', '50', '101', '101', '192', '153', '153', '101', '101', '10', '192', '25', '140', '50', '241', '102', '102', '32', '150', '101', '102', '153', '102', '101', '153', '102', '25', '10', '125', '18', '125', '177', '209', '102', '10', '78', '78', '137', '18', '237', '241', '175', '195', '175', '96', '176', '237', '96', '96', '101', '193', '238', '170', '238', '102', '222', '268', '238', '102', '244', '34', '18', '244', '202', '149', '149', '193', '102', '170', '194', '75', '101', '222', '238', '50', '195', '135', '22', '93', '18', '102', '102', '93', '93', '191', '147', '244', '96', '2', '241', '241', '93', '189', '189', '177', '35', '137', '197', '191', '40', '40', '195', '176', '35', '200', '200', '237', '135', '140', '140', '93', '156', '200', '78', '191', '22', '81', '238', '101', '213', '101', '78', '109', '19', '140', '196', '78', '78', '76', '218', '14', '265', '101', '241', '155', '4', '50', '78', '78', '238', '135', '191', '265', '81', '238', '238', '102', '102', '102', '140', '162', '32', '17', '17', '171', '171', '191', '101', '102', '192', '101', '17', '171', '135', '102', '242', '195', '237', '21', '18', '238', '191', '278', '140', '149', '97', '260', '155', '276', '192', '196', '21', '191', '2', '97', '0', '150', '221', '213', '209', '238', '80', '260', '194', '104', '96', '193', '14', '240', '193', '117', '117', '155', '17', '264', '260', '209', '209', '2', '137', '71', '72', '101', '205', '173', '191', '118', '24', '41', '197', '78', '94', '237', '156', '193', '153', '154', '153', '209', '110', '78', '78', '213', '238', '78', '191', '93', '79', '177', '267', '133', '133', '46', '139', '40', '143', '74', '241', '202', '50', '3', '102', '78', '149', '191', '156', '18', '93', '195', '192', '137', '240', '135', '204', '189', '22', '56', '154', '25', '192', '130', '277', '191', '80', '39', '21', '21', '238', '238', '23', '192', '191', '192', '94', '94', '80', '139', '17', '10', '22', '227', '227', '140', '102', '21', '17', '227', '227', '23', '243', '264', '18', '274', '149', '238', '30', '221', '22', '238', '193', '135', '205', '195', '238', '238', '197', '130', '195', '18', '104', '102', '14', '59', '104', '18', '18', '258', '34', '241', '138', '188', '176', '102', '1', '140', '140', '22', '22', '278', '10', '96', '35', '145', '120', '33', '33', '22', '33', '22', '35', '22', '19', '33', '102', '192', '192', '175', '22', '72', '191', '222', '125', '22', '79', '18', '194', '18', '250', '192', '192', '96', '48', '274', '273', '132', '133', '192', '40', '171', '171', '246', '191', '10', '166', '197', '195', '125', '125', '200', '153', '192', '149', '110', '139', '194', '48', '102', '102', '104', '203', '238', '238', '238', '78', '18', '238', '121', '18', '238', '96', '238', '238', '13', '191', '18', '18', '240', '96', '40', '188', '18', '137', '175', '28', '28', '28', '149', '191', '191', '207', '239', '239', '153', '153', '153', '153', '153', '153', '154', '153', '154', '153', '153', '153', '18', '0', '27', '18', '25', '96', '96', '137', '33', '10', '130', '153', '153', '154', '180', '137', '44', '241', '153', '268', '25', '161', '25', '39', '242', '4', '238', '195', '102', '135', '13', '229', '140', '1', '79', '192', '229', '191', '118', '21', '260', '203', '18', '177', '191', '10', '238', '10', '133', '14', '175', '175', '192', '18', '153', '153', '254', '140', '149', '22', '22', '22', '237', '40', '135', '191', '192', '192', '62', '205', '19', '238', '176', '214', '200', '96', '15', '153', '191', '108', '29', '166', '28', '193', '197', '96', '195', '108', '102', '241', '195', '10', '33', '166', '197', '168', '2', '96', '26', '33', '204', '258', '149', '153', '153', '40', '137', '241', '166', '221', '23', '221', '153', '15', '113', '113', '78', '171', '40', '71', '21', '71', '80', '15', '191', '191', '135', '110', '10', '259', '102', '149', '149', '109', '109', '175', '97', '97', '97', '153', '15', '238', '193', '175', '38', '38', '137', '18', '235', '155', '137', '265', '110', '18', '18', '256', '191', '224', '173', '104', '10', '152', '24', '24', '24', '17', '40', '40', '40', '40', '40', '40', '112', '6', '18', '18', '210', '96', '191', '27', '213', '17', '153', '26', '133', '18', '18', '153', '38', '96', '40', '38', '40', '40', '10', '17', '17', '95', '71', '192', '17', '259', '213', '200', '15', '11', '207', '194', '18', '135', '238', '191', '241', '191', '116', '35', '147', '156', '67', '78', '17', '227', '191', '117', '188', '130', '72', '96', '235', '97', '18', '150', '203', '25', '126', '149', '200', '94', '4', '4', '1', '244', '228', '252', '149', '191', '149', '15', '17', '17', '29', '18', '18', '116', '23', '237', '94', '184', '268', '197', '175', '243', '28', '16', '10', '50', '191', '102', '102', '153', '149', '111', '78', '191', '135', '18', '35', '78', '238', '208', '93', '95', '258', '17', '111', '111', '262', '22', '22', '104', '161', '209', '21', '195', '12', '116', '241', '242', '239', '17', '97', '16', '40', '117', '94', '96', '78', '15', '103', '195', '135', '235', '247', '135', '130', '260', '239', '176', '93', '191', '209', '239', '93', '264', '30', '18', '30', '18', '18', '103', '191', '97', '17', '95', '115', '209', '140', '22', '22', '241', '10', '153', '3', '188', '238', '226', '195', '97', '96', '238', '97', '130', '97', '116', '96', '96', '242', '45', '149', '166', '155', '96', '116', '264', '10', '237', '192', '71', '22', '71', '175', '149', '116', '237', '274', '18', '235', '176', '116', '202', '250', '250', '78', '2', '196', '2', '97', '143', '18', '111', '111', '80', '78', '153', '80', '40', '110', '30', '112', '30', '111', '215', '112', '149', '78', '78', '209', '110', '110', '238', '109', '239', '112', '151', '154', '16', '16', '16', '78', '137', '179', '72', '149', '10', '111', '268', '96', '170', '111', '38', '192', '153', '183', '195', '30', '30', '147', '195', '147', '93', '264', '153', '25', '18', '19', '22', '74', '40', '93', '166', '15', '127', '147', '192', '260', '238', '103', '269', '127', '19', '19', '153', '18', '78', '238', '30', '30', '241', '16', '235', '250', '204', '238', '218', '40', '93', '184', '18', '0', '241', '195', '94', '154', '109', '156', '10', '21', '30', '197', '39', '78', '24', '107', '142', '142', '228', '229', '229', '229', '232', '179', '239', '142', '232', '23', '175', '147', '137', '137', '137', '171', '191', '117', '128', '127', '204', '98', '147', '235', '239', '153', '15', '204', '255', '80', '204', '204', '149', '113', '19', '167', '22', '250', '17', '17', '17', '18', '17', '67', '121', '125', '26', '97', '97', '97', '97', '247', '124', '163', '147', '112', '111', '208', '108', '38', '209', '154', '154', '194', '135', '22', '147', '241', '120', '218', '260', '156', '155', '241', '192', '215', '180', '179', '149', '40', '193', '192', '150', '102', '93', '1', '149', '137', '133', '102', '102', '102', '78', '145', '21', '96', '175', '175', '78', '123', '30', '95', '19', '18', '110', '18', '152', '152', '147', '26', '135', '260', '158', '203', '195', '158', '112', '156', '125', '213', '230', '30', '15', '125', '106', '111', '195', '181', '130', '195', '130', '239', '15', '194', '16', '15', '111', '61', '72', '57', '128', '239', '239', '203', '25', '218', '10', '155', '191', '46', '98', '175', '260', '102', '21', '191', '260', '191', '239', '15', '30', '93', '147', '103', '78', '177', '30', '95', '155', '155', '155', '103', '103', '112', '241', '191', '191', '17', '78', '153', '150', '113', '115', '147', '78', '40', '163', '33', '15', '149', '103', '192', '191', '96', '106', '115', '143', '235', '200', '204', '238', '204', '204', '74', '196', '113', '246', '102', '150', '16', '176', '20', '195', '128', '197', '15', '71', '71', '193', '24', '24', '24', '170', '204', '209', '111', '144', '112', '22', '222', '224', '28', '209', '155', '213', '275', '196', '239', '195', '25', '103', '149', '128', '19', '96', '102', '135', '17', '232', '155', '191', '20', '18', '241', '30', '25', '205', '97', '97', '96', '72', '182', '135', '135', '20', '149', '149', '78', '135', '40', '18', '153', '10', '26', '158', '205', '157', '157', '135', '135', '140', '155', '192', '26', '268', '263', '245', '155', '15', '177', '159', '212', '161', '239', '200', '40', '35', '153', '197', '259', '15', '254', '175', '10', '30', '239', '67', '195', '200', '191', '22', '66', '191', '112', '133', '128', '235', '161', '155', '17', '93', '36', '113', '239', '18', '18', '191', '165', '163', '163', '238', '79', '158', '40', '102', '40', '200', '191', '60', '241', '245', '96', '78', '30', '15', '173', '192', '192', '158', '64', '113', '200', '74', '128', '135', '115', '115', '10', '74', '102', '102', '115', '111', '238', '244', '109', '109', '238', '152', '10', '237', '18', '50', '16', '173', '204', '15', '208', '276', '43', '109', '258', '222', '15', '94', '235', '203', '79', '128', '113', '3', '204', '106', '133', '214', '218', '25', '98', '166', '71', '30', '175', '175', '97', '135', '108', '19', '18', '196', '35', '12', '196', '78', '166', '155', '72', '155', '22', '109', '258', '196', '263', '161', '204', '210', '14', '208', '208', '208', '208', '204', '102', '102', '213', '155', '251', '153', '153', '153', '154', '154', '237', '97', '19', '28', '94', '136', '112', '19', '97', '152', '153', '153', '111', '111', '135', '135', '105', '17', '105', '152', '152', '152', '153', '79', '266', '135', '120', '125', '149', '50', '162', '72', '17', '147', '109', '115', '24', '105', '244', '149', '238', '203', '35', '202', '177', '173', '239', '19', '19', '179', '237', '14', '18', '185', '58', '158', '144', '213', '237', '155', '250', '239', '93', '237', '97', '195', '157', '107', '109', '246', '165', '98', '49', '2', '266', '149', '20', '18', '71', '108', '22', '115', '239', '40', '178', '232', '238', '218', '118', '118', '210', '235', '46', '228', '192', '112', '149', '96', '111', '21', '95', '95', '95', '18', '57', '206', '18', '239', '157', '80', '196', '112', '176', '99', '106', '71', '235', '135', '18', '209', '78', '20', '20', '103', '33', '3', '103', '166', '79', '79', '156', '229', '18', '238', '2', '16', '128', '200', '117', '22', '197', '166', '78', '237', '241', '107', '53', '189', '204', '243', '163', '221', '15', '200', '221', '152', '23', '193', '107', '195', '99', '97', '19', '125', '276', '113', '193', '64', '78', '197', '117', '175', '135', '135', '78', '93', '154', '153', '256', '256', '135', '137', '135', '137', '149', '110', '15', '159', '21', '158', '192', '192', '117', '29', '97', '258', '78', '46', '5', '80', '15', '105', '260', '161', '191', '2', '138', '117', '215', '176', '156', '194', '265', '115', '181', '21', '14', '106', '106', '108', '200', '156', '103', '102', '29', '17', '204', '175', '227', '118', '18', '239', '204', '258', '78', '155', '213', '109', '192', '186', '186', '15', '241', '133', '30', '30', '239', '30', '218', '109', '138', '241', '241', '45', '135', '112', '232', '104', '106', '200', '53', '23', '112', '17', '117', '59', '260', '195', '106', '117', '159', '204', '200', '228', '165', '165', '261', '200', '227', '166', '93', '21', '80', '112', '152', '196', '203', '228', '98', '102', '15', '139', '110', '13', '238', '6', '171', '94', '208', '155', '112', '203', '239', '191', '39', '136', '203', '195', '46', '203', '148', '192', '210', '23', '4', '153', '29', '165', '192', '204', '232', '94', '112', '239', '98', '71', '248', '17', '155', '10', '78', '191', '191', '111', '25', '25', '19', '19', '17', '235', '102', '265', '195', '235', '163', '238', '71', '197', '107', '155', '30', '72', '19', '259', '80', '155', '23', '64', '239', '197', '156', '57', '203', '258', '200', '205', '246', '238', '258', '227', '238', '238', '19', '148', '161', '111', '105', '202', '109', '262', '133', '137', '221', '135', '52', '170', '111', '36', '30', '156', '102', '268', '115', '22', '153', '133', '259', '98', '235', '135', '191', '25', '235', '25', '29', '94', '195', '203', '212', '112', '135', '245', '260', '200', '96', '158', '23', '165', '22', '18', '249', '240', '109', '43', '37', '15', '241', '241', '23', '135', '240', '108', '98', '268', '275', '191', '136', '263', '259', '196', '98', '98', '204', '248', '123', '105', '204', '240', '9', '9', '98', '218', '189', '238', '246', '155', '130', '157', '74', '109', '29', '189', '203', '213', '195', '241', '159', '238', '204', '218', '105', '23', '16', '263', '206', '150', '152', '204', '202', '173', '267', '18', '153', '155', '196', '2', '97', '158', '158', '204', '98', '202', '98', '204', '258', '98', '22', '113', '113', '135', '71', '237', '155', '163', '250', '233', '26', '26', '26', '26', '26', '19', '170', '18', '15', '19', '191', '30', '30', '48', '35', '30', '237', '46', '119', '16', '78', '239', '150', '245', '235', '158', '27', '241', '30', '38', '107', '75', '241', '135', '106', '106', '98', '159', '96', '135', '111', '176', '175', '200', '18', '14', '242', '163', '187', '20', '12', '191', '23', '106', '171', '25', '78', '162', '191', '157', '94', '237', '246', '203', '108', '238', '204', '204', '1', '102', '154', '106', '170', '102', '200', '2', '135', '106', '258', '258', '227', '26', '15', '25', '71', '203', '96', '3', '241', '200', '30', '108', '98', '108', '27', '106', '26', '150', '259', '78', '113', '113', '158', '202', '16', '136', '239', '156', '239', '35', '276', '252', '115', '22', '22', '258', '172', '3', '154', '3', '98', '237', '102', '235', '112', '206', '98', '57', '118', '153', '18', '26', '30', '196', '163', '191', '153', '72', '24', '156', '117', '122', '30', '135', '192', '98', '28', '30', '30', '95', '241', '155', '17', '195', '31', '103', '202', '98', '237', '94', '112', '163', '163', '94', '163', '112', '19', '195', '191', '15', '135', '78', '109', '160', '152', '97', '98', '107', '167', '97', '138', '138', '26', '191', '97', '97', '30', '30', '206', '202', '13', '30', '240', '111', '261', '79', '0', '111', '19', '152', '107', '107', '18', '98', '28', '258', '244', '106', '237', '204', '191', '15', '183', '202', '16', '111', '166', '268', '202', '12', '30', '78', '110', '133', '71', '197', '111', '52', '79', '171', '156', '94', '13', '118', '111', '71', '133', '18', '105', '30', '22', '133', '60', '237', '191', '6', '126', '3', '238', '276', '78', '162', '150', '115', '191', '107', '200', '148', '135', '72', '155', '241', '4', '108', '160', '203', '20', '133', '2', '80', '80', '154', '18', '15', '115', '239', '167', '167', '35', '268', '18', '221', '18', '195', '204', '195', '216', '133', '112', '104', '223', '191', '115', '30', '30', '30', '30', '30', '202', '204', '165', '200', '200', '29', '191', '237', '267', '18', '175', '81', '264', '264', '178', '115', '102', '178', '212', '273', '202', '109', '259', '166', '135', '155', '219', '161', '18', '94', '241', '191', '196', '191', '98', '26', '26', '204', '204', '135', '27', '238', '260', '182', '26', '98', '94', '93', '238', '156', '19', '111', '79', '16', '192', '113', '19', '241', '18', '202', '260', '272', '106', '163', '156', '106', '17', '109', '221', '78', '203', '7', '136', '197', '193', '191', '98', '112', '200', '95', '221', '98', '161', '191', '3', '106', '189', '111', '25', '196', '196', '241', '163', '200', '24', '78', '176', '3', '98', '102', '98', '19', '161', '224', '80', '18', '191', '112', '105', '254', '71', '200', '137', '29', '20', '165', '8', '106', '23', '106', '155', '200', '265', '135', '126', '203', '188', '161', '158', '258', '255', '98', '203', '50', '35', '102', '158', '175', '40', '239', '26', '66', '57', '193', '155', '15', '135', '10', '107', '106', '229', '18', '191', '204', '107', '99', '235', '238', '18', '205', '205', '35', '238', '208', '203', '110', '110', '109', '112', '23', '18', '19', '182', '156', '18', '18', '156', '111', '135', '137', '133', '137', '240', '99', '197', '113', '15', '165', '39', '97', '97', '15', '150', '163', '109', '268', '15', '15', '238', '203', '67', '161', '197', '18', '196', '97', '161', '197', '161', '185', '238', '191', '30', '121', '133', '160', '160', '74', '135', '75', '54', '135', '196', '23', '241', '166', '194', '239', '135', '240', '208', '238', '98', '135', '152', '176', '22', '174', '113', '241', '237', '57', '195', '191', '24', '165', '165', '204', '204', '56', '109', '27', '153', '252', '228', '165', '252', '200', '28', '158', '135', '196', '22', '18', '133', '30', '204', '237', '196', '93', '106', '94', '25', '93', '159', '4', '195', '23', '237', '25', '199', '57', '26', '276', '274', '204', '75', '18', '204', '108', '22', '133', '191', '111', '204', '203', '133', '237', '166', '4', '62', '202', '166', '161', '165', '74', '109', '118', '200', '22', '206', '25', '209', '17', '155', '204', '64', '23', '30', '202', '149', '171', '135', '136', '184', '26', '18', '159', '195', '18', '166', '156', '98', '133', '38', '40', '18', '18', '166', '109', '109', '209', '22', '195', '163', '163', '108', '18', '15', '99', '135', '174', '26', '19', '166', '133', '259', '151', '235', '204', '228', '133', '30', '161', '111', '242', '84', '238', '98', '195', '78', '160', '28', '25', '264', '165', '41', '19', '55', '46', '202', '121', '15', '32', '98', '238', '0', '78', '31', '30', '203', '264', '203', '135', '104', '111', '78', '212', '111', '98', '126', '22', '2', '30', '237', '212', '265', '163', '106', '106', '235', '109', '223', '161', '202', '150', '170', '255', '112', '114', '170', '98', '96', '197', '36', '18', '228', '14', '255', '204', '102', '239', '0', '185', '30', '22', '23', '246', '199', '133', '191', '78', '195', '153', '112', '191', '200', '170', '171', '200', '78', '128', '25', '153', '237', '99', '94', '208', '107', '208', '208', '78', '204', '204', '27', '191', '272', '134', '158', '158', '174', '118', '64', '18', '258', '150', '189', '18', '153', '241', '204', '112', '106', '135', '71', '148', '7', '133', '140', '187', '23', '223', '19', '19', '2', '136', '2', '200', '161', '15', '98', '158', '148', '3', '250', '153', '235', '165', '220', '176', '110', '240', '151', '148', '133', '74', '193', '46', '27', '173', '94', '176', '105', '256', '161', '226', '18', '78', '191', '171', '243', '26', '0', '108', '167', '159', '239', '150', '112', '155', '79', '241', '202', '165', '254', '107', '202', '156', '166', '193', '239', '135', '15', '203', '187', '148', '25', '15', '22', '194', '239', '56', '191', '176', '262', '200', '110', '72', '99', '102', '135', '98', '113', '15', '15', '159', '262', '171', '151', '98', '204', '213', '255', '26', '243', '175', '156', '98', '107', '191', '165', '241', '26', '165', '171', '30', '160', '166', '266', '11', '19', '211', '93', '254', '30', '239', '115', '273', '205', '112', '237', '263', '74', '98', '79', '122', '94', '265', '155', '191', '16', '235', '229', '98', '191', '227', '17', '111', '267', '135', '202', '195', '10', '180', '196', '196', '133', '135', '133', '95', '111', '122', '258', '191', '3', '114', '107', '19', '264', '74', '56', '200', '200', '133', '161', '97', '15', '97', '197', '196', '253', '93', '202', '258', '106', '153', '26', '109', '40', '149', '8', '196', '15', '119', '155', '23', '237', '193', '264', '148', '151', '238', '200', '94', '75', '148', '175', '24', '195', '171', '30', '203', '200', '202', '16', '158', '223', '191', '22', '264', '135', '35', '57', '94', '191', '196', '202', '98', '18', '196', '3', '133', '78', '191', '208', '258', '18', '200', '29', '135', '109', '110', '2', '185', '98', '133', '19', '22', '204', '95', '238', '25', '133', '250', '135', '135', '3', '47', '158', '187', '2', '78', '115', '155', '3', '218', '166', '112', '266', '22', '195', '135', '72', '97', '197', '209', '127', '80', '2', '98', '14', '239', '15', '113', '165', '204', '78', '204', '191', '175', '200', '193', '18', '196', '133', '109', '237', '56', '196', '3', '163', '163', '163', '195', '15', '263', '18', '84', '18', '133', '107', '18', '102', '156', '238', '102', '26', '244', '82', '75', '23', '258', '238', '136', '3', '204', '184', '200', '196', '158', '106', '27', '238', '155', '152', '50', '50', '2', '235', '98', '203', '166', '109', '25', '18', '151', '133', '200', '26', '26', '26', '19', '107', '156', '23', '26', '3', '70', '225', '41', '26', '239', '26', '186', '26', '34', '272', '272', '104', '78', '260', '18', '204', '26', '78', '18', '210', '135', '203', '71', '207', '26', '133', '13', '25', '26', '115', '94', '264', '135', '244', '79', '30', '263', '22', '109', '205', '176', '106', '22', '17', '151', '106', '191', '160', '160', '165', '78', '177', '196', '260', '166', '35', '191', '260', '98', '18', '237', '18', '161', '18', '197', '133', '30', '35', '200', '170', '155', '245', '19', '200', '160', '96', '172', '165', '55', '137', '241', '133', '191', '112', '25', '126', '238', '18', '18', '108', '135', '161', '135', '73', '112', '107', '8', '112', '109', '195', '25', '30', '105', '161', '241', '110', '75', '238', '215', '238', '104', '25', '3', '155', '30', '16', '3', '175', '99', '3', '200', '23', '3', '199', '203', '191', '196', '196', '204', '98', '105', '133', '197', '135', '161', '39', '15', '30', '133', '55', '191', '203', '133', '203', '140', '200', '149', '30', '96', '135', '23', '26', '3', '18', '155', '158', '135', '135', '18', '250', '18', '22', '114', '18', '18', '166', '18', '30', '18', '239', '18', '161', '50', '50', '49', '51', '23', '55', '98', '78', '210', '211', '18', '239', '78', '13', '193', '114', '78', '18', '25', '133', '19', '176', '4', '173', '50', '108', '17', '4', '204', '94', '112', '207', '109', '115', '23', '235', '197', '194', '197', '213', '148', '40', '157', '107', '200', '147', '14', '14', '18', '238', '18', '238', '18', '238', '57', '200', '135', '30', '18', '191', '191', '200', '166', '151', '161', '73', '19', '73', '239', '239', '165', '108', '2', '93', '166', '133', '133', '173', '35', '193', '114', '74', '19', '40', '58', '104', '16', '260', '160', '158', '112', '210', '158', '167', '203', '203', '3', '3', '109', '30', '133', '258', '70', '204', '200', '152', '155', '192', '204', '108', '191', '204', '26', '98', '26', '98', '264', '158', '18', '150', '137', '153', '196', '199', '191', '115', '168', '157', '149', '151', '78', '137', '160', '259', '166', '196', '171', '175', '199', '166', '161', '70', '13', '166', '152', '152', '206', '58', '18', '207', '58', '74', '207', '29', '46', '109', '135', '74', '174', '109', '40', '10', '30', '258', '16', '111', '30', '15', '239', '204', '48', '25', '196', '0', '156', '72', '72', '96', '22', '72', '133', '133', '238', '26', '3', '50', '152', '193', '235', '136', '153', '50', '1', '19', '239', '30', '25', '179', '161', '30', '191', '3', '27', '133', '3', '40', '23', '133', '150', '151', '265', '32', '191', '3', '25', '170', '238', '40', '259', '166', '137', '241', '133', '209', '159', '78', '78', '153', '26', '52', '241', '36', '250', '96', '30', '30', '38', '18', '18', '238', '99', '72', '111', '3', '244', '191', '226', '3', '191', '114', '135', '109', '40', '79', '202', '78', '114', '238', '10', '150', '108', '163', '108', '113', '197', '109', '115', '161', '3', '238', '3', '237', '161', '107', '155', '78', '202', '112', '38', '195', '137', '250', '133', '237', '161', '25', '26', '25', '15', '15', '239', '166', '114', '229', '111', '40', '135', '94', '102', '162', '204', '237', '17', '166', '136', '22', '98', '99', '99', '166', '35', '196', '3', '203', '135', '135', '84', '98', '266', '238', '13', '166', '78', '193', '98', '241', '104', '153', '192', '200', '10', '95', '95', '148', '107', '14', '17', '191', '239', '19', '203', '255', '197', '213', '94', '3', '107', '23', '35', '10', '237', '235', '78', '29', '18', '160', '200', '150', '99', '109', '155', '161', '133', '114', '18', '18', '18', '264', '133', '104', '161', '70', '17', '204', '195', '204', '241', '99', '82', '241', '241', '161', '18', '235', '175', '2', '2', '202', '50', '230', '26', '40', '40', '200', '78', '156', '26', '112', '26', '30', '71', '184', '200', '135', '153', '23', '106', '125', '175', '133', '133', '7', '38', '196', '161', '0', '49', '241', '158', '15', '21', '104', '2', '3', '176', '259', '135', '78', '75', '112', '200', '1', '207', '238', '161', '150', '158', '2', '45', '31', '237', '149', '3', '26', '165', '135', '152', '115', '133', '18', '40', '166', '30', '173', '40', '26', '78', '50', '48', '50', '13', '45', '200', '200', '18', '151', '205', '135', '38', '117', '196', '135', '155', '111', '196', '196', '166', '23', '25', '161', '18', '124', '259', '165', '58', '170', '204', '250', '103', '160', '238', '194', '163', '241', '255', '235', '17', '133', '26', '135', '196', '237', '114', '258', '193', '248', '40', '195', '84', '238', '146', '199', '40', '161', '22', '137', '43', '166', '156', '4', '104', '133', '177', '211', '193', '78', '203', '246', '237', '264', '3', '149', '78', '78', '111', '239', '13', '238', '176', '112', '150', '156', '3', '6', '118', '178', '178', '21', '186', '150', '241', '50', '243', '191', '121', '278', '29', '200', '18', '3', '197', '191', '98', '173', '195', '25', '195', '264', '221', '15', '156', '25', '200', '211', '195', '26', '35', '2', '135', '126', '113', '167', '191', '264', '196', '133', '18', '26', '146', '203', '219', '23', '160', '160', '135', '259', '3', '159', '120', '160', '106', '240', '133', '115', '258', '38', '22', '3', '26', '22', '107', '49', '48', '25', '237', '72', '219', '196', '196', '111', '135', '196', '18', '166', '172', '135', '26', '258', '70', '98', '40', '166', '23', '96', '3', '30', '155', '126', '146', '114', '242', '237', '157', '238', '241', '40', '133', '200', '161', '175', '200', '175', '84', '266', '18', '99', '30', '242', '151', '153', '166', '199', '228', '3', '237', '132', '110', '135', '80', '15', '15', '15', '84', '133', '15', '177', '3', '202', '255', '71', '115', '235', '19', '18', '18', '102', '58', '78', '165', '166', '264', '12', '150', '15', '2', '18', '135', '109', '200', '258', '128', '3', '199', '197', '239', '109', '222', '239', '107', '125', '132', '204', '204', '133', '135', '30', '158', '203', '15', '264', '237', '133', '196', '3', '252', '26', '98', '79', '199', '210', '0', '99', '30', '26', '259', '6', '49', '166', '98', '108', '9', '133', '102', '155', '98', '114', '78', '35', '166', '188', '202', '235', '30', '149', '165', '176', '138', '29', '153', '25', '25', '137', '133', '166', '135', '125', '71', '193', '154', '165', '98', '23', '56', '198', '26', '26', '0', '109', '136', '163', '151', '25', '15', '241', '195', '204', '191', '195', '161', '175', '133', '237', '78', '165', '258', '135', '78', '135', '258', '18', '235', '30', '187', '150', '275', '149', '2', '36', '112', '22', '161', '238', '203', '112', '265', '75', '133', '205', '258', '158', '18', '26', '18', '25', '26', '15', '26', '239', '15', '240', '18', '71', '71', '78', '135', '3', '148', '166', '26', '176', '2', '197', '150', '176', '19', '18', '97', '166', '18', '153', '250', '133', '210', '20', '192', '196', '26', '191', '2', '81', '70', '5', '79', '3', '30', '193', '193', '69', '160', '176', '246', '108', '54', '250', '135', '200', '80', '3', '167', '18', '166', '200', '152', '23', '149', '19', '14', '13', '237', '196', '19', '112', '254', '204', '191', '23', '264', '199', '209', '137', '31', '135', '238', '78', '18', '191', '15', '164', '195', '74', '163', '163', '41', '99', '26', '135', '26', '40', '40', '250', '200', '200', '26', '237', '4', '195', '158', '35', '71', '191', '3', '218', '274', '58', '3', '23', '177', '132', '49', '62', '238', '166', '97', '264', '193', '29', '26', '49', '241', '197', '200', '64', '227', '258', '44', '15', '133', '175', '238', '109', '254', '237', '198', '175', '166', '213', '174', '196', '107', '259', '98', '10', '196', '210', '191', '96', '166', '135', '15', '197', '78', '78', '195', '176', '18', '98', '129', '170', '18', '247', '203', '26', '158', '40', '64', '200', '163', '196', '19', '205', '197', '193', '191', '69', '96', '74', '35', '36', '64', '133', '172', '19', '266', '96', '94', '103', '258', '193', '264', '98', '254', '50', '193', '25', '170', '75', '84', '27', '252', '203', '94', '94', '19', '98', '137', '12', '153', '3', '155', '135', '238', '15', '196', '15', '26', '84', '49', '27', '135', '203', '48', '133', '129', '133', '258', '155', '35', '195', '71', '241', '135', '166', '193', '165', '104', '196', '117', '196', '26', '237', '109', '95', '196', '60', '40', '158', '193', '250', '104', '94', '178', '27', '193', '78', '106', '79', '204', '133', '237', '235', '135', '202', '70', '15', '264', '241', '254', '110', '199', '49', '255', '30', '193', '238', '204', '102', '97', '161', '43', '228', '1', '6', '196', '193', '26', '15', '128', '46', '155', '32', '133', '69', '47', '135', '78', '205', '200', '200', '202', '3', '95', '191', '78', '26', '248', '258', '191', '240', '49', '110', '38', '193', '200', '200', '96', '200', '204', '135', '9', '200', '258', '15', '46', '15', '149', '155', '197', '2', '166', '151', '72', '135', '97', '211', '238', '135', '163', '161', '26', '163', '129', '241', '239', '241', '16', '26', '18', '263', '260', '166', '3', '264', '195', '79', '132', '251', '50', '133', '15', '25', '15', '191', '15', '78', '50', '78', '71', '227', '69', '200', '69', '109', '64', '133', '197', '196', '3', '45', '238', '26', '132', '104', '196', '193', '136', '80', '26', '75', '117', '14', '78', '75', '98', '15', '2', '15', '50', '255', '136', '3', '112', '173', '69', '191', '19', '239', '235', '109', '70', '135', '78', '165', '238', '262', '199', '35', '203', '224', '193', '15', '18', '150', '238', '197', '238', '193', '135', '3', '150', '187', '199', '239', '109', '152', '27', '257', '6', '235', '104', '191', '133', '166', '155', '26', '16', '136', '198', '98', '154', '235', '27', '102', '30', '191', '15', '132', '132', '114', '107', '241', '241', '156', '199', '241', '49', '62', '117', '258', '30', '163', '133', '238', '200', '3', '15', '109', '2', '196', '165', '166', '159', '210', '35', '15', '17', '200', '241', '106', '203', '197', '197', '79', '98', '98', '23', '204', '48', '129', '57', '258', '202', '215', '238', '70', '64', '193', '189', '57', '200', '0', '98', '135', '204', '25', '30', '204', '27', '26', '78', '135', '109', '3', '197', '3', '167', '35', '196', '225', '2', '194', '200', '15', '161', '198', '23', '133', '238', '195', '35', '50', '135', '152', '163', '21', '15', '94', '26', '94', '53', '260', '196', '154', '277', '200', '118', '6', '238', '50', '209', '50', '3', '238', '3', '15', '107', '135', '199', '15', '135', '26', '258', '251', '203', '19', '259', '195', '199', '238', '15', '254', '112', '211', '200', '129', '84', '196', '3', '208', '259', '135', '104', '29', '136', '31', '78', '13', '3', '64', '75', '55', '114', '160', '96', '14', '18', '207', '75', '48', '15', '49', '97', '117', '196', '99', '258', '195', '264', '264', '78', '197', '30', '238', '109', '158', '32', '195', '149', '227', '65', '167', '162', '204', '250', '151', '151', '102', '26', '29', '26', '193', '74', '50', '193', '186', '15', '74', '259', '57', '188', '277', '277', '195', '195', '212', '78', '197', '64', '129', '259', '200', '199', '129', '4', '235', '197', '196', '161', '3', '255', '199', '102', '3', '13', '94', '53', '98', '15', '129', '15', '155', '71', '179', '50', '29', '136', '80', '254', '198', '162', '237', '3', '18', '260', '238', '117', '179', '156', '19', '45', '197', '78', '199', '102', '102', '237', '155', '69', '69', '163', '239', '15', '196', '80', '191', '30', '71', '158', '259', '2', '106', '148', '15', '135', '135', '135', '196', '19', '123', '15', '104', '19', '128', '230', '69', '98', '197', '241', '161', '195', '200', '3', '98', '78', '165', '163', '19', '196', '31', '173', '203', '55', '69', '209', '239', '79', '158', '254', '135', '51', '94', '162', '255', '95', '18', '133', '109', '50', '94', '199', '15', '197', '200', '133', '116', '50', '196', '161', '69', '133', '3', '258', '3', '197', '29', '3', '157', '197', '193', '4', '94', '195', '17', '48', '152', '8', '15', '155', '133', '195', '136', '150', '102', '237', '203', '164', '164', '250', '154', '13', '191', '58', '94', '70', '93', '264', '15', '30', '133', '237', '267', '199', '193', '76', '241', '157', '26', '200', '109', '179', '55', '98', '200', '129', '238', '235', '208', '109', '239', '258', '196', '193', '18', '107', '210', '70', '40', '129', '259', '193', '30', '158', '252', '40', '210', '78', '15', '78', '96', '208', '113', '258', '64', '80', '15', '133', '241', '15', '15', '258', '135', '109', '251', '251', '109', '74', '196', '170', '240', '254', '258', '204', '199', '26', '109', '197', '57', '45', '161', '57', '114', '3', '15', '264', '106', '57', '27', '18', '258', '29', '84', '161', '135', '114', '24', '30', '107', '107', '172', '209', '170', '18', '19', '133', '191', '195', '135', '156', '196', '94', '3', '84', '225', '199', '238', '106', '104', '70', '50', '12', '155', '114', '176', '237', '200', '108', '70', '240', '196', '19', '22', '276', '199', '238', '205', '104', '193', '108', '115', '259', '102', '19', '258', '18', '106', '161', '191', '114', '64', '15', '204', '35', '155', '40', '238', '136', '264', '133', '186', '15', '258', '166', '26', '6', '114', '114', '133', '114', '114', '13', '13', '13', '237', '250', '245', '136', '78', '13', '48', '18', '13', '25', '26', '93', '125', '50', '94', '148', '78', '194', '200', '25', '93', '274', '266', '70', '22', '3', '208', '240', '211', '135', '31', '18', '153', '18', '2', '15', '15', '15', '156', '15', '70', '73', '173', '13', '112', '69', '204', '70', '248', '135', '70', '3', '193', '193', '22', '162', '238', '57', '98', '15', '199', '199', '52', '151', '73', '5', '108', '23', '69', '95', '220', '106', '163', '152', '163', '108', '15', '58', '250', '19', '177', '245', '264', '93', '204', '25', '78', '155', '69', '155', '239', '241', '155', '200', '18', '3', '13', '196', '197', '204', '272', '95', '135', '165', '22', '210', '35', '69', '18', '170', '3', '25', '156', '78', '135', '102', '107', '255', '19', '70', '70', '109', '198', '156', '64', '170', '138', '78', '203', '166', '108', '114', '179', '228', '74', '49', '14', '264', '26', '195', '78', '107', '75', '18', '106', '151', '70', '58', '15', '196', '238', '109', '104', '104', '109', '136', '148', '210', '3', '161', '15', '104', '238', '208', '133', '108', '241', '18', '209', '74', '156', '114', '199', '174', '35', '18', '170', '135', '238', '244', '155', '162', '197', '3', '98', '227', '267', '238', '193', '50', '238', '98', '112', '124', '25', '114', '241', '159', '154', '78', '161', '136', '114', '153', '225', '154', '13', '50', '11', '25', '15', '209', '11', '166', '203', '259', '237', '22', '114', '276', '98', '278', '150', '198', '18', '15', '78', '15', '162', '26', '3', '196', '3', '102', '189', '71', '69', '16', '18', '152', '133', '135', '162', '241', '15', '104', '15', '259', '202', '2', '238', '207', '15', '204', '30', '215', '112', '210', '42', '96', '193', '15', '110', '163', '211', '78', '108', '15', '31', '27', '15', '31', '196', '195', '96', '15', '161', '94', '193', '195', '198', '98', '261', '258', '76', '15', '104', '258', '135', '18', '15', '202', '258', '108', '156', '13', '22', '251', '199', '155', '258', '211', '3', '207', '202', '259', '107', '202', '259', '135', '135', '148', '158', '154', '99', '99', '3', '207', '94', '25', '162', '31', '241', '25', '15', '114', '18', '114', '148', '19', '15', '203', '191', '19', '13', '193', '204', '25', '166', '155', '196', '98', '162', '197', '29', '152', '191', '133', '199', '230', '40', '25', '204', '79', '3', '19', '165', '204', '108', '208', '3', '251', '274', '15', '166', '264', '35', '13', '193', '114', '258', '13', '19', '133', '23', '17', '199', '204', '153', '250', '204', '16', '12', '107', '191', '18', '3', '15', '165', '15', '69', '17', '165', '17', '15', '15', '156', '23', '23', '15', '197', '104', '197', '26', '208', '135', '211', '250', '18', '204', '25', '196', '53', '69', '104', '104', '19', '78', '241', '230', '30', '195', '96', '26', '109', '196', '69', '238', '15', '18', '133', '26', '106', '109', '98', '15', '258', '274', '193', '203', '58', '57', '274', '2', '202', '161', '136', '135', '155', '241', '35', '151', '137', '5', '114', '96', '16', '209', '158', '15', '266', '161', '15', '25', '274', '202', '3', '161', '237', '267', '204', '125', '15', '151', '155', '259', '151', '210', '238', '15', '26', '132', '50', '173', '197', '114', '241', '15', '258', '109', '166', '274', '13', '191', '15', '26', '164', '258', '163', '15', '264', '11', '260', '148', '150', '207', '196', '98', '272', '133', '196', '107', '26', '191', '163', '191', '135', '3', '18', '18', '119', '69', '196', '15', '156', '135', '112', '258', '155', '256', '199', '204', '212', '18', '135', '254', '72', '258', '254', '15', '133', '18', '259', '193', '258', '204', '40', '18', '29', '155', '104', '107', '274', '241', '25', '26', '69', '148', '166', '46', '94', '135', '15', '155', '104', '241', '32', '274', '249', '15', '197', '104', '44', '259', '69', '191', '15', '198', '193', '133', '17', '258', '248', '166', '197', '200', '250', '204', '3', '225', '238', '163', '69', '15', '15', '26', '202', '19', '209', '78', '212', '15', '78', '243', '110', '191', '15', '18', '107', '38', '208', '166', '239', '237', '199', '15', '204', '258', '173', '172', '200', '274', '243', '19', '191', '135', '203', '69', '149', '148', '196', '256', '204', '97', '191', '26', '114', '274', '200', '69', '69', '64', '69', '239', '25', '274', '203', '78', '135', '196', '202', '17', '2', '2', '3', '98', '98', '62', '2', '95', '96', '2', '207', '78', '200', '26', '64', '18', '93', '49', '57', '154', '200', '69', '94', '98', '98', '56', '56', '166', '98', '212', '226', '226', '15', '69', '15', '273', '272', '104', '193', '3', '98', '199', '98', '25', '125', '259', '249', '108', '40', '26', '198', '218', '73', '148', '69', '204', '241', '35', '3', '274', '70', '18', '71', '35', '172', '133', '106', '69', '107', '199', '219', '19', '133', '69', '135', '109', '57', '191', '135', '252', '104', '3', '94', '3', '72', '126', '16', '36', '84', '241', '3', '218', '98', '108', '46', '19', '196', '118', '104', '29', '238', '69', '10', '204', '264', '259', '124', '242', '107', '133', '25', '64', '30', '194', '259', '159', '264', '274', '52', '200', '104', '29', '30', '193', '98', '94', '114', '204', '40', '18', '198', '97', '3', '99', '274', '239', '177', '15', '18', '126', '126', '208', '148', '15', '104', '29', '36', '218', '70', '15', '23', '278', '23', '36', '191', '210', '51', '245', '167', '72', '237', '25', '203', '36', '196', '193', '196', '15', '151', '207', '203', '103', '166', '135', '110', '27', '13', '27', '191', '98', '57', '166', '50', '21', '15', '197', '23', '2', '70', '69', '193', '274', '104', '166', '258', '126', '238', '161', '18', '193', '69', '140', '170', '197', '204', '165', '203', '170', '15', '58', '35', '83', '212', '150', '73', '25', '15', '64', '133', '210', '108', '79', '193', '15', '126', '165', '15', '208', '98', '3', '200', '126', '3', '40', '165', '98', '69', '15', '165', '133', '96', '173', '203', '133', '238', '248', '15', '7', '38', '207', '108', '165', '199', '18', '162', '27', '3', '72', '65', '250', '209', '166', '64', '209', '237', '212', '64', '225', '120', '108', '203', '135', '248', '69', '251', '135', '204', '160', '160', '135', '148', '15', '15', '23', '207', '274', '29', '28', '194', '150', '40', '104', '208', '125', '70', '18', '3', '19', '102', '104', '23', '29', '207', '133', '204', '111', '2', '125', '126', '7', '194', '209', '203', '83', '126', '126', '258', '119', '162', '241', '133', '18', '208', '197', '209', '15', '78', '80', '110', '237', '104', '30', '208', '103', '15', '200', '15', '238', '254', '108', '173', '40', '69', '175', '17', '155', '136', '26', '3', '135', '13', '241', '161', '166', '170', '162', '133', '53', '211', '204', '15', '161', '238', '110', '18', '191', '18', '50', '71', '30', '17', '208', '248', '203', '203', '196', '210', '258', '204', '3', '18', '196', '125', '173', '199', '249', '18', '104', '165', '241', '2', '193', '238', '78', '15', '166', '15', '126', '3', '237', '161', '52', '207', '3', '3', '50', '56', '22', '56', '69', '93', '135', '207', '69', '99', '3', '133', '68', '133', '161', '108', '126', '250', '60', '155', '98', '18', '16', '78', '78', '240', '235', '176', '22', '199', '160', '235', '274', '209', '64', '173', '137', '128', '25', '18', '160', '203', '70', '165', '193', '110', '207', '64', '85', '252', '78', '51', '2', '99', '19', '22', '198', '204', '166', '166', '151', '56', '133', '210', '26', '149', '109', '30', '209', '209', '51', '3', '193', '119', '208', '93', '165', '248', '55', '203', '204', '58', '18', '199', '211', '83', '32', '135', '212', '156', '19', '235', '173', '191', '209', '203', '146', '208', '58', '158', '102', '199', '200', '200', '173', '70', '52', '107', '211', '213', '237', '196', '205', '104', '135', '258', '255', '96', '19', '124', '128', '57', '199', '98', '166', '102', '40', '15', '238', '3', '148', '225', '126', '135', '114', '193', '204', '15', '209', '209', '193', '259', '64', '161', '26', '274', '126', '18', '196', '0', '60', '207', '17', '71', '218', '239', '248', '38', '154', '61', '128', '199', '196', '173', '25', '46', '98', '258', '108', '3', '108', '196', '93', '93', '99', '172', '69', '241', '196', '30', '135', '278', '109', '18', '5', '209', '57', '159', '70', '193', '163', '249', '196', '196', '72', '103', '133', '17', '204', '199', '166', '202', '29', '57', '193', '150', '98', '119', '25', '205', '18', '173', '166', '209', '57', '25', '58', '258', '26', '15', '15', '101', '26', '74', '1', '4', '74', '263']\n",
            "     filenum                                               text  label\n",
            "0          1  ent waves which Lehr and Wyatt recorded on the...    209\n",
            "1          2  many times by the writers of the Old Testament...     63\n",
            "2          4  sued, recovery may be had for an involuntary, ...    108\n",
            "3          6  of the manufacturer.' The same result was reac...    200\n",
            "4          7  in the interest of either the consumer or the ...    203\n",
            "...      ...                                                ...    ...\n",
            "4499    8411  Dixon, supra, at P. at And the California Supr...     15\n",
            "4500    8412  improper venue because it was filed in the wro...     15\n",
            "4501    8414  hearing and the arguments of counsel, you find...     26\n",
            "4502    8417  re-examination, they should be reaffirmed in a...      4\n",
            "4503    8406  (emphasis added). And those listed locations a...     57\n",
            "\n",
            "[4504 rows x 3 columns]\n"
          ]
        }
      ],
      "source": [
        "fh = open('sc_model_5.txt')\n",
        "filenums=[]\n",
        "records=[]\n",
        "for record in fh:\n",
        "    record=record.split(\"---\")\n",
        "    filenum=record[0]\n",
        "    modelnum=record[1]\n",
        "    text=record[2]\n",
        "    filenums.append(filenum)\n",
        "    records.append(text)\n",
        "    \n",
        "print(len(filenums))\n",
        "print(len(records))\n",
        "\n",
        "summarized_data = pd.DataFrame(list(zip(filenums, records)),columns =['filenum','text'])\n",
        "len_list = [len(ele.split()) for ele in records]\n",
        "res = 0 if len(len_list) == 0 else (float(sum(len_list)) / len(len_list))\n",
        "print(\"Average Length %s\" % res)\n",
        "\n",
        "#temp_file = open(\"labels_sc.txt\", \"r\")\n",
        "temp_file = open(\"labels_sc_279.txt\", \"r\")\n",
        "\n",
        "data = temp_file.read()\n",
        "\n",
        "label_list = data.split(\"\\n\")\n",
        "print(label_list)\n",
        "label_list = label_list[0:-1]\n",
        "print(label_list)\n",
        "label_list = [int(i) for i in label_list]\n",
        "temp_file.close()\n",
        "\n",
        "\n",
        "new_label_list=[]\n",
        "for num in filenums:\n",
        "  new_label_list.append(label_list[int(num)-1])\n",
        "summarized_data['label'] = new_label_list\n",
        "\n",
        "print(summarized_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FL0rlksFwxTx",
        "outputId": "8249f561-dc44-402b-eae9-1dc0288649d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_1[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_roberta_model[0][0]']       \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 279)          143127      ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[209  63 108 ...  26   4  57]\n",
            "[   1    2    4 ... 8414 8417 8406]\n",
            "[5596 6574 6333 3587 5524 7527 3634 3578 8103 3832 5607 7486 5997 7826\n",
            " 4157 5688 5253 7691 1418 5606 4518 7620 5550 7405 4020 7918 4878 7248\n",
            " 5565 8257 7965 6207 5878 2772 6330 3909 7196 6719 8393 8172 1609 7985\n",
            " 6052 2269 4125 5143 6021 6606 4658 4532 7594 7556 3970 7604 6879 3712\n",
            " 7567 7506 7513 5690 5015 4530 6381 6005 7505   55 2964 7020 7311 2205\n",
            " 7162 6518 5770 4156 5782 5009 6354 3443 4771 4592 7296 6835 7576 7074\n",
            " 6340 2419 6864 4549 5243 4586 7874 8050 6616 7589 1625  959 2805 3811\n",
            " 4398 6222 5615 5620 6508 4282 7179 4610 5344 5537 6001 4487 7244 3514\n",
            " 5452 5125 8398 6699 7590 4392 8361 4664 8309 6818 4499 4256 1404 4369\n",
            " 8031 3588 5641 8412  407 6779 6506 4550 1672  110 8038 6712 5025  298\n",
            " 7427 7888   62 2405  308 6004 6086 2123 2862 6973 1516 4742 3885  265\n",
            " 6050 3702  848 5436  644 3964 6878 4186 5078 5490 6948 7342 8390  267\n",
            " 6200 5466 5182 7571 3680   40 4885 6320 4254 7396 3775 4958 1419 8081\n",
            " 5564 2688  926 3022 5017 7461 4219 6394 6556 6840 6632 8258 7200 4597\n",
            " 4480 6947 7481  849 4496 4574 5240 2121 8294 7375 3065 2370 8288 8267\n",
            " 3802  572 8066  745 7199 5071 5902 6806 7832  839  205 6122 6622 7109\n",
            " 5831 7256 6668 6965 4778 6776 7346 4346   73 2537 8086 5588  230 6782\n",
            " 1535 5415  348 7688 8250 8319 1635 3248 6526 6742 1852 8224 5931 5870\n",
            "  242 6870 3691 5491 7212 6763 4635 7634 4428 5544 3465 3321  283 6090\n",
            "  343 7935 5755  100 5594  886 3758  799 7498 5592 7670 6865 3652 5918\n",
            " 7883 6192 3617  271 5498 6315 5269 8329 6389 5895 7120 6188 8182 1686\n",
            " 2885 2069 6197 8301 3915 1887 1102 5505 5580 4431   49 3781 3892 3043\n",
            " 7719 5972 7261 7352 7909 5802 5462 6551 6488  812 8076 4769  147 3345\n",
            " 3310 6301 6185 7173 3158 7011 4649 7338 2307 7894 5129 8237  130 5354\n",
            " 7448 2832  931 4414 2504 6309 6659 5428 1478 7554 4290  467 7409 7989\n",
            " 3579 6162 5358 6545 4263  376 4825 7816 5523 4091 3803 2351 6654 7548\n",
            " 4270 5699 7129  208  276 5908 7866 4582 5631 2974 3609 4741 7306 2417\n",
            " 7509 3311 1745 4338 3558 2884 7633 6929 1581 8189 7471 4686 7208  297\n",
            " 5702 5340 4274 2878 7334 7773  425 1947 7603  573 7844 3396 5013 5339\n",
            " 2988 8388 8075 1237 3168 6137 5819 8017 5903 1278 2637 4332 1707 4319\n",
            " 4394 7999 6263 5312 1194 4696 3517 3805 6647 7300 5470 5300 8332 8098\n",
            " 4026 5197  365 3501 2472 7682 6223 4303 5193 7021 3503 4507 7007 5909\n",
            " 8029 5645  146]\n"
          ]
        }
      ],
      "source": [
        "gc.collect()\n",
        "tf.keras.backend.clear_session()\n",
        "dbert_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "dbert_model = TFRobertaModel.from_pretrained('roberta-base')\n",
        "max_len=512\n",
        "sentences=summarized_data['text']\n",
        "labels=summarized_data['label']\n",
        "filenumbers=summarized_data['filenum']\n",
        "len(sentences),len(labels),len(filenumbers)\n",
        "\n",
        "model_5=create_model()\n",
        "input_ids=[]\n",
        "attention_masks=[]\n",
        "\n",
        "for sent in sentences:\n",
        "  dbert_inps=dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n",
        "  input_ids.append(dbert_inps['input_ids'])\n",
        "  attention_masks.append(dbert_inps['attention_mask'])\n",
        "input_ids=np.asarray(input_ids)\n",
        "\n",
        "attention_masks=np.array(attention_masks)\n",
        "labels=np.array(labels)\n",
        "filenumbers=[int(i) for i in filenumbers]\n",
        "filenumbers=np.array(filenumbers)\n",
        "print(labels)\n",
        "print(filenumbers)\n",
        "\n",
        "train_inp,val_inp,train_label,val_label,train_mask,val_mask,train_fnum,val_fnum=train_test_split(input_ids,labels,attention_masks,filenumbers,test_size=0.1,random_state=42)\n",
        "\n",
        "new_train_inp_5=[]\n",
        "new_val_inp_5=[]\n",
        "new_train_label_5=[]\n",
        "new_val_label_5=[]\n",
        "new_train_mask_5=[]\n",
        "new_train_fnum_5=[]\n",
        "new_val_fnum_5=[]\n",
        "new_val_mask_5=[]\n",
        "for i in range(len(train_fnum)):\n",
        "  if train_fnum[i] in filenumbers:\n",
        "    new_train_inp_5.append(train_inp[i])\n",
        "    #new_val_inp.append(val_inp[i])\n",
        "    new_train_label_5.append(train_label[i])\n",
        "    #new_val_label.append(val_label[i])\n",
        "    new_train_mask_5.append(train_mask[i])\n",
        "    #new_val_mask.append(val_mask[i])\n",
        "    new_train_fnum_5.append(train_fnum[i])\n",
        "\n",
        "for i in range(len(val_fnum)):\n",
        "  if val_fnum[i] in filenumbers:\n",
        "    #new_train_inp.append(train_inp[i])\n",
        "    new_val_inp_5.append(val_inp[i])\n",
        "    #new_train_label.append(train_label[i])\n",
        "    new_val_label_5.append(val_label[i])\n",
        "    #new_train_mask.append(train_mask[i])\n",
        "    new_val_mask_5.append(val_mask[i])\n",
        "    new_val_fnum_5.append(val_fnum[i])\n",
        "\n",
        "# print(new_train_label)\n",
        "# print(len(new_train_label))\n",
        "# print(new_val_label)\n",
        "# print(len(new_val_label))\n",
        "new_train_inp_5=np.array(new_train_inp_5)\n",
        "new_val_inp_5=np.array(new_val_inp_5)\n",
        "new_train_label_5=np.array(new_train_label_5)\n",
        "new_val_label_5=np.array(new_val_label_5)\n",
        "new_train_mask_5=np.array(new_train_mask_5)\n",
        "new_train_fnum_5=np.array(new_train_fnum_5)\n",
        "new_val_fnum_5=np.array(new_val_fnum_5)\n",
        "new_val_mask_5=np.array(new_val_mask_5)\n",
        "\n",
        "print(new_val_fnum_5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "C9x3SEyjQiEC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9569ca5-032f-4a7b-b97c-0fdbfcfc66f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_1[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_roberta_model[0][0]']       \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 279)          143127      ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[209  63 108 ...  26   4  57]\n",
            "[   1    2    4 ... 8414 8417 8406]\n",
            "[6622   35 6713 4989 8233 7444 5061 8344 5464  317 4515 3001 6991 2875\n",
            " 3885 6647 4857 4504  214 8340 2969 3847 4609 7358 2752 7651 8188 7073\n",
            " 6483 4952 5072 1967 4071 7590 5030 7297  626 6029 6302 7256 3993 4263\n",
            " 3835 4293 8129 4837 3843 1418 5274 6205 5997 6685 3310 7721 2360 7197\n",
            " 4252 8343  909 4354  649 3488 2496 5765 4004 4052 1403 6697 6293 6380\n",
            " 4602 6057 4236 7978 5083 6552  361 7706 4186 8351  812 6600 8264 8064\n",
            " 7121 8279 5556 5119 7947 3350 7152 4282 5025 5051 3615 8062 1489 7351\n",
            "  234 7416 7921 6574 6257 8396 4520 3300 5640  240 7810 1321 6580 5404\n",
            " 8273 6742 7861 7975 1018 2140 7061 7767 4990 8124 5927 6051 6916 2846\n",
            " 6818 7052 7775 7582 8116 4767 5248 6055 8331 1564 7326 3065 6764 6854\n",
            " 4915 3583 5902 4229 7565 3712 7224 4502 3827 6525 6318 2686 5369 8235\n",
            " 3351 8021  102  224 8294 7707 2985 5146 3709 6824 2281 6837 5436 2649\n",
            " 4886  966 5476 8387 6166 4387  350 6123 5236 5041  734 3412 6085  489\n",
            " 4982 5115 6743 7173 5724 7689 7787 8302 3150 2247 8128 6660 4610 2866\n",
            " 6905 7096  768 4411 6087 2096 4903 7546 6630 2928 4640 7125 1792 1700\n",
            " 7928 8095  737 5849 2359 6015 4361 6081 5014 5167 1023 8009  268 4777\n",
            " 5702 6129 7181 1860 7192 5467   67 6134 4270 4611 6746 7696 3466 3950\n",
            " 6125 8370 6142 6131 5548 1721 5067 4015 7092   25 1749 4878 8248 6836\n",
            " 5310 6960 2339 1650 6227 4104 5130 7089 6993 1289 4305 5585 7933 6284\n",
            " 7057 3807 8347 5280 2841 7693 8090 4171 5260  133 7451 3957 1719 5435\n",
            " 6216 4858 4417 8355 4492 2648 5278 7308  158 7543 2168 5913 7111 3414\n",
            " 7592   39 7157 5647 4555 3956 3617 4630 4384 6596 4539 6749 7202 2413\n",
            " 4749 5122 5929  252 5565  336 6008 4564 5580 7237 5958 6918 6487 2989\n",
            " 7448 3382  467 3784 4419 1117 7954 7122 3174 1534 4164 7526  498 5944\n",
            " 4572 2307 2929 7480 4412 4563 7692 1869 2472 6966 6549 1594 2276 7269\n",
            " 2545 2216 6587 6315 5297 5282 6399 3917 6342 5721 7909 8382 4383 7130\n",
            "  485 4936 7674 5372 7109 4605 5211 5462 8065   50 4080 3334 8167 8378\n",
            " 5595 1873 8207 7076 3719 4627  231 2405 6973 3246 5931 8208 4825 6203\n",
            " 3019 5822   63 5854 5787 4764 6701 3822 8318 8169 8218 7549  707 1076\n",
            " 2465 4636 7025 7127 4026 4961 7531  744 6684 4297 7939 7384  446 6675\n",
            " 7752 5279 8196 3020 4192 4253 2887 1898 3208 5973 7033 1431 5915 8312\n",
            " 5799 7105  511 6404 1561]\n",
            "Mon Jun  6 21:10:16 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    38W / 300W |   4723MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "509/509 [==============================] - 191s 347ms/step - loss: 14.4998 - accuracy: 0.0635 - val_loss: 13.1019 - val_accuracy: 0.2164\n",
            "Epoch 2/5\n",
            "509/509 [==============================] - 177s 347ms/step - loss: 12.5556 - accuracy: 0.2822 - val_loss: 11.8001 - val_accuracy: 0.3850\n",
            "Epoch 3/5\n",
            "509/509 [==============================] - 177s 348ms/step - loss: 11.3991 - accuracy: 0.4162 - val_loss: 11.1022 - val_accuracy: 0.4191\n",
            "Epoch 4/5\n",
            "509/509 [==============================] - 177s 348ms/step - loss: 10.6247 - accuracy: 0.4967 - val_loss: 10.7292 - val_accuracy: 0.4374\n",
            "Epoch 5/5\n",
            "509/509 [==============================] - 177s 348ms/step - loss: 9.9943 - accuracy: 0.5656 - val_loss: 10.3222 - val_accuracy: 0.4829\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_3[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_roberta_model[1][0]']       \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 279)          143127      ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.45724465558194777\n",
            "Weighted F1: 0.3902413356389285\n",
            "Micro F1: 0.45724465558194777\n",
            "Weighted Precision: 0.3672576709734887\n",
            "Micro Precision: 0.45724465558194777\n",
            "Weighted Recall: 0.45724465558194777\n",
            "Micro Recall: 0.45724465558194777\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_1[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_roberta_model[0][0]']       \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 279)          143127      ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[209  63 108 ...  26   4  57]\n",
            "[   1    2    4 ... 8414 8417 8406]\n",
            "[6622   35 6713 4989 8233 7444 5061 8344 5464  317 4515 3001 6991 2875\n",
            " 3885 6647 4857 4504  214 8340 2969 3847 4609 7358 2752 7651 8188 7073\n",
            " 6483 4952 5072 1967 4071 7590 5030 7297  626 6029 6302 7256 3993 4263\n",
            " 3835 4293 8129 4837 3843 1418 5274 6205 5997 6685 3310 7721 2360 7197\n",
            " 4252 8343  909 4354  649 3488 2496 5765 4004 4052 1403 6697 6293 6380\n",
            " 4602 6057 4236 7978 5083 6552  361 7706 4186 8351  812 6600 8264 8064\n",
            " 7121 8279 5556 5119 7947 3350 7152 4282 5025 5051 3615 8062 1489 7351\n",
            "  234 7416 7921 6574 6257 8396 4520 3300 5640  240 7810 1321 6580 5404\n",
            " 8273 6742 7861 7975 1018 2140 7061 7767 4990 8124 5927 6051 6916 2846\n",
            " 6818 7052 7775 7582 8116 4767 5248 6055 8331 1564 7326 3065 6764 6854\n",
            " 4915 3583 5902 4229 7565 3712 7224 4502 3827 6525 6318 2686 5369 8235\n",
            " 3351 8021  102  224 8294 7707 2985 5146 3709 6824 2281 6837 5436 2649\n",
            " 4886  966 5476 8387 6166 4387  350 6123 5236 5041  734 3412 6085  489\n",
            " 4982 5115 6743 7173 5724 7689 7787 8302 3150 2247 8128 6660 4610 2866\n",
            " 6905 7096  768 4411 6087 2096 4903 7546 6630 2928 4640 7125 1792 1700\n",
            " 7928 8095  737 5849 2359 6015 4361 6081 5014 5167 1023 8009  268 4777\n",
            " 5702 6129 7181 1860 7192 5467   67 6134 4270 4611 6746 7696 3466 3950\n",
            " 6125 8370 6142 6131 5548 1721 5067 4015 7092   25 1749 4878 8248 6836\n",
            " 5310 6960 2339 1650 6227 4104 5130 7089 6993 1289 4305 5585 7933 6284\n",
            " 7057 3807 8347 5280 2841 7693 8090 4171 5260  133 7451 3957 1719 5435\n",
            " 6216 4858 4417 8355 4492 2648 5278 7308  158 7543 2168 5913 7111 3414\n",
            " 7592   39 7157 5647 4555 3956 3617 4630 4384 6596 4539 6749 7202 2413\n",
            " 4749 5122 5929  252 5565  336 6008 4564 5580 7237 5958 6918 6487 2989\n",
            " 7448 3382  467 3784 4419 1117 7954 7122 3174 1534 4164 7526  498 5944\n",
            " 4572 2307 2929 7480 4412 4563 7692 1869 2472 6966 6549 1594 2276 7269\n",
            " 2545 2216 6587 6315 5297 5282 6399 3917 6342 5721 7909 8382 4383 7130\n",
            "  485 4936 7674 5372 7109 4605 5211 5462 8065   50 4080 3334 8167 8378\n",
            " 5595 1873 8207 7076 3719 4627  231 2405 6973 3246 5931 8208 4825 6203\n",
            " 3019 5822   63 5854 5787 4764 6701 3822 8318 8169 8218 7549  707 1076\n",
            " 2465 4636 7025 7127 4026 4961 7531  744 6684 4297 7939 7384  446 6675\n",
            " 7752 5279 8196 3020 4192 4253 2887 1898 3208 5973 7033 1431 5915 8312\n",
            " 5799 7105  511 6404 1561]\n",
            "Mon Jun  6 21:26:51 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P0    38W / 300W |  15237MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "509/509 [==============================] - 192s 348ms/step - loss: 14.6475 - accuracy: 0.0347 - val_loss: 13.5625 - val_accuracy: 0.1276\n",
            "Epoch 2/5\n",
            "509/509 [==============================] - 177s 348ms/step - loss: 12.8459 - accuracy: 0.2263 - val_loss: 11.9262 - val_accuracy: 0.3508\n",
            "Epoch 3/5\n",
            "509/509 [==============================] - 177s 348ms/step - loss: 11.5217 - accuracy: 0.3931 - val_loss: 11.0951 - val_accuracy: 0.4328\n",
            "Epoch 4/5\n",
            "509/509 [==============================] - 177s 349ms/step - loss: 10.6777 - accuracy: 0.4807 - val_loss: 10.6202 - val_accuracy: 0.4556\n",
            "Epoch 5/5\n",
            "509/509 [==============================] - 177s 348ms/step - loss: 10.0367 - accuracy: 0.5387 - val_loss: 10.2148 - val_accuracy: 0.4897\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_3[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_roberta_model[1][0]']       \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 279)          143127      ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.47743467933491684\n",
            "Weighted F1: 0.40827144344514116\n",
            "Micro F1: 0.47743467933491684\n",
            "Weighted Precision: 0.3803192231603759\n",
            "Micro Precision: 0.47743467933491684\n",
            "Weighted Recall: 0.47743467933491684\n",
            "Micro Recall: 0.47743467933491684\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_1[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_roberta_model[0][0]']       \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 279)          143127      ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[209  63 108 ...  26   4  57]\n",
            "[   1    2    4 ... 8414 8417 8406]\n",
            "[6622   35 6713 4989 8233 7444 5061 8344 5464  317 4515 3001 6991 2875\n",
            " 3885 6647 4857 4504  214 8340 2969 3847 4609 7358 2752 7651 8188 7073\n",
            " 6483 4952 5072 1967 4071 7590 5030 7297  626 6029 6302 7256 3993 4263\n",
            " 3835 4293 8129 4837 3843 1418 5274 6205 5997 6685 3310 7721 2360 7197\n",
            " 4252 8343  909 4354  649 3488 2496 5765 4004 4052 1403 6697 6293 6380\n",
            " 4602 6057 4236 7978 5083 6552  361 7706 4186 8351  812 6600 8264 8064\n",
            " 7121 8279 5556 5119 7947 3350 7152 4282 5025 5051 3615 8062 1489 7351\n",
            "  234 7416 7921 6574 6257 8396 4520 3300 5640  240 7810 1321 6580 5404\n",
            " 8273 6742 7861 7975 1018 2140 7061 7767 4990 8124 5927 6051 6916 2846\n",
            " 6818 7052 7775 7582 8116 4767 5248 6055 8331 1564 7326 3065 6764 6854\n",
            " 4915 3583 5902 4229 7565 3712 7224 4502 3827 6525 6318 2686 5369 8235\n",
            " 3351 8021  102  224 8294 7707 2985 5146 3709 6824 2281 6837 5436 2649\n",
            " 4886  966 5476 8387 6166 4387  350 6123 5236 5041  734 3412 6085  489\n",
            " 4982 5115 6743 7173 5724 7689 7787 8302 3150 2247 8128 6660 4610 2866\n",
            " 6905 7096  768 4411 6087 2096 4903 7546 6630 2928 4640 7125 1792 1700\n",
            " 7928 8095  737 5849 2359 6015 4361 6081 5014 5167 1023 8009  268 4777\n",
            " 5702 6129 7181 1860 7192 5467   67 6134 4270 4611 6746 7696 3466 3950\n",
            " 6125 8370 6142 6131 5548 1721 5067 4015 7092   25 1749 4878 8248 6836\n",
            " 5310 6960 2339 1650 6227 4104 5130 7089 6993 1289 4305 5585 7933 6284\n",
            " 7057 3807 8347 5280 2841 7693 8090 4171 5260  133 7451 3957 1719 5435\n",
            " 6216 4858 4417 8355 4492 2648 5278 7308  158 7543 2168 5913 7111 3414\n",
            " 7592   39 7157 5647 4555 3956 3617 4630 4384 6596 4539 6749 7202 2413\n",
            " 4749 5122 5929  252 5565  336 6008 4564 5580 7237 5958 6918 6487 2989\n",
            " 7448 3382  467 3784 4419 1117 7954 7122 3174 1534 4164 7526  498 5944\n",
            " 4572 2307 2929 7480 4412 4563 7692 1869 2472 6966 6549 1594 2276 7269\n",
            " 2545 2216 6587 6315 5297 5282 6399 3917 6342 5721 7909 8382 4383 7130\n",
            "  485 4936 7674 5372 7109 4605 5211 5462 8065   50 4080 3334 8167 8378\n",
            " 5595 1873 8207 7076 3719 4627  231 2405 6973 3246 5931 8208 4825 6203\n",
            " 3019 5822   63 5854 5787 4764 6701 3822 8318 8169 8218 7549  707 1076\n",
            " 2465 4636 7025 7127 4026 4961 7531  744 6684 4297 7939 7384  446 6675\n",
            " 7752 5279 8196 3020 4192 4253 2887 1898 3208 5973 7033 1431 5915 8312\n",
            " 5799 7105  511 6404 1561]\n",
            "Mon Jun  6 21:43:27 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P0    38W / 300W |  15237MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "509/509 [==============================] - 192s 348ms/step - loss: 14.6308 - accuracy: 0.0352 - val_loss: 13.4678 - val_accuracy: 0.1640\n",
            "Epoch 2/5\n",
            "509/509 [==============================] - 177s 348ms/step - loss: 12.7583 - accuracy: 0.2394 - val_loss: 11.7964 - val_accuracy: 0.3531\n",
            "Epoch 3/5\n",
            "509/509 [==============================] - 177s 348ms/step - loss: 11.4332 - accuracy: 0.3931 - val_loss: 11.0536 - val_accuracy: 0.4191\n",
            "Epoch 4/5\n",
            "509/509 [==============================] - 177s 348ms/step - loss: 10.6208 - accuracy: 0.4733 - val_loss: 10.5413 - val_accuracy: 0.4624\n",
            "Epoch 5/5\n",
            "509/509 [==============================] - 177s 348ms/step - loss: 9.9636 - accuracy: 0.5380 - val_loss: 10.1453 - val_accuracy: 0.4875\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_3[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_roberta_model[1][0]']       \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 279)          143127      ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.4643705463182898\n",
            "Weighted F1: 0.40098170347809964\n",
            "Micro F1: 0.4643705463182898\n",
            "Weighted Precision: 0.4012626588413182\n",
            "Micro Precision: 0.4643705463182898\n",
            "Weighted Recall: 0.4643705463182898\n",
            "Micro Recall: 0.4643705463182898\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_1[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_roberta_model[0][0]']       \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 279)          143127      ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[209  63 108 ...  26   4  57]\n",
            "[   1    2    4 ... 8414 8417 8406]\n",
            "[6622   35 6713 4989 8233 7444 5061 8344 5464  317 4515 3001 6991 2875\n",
            " 3885 6647 4857 4504  214 8340 2969 3847 4609 7358 2752 7651 8188 7073\n",
            " 6483 4952 5072 1967 4071 7590 5030 7297  626 6029 6302 7256 3993 4263\n",
            " 3835 4293 8129 4837 3843 1418 5274 6205 5997 6685 3310 7721 2360 7197\n",
            " 4252 8343  909 4354  649 3488 2496 5765 4004 4052 1403 6697 6293 6380\n",
            " 4602 6057 4236 7978 5083 6552  361 7706 4186 8351  812 6600 8264 8064\n",
            " 7121 8279 5556 5119 7947 3350 7152 4282 5025 5051 3615 8062 1489 7351\n",
            "  234 7416 7921 6574 6257 8396 4520 3300 5640  240 7810 1321 6580 5404\n",
            " 8273 6742 7861 7975 1018 2140 7061 7767 4990 8124 5927 6051 6916 2846\n",
            " 6818 7052 7775 7582 8116 4767 5248 6055 8331 1564 7326 3065 6764 6854\n",
            " 4915 3583 5902 4229 7565 3712 7224 4502 3827 6525 6318 2686 5369 8235\n",
            " 3351 8021  102  224 8294 7707 2985 5146 3709 6824 2281 6837 5436 2649\n",
            " 4886  966 5476 8387 6166 4387  350 6123 5236 5041  734 3412 6085  489\n",
            " 4982 5115 6743 7173 5724 7689 7787 8302 3150 2247 8128 6660 4610 2866\n",
            " 6905 7096  768 4411 6087 2096 4903 7546 6630 2928 4640 7125 1792 1700\n",
            " 7928 8095  737 5849 2359 6015 4361 6081 5014 5167 1023 8009  268 4777\n",
            " 5702 6129 7181 1860 7192 5467   67 6134 4270 4611 6746 7696 3466 3950\n",
            " 6125 8370 6142 6131 5548 1721 5067 4015 7092   25 1749 4878 8248 6836\n",
            " 5310 6960 2339 1650 6227 4104 5130 7089 6993 1289 4305 5585 7933 6284\n",
            " 7057 3807 8347 5280 2841 7693 8090 4171 5260  133 7451 3957 1719 5435\n",
            " 6216 4858 4417 8355 4492 2648 5278 7308  158 7543 2168 5913 7111 3414\n",
            " 7592   39 7157 5647 4555 3956 3617 4630 4384 6596 4539 6749 7202 2413\n",
            " 4749 5122 5929  252 5565  336 6008 4564 5580 7237 5958 6918 6487 2989\n",
            " 7448 3382  467 3784 4419 1117 7954 7122 3174 1534 4164 7526  498 5944\n",
            " 4572 2307 2929 7480 4412 4563 7692 1869 2472 6966 6549 1594 2276 7269\n",
            " 2545 2216 6587 6315 5297 5282 6399 3917 6342 5721 7909 8382 4383 7130\n",
            "  485 4936 7674 5372 7109 4605 5211 5462 8065   50 4080 3334 8167 8378\n",
            " 5595 1873 8207 7076 3719 4627  231 2405 6973 3246 5931 8208 4825 6203\n",
            " 3019 5822   63 5854 5787 4764 6701 3822 8318 8169 8218 7549  707 1076\n",
            " 2465 4636 7025 7127 4026 4961 7531  744 6684 4297 7939 7384  446 6675\n",
            " 7752 5279 8196 3020 4192 4253 2887 1898 3208 5973 7033 1431 5915 8312\n",
            " 5799 7105  511 6404 1561]\n",
            "Mon Jun  6 22:00:02 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P0    38W / 300W |  15237MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "509/509 [==============================] - 192s 348ms/step - loss: 14.6364 - accuracy: 0.0499 - val_loss: 13.3250 - val_accuracy: 0.1708\n",
            "Epoch 2/5\n",
            "509/509 [==============================] - 177s 348ms/step - loss: 12.7597 - accuracy: 0.2590 - val_loss: 11.9260 - val_accuracy: 0.3667\n",
            "Epoch 3/5\n",
            "509/509 [==============================] - 177s 348ms/step - loss: 11.6005 - accuracy: 0.3995 - val_loss: 11.2555 - val_accuracy: 0.4237\n",
            "Epoch 4/5\n",
            "509/509 [==============================] - 177s 347ms/step - loss: 10.8326 - accuracy: 0.4760 - val_loss: 10.8281 - val_accuracy: 0.4487\n",
            "Epoch 5/5\n",
            "509/509 [==============================] - 177s 347ms/step - loss: 10.1819 - accuracy: 0.5533 - val_loss: 10.4299 - val_accuracy: 0.4761\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_3[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_roberta_model[1][0]']       \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 279)          143127      ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.46318289786223277\n",
            "Weighted F1: 0.3994013542599501\n",
            "Micro F1: 0.46318289786223277\n",
            "Weighted Precision: 0.3831667058934412\n",
            "Micro Precision: 0.46318289786223277\n",
            "Weighted Recall: 0.46318289786223277\n",
            "Micro Recall: 0.46318289786223277\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_1[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_roberta_model[0][0]']       \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 279)          143127      ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[209  63 108 ...  26   4  57]\n",
            "[   1    2    4 ... 8414 8417 8406]\n",
            "[6622   35 6713 4989 8233 7444 5061 8344 5464  317 4515 3001 6991 2875\n",
            " 3885 6647 4857 4504  214 8340 2969 3847 4609 7358 2752 7651 8188 7073\n",
            " 6483 4952 5072 1967 4071 7590 5030 7297  626 6029 6302 7256 3993 4263\n",
            " 3835 4293 8129 4837 3843 1418 5274 6205 5997 6685 3310 7721 2360 7197\n",
            " 4252 8343  909 4354  649 3488 2496 5765 4004 4052 1403 6697 6293 6380\n",
            " 4602 6057 4236 7978 5083 6552  361 7706 4186 8351  812 6600 8264 8064\n",
            " 7121 8279 5556 5119 7947 3350 7152 4282 5025 5051 3615 8062 1489 7351\n",
            "  234 7416 7921 6574 6257 8396 4520 3300 5640  240 7810 1321 6580 5404\n",
            " 8273 6742 7861 7975 1018 2140 7061 7767 4990 8124 5927 6051 6916 2846\n",
            " 6818 7052 7775 7582 8116 4767 5248 6055 8331 1564 7326 3065 6764 6854\n",
            " 4915 3583 5902 4229 7565 3712 7224 4502 3827 6525 6318 2686 5369 8235\n",
            " 3351 8021  102  224 8294 7707 2985 5146 3709 6824 2281 6837 5436 2649\n",
            " 4886  966 5476 8387 6166 4387  350 6123 5236 5041  734 3412 6085  489\n",
            " 4982 5115 6743 7173 5724 7689 7787 8302 3150 2247 8128 6660 4610 2866\n",
            " 6905 7096  768 4411 6087 2096 4903 7546 6630 2928 4640 7125 1792 1700\n",
            " 7928 8095  737 5849 2359 6015 4361 6081 5014 5167 1023 8009  268 4777\n",
            " 5702 6129 7181 1860 7192 5467   67 6134 4270 4611 6746 7696 3466 3950\n",
            " 6125 8370 6142 6131 5548 1721 5067 4015 7092   25 1749 4878 8248 6836\n",
            " 5310 6960 2339 1650 6227 4104 5130 7089 6993 1289 4305 5585 7933 6284\n",
            " 7057 3807 8347 5280 2841 7693 8090 4171 5260  133 7451 3957 1719 5435\n",
            " 6216 4858 4417 8355 4492 2648 5278 7308  158 7543 2168 5913 7111 3414\n",
            " 7592   39 7157 5647 4555 3956 3617 4630 4384 6596 4539 6749 7202 2413\n",
            " 4749 5122 5929  252 5565  336 6008 4564 5580 7237 5958 6918 6487 2989\n",
            " 7448 3382  467 3784 4419 1117 7954 7122 3174 1534 4164 7526  498 5944\n",
            " 4572 2307 2929 7480 4412 4563 7692 1869 2472 6966 6549 1594 2276 7269\n",
            " 2545 2216 6587 6315 5297 5282 6399 3917 6342 5721 7909 8382 4383 7130\n",
            "  485 4936 7674 5372 7109 4605 5211 5462 8065   50 4080 3334 8167 8378\n",
            " 5595 1873 8207 7076 3719 4627  231 2405 6973 3246 5931 8208 4825 6203\n",
            " 3019 5822   63 5854 5787 4764 6701 3822 8318 8169 8218 7549  707 1076\n",
            " 2465 4636 7025 7127 4026 4961 7531  744 6684 4297 7939 7384  446 6675\n",
            " 7752 5279 8196 3020 4192 4253 2887 1898 3208 5973 7033 1431 5915 8312\n",
            " 5799 7105  511 6404 1561]\n",
            "Mon Jun  6 22:16:38 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P0    38W / 300W |  15237MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "509/509 [==============================] - 191s 347ms/step - loss: 14.5515 - accuracy: 0.0561 - val_loss: 13.2254 - val_accuracy: 0.1936\n",
            "Epoch 2/5\n",
            "509/509 [==============================] - 177s 347ms/step - loss: 12.6172 - accuracy: 0.2802 - val_loss: 11.8703 - val_accuracy: 0.3736\n",
            "Epoch 3/5\n",
            "509/509 [==============================] - 176s 346ms/step - loss: 11.4250 - accuracy: 0.4145 - val_loss: 11.1637 - val_accuracy: 0.3986\n",
            "Epoch 4/5\n",
            "509/509 [==============================] - 176s 346ms/step - loss: 10.6328 - accuracy: 0.4962 - val_loss: 10.6827 - val_accuracy: 0.4465\n",
            "Epoch 5/5\n",
            "509/509 [==============================] - 177s 347ms/step - loss: 10.0216 - accuracy: 0.5599 - val_loss: 10.3011 - val_accuracy: 0.4875\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_3[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_roberta_model[1][0]']       \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 279)          143127      ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.4643705463182898\n",
            "Weighted F1: 0.39685929386852076\n",
            "Micro F1: 0.4643705463182898\n",
            "Weighted Precision: 0.37912778029351246\n",
            "Micro Precision: 0.4643705463182898\n",
            "Weighted Recall: 0.4643705463182898\n",
            "Micro Recall: 0.4643705463182898\n",
            "Average Accuracy: 0.4653206650831354\n",
            "Average Weighted F1: 0.39915102613812803\n",
            "Average Micro F1: 0.4653206650831354\n",
            "Average Weighted Precision: 0.38222680783242724\n",
            "Average Micro Precision: 0.4653206650831354\n",
            "Average Weighted Recall: 0.4653206650831354\n",
            "Average Micro Recall: 0.4653206650831354\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "total_accuracy=0\n",
        "total_weighted_f1=0\n",
        "total_micro_f1=0\n",
        "total_weighted_precision=0\n",
        "total_micro_precision=0\n",
        "total_weighted_recall=0\n",
        "total_micro_recall=0\n",
        "\n",
        "for f in range(5):\n",
        "  gc.collect()\n",
        "  tf.keras.backend.clear_session()\n",
        "  dbert_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "  dbert_model = TFRobertaModel.from_pretrained('roberta-base')\n",
        "  max_len=512\n",
        "  sentences=summarized_data['text']\n",
        "  labels=summarized_data['label']\n",
        "  filenumbers=summarized_data['filenum']\n",
        "  len(sentences),len(labels),len(filenumbers)\n",
        "  \n",
        "  model_5=create_model()\n",
        "  input_ids=[]\n",
        "  attention_masks=[]\n",
        "\n",
        "  for sent in sentences:\n",
        "    dbert_inps=dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n",
        "    input_ids.append(dbert_inps['input_ids'])\n",
        "    attention_masks.append(dbert_inps['attention_mask'])\n",
        "  input_ids=np.asarray(input_ids)\n",
        "\n",
        "  attention_masks=np.array(attention_masks)\n",
        "  labels=np.array(labels)\n",
        "  filenumbers=[int(i) for i in filenumbers]\n",
        "  filenumbers=np.array(filenumbers)\n",
        "  print(labels)\n",
        "  print(filenumbers)\n",
        "\n",
        "  #train_inp,val_inp,train_label,val_label,train_mask,val_mask,train_fnum,val_fnum=train_test_split(input_ids,labels,attention_masks,filenumbers,test_size=0.1,random_state=42)\n",
        "\n",
        "  new_train_inp_5=[]\n",
        "  new_val_inp_5=[]\n",
        "  new_train_label_5=[]\n",
        "  new_val_label_5=[]\n",
        "  new_train_mask_5=[]\n",
        "  new_train_fnum_5=[]\n",
        "  new_val_fnum_5=[]\n",
        "  new_val_mask_5=[]\n",
        "  for i in range(len(train_fnum)):\n",
        "    if train_fnum[i] in filenumbers:\n",
        "      new_train_inp_5.append(train_inp[i])\n",
        "      #new_val_inp.append(val_inp[i])\n",
        "      new_train_label_5.append(train_label[i])\n",
        "      #new_val_label.append(val_label[i])\n",
        "      new_train_mask_5.append(train_mask[i])\n",
        "      #new_val_mask.append(val_mask[i])\n",
        "      new_train_fnum_5.append(train_fnum[i])\n",
        "\n",
        "  for i in range(len(val_fnum)):\n",
        "    if val_fnum[i] in filenumbers:\n",
        "      #new_train_inp.append(train_inp[i])\n",
        "      new_val_inp_5.append(val_inp[i])\n",
        "      #new_train_label.append(train_label[i])\n",
        "      new_val_label_5.append(val_label[i])\n",
        "      #new_train_mask.append(train_mask[i])\n",
        "      new_val_mask_5.append(val_mask[i])\n",
        "      new_val_fnum_5.append(val_fnum[i])\n",
        "\n",
        "  # print(new_train_label)\n",
        "  # print(len(new_train_label))\n",
        "  # print(new_val_label)\n",
        "  # print(len(new_val_label))\n",
        "  new_train_inp_5=np.array(new_train_inp_5)\n",
        "  new_val_inp_5=np.array(new_val_inp_5)\n",
        "  new_train_label_5=np.array(new_train_label_5)\n",
        "  new_val_label_5=np.array(new_val_label_5)\n",
        "  new_train_mask_5=np.array(new_train_mask_5)\n",
        "  new_train_fnum_5=np.array(new_train_fnum_5)\n",
        "  new_val_fnum_5=np.array(new_val_fnum_5)\n",
        "  new_val_mask_5=np.array(new_val_mask_5)\n",
        "\n",
        "  print(new_val_fnum_5)\n",
        "  \n",
        "  log_dir='dbert_model'\n",
        "\n",
        "  model_save_path='./drive/MyDrive/Ensemble/roberta-ensemble-512-model_5-'+str(f)+'-279labels.h5'\n",
        "\n",
        "  loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "  accuracy = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
        "  callbacks= [tf.keras.callbacks.ModelCheckpoint(filepath=model_save_path,monitor='val_accuracy',mode='max',save_best_only=True,save_weights_only=True),keras.callbacks.TensorBoard(log_dir=log_dir)]\n",
        "  model_5.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])\n",
        "  gpu_info = !nvidia-smi\n",
        "  gpu_info = '\\n'.join(gpu_info)\n",
        "  if gpu_info.find('failed') >= 0:\n",
        "    print('Not connected to a GPU')\n",
        "  else:\n",
        "    print(gpu_info)\n",
        "  \n",
        "  history=model_5.fit([new_train_inp_5,new_train_mask_5],new_train_label_5,batch_size=8,epochs=5,validation_data=([new_val_inp_5,new_val_mask_5],new_val_label_5),callbacks=callbacks)\n",
        "\n",
        "  pred_labels=[]\n",
        "\n",
        "  model_saved_5= create_model()\n",
        "  model_saved_5.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])\n",
        "  model_saved_5.load_weights('./drive/MyDrive/Ensemble/roberta-ensemble-512-model_4-'+str(f)+'-279labels.h5')\n",
        "\n",
        "  for i in range(0,len(val_inp)):\n",
        "    pred=model_saved_5.predict([val_inp[i].reshape(1,512),val_mask[i].reshape(1,512)])\n",
        "    pred_label = pred.argmax(axis=1)\n",
        "    pred_labels.append(pred_label)\n",
        "  accuracy=accuracy_score(val_label, pred_labels)\n",
        "  print(\"Accuracy: \"+str(accuracy))\n",
        "  total_accuracy=total_accuracy+accuracy\n",
        "  \n",
        "  weighted_f1=f1_score(val_label,pred_labels, average='weighted')\n",
        "  print(\"Weighted F1: \"+ str(weighted_f1))\n",
        "  total_weighted_f1=total_weighted_f1+weighted_f1\n",
        "  micro_f1=f1_score(val_label,pred_labels, average='micro')\n",
        "  print(\"Micro F1: \"+ str(micro_f1))\n",
        "  total_micro_f1=total_micro_f1+micro_f1\n",
        "\n",
        "  weighted_precision=precision_score(val_label, pred_labels, average='weighted')\n",
        "  print(\"Weighted Precision: \" + str(weighted_precision))\n",
        "  total_weighted_precision=total_weighted_precision+weighted_precision\n",
        "  micro_precision=precision_score(val_label, pred_labels, average='micro')\n",
        "  print(\"Micro Precision: \" + str(micro_precision))\n",
        "  total_micro_precision=total_micro_precision+micro_precision\n",
        "\n",
        "  weighted_recall=recall_score(val_label, pred_labels, average='weighted')\n",
        "  print(\"Weighted Recall: \" + str(weighted_recall))\n",
        "  total_weighted_recall=total_weighted_recall+weighted_recall\n",
        "  micro_recall=recall_score(val_label, pred_labels, average='micro')\n",
        "  print(\"Micro Recall: \" + str(micro_recall))\n",
        "  total_micro_recall=total_micro_recall+micro_recall\n",
        "\n",
        "\n",
        "print(\"Average Accuracy: \"+str(total_accuracy/5))\n",
        "print(\"Average Weighted F1: \"+str(total_weighted_f1/5))\n",
        "print(\"Average Micro F1: \"+str(total_micro_f1/5))\n",
        "print(\"Average Weighted Precision: \"+str(total_weighted_precision/5))\n",
        "print(\"Average Micro Precision: \"+str(total_micro_precision/5))\n",
        "print(\"Average Weighted Recall: \"+str(total_weighted_recall/5))\n",
        "print(\"Average Micro Recall: \"+str(total_micro_recall/5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "aiQQ3PLLiLdn"
      },
      "outputs": [],
      "source": [
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "accuracy = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
        "\n",
        "def load_model(loss,accuracy,optimizer,id):\n",
        "  model= create_model()\n",
        "  model.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])\n",
        "  model.load_weights('./drive/MyDrive/Ensemble/roberta-ensemble-512-model_'+str(id)+'-279labels.h5')\n",
        "  return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcfQEe8-lDZ7",
        "outputId": "87a639f0-44a0-4a6f-f96f-b5eaf1dcd796"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6622   35 2227 6713 4989 8233 7444 5061 3942 8344  454 5464  317 2883\n",
            " 4515  796 3001 3410 6991 2875 1125 1657  354 3885 6647 4857 4504  214\n",
            " 3230 8340 2969 3847 4609 5092 6110 1346 5993 7358 2752 7651 7301 8188\n",
            " 7073 4782 6069 2346 6445 6483  542 1663 4952 5072 1967 4071 7590  860\n",
            " 6418 1997 5030 7297 2570  626 5463 6029 6302 3819 7256 2950 5667 1235\n",
            " 3993 1439 1323 4263 3835 2134 3054 2624 5801 4293 8129  320 5866 3247\n",
            " 5777 2373 5518   19 6059 6520 4837 3843 5695 1666  858 1418 5409 5274\n",
            " 6205 3115 3167 2356 8418 5997 6685  223 3444 3310 7721 5928 6139 2360\n",
            " 7197 4252 8343  909 4093 4354 4752 1421  649  829 1916 1003 3488 2496\n",
            " 5765 6612 4004 4052 1403 6697 5268 1010 6293  264 6380 4602 6057 1480\n",
            " 1851 4577 4236 7978 1350 5256 5083 6552 1058  361 7706 4186 2590 4452\n",
            " 4396 5386 2522  749  870 8351 3834  812 5108  463 2274 1743 6600 8264\n",
            " 8064 7121 8279 5556 1845 5119 2516 4673 2652 7947 8215 1882 3350 7617\n",
            " 7152 5199 3965 1374 4282 7672 1092 3342 8003 5025 5051  535 6024  933\n",
            " 3969 1927 3615 1221 8062 4920 1489 3296 7442 5459 2031 7351 1633  234\n",
            " 2313 8015 7416 7921 6574 6257 3535 8396  941 4520 3300 5640  240 7810\n",
            " 7425 1321 6580  928 1170 5404 1920 8273 5649 1919 6742 7005   94 7459\n",
            "    9 7861 7975 1018 5029 2322 2140 1415 7061 7767 4990 6261 8124 5927\n",
            " 6051 1739 3269 6916 5959 1503 4906 2846 6818 7052 7775 7582 8116 4240\n",
            " 4767 5248  430 1812 2175 6055 8331 3916  998 7271 1564 7326 1077 1559\n",
            "  292 3065 6764 6854 4915 5552 3583 1917  868 5902 2581 4229 2593 2144\n",
            " 7565 3712 7224 4502 1406 1651 6256 3827 6525 6318  791 2686 3616 5369\n",
            " 8235 1704 1011 3351 8089 8021  102  224 5671 8294 5874 1510 2459 7707\n",
            " 2985 6624 5146 1428 7381 3709 6824 6308  701 2281 6837 5436 1741 3520\n",
            " 5995 2649 4886  966 2863 5476 4058 1246 2406 1199 8387  513 5449 1028\n",
            " 2226 6166 4387  350 6123  678 5236 5041  734 3743 3439 3412 6085  587\n",
            " 2477  489 5760 3163 3376 4982  321 5115 4655 5575 3839 6743 7173  624\n",
            " 5724 7689 7787 1748  520  713 3214 8302 3150 5713 2247 8128 3044 4607\n",
            " 2089 6660 3505  983 4610 2866 4161 6905 7096  768 1062 4411 6087 2096\n",
            " 3536   21 4903 7546 6630 2928 4640 4510 7125 2816 1792 2063 1700 2305\n",
            " 7928 8095  737 5849 2311  922 3423 2359 6015 4489 4361 3250 5076 6081\n",
            " 6346 5014 5167 1023 8009 3335  268 2211 4777 5221 4323 6168 2580  759\n",
            "  711 5702 6129 7181 1860 5385 4701 7192 1347 1692 8278 5467 3378 6579\n",
            "   67 2693 6134 4841 4270 3073 8397 4611 1057 2943 6746 7696 3466 2573\n",
            " 1983 3950 4693 6125 4033  474 8370 6142 6131 3938  995 5548 2186 2112\n",
            " 1721 4153 5067 4015 5587 7092 3156   25 1749 5554 4878 1197 8248  458\n",
            " 8035 6836 1006 5310 2698 6960 2339 1650 6227 4104 5130 7089 6993 2400\n",
            " 1144 1289 4305 5585 2524 7933 2587 6284 7057 5324 3807 5790 7373 8347\n",
            " 2390 4815 5280 2841 7220 1384 6246 4205 4408 7693 8090 2671 5077 4171\n",
            " 5260  133 7451 6248 3957 6793 1719 7379 4743 4746 5435 6216 4858 4417\n",
            " 8355 4492 2648 5104 5278 6539  743 7308 5704  158 1185 7543 2168 5913\n",
            " 7111  864 3414 5788 7592   39 7157 5647 4555 1671 3956 1045 3617 4630\n",
            " 4384 3033 6596   96 4539 6749 7202 4710 2886 2413 4749 5122 8415 5052\n",
            " 1598 5929  252 5565  336 6008 4564 5580 7237 1833 2803 1466 1568 5958\n",
            " 7657 4327 5091 6918 6487 4082 2989 7448 4715   81 3382  467 2689 3784\n",
            " 4624 2317  764 4419 1117 2962 2571 3283 7954 7122 1430 6392  505 3174\n",
            " 1534  323 4164 3659 7526  498 5944 3510 4572 2307 2929 7480 4412  731\n",
            " 4563 7692 1869 2472 6966 4238 4702 6549 1594 2276 7269 2545 2216  605\n",
            " 6587 6315 5297 5282 4816 1607 3200 1619 5504 2536 6399 3459 3917 1212\n",
            " 6342 5754 5721 7385 7909 6126  555 7382 8382 4383 1777 5502 7130  485\n",
            " 5862 4936 7674 3575 5372 3893 7109 4605 5211 1895 5462 8065   50 4080\n",
            " 3334 8167 8378 5595 1873 8207 7076 4106 3719 4627 2090  395 2685  231\n",
            " 2405 5419 6973 3480 3246 5414 3593 5931 6252 8208 4825 6203 3019 1105\n",
            " 5822 6725   63 5854 6543 5787 4764 6701  334 6562 3822 1627 4861 3051\n",
            " 1315 4805 1149  246 1484 2629 8318 3862 1089 8169 2778 2036 8218 2756\n",
            " 1299 7549 2639  707 1284 1076 2465 3815  540 4636 3188  697   80 1455\n",
            "  381 7025 7127 3992   52 4026 2777 2038 4961  383 7531  744 1453 6640\n",
            " 1047 6684 1502 4297 7939  150 7384  446 7738 3287 6675  170 7752 5279\n",
            " 8196 3020 1203 4192 1552 2085 4253 2887 2556 5729 4127 1898 8362 3208\n",
            " 5973 7033 1431 5915 8312  881 5799 7105  511 3116 6404 2852 1561 4150\n",
            " 4835 1178]\n",
            "[6622   35 6713 4989 8233 7444 5061 3942 8344  454 5464  317 2883 4515\n",
            " 3001 3410 6991 2875 1125 3885 6647 4857 4504  214 3230 8340 2969 3847\n",
            " 4609 5092 1346 5993 7358 2752 7651 7301 8188 7073 4782 6069 2346 6445\n",
            " 6483  542 1663 4952 5072 1967 4071 7590  860 6418 5030 7297  626 5463\n",
            " 6029 6302 3819 7256 2950 5667 1235 3993 1439 1323 4263 3835 2134 2624\n",
            " 5801 4293 8129  320 5866 3247 5777   19 6059 6520 4837 3843 5695 1666\n",
            "  858 1418 5409 5274 6205 3167 2356 8418 5997 6685  223 3444 3310 7721\n",
            " 5928 2360 7197 4252 8343  909 4093 4354 4752 1421  649  829 1916 1003\n",
            " 3488 2496 5765 6612 4004 4052 1403 6697 5268 1010 6293 6380 4602 6057\n",
            " 1851 4577 4236 7978 1350 5083 6552 1058  361 7706 4186 2590 4452 4396\n",
            "  749  870 8351 3834  812 5108  463 1743 6600 8264 8064 7121 8279 5556\n",
            " 1845 5119 2516 4673 7947 8215 1882 3350 7617 7152 5199 4282 1092 3342\n",
            " 8003 5025 5051  535  933 3969 1927 3615 1221 8062 1489 3296 7442 5459\n",
            " 2031 7351  234 2313 8015 7416 7921 6574 6257 8396  941 4520 3300 5640\n",
            "  240 7810 7425 1321 6580  928 1170 5404 8273 6742 7005   94 7459    9\n",
            " 7861 7975 1018 5029 2322 2140 1415 7061 7767 4990 8124 5927 6051 3269\n",
            " 6916 5959 2846 6818 7052 7775 7582 8116 4240 4767 5248  430 1812 2175\n",
            " 6055 8331 3916  998 7271 1564 7326 1077 1559  292 3065 6764 6854 4915\n",
            " 5552 3583 1917  868 5902 4229 2144 7565 3712 7224 4502 1406 1651 6256\n",
            " 3827 6525 6318 2686 3616 5369 8235 1704 1011 3351 8089 8021  102  224\n",
            " 5671 8294 1510 7707 2985 6624 5146 1428 7381 3709 6824 6308  701 2281\n",
            " 6837 5436 1741 3520 5995 2649 4886  966 2863 5476 4058 1246 2406 1199\n",
            " 8387  513 5449 1028 2226 6166 4387  350 6123  678 5236 5041  734 3412\n",
            " 6085  587  489 5760 3163 3376 4982 5115 4655 5575 3839 6743 7173 5724\n",
            " 7689 7787  520  713 3214 8302 3150 5713 2247 8128 4607 6660 3505 4610\n",
            " 2866 4161 6905 7096  768 1062 4411 6087 2096   21 4903 7546 6630 2928\n",
            " 4640 7125 1792 2063 1700 2305 7928 8095  737 5849  922 3423 2359 6015\n",
            " 4489 4361 6081 6346 5014 5167 1023 8009 3335  268 4777 5221 4323 6168\n",
            "  759  711 5702 6129 7181 1860 7192 1347 1692 5467 3378 6579   67 2693\n",
            " 6134 4841 4270 3073 8397 4611 2943 6746 7696 3466 3950 4693 6125 8370\n",
            " 6142 6131  995 5548 2186 2112 1721 5067 4015 5587 7092 3156   25 1749\n",
            " 5554 4878 1197 8248  458 8035 6836 1006 5310 2698 6960 2339 1650 6227\n",
            " 4104 5130 7089 6993 2400 1289 4305 5585 2524 7933 6284 7057 5324 3807\n",
            " 5790 8347 2390 5280 2841 7220 1384 7693 8090 2671 4171 5260  133 7451\n",
            " 3957 6793 1719 7379 4743 4746 5435 6216 4858 4417 8355 4492 2648 5104\n",
            " 5278 6539  743 7308  158 7543 2168 5913 7111  864 3414 5788 7592   39\n",
            " 7157 5647 4555 3956 1045 3617 4630 4384 6596   96 4539 6749 7202 4710\n",
            " 2413 4749 5122 5052 1598 5929  252 5565  336 6008 4564 5580 7237 1833\n",
            " 1466 1568 5958 7657 4327 5091 6918 6487 4082 2989 7448 4715   81 3382\n",
            "  467 3784 4624  764 4419 1117 2962 7954 7122 1430 6392  505 3174 1534\n",
            "  323 4164 3659 7526  498 5944 3510 4572 2307 2929 7480 4412  731 4563\n",
            " 7692 1869 2472 6966 4238 4702 6549 1594 2276 7269 2545 2216  605 6587\n",
            " 6315 5297 5282 4816 3200 1619 5504 2536 6399 3459 3917 6342 5754 5721\n",
            " 7385 7909 6126 7382 8382 4383 5502 7130  485 5862 4936 7674 3575 5372\n",
            " 3893 7109 4605 5211 5462 8065   50 4080 3334 8167 8378 5595 1873 8207\n",
            " 7076 4106 3719 4627  395 2685  231 2405 5419 6973 3480 3246 5414 3593\n",
            " 5931 6252 8208 4825 6203 3019 1105 5822   63 5854 6543 5787 4764 6701\n",
            "  334 6562 3822 4861 3051 1315 4805  246 8318 1089 8169 8218 2756 7549\n",
            "  707 1284 1076 2465 3815  540 4636 3188  697   80 1455  381 7025 7127\n",
            " 3992 4026 2777 4961  383 7531  744 6640 1047 6684 4297 7939  150 7384\n",
            "  446 7738 3287 6675 7752 5279 8196 3020 1203 4192 1552 4253 2887 2556\n",
            " 5729 1898 3208 5973 7033 1431 5915 8312 5799 7105  511 6404 2852 1561\n",
            " 4150 4835 1178]\n",
            "[6622   35 6713 4989 8233 7444 5061 3942 8344  454 5464  317 4515 3001\n",
            " 6991 2875 1125 3885 6647 4857 4504  214 8340 2969 3847 4609 5092 1346\n",
            " 5993 7358 2752 7651 7301 8188 7073 4782 6069 2346 6445 6483 1663 4952\n",
            " 5072 1967 4071 7590  860 6418 5030 7297  626 5463 6029 6302 3819 7256\n",
            " 5667 1235 3993 1439 1323 4263 3835 2624 5801 4293 8129  320 5866 3247\n",
            " 5777   19 6059 4837 3843 1666  858 1418 5409 5274 6205 3167 2356 5997\n",
            " 6685  223 3444 3310 7721 5928 2360 7197 4252 8343  909 4354 4752 1421\n",
            "  649  829 1916 3488 2496 5765 6612 4004 4052 1403 6697 5268 1010 6293\n",
            " 6380 4602 6057 1851 4577 4236 7978 5083 6552 1058  361 7706 4186 4452\n",
            " 4396  749 8351 3834  812 5108  463 1743 6600 8264 8064 7121 8279 5556\n",
            " 5119 4673 7947 1882 3350 7617 7152 5199 4282 1092 8003 5025 5051  933\n",
            " 3969 1927 3615 1221 8062 1489 3296 7442 5459 2031 7351  234 2313 8015\n",
            " 7416 7921 6574 6257 8396  941 4520 3300 5640  240 7810 7425 1321 6580\n",
            "  928 1170 5404 8273 6742 7005   94 7459    9 7861 7975 1018 5029 2140\n",
            " 1415 7061 7767 4990 8124 5927 6051 3269 6916 5959 2846 6818 7052 7775\n",
            " 7582 8116 4240 4767 5248  430 1812 2175 6055 8331 3916  998 1564 7326\n",
            " 1559  292 3065 6764 6854 4915 5552 3583 1917 5902 4229 2144 7565 3712\n",
            " 7224 4502 1406 1651 6256 3827 6525 6318 2686 3616 5369 8235 1704 1011\n",
            " 3351 8089 8021  102  224 5671 8294 7707 2985 6624 5146 1428 7381 3709\n",
            " 6824 6308  701 2281 6837 5436 1741 3520 5995 2649 4886  966 5476 2406\n",
            " 1199 8387  513 5449 1028 2226 6166 4387  350 6123  678 5236 5041  734\n",
            " 3412 6085  587  489 5760 3376 4982 5115 4655 5575 3839 6743 7173 5724\n",
            " 7689 7787  520 8302 3150 5713 2247 8128 4607 6660 3505 4610 2866 4161\n",
            " 6905 7096  768 1062 4411 6087 2096 4903 7546 6630 2928 4640 7125 1792\n",
            " 2063 1700 2305 7928 8095  737 5849  922 2359 6015 4489 4361 6081 6346\n",
            " 5014 5167 1023 8009 3335  268 4777 5221 4323 6168  759  711 5702 6129\n",
            " 7181 1860 7192 1347 5467 3378 6579   67 2693 6134 4841 4270 4611 2943\n",
            " 6746 7696 3466 3950 4693 6125 8370 6142 6131  995 5548 2112 1721 5067\n",
            " 4015 7092 3156   25 1749 5554 4878 8248  458 6836 1006 5310 2698 6960\n",
            " 2339 1650 6227 4104 5130 7089 6993 2400 1289 4305 5585 7933 6284 7057\n",
            " 3807 5790 8347 2390 5280 2841 7220 1384 7693 8090 4171 5260  133 7451\n",
            " 3957 6793 1719 7379 4743 4746 5435 6216 4858 4417 8355 4492 2648 5104\n",
            " 5278 6539  743 7308  158 7543 2168 5913 7111 3414 5788 7592   39 7157\n",
            " 5647 4555 3956 1045 3617 4630 4384 6596   96 4539 6749 7202 2413 4749\n",
            " 5122 5052 1598 5929  252 5565  336 6008 4564 5580 7237 1466 1568 5958\n",
            " 7657 4327 5091 6918 6487 4082 2989 7448 4715 3382  467 3784 4624  764\n",
            " 4419 1117 7954 7122 6392  505 3174 1534  323 4164 3659 7526  498 5944\n",
            " 4572 2307 2929 7480 4412  731 4563 7692 1869 2472 6966 4238 6549 1594\n",
            " 2276 7269 2545 2216  605 6587 6315 5297 5282 4816 1619 5504 2536 6399\n",
            " 3917 6342 5754 5721 7385 7909 6126 7382 8382 4383 5502 7130  485 5862\n",
            " 4936 7674 3575 5372 3893 7109 4605 5211 5462 8065   50 4080 3334 8167\n",
            " 8378 5595 1873 8207 7076 4106 3719 4627  395  231 2405 5419 6973 3480\n",
            " 3246 5414 5931 8208 4825 6203 3019 5822   63 5854 6543 5787 4764 6701\n",
            " 6562 3822 4861 3051 1315 4805 8318 1089 8169 8218 7549  707 1284 1076\n",
            " 2465 3815 4636 3188  697   80 1455  381 7025 7127 3992 4026 2777 4961\n",
            " 7531  744 6640 1047 6684 4297 7939 7384  446 3287 6675 7752 5279 8196\n",
            " 3020 1203 4192 1552 4253 2887 2556 5729 1898 3208 5973 7033 1431 5915\n",
            " 8312 5799 7105  511 6404 2852 1561 4150 4835 1178]\n",
            "[6622   35 6713 4989 8233 7444 5061 3942 8344 5464  317 4515 3001 6991\n",
            " 2875 1125 3885 6647 4857 4504  214 8340 2969 3847 4609 1346 5993 7358\n",
            " 2752 7651 8188 7073 4782 6069 2346 6483 1663 4952 5072 1967 4071 7590\n",
            "  860 6418 5030 7297  626 5463 6029 6302 7256 5667 1235 3993 1323 4263\n",
            " 3835 2624 5801 4293 8129 5866 5777 6059 4837 3843 1418 5409 5274 6205\n",
            " 2356 5997 6685 3444 3310 7721 5928 2360 7197 4252 8343  909 4354 4752\n",
            "  649 1916 3488 2496 5765 6612 4004 4052 1403 6697 5268 6293 6380 4602\n",
            " 6057 1851 4577 4236 7978 5083 6552  361 7706 4186 4452 4396 8351 3834\n",
            "  812 5108  463 6600 8264 8064 7121 8279 5556 5119 7947 3350 7152 5199\n",
            " 4282 1092 5025 5051  933 1927 3615 1221 8062 1489 7442 5459 2031 7351\n",
            "  234 8015 7416 7921 6574 6257 8396  941 4520 3300 5640  240 7810 7425\n",
            " 1321 6580  928 1170 5404 8273 6742   94    9 7861 7975 1018 2140 1415\n",
            " 7061 7767 4990 8124 5927 6051 3269 6916 5959 2846 6818 7052 7775 7582\n",
            " 8116 4240 4767 5248 1812 2175 6055 8331  998 1564 7326 1559  292 3065\n",
            " 6764 6854 4915 5552 3583 5902 4229 2144 7565 3712 7224 4502 1406 1651\n",
            " 6256 3827 6525 6318 2686 3616 5369 8235 1704 1011 3351 8089 8021  102\n",
            "  224 5671 8294 7707 2985 6624 5146 7381 3709 6824 6308 2281 6837 5436\n",
            " 1741 3520 2649 4886  966 5476 1199 8387 5449 1028 2226 6166 4387  350\n",
            " 6123 5236 5041  734 3412 6085  587  489 5760 4982 5115 5575 3839 6743\n",
            " 7173 5724 7689 7787 8302 3150 2247 8128 6660 3505 4610 2866 4161 6905\n",
            " 7096  768 1062 4411 6087 2096 4903 7546 6630 2928 4640 7125 1792 2063\n",
            " 1700 7928 8095  737 5849  922 2359 6015 4489 4361 6081 6346 5014 5167\n",
            " 1023 8009 3335  268 4777 5221 4323 6168  711 5702 6129 7181 1860 7192\n",
            " 1347 5467 6579   67 2693 6134 4841 4270 4611 2943 6746 7696 3466 3950\n",
            " 6125 8370 6142 6131  995 5548 1721 5067 4015 7092 3156   25 1749 5554\n",
            " 4878 8248 6836 5310 2698 6960 2339 1650 6227 4104 5130 7089 6993 2400\n",
            " 1289 4305 5585 7933 6284 7057 3807 5790 8347 5280 2841 7220 1384 7693\n",
            " 8090 4171 5260  133 7451 3957 1719 7379 4743 4746 5435 6216 4858 4417\n",
            " 8355 4492 2648 5104 5278  743 7308  158 7543 2168 5913 7111 3414 7592\n",
            "   39 7157 5647 4555 3956 3617 4630 4384 6596   96 4539 6749 7202 2413\n",
            " 4749 5122 5052 5929  252 5565  336 6008 4564 5580 7237 1466 1568 5958\n",
            " 7657 4327 5091 6918 6487 4082 2989 7448 4715 3382  467 3784 4624 4419\n",
            " 1117 7954 7122 6392 3174 1534 4164 3659 7526  498 5944 4572 2307 2929\n",
            " 7480 4412  731 4563 7692 1869 2472 6966 4238 6549 1594 2276 7269 2545\n",
            " 2216  605 6587 6315 5297 5282 1619 5504 6399 3917 6342 5754 5721 7385\n",
            " 7909 6126 7382 8382 4383 5502 7130  485 4936 7674 5372 3893 7109 4605\n",
            " 5211 5462 8065   50 4080 3334 8167 8378 5595 1873 8207 7076 4106 3719\n",
            " 4627  395  231 2405 6973 3480 3246 5414 5931 8208 4825 6203 3019 5822\n",
            "   63 5854 6543 5787 4764 6701 6562 3822 4861 3051 1315 8318 1089 8169\n",
            " 8218 7549  707 1284 1076 2465 3815 4636   80 7025 7127 4026 2777 4961\n",
            " 7531  744 6640 6684 4297 7939 7384  446 3287 6675 7752 5279 8196 3020\n",
            " 1203 4192 4253 2887 2556 5729 1898 3208 5973 7033 1431 5915 8312 5799\n",
            " 7105  511 6404 2852 1561 4150 1178]\n",
            "[6622   35 6713 4989 8233 7444 5061 8344 5464  317 4515 3001 6991 2875\n",
            " 3885 6647 4857 4504  214 8340 2969 3847 4609 1346 5993 7358 2752 7651\n",
            " 8188 7073 6069 2346 6483 4952 5072 1967 4071 7590 5030 7297  626 5463\n",
            " 6029 6302 7256 3993 4263 3835 5801 4293 8129 5866 5777 4837 3843 1418\n",
            " 5274 6205 2356 5997 6685 3310 7721 5928 2360 7197 4252 8343  909 4354\n",
            " 4752  649 3488 2496 5765 6612 4004 4052 1403 6697 5268 6293 6380 4602\n",
            " 6057 4577 4236 7978 5083 6552  361 7706 4186 8351  812 5108 6600 8264\n",
            " 8064 7121 8279 5556 5119 7947 3350 7152 5199 4282 5025 5051 1927 3615\n",
            " 1221 8062 1489 2031 7351  234 8015 7416 7921 6574 6257 8396  941 4520\n",
            " 3300 5640  240 7810 7425 1321 6580 5404 8273 6742   94    9 7861 7975\n",
            " 1018 2140 1415 7061 7767 4990 8124 5927 6051 3269 6916 2846 6818 7052\n",
            " 7775 7582 8116 4767 5248 1812 6055 8331  998 1564 7326  292 3065 6764\n",
            " 6854 4915 3583 5902 4229 2144 7565 3712 7224 4502 1406 1651 3827 6525\n",
            " 6318 2686 5369 8235 1704 1011 3351 8021  102  224 8294 7707 2985 5146\n",
            " 3709 6824 2281 6837 5436 2649 4886  966 5476 1199 8387 5449 1028 2226\n",
            " 6166 4387  350 6123 5236 5041  734 3412 6085  587  489 4982 5115 5575\n",
            " 3839 6743 7173 5724 7689 7787 8302 3150 2247 8128 6660 3505 4610 2866\n",
            " 4161 6905 7096  768 1062 4411 6087 2096 4903 7546 6630 2928 4640 7125\n",
            " 1792 2063 1700 7928 8095  737 5849  922 2359 6015 4489 4361 6081 6346\n",
            " 5014 5167 1023 8009  268 4777 5221 4323 6168 5702 6129 7181 1860 7192\n",
            " 5467 6579   67 2693 6134 4270 4611 6746 7696 3466 3950 6125 8370 6142\n",
            " 6131  995 5548 1721 5067 4015 7092 3156   25 1749 4878 8248 6836 5310\n",
            " 6960 2339 1650 6227 4104 5130 7089 6993 1289 4305 5585 7933 6284 7057\n",
            " 3807 5790 8347 5280 2841 7693 8090 4171 5260  133 7451 3957 1719 4743\n",
            " 4746 5435 6216 4858 4417 8355 4492 2648 5104 5278 7308  158 7543 2168\n",
            " 5913 7111 3414 7592   39 7157 5647 4555 3956 3617 4630 4384 6596 4539\n",
            " 6749 7202 2413 4749 5122 5929  252 5565  336 6008 4564 5580 7237 5958\n",
            " 7657 4327 5091 6918 6487 2989 7448 4715 3382  467 3784 4419 1117 7954\n",
            " 7122 3174 1534 4164 7526  498 5944 4572 2307 2929 7480 4412 4563 7692\n",
            " 1869 2472 6966 6549 1594 2276 7269 2545 2216  605 6587 6315 5297 5282\n",
            " 1619 5504 6399 3917 6342 5721 7909 6126 8382 4383 5502 7130  485 4936\n",
            " 7674 5372 7109 4605 5211 5462 8065   50 4080 3334 8167 8378 5595 1873\n",
            " 8207 7076 4106 3719 4627  231 2405 6973 3480 3246 5931 8208 4825 6203\n",
            " 3019 5822   63 5854 6543 5787 4764 6701 6562 3822 4861 1315 8318 8169\n",
            " 8218 7549  707 1076 2465 4636   80 7025 7127 4026 4961 7531  744 6684\n",
            " 4297 7939 7384  446 6675 7752 5279 8196 3020 4192 4253 2887 2556 5729\n",
            " 1898 3208 5973 7033 1431 5915 8312 5799 7105  511 6404 2852 1561]\n",
            "[6622   35 6713 4989 8233 7444 5061 8344 5464  317 4515 3001 6991 2875\n",
            " 3885 6647 4857 4504  214 8340 2969 3847 4609 7358 2752 7651 8188 7073\n",
            " 6483 4952 5072 1967 4071 7590 5030 7297  626 6029 6302 7256 3993 4263\n",
            " 3835 4293 8129 4837 3843 1418 5274 6205 5997 6685 3310 7721 2360 7197\n",
            " 4252 8343  909 4354  649 3488 2496 5765 4004 4052 1403 6697 6293 6380\n",
            " 4602 6057 4236 7978 5083 6552  361 7706 4186 8351  812 6600 8264 8064\n",
            " 7121 8279 5556 5119 7947 3350 7152 4282 5025 5051 3615 8062 1489 7351\n",
            "  234 7416 7921 6574 6257 8396 4520 3300 5640  240 7810 1321 6580 5404\n",
            " 8273 6742 7861 7975 1018 2140 7061 7767 4990 8124 5927 6051 6916 2846\n",
            " 6818 7052 7775 7582 8116 4767 5248 6055 8331 1564 7326 3065 6764 6854\n",
            " 4915 3583 5902 4229 7565 3712 7224 4502 3827 6525 6318 2686 5369 8235\n",
            " 3351 8021  102  224 8294 7707 2985 5146 3709 6824 2281 6837 5436 2649\n",
            " 4886  966 5476 8387 6166 4387  350 6123 5236 5041  734 3412 6085  489\n",
            " 4982 5115 6743 7173 5724 7689 7787 8302 3150 2247 8128 6660 4610 2866\n",
            " 6905 7096  768 4411 6087 2096 4903 7546 6630 2928 4640 7125 1792 1700\n",
            " 7928 8095  737 5849 2359 6015 4361 6081 5014 5167 1023 8009  268 4777\n",
            " 5702 6129 7181 1860 7192 5467   67 6134 4270 4611 6746 7696 3466 3950\n",
            " 6125 8370 6142 6131 5548 1721 5067 4015 7092   25 1749 4878 8248 6836\n",
            " 5310 6960 2339 1650 6227 4104 5130 7089 6993 1289 4305 5585 7933 6284\n",
            " 7057 3807 8347 5280 2841 7693 8090 4171 5260  133 7451 3957 1719 5435\n",
            " 6216 4858 4417 8355 4492 2648 5278 7308  158 7543 2168 5913 7111 3414\n",
            " 7592   39 7157 5647 4555 3956 3617 4630 4384 6596 4539 6749 7202 2413\n",
            " 4749 5122 5929  252 5565  336 6008 4564 5580 7237 5958 6918 6487 2989\n",
            " 7448 3382  467 3784 4419 1117 7954 7122 3174 1534 4164 7526  498 5944\n",
            " 4572 2307 2929 7480 4412 4563 7692 1869 2472 6966 6549 1594 2276 7269\n",
            " 2545 2216 6587 6315 5297 5282 6399 3917 6342 5721 7909 8382 4383 7130\n",
            "  485 4936 7674 5372 7109 4605 5211 5462 8065   50 4080 3334 8167 8378\n",
            " 5595 1873 8207 7076 3719 4627  231 2405 6973 3246 5931 8208 4825 6203\n",
            " 3019 5822   63 5854 5787 4764 6701 3822 8318 8169 8218 7549  707 1076\n",
            " 2465 4636 7025 7127 4026 4961 7531  744 6684 4297 7939 7384  446 6675\n",
            " 7752 5279 8196 3020 4192 4253 2887 1898 3208 5973 7033 1431 5915 8312\n",
            " 5799 7105  511 6404 1561]\n",
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_5 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_6 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_5[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_6[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_2 (Sl  (None, 768)         0           ['tf_roberta_model[2][0]']       \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_2[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_39 (Dropout)           (None, 512)          0           ['dense_4[0][0]']                \n",
            "                                                                                                  \n",
            " dense_5 (Dense)                (None, 279)          143127      ['dropout_39[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_7 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_8 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_7[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_8[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_3 (Sl  (None, 768)         0           ['tf_roberta_model[3][0]']       \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_6 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_3[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_40 (Dropout)           (None, 512)          0           ['dense_6[0][0]']                \n",
            "                                                                                                  \n",
            " dense_7 (Dense)                (None, 279)          143127      ['dropout_40[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_9 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_10 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_9[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_10[0][0]']               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_4 (Sl  (None, 768)         0           ['tf_roberta_model[4][0]']       \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_8 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_4[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_41 (Dropout)           (None, 512)          0           ['dense_8[0][0]']                \n",
            "                                                                                                  \n",
            " dense_9 (Dense)                (None, 279)          143127      ['dropout_41[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model: \"model_5\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_11 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_12 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_11[0][0]',               \n",
            " el)                            thPoolingAndCrossAt               'input_12[0][0]']               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_5 (Sl  (None, 768)         0           ['tf_roberta_model[5][0]']       \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_10 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_5[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_42 (Dropout)           (None, 512)          0           ['dense_10[0][0]']               \n",
            "                                                                                                  \n",
            " dense_11 (Dense)               (None, 279)          143127      ['dropout_42[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model: \"model_6\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_13 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_14 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_13[0][0]',               \n",
            " el)                            thPoolingAndCrossAt               'input_14[0][0]']               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_6 (Sl  (None, 768)         0           ['tf_roberta_model[6][0]']       \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_12 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_6[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_43 (Dropout)           (None, 512)          0           ['dense_12[0][0]']               \n",
            "                                                                                                  \n",
            " dense_13 (Dense)               (None, 279)          143127      ['dropout_43[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model: \"model_7\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_15 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_16 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_15[0][0]',               \n",
            " el)                            thPoolingAndCrossAt               'input_16[0][0]']               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_7 (Sl  (None, 768)         0           ['tf_roberta_model[7][0]']       \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_14 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_7[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_44 (Dropout)           (None, 512)          0           ['dense_14[0][0]']               \n",
            "                                                                                                  \n",
            " dense_15 (Dense)               (None, 279)          143127      ['dropout_44[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "[30, 237, 137, 50, 104, 203, 149, 166, 108, 15, 200, 161, 191, 156, 205, 176, 96, 156, 239, 97, 197, 241, 133, 161, 98, 3, 97, 191, 21, 196, 193, 19, 204, 64, 2, 30, 15, 114, 96, 135, 96, 69, 58, 115, 114, 191, 69, 151, 161, 202, 135, 78, 22, 175, 96, 196, 264, 125, 258, 238, 153, 104, 166, 109, 15, 25, 108, 18, 202, 64, 153, 194, 173, 25, 98, 96, 241, 161, 23, 135, 204, 76, 3, 25, 173, 238, 239, 78, 80, 71, 121, 203, 26, 121, 25, 50, 104, 202, 200, 25, 175, 153, 74, 133, 72, 259, 173, 156, 204, 21, 3, 239, 50, 26, 3, 240, 135, 153, 160, 50, 196, 197, 175, 50, 109, 237, 149, 107, 204, 106, 235, 21, 200, 118, 258, 208, 15, 18, 150, 94, 15, 22, 200, 237, 25, 153, 191, 135, 196, 258, 104, 23, 137, 175, 156, 0, 149, 200, 191, 104, 239, 191, 156, 156, 78, 50, 133, 193, 160, 176, 135, 163, 2, 107, 195, 235, 153, 237, 78, 175, 104, 50, 237, 137, 202, 191, 203, 258, 78, 10, 26, 204, 3, 10, 239, 238, 74, 78, 57, 237, 137, 239, 18, 95, 200, 196, 215, 237, 125, 192, 191, 98, 114, 165, 19, 58, 109, 193, 117, 97, 238, 238, 196, 98, 30, 238, 149, 97, 155, 114, 96, 58, 241, 25, 193, 94, 238, 15, 3, 135, 64, 213, 208, 193, 10, 10, 175, 204, 23, 241, 98, 115, 150, 29, 235, 95, 30, 25, 241, 115, 197, 165, 69, 135, 237, 133, 25, 202, 197, 191, 96, 237, 209, 94, 175, 19, 221, 98, 104, 147, 156, 78, 97, 200, 19, 26, 146, 78, 25, 135, 96, 175, 153, 50, 3, 104, 30, 165, 22, 140, 237, 22, 109, 64, 3, 218, 161, 147, 165, 78, 175, 29, 207, 165, 21, 15, 23, 200, 241, 140, 241, 133, 161, 155, 19, 238, 25, 202, 102, 19, 176, 163, 50, 26, 25, 152, 15, 200, 98, 218, 22, 207, 173, 139, 21, 3, 133, 21, 115, 195, 62, 151, 135, 238, 40, 121, 25, 19, 264, 204, 16, 78, 137, 25, 153, 196, 158, 135, 140, 109, 237, 23, 109, 109, 133, 135, 166, 96, 194, 25, 3, 193, 200, 200, 50, 237, 193, 15, 78, 40, 156, 191, 215, 176, 239, 200, 203, 191, 203, 57, 238, 102, 30, 151, 166, 96, 149, 118, 106, 133, 195, 147, 26, 135, 26, 264, 10, 175, 163, 149, 200, 25, 3, 78, 125, 22, 156, 209, 200, 18, 94, 150, 96, 165, 193, 207, 204, 156, 193, 149, 25, 102, 161, 170, 258, 19, 96, 191, 104, 238, 191, 204, 191, 101, 78, 57, 195, 30, 241, 166, 200, 193, 176, 40, 161, 18, 115, 149, 163, 166, 78, 22, 135, 135, 25, 153, 21, 196, 203, 30, 264, 172, 196, 160, 96, 241, 215, 133, 218, 239, 29, 153, 156, 163, 113, 19, 149, 30, 108, 98, 2, 200, 154, 137, 26, 50, 161, 18, 239, 21, 137, 140, 78, 3, 50, 57, 133, 192, 237, 135, 112, 163, 133, 135, 153, 238, 22, 21, 175, 58, 18, 237, 192, 98, 204, 191, 25, 208, 26, 161, 200, 166, 18, 2, 15, 26, 238, 238, 196, 4, 204, 193, 101, 15, 204, 239, 203, 112, 165, 161, 173, 106, 238, 237, 196, 80, 195, 196, 241, 58, 96, 18, 137, 203, 64, 202, 15, 18, 209, 202, 238, 193, 22, 15, 121, 104, 165, 204, 78, 155, 155, 173, 241, 155, 23, 196, 196, 192, 30, 200, 74, 26, 140, 96, 238, 72, 196, 18, 203, 114, 135, 237, 235, 2, 19, 200, 196, 237, 237, 133, 211, 238, 78, 153, 22, 104, 192, 204, 113, 133, 22, 153, 147, 80, 192, 239, 191, 239, 15, 29, 137, 26, 202, 71, 221, 15, 30, 204, 26, 213, 149, 153, 200, 235, 96, 151, 2, 104, 204, 148, 79, 155, 205, 78, 78, 23, 176, 221, 74, 18, 147, 161, 152, 26, 200, 238, 155, 191, 40, 209, 133, 191, 133, 113, 196, 194, 166, 200, 112, 238, 30, 175, 3, 258, 163, 3, 15, 23, 114, 191, 176, 151, 15, 97, 19, 221, 208, 40, 18, 96, 199, 160, 135, 203, 21, 18, 57, 137, 194, 108, 199, 200, 215, 209, 149, 176, 23, 96, 137, 203, 235, 200, 191, 112, 156, 133, 193, 15, 158, 18, 25, 203, 26, 112, 200, 3, 80, 200, 200, 94, 191, 69, 192, 133, 202, 175, 218, 23, 208, 10, 130, 101, 40, 72, 161, 25, 200, 125, 3, 21, 241, 104, 153, 15, 30, 205, 121, 35, 156, 176, 25, 195, 200, 58, 80, 204, 114, 18, 173, 19, 213, 104, 98, 79, 15, 151, 218, 191, 10, 200, 194, 18, 23, 96, 3, 139, 196, 96, 3, 196, 196, 160, 35, 22, 118, 97, 258, 2, 133, 133, 28, 23, 78, 204, 0, 30, 2, 237, 195, 23, 237, 80, 196, 258, 215, 26, 2, 78, 78, 196, 94, 160]\n",
            "[15, 237, -1, 16, 104, 203, 135, 166, 109, 15, 200, 161, 58, 156, 205, -1, 96, 156, 15, 97, 197, -1, -1, 161, 98, 202, 97, 191, 21, 196, 193, 19, 204, 70, -1, 30, 15, 114, 96, 135, 99, 69, 58, 106, 13, 191, 69, 151, 161, 202, 135, 80, 135, 176, 96, 237, 258, -1, 258, 238, -1, 104, 166, 109, 15, 50, 108, 18, 213, 53, 153, 194, 173, 25, 98, 96, -1, 161, 23, 135, 26, 76, 3, 25, 173, -1, -1, 78, 200, 71, 121, 191, 69, 121, 25, 62, 104, 3, 200, -1, 175, 153, 74, 133, 72, 258, 173, 156, 204, 21, -1, 258, 50, 71, 3, 240, 135, 153, 235, 50, 197, 195, 2, 62, 109, 237, 135, 107, 204, 106, 258, 21, 196, 117, 258, -1, 15, 18, 26, -1, 15, 22, 3, 237, 25, -1, 192, 135, 196, 258, 104, 15, 137, 175, 156, -1, -1, 200, 191, 104, 158, 191, 156, 156, -1, 50, 133, 258, 160, 176, 135, 163, 2, 107, 197, 258, -1, 237, 78, 238, 104, 50, 237, 137, -1, -1, 203, -1, 78, 10, 26, 204, 3, 155, -1, 238, 4, 78, 57, 237, 135, -1, 18, 95, 200, 196, 213, 237, -1, 191, 191, 107, 114, 264, 18, 56, -1, 193, 117, 99, 238, 238, 161, 107, 15, 176, 135, 97, 155, 114, -1, 58, -1, -1, 193, 94, 238, 15, 112, 135, 70, 213, 208, 193, 10, 10, 175, 204, 25, -1, 106, 115, 150, -1, 161, 95, 30, -1, -1, 111, 194, 237, 69, 135, 237, 196, 25, 202, 197, 191, 96, 237, 209, 102, 176, 19, 221, 107, 104, 147, 173, 78, 97, 200, 19, 26, 196, 78, 25, 135, -1, 175, -1, 50, 161, 104, 30, 165, 24, 135, 98, 22, 109, 64, -1, 221, 96, 98, 98, 78, 172, 155, 207, 165, 21, 15, 25, 200, -1, 140, -1, 158, 4, 155, 19, 2, 15, 202, 102, 18, 176, 163, 96, 26, 15, 152, 15, 200, 98, 221, 22, 207, 173, 139, 21, 3, 238, 21, 117, 195, 78, 150, 202, 213, 40, 121, 25, 19, 258, -1, -1, 78, 137, 25, -1, 196, 158, 147, 202, 109, -1, 23, 109, 109, 133, 135, 166, -1, 71, 25, 3, -1, 200, 200, 69, 237, 193, 15, 78, 40, -1, 191, -1, 177, 135, -1, 203, 191, 203, 57, 238, 102, 30, 151, 107, 97, -1, 118, 106, 196, 3, 147, 26, -1, 26, -1, 10, 2, 163, 149, 58, 15, 3, 78, -1, 22, 237, 209, 200, 18, 94, -1, -1, 29, 193, 264, 204, 155, 193, 149, 25, -1, 161, 170, 258, 19, -1, 191, 104, 238, 191, 204, 191, -1, -1, 57, 197, 30, -1, 166, 200, 193, 176, 40, 161, 18, 113, 149, 150, 166, -1, 22, 135, 135, 25, -1, -1, 196, 203, 30, -1, -1, 258, 202, 96, -1, 213, 133, 221, 21, 140, -1, 156, 163, 113, 19, 149, 70, 109, 98, 173, 200, 154, 156, 15, 237, 161, 18, 15, 135, 137, 135, 78, 208, 50, 57, 133, 192, -1, 135, 112, 163, 196, 135, -1, 238, 22, 15, 171, 205, -1, 237, 192, -1, 204, 191, 50, 208, -1, -1, -1, 166, 18, 258, -1, 26, 191, 238, 135, -1, 204, 193, 101, 15, 204, 237, 203, 111, 165, 161, 173, 106, 238, 237, 196, 80, 195, 3, -1, 53, -1, 18, 137, 203, 50, 202, 15, 18, 209, 202, 238, 193, 29, -1, 115, 104, 162, 204, 78, -1, 114, 173, 97, 57, 23, 196, -1, 192, 30, 200, -1, 26, 140, 96, 135, 72, 191, 18, 203, 114, 135, 237, -1, 2, 19, 200, 196, 237, 237, 133, 211, 238, 136, 153, 25, 104, 191, 161, -1, 239, 155, -1, 147, 80, 192, 196, -1, -1, 15, 15, 137, 15, 202, 71, 221, 15, 30, 204, 15, 213, 135, 153, 200, 202, 96, 150, 3, 104, 204, 148, 78, 155, 205, 78, 78, 15, 176, 221, 74, 18, 147, 161, 114, 26, 200, 98, 158, -1, 29, 209, 133, 191, 238, 113, 196, -1, 166, 200, 112, 238, 30, 199, -1, 240, 163, 238, -1, 23, 114, 191, 176, 151, 15, 106, 18, 221, 136, 155, 18, -1, 199, 160, 135, 203, 21, 18, 57, 137, 194, 109, 199, 200, 238, 209, -1, 176, 23, 121, 137, 203, 258, 200, 191, 111, 156, 133, 71, 15, 158, 18, 25, 203, 26, -1, 200, 3, 111, 200, 200, 102, 191, 69, 192, -1, 202, 176, 221, 23, -1, 10, -1, -1, 40, -1, 204, 25, -1, -1, 3, 21, -1, 104, -1, 15, 30, 148, 140, 96, 156, 176, 25, 194, 200, 53, 80, 204, 114, 18, -1, 19, 213, -1, 98, 78, 15, 150, -1, 191, 10, 200, -1, 18, 23, 96, 3, 29, 196, 96, 3, -1, 196, 196, 69, 71, 118, 97, 237, -1, 133, 156, 25, 23, -1, 161, -1, 72, 175, 237, 195, 25, 166, -1, 196, 258, 213, -1, 2, 78, 78, 196, 94, 56]\n",
            "[30, 237, -1, 16, 104, 237, 135, 166, 108, 15, 200, 161, 58, -1, 205, -1, 96, -1, 15, 97, 197, -1, -1, 161, 98, 3, 97, 191, -1, 196, 193, 19, 204, 18, -1, 155, 15, 114, 96, 135, 96, 69, 3, 156, 114, 191, 69, 151, -1, 202, 135, 78, 139, 175, 96, 194, 237, -1, 258, 98, -1, 104, 166, 109, 15, 25, 207, -1, 202, 18, 153, 194, 173, 25, 98, -1, -1, 161, 23, 135, 35, 160, 3, 25, 173, -1, -1, 78, 200, -1, 121, 191, -1, 121, 25, 78, 104, 3, 200, -1, 175, 153, -1, 133, 72, 237, 173, 156, 204, 21, -1, 258, 50, 71, 3, 240, -1, 153, 156, 50, 195, 194, 175, -1, 108, 71, 135, 107, 204, 106, 258, 21, 78, 147, 258, -1, 155, 19, 26, -1, 15, 22, 200, 195, -1, -1, 191, 136, 196, 258, 104, 15, -1, 175, 156, -1, -1, 200, -1, 104, 156, 191, 156, 160, -1, 50, 160, 258, 160, 179, 135, 163, -1, 98, -1, 235, -1, 237, -1, 238, 104, 50, 237, 137, -1, -1, 203, -1, 78, -1, 26, 161, 3, -1, -1, 197, 74, 78, 57, 237, 135, -1, 18, 95, 200, 196, 213, 237, -1, 191, 191, 107, 98, 237, 18, 18, -1, 193, 117, 97, 238, 250, 196, 98, 30, 176, 149, 97, 58, 114, -1, 58, -1, -1, 193, 94, 238, 15, 3, 135, 153, 213, 208, -1, 10, 10, 175, 204, 25, -1, 98, 115, 150, -1, 196, 95, 30, -1, -1, 111, 194, 165, 69, 135, 258, 196, 25, 202, 197, 191, 96, 98, 209, 94, 175, -1, 221, 98, -1, 147, 173, 78, 97, 200, 19, 26, 146, 78, -1, 135, -1, 175, -1, 50, 196, 104, 30, 165, 22, 135, 133, 22, 108, 64, -1, 221, 106, 98, 165, 78, 3, 10, 3, 165, 21, 15, 25, 200, -1, -1, -1, 133, 161, 155, 19, 195, 25, 202, 102, 19, 176, 163, 50, 26, 25, 152, 15, 200, 98, 221, -1, 207, -1, -1, 21, 202, 25, 21, 200, 197, 78, 150, 202, 197, 22, 121, 25, 19, 237, -1, -1, 78, 137, 25, -1, 196, 158, -1, 135, 109, -1, 25, 109, 109, 133, 135, 237, -1, 71, 25, 3, -1, 200, -1, -1, 237, 193, 15, 78, 40, -1, 191, -1, 176, 196, -1, 203, 209, 203, 57, 238, 93, 30, 152, 166, 97, -1, -1, 107, 133, 3, 147, 26, -1, 26, -1, 10, 172, 163, 19, 58, 25, 218, 78, -1, 22, -1, 209, 98, 18, 94, -1, -1, 18, 193, 235, 204, 155, 193, 149, 25, -1, 161, 170, 258, 19, -1, 191, 104, 238, 191, 204, 191, -1, -1, 57, 195, -1, -1, 166, 200, 193, 176, 10, 161, 18, 115, -1, -1, 166, -1, 22, 136, 135, 25, -1, -1, 196, 203, 30, -1, -1, 250, 240, 96, -1, 213, 156, -1, 21, 29, -1, 156, 163, -1, 19, 149, 18, 108, 238, 3, -1, 153, 137, -1, 50, 161, 18, 15, 250, 137, 135, 78, 3, 50, 57, 133, 192, -1, 135, 112, 163, -1, 135, -1, 238, 22, -1, 3, 58, -1, 196, 203, -1, 204, 191, 25, 211, -1, -1, -1, 166, 18, -1, -1, 26, 191, 238, 161, -1, 161, 193, 96, 15, 204, 237, 203, 3, 165, 161, 173, 106, 238, 237, 196, 80, 194, 3, -1, 58, -1, 18, 137, 203, 64, -1, 15, 18, 209, 235, 238, 193, 40, -1, 98, 104, 165, 204, 78, -1, 114, 173, 97, 57, 23, -1, -1, 192, 30, 200, -1, 26, 140, 71, 135, 50, 135, 18, 203, 114, 135, -1, -1, 3, 19, 200, 196, 237, 237, 133, 211, 258, 135, 153, 106, -1, 191, 161, -1, 155, 22, -1, 147, 80, 192, -1, -1, -1, 30, 25, -1, 26, 202, 71, 221, 15, 30, 161, 26, 213, 149, -1, 200, 3, 96, 152, 3, 104, 204, 148, 78, 155, 205, 78, -1, 15, 176, 221, 74, 18, 147, 161, 152, 26, 200, 176, 158, -1, -1, 209, 133, 191, 237, -1, 196, -1, 166, 200, 112, 15, 30, 199, -1, 258, 163, 221, -1, 23, 114, 191, 176, 151, 15, 97, 19, 221, 191, 29, 18, -1, 199, 160, 135, 203, 21, 18, 57, 137, 194, 108, 199, 200, 238, 209, -1, 176, -1, 121, 135, 203, 258, 200, 191, 112, -1, 3, -1, 15, 110, 19, 25, -1, 26, -1, 200, 3, 112, 200, 200, 94, -1, 69, 192, -1, 235, 175, 221, 23, -1, -1, -1, -1, 40, -1, 161, 25, -1, -1, 3, -1, -1, 104, -1, 15, 30, 135, 140, 71, -1, 176, 25, 194, 200, 58, 80, 204, 114, 18, -1, 19, 213, -1, 98, -1, 15, 150, -1, 237, 10, 200, -1, 18, 23, -1, 3, 139, -1, 71, 196, -1, 196, 196, 155, 22, 117, 97, 258, -1, 133, 155, 25, 23, -1, 204, -1, 30, 2, 237, 195, 23, 237, -1, 196, 258, 215, -1, 175, 78, 78, 196, 94, 57]\n",
            "[30, 237, -1, 16, 104, 203, 135, 166, 108, 15, -1, 161, 58, -1, 205, -1, 96, -1, 15, 97, 197, -1, -1, 161, 98, 3, 97, 191, -1, 196, 193, 19, 204, -1, -1, 30, 15, 114, 96, 135, -1, 69, 58, 160, 114, 191, -1, 151, -1, 202, 135, 78, 22, 175, 96, 237, 258, -1, 135, 238, -1, 104, 166, 109, 15, -1, 108, -1, 238, 18, 153, -1, 173, 25, 98, -1, -1, 161, 23, 135, 69, -1, 3, -1, 173, -1, -1, -1, 3, -1, 104, 191, -1, -1, -1, 78, 104, 3, 200, -1, -1, 153, -1, 133, 72, -1, 173, 156, 204, 21, -1, 258, 30, 71, 3, 258, -1, 153, 106, -1, 195, -1, 172, -1, 109, 15, 149, 107, 204, 106, 258, 21, 3, -1, 258, -1, 15, 18, 26, -1, 15, 22, 200, 197, -1, -1, 191, 135, -1, 238, 104, 15, -1, 175, 156, -1, -1, -1, -1, 104, 158, 191, 156, 160, -1, -1, 133, 193, 160, 238, 135, 163, -1, 3, -1, -1, -1, 237, -1, -1, 135, -1, 237, 160, -1, -1, 203, -1, 78, -1, -1, 204, 3, -1, -1, 238, -1, 78, 57, 237, 135, -1, 18, -1, 200, 196, 213, 258, -1, 191, -1, 98, 114, 237, 18, 58, -1, 193, 117, 97, 238, 238, 161, 98, 30, 176, 135, 97, 18, 114, -1, 58, -1, -1, 193, -1, 238, -1, 172, 135, 69, 213, -1, -1, 10, 10, 175, 204, 23, -1, 258, 115, 150, -1, 238, 95, 30, -1, -1, 111, 197, 165, 69, 135, 114, 196, 25, 202, -1, 191, 96, 98, 209, -1, 175, -1, 221, 98, -1, 147, 98, 78, 97, 200, 19, 26, 258, -1, -1, 135, -1, 175, -1, 15, 238, 104, 30, 165, 22, 135, 98, 22, 109, 64, -1, 221, 161, 147, 203, 78, 172, 29, 3, 165, 21, 15, 23, 200, -1, -1, -1, 158, 161, 10, 19, -1, 25, 202, 102, 18, -1, 163, 29, 26, 15, 152, -1, 200, 98, 221, -1, 207, -1, -1, -1, 3, 208, -1, 200, 195, 78, 150, 135, 238, 29, -1, 25, 19, 237, -1, -1, 78, 137, 25, -1, 196, 158, -1, -1, 109, -1, 25, -1, 109, 133, 135, 237, -1, 71, 25, 3, -1, -1, -1, -1, 237, 193, -1, 78, 40, -1, -1, -1, 173, 196, -1, 203, 191, 203, 69, 238, 93, 30, 151, 166, 97, -1, -1, 106, 196, 3, 147, 26, -1, 26, -1, 10, 172, 163, -1, 58, 25, 3, 78, -1, 22, -1, 209, 200, 18, 94, -1, -1, 165, 193, 207, 204, 15, 193, 149, 25, -1, 161, 170, 258, 19, -1, -1, 104, 238, 191, 204, 191, -1, -1, 57, 195, -1, -1, 166, -1, 193, 176, 40, 161, 18, 121, -1, -1, 166, -1, 22, 135, 135, 25, -1, -1, 196, -1, 30, -1, -1, 196, 258, 96, -1, 238, 133, -1, -1, 29, -1, 15, 163, -1, 19, 149, 18, 109, 238, 3, -1, 155, -1, -1, 15, -1, 18, 15, 13, 137, 22, 78, 3, 237, 57, 196, 192, -1, 135, 112, 163, -1, 135, -1, 238, 22, -1, 191, 58, -1, 196, -1, -1, 204, 191, 25, 191, -1, -1, -1, 114, 18, -1, -1, 26, 238, 238, 196, -1, 204, -1, 96, 15, 204, 237, 203, 112, 165, 161, 173, 106, 238, 258, 196, -1, 258, 3, -1, 140, -1, 18, 137, 203, 69, -1, 15, -1, 209, 160, 238, 193, 22, -1, 98, -1, 238, 204, 78, -1, 114, 173, 97, 57, 23, -1, -1, 192, 30, 200, -1, 26, -1, 96, 135, 72, 191, 18, 203, 114, 135, -1, -1, 172, 19, 200, 196, 237, 98, 133, 211, 238, 208, 153, 25, -1, 191, 161, -1, 133, 22, -1, -1, 193, 192, -1, -1, -1, 30, 25, -1, 15, -1, 71, 221, -1, 30, 204, 15, 238, 149, -1, 200, 3, 96, 151, 2, 104, 204, 148, 78, 15, 58, 78, -1, 15, 175, 221, 74, 18, 147, 161, 114, 26, 200, 98, -1, -1, -1, 209, 133, -1, 158, -1, 195, -1, 166, 200, 112, 238, 30, 199, -1, 258, 163, 238, -1, 155, 114, 191, -1, 150, 15, -1, 18, 221, 238, 29, 18, -1, 199, 160, 135, 203, 21, 18, 57, 137, 197, 109, 199, 200, 238, 209, -1, 176, -1, 121, 135, -1, 258, 200, 191, 112, -1, 258, -1, 15, 158, 18, 18, -1, 26, -1, 200, 3, 3, 200, 200, 94, -1, 69, 192, -1, 202, 175, 221, -1, -1, -1, -1, -1, 40, -1, 204, 25, -1, -1, 3, -1, -1, 104, -1, 15, 30, 135, 140, 96, -1, 173, -1, -1, 200, -1, -1, 204, 114, -1, -1, 19, 238, -1, 98, -1, 15, 151, -1, 191, -1, 200, -1, 18, 23, -1, 3, 165, -1, 71, 3, -1, 196, 235, 69, 22, 118, 97, -1, -1, 133, 133, 25, 23, -1, 204, -1, 72, 172, 238, 238, 23, 15, -1, 196, 258, 238, -1, 172, 78, 78, 196, -1, 69]\n",
            "[30, 196, -1, 16, 104, 203, 135, 166, -1, 15, -1, 161, 197, -1, 140, -1, 133, -1, 15, 97, -1, -1, -1, 161, 98, 3, 97, 191, -1, 196, 193, 19, 204, -1, -1, 158, 15, 114, 96, 135, -1, 69, 191, -1, 114, 191, -1, 151, -1, -1, 135, 78, 22, 175, 96, -1, -1, -1, 258, 238, -1, 104, 166, 109, 15, -1, 108, -1, -1, -1, 153, -1, -1, 25, 98, -1, -1, -1, 23, 135, 204, -1, 3, -1, 173, -1, -1, -1, -1, -1, 121, 203, -1, -1, -1, 78, -1, 135, 200, -1, -1, 153, -1, 133, 158, -1, -1, 156, 204, 21, -1, 158, 69, 38, 3, 258, -1, 135, 259, -1, 197, -1, -1, -1, 109, 35, 135, 107, 204, 106, 258, 21, 78, -1, 258, -1, 15, 18, 26, -1, -1, 23, 3, 197, -1, -1, 192, 135, -1, 196, 104, 23, -1, -1, -1, -1, -1, -1, -1, 104, -1, 191, 156, -1, -1, -1, 133, 193, 160, 176, 135, 163, -1, 3, -1, -1, -1, 237, -1, -1, 104, -1, 237, 137, -1, -1, 203, -1, -1, -1, -1, 204, 238, -1, -1, -1, -1, 78, 57, 237, 135, -1, 18, -1, -1, -1, 213, 237, -1, 192, -1, 107, 114, 237, 18, 197, -1, 193, 117, 97, 3, 238, 161, 98, 30, 176, 135, -1, -1, 114, -1, 58, -1, -1, 193, -1, 213, -1, 112, 135, 69, 213, -1, -1, 10, 10, 175, 204, 23, -1, 133, 115, 78, -1, 204, 95, -1, -1, -1, 112, 197, 165, 69, 135, 166, -1, 25, 202, -1, 191, -1, 133, 191, -1, 175, -1, 221, 98, -1, -1, 173, 78, 97, 200, 19, -1, 133, -1, -1, 135, -1, 175, -1, 70, 3, 104, 30, 165, 22, 140, -1, 22, 109, 64, -1, 221, -1, 147, 165, 78, 175, 155, -1, 165, 158, 15, -1, 200, -1, -1, -1, 133, 109, -1, 19, -1, -1, 202, 102, -1, -1, 163, 155, 26, -1, -1, -1, 200, 98, 221, -1, 207, -1, -1, -1, 3, 166, -1, 118, 197, 78, 150, 135, 238, 40, -1, 25, 19, 258, -1, -1, 78, 137, 25, -1, 196, -1, -1, -1, 109, -1, 25, -1, 109, 133, 135, 166, -1, 50, 25, 3, -1, -1, -1, -1, 258, 193, -1, 78, 40, -1, -1, -1, 2, 117, -1, 203, 209, 203, 57, 78, 93, 30, 151, 107, 97, -1, -1, 106, 133, 195, 147, 26, -1, 26, -1, 10, 175, 163, -1, 148, 25, 218, 78, -1, 22, -1, 209, 200, 18, 94, -1, -1, 165, 193, 207, 204, 156, 193, -1, 25, -1, 161, 170, 258, 19, -1, -1, -1, 238, 135, 204, 191, -1, -1, 57, -1, -1, -1, 166, -1, 193, 2, 40, 161, -1, 104, -1, -1, 166, -1, -1, 135, 135, 25, -1, -1, 196, -1, 30, -1, -1, 196, 3, 96, -1, 238, 133, -1, -1, 140, -1, 158, 163, -1, 19, 149, 197, 204, -1, 173, -1, 154, -1, -1, 50, -1, 18, -1, 135, 137, 135, 78, 3, 29, 69, 133, -1, -1, 135, 112, 163, -1, 135, -1, 238, 22, -1, 191, 58, -1, 237, -1, -1, 204, 191, -1, -1, -1, -1, -1, 166, 18, -1, -1, 26, 238, 238, 196, -1, 204, -1, 18, -1, 204, 114, 203, 111, 165, 161, 173, 106, 238, 237, 196, -1, -1, 3, -1, 64, -1, 19, 137, 203, 35, -1, 15, -1, 209, 202, 112, 193, 40, -1, 98, -1, 165, 204, 78, -1, 155, -1, 97, 57, 23, -1, -1, 192, 30, 200, -1, -1, -1, 38, 196, 72, 161, 18, 203, 114, 135, -1, -1, -1, -1, 200, 196, 237, 237, 133, 211, -1, 135, 135, 25, -1, 192, 161, -1, 156, -1, -1, -1, 3, 192, -1, -1, -1, 15, 25, -1, -1, -1, 71, 221, -1, 30, -1, 26, 213, 149, -1, 200, 3, 96, 150, 3, -1, 204, 148, 78, 155, 205, -1, -1, 15, 175, 221, 74, 18, 147, 161, 114, 26, 200, 2, -1, -1, -1, 209, 133, -1, 96, -1, 197, -1, 166, -1, 112, -1, 30, 199, -1, -1, 163, 221, -1, 23, 114, 191, -1, 151, 15, -1, 19, -1, 3, 40, 18, -1, 199, 160, 135, 203, 21, 18, 57, 137, 197, 106, 199, 200, 238, 209, -1, -1, -1, 121, 135, -1, 258, 200, 191, -1, -1, 133, -1, 15, 158, 18, 25, -1, 26, -1, 200, 3, 112, 200, 200, 94, -1, 69, 192, -1, 202, -1, 221, -1, -1, -1, -1, -1, 40, -1, -1, 25, -1, -1, 3, -1, -1, 104, -1, 15, -1, 148, 140, -1, -1, 2, -1, -1, 78, -1, -1, 204, 114, -1, -1, 19, -1, -1, 98, -1, 15, 151, -1, -1, -1, 200, -1, 18, 23, -1, 3, 165, -1, -1, 3, -1, 196, 196, 69, 40, -1, 97, -1, -1, 133, 156, 155, 23, -1, 204, -1, 72, 2, 258, 197, 25, 166, -1, 196, 258, 213, -1, 2, 78, 78, -1, -1, -1]\n",
            "[15, 196, -1, 16, 104, 203, 135, 166, -1, 15, -1, 161, 197, -1, 202, -1, 96, -1, 15, 97, -1, -1, -1, 161, 98, 202, 97, 191, -1, 196, 193, 19, 204, -1, -1, -1, -1, 114, 98, 135, -1, 69, 191, -1, -1, -1, -1, 151, -1, -1, 135, 78, 40, 176, 96, -1, -1, -1, 258, 98, -1, 104, -1, 109, 15, -1, 108, -1, -1, -1, 153, -1, -1, 25, 98, -1, -1, -1, -1, 135, 69, -1, -1, -1, -1, -1, -1, -1, -1, -1, 121, 191, -1, -1, -1, 78, -1, 3, 200, -1, -1, -1, -1, 133, 15, -1, -1, 18, 204, -1, -1, 258, 30, 35, 3, 195, -1, 154, -1, -1, 197, -1, -1, -1, 109, 237, 135, -1, 204, 106, 161, 21, -1, -1, 258, -1, 15, 19, 26, -1, -1, -1, 3, 197, -1, -1, 192, 136, -1, 258, 104, 15, -1, -1, -1, -1, -1, -1, -1, 104, -1, 191, -1, -1, -1, -1, 198, 258, 258, 176, 135, 163, -1, 107, -1, -1, -1, 237, -1, -1, 104, -1, 195, -1, -1, -1, 203, -1, -1, -1, -1, 204, 204, -1, -1, -1, -1, -1, 57, -1, 133, -1, 18, -1, -1, -1, -1, 133, -1, 191, -1, -1, 114, 258, 18, 202, -1, 193, -1, 97, 238, 238, 196, 98, -1, 176, 135, -1, -1, 114, -1, 70, -1, -1, 193, -1, -1, -1, -1, 135, 70, 213, -1, -1, 10, -1, 176, 204, 25, -1, 98, 115, 79, -1, -1, 95, -1, -1, -1, 111, 197, 165, 69, 135, 166, -1, 25, 202, -1, -1, -1, 98, 209, -1, -1, -1, 221, 98, -1, -1, -1, 78, 97, 200, 19, -1, 98, -1, -1, 135, -1, 175, -1, -1, 3, 104, 30, 165, -1, -1, -1, 22, 109, 238, -1, 238, -1, 118, 196, -1, -1, 15, -1, 165, 21, 15, -1, 200, -1, -1, -1, 133, 109, -1, 19, -1, -1, 202, 102, -1, -1, 163, 29, 26, -1, -1, -1, 200, 98, 238, -1, 170, -1, -1, -1, -1, 166, -1, -1, -1, -1, 151, 202, 238, 40, -1, 25, 19, 258, -1, -1, 78, 137, -1, -1, 196, -1, -1, -1, 109, -1, 25, -1, -1, -1, 135, 237, -1, 196, 25, 3, -1, -1, -1, -1, 258, 193, -1, 78, 40, -1, -1, -1, 173, -1, -1, 203, 191, -1, 69, 238, 102, -1, 152, 166, 97, -1, -1, 106, 133, 196, 147, 26, -1, 26, -1, 10, -1, 163, -1, 191, 25, 3, 78, -1, -1, -1, 209, 200, -1, 94, -1, -1, 165, -1, 207, 204, 133, 193, -1, 25, -1, 204, -1, -1, -1, -1, -1, -1, 238, 191, 204, 191, -1, -1, 57, -1, -1, -1, 166, -1, -1, 176, -1, 161, -1, 104, -1, -1, 166, -1, -1, 136, 135, 25, -1, -1, 196, -1, 40, -1, -1, 258, 258, 96, -1, -1, 196, -1, -1, 29, -1, 156, 163, -1, 19, -1, 57, 109, -1, 2, -1, 70, -1, -1, 30, -1, 18, -1, 135, 137, 22, 78, 3, 29, 69, 133, -1, -1, 135, 112, 163, -1, 135, -1, 238, 22, -1, 171, -1, -1, 196, -1, -1, 204, 191, -1, -1, -1, -1, -1, 166, 18, -1, -1, 26, 238, 238, 196, -1, 204, -1, 149, -1, -1, -1, 203, 3, 165, 161, 173, 106, 238, -1, 196, -1, -1, 238, -1, 191, -1, 18, 137, 203, 30, -1, 15, -1, 209, 160, 238, 193, 40, -1, 121, -1, 238, 204, 78, -1, 158, -1, 97, 202, 23, -1, -1, 192, 30, 200, -1, -1, -1, 96, 135, 72, 191, 18, 203, 114, 135, -1, -1, -1, -1, 200, -1, -1, -1, 133, 211, -1, 136, 154, -1, -1, 192, 204, -1, 15, -1, -1, -1, 3, 192, -1, -1, -1, 30, 25, -1, -1, -1, 50, 238, -1, 30, -1, 26, 213, 149, -1, 200, 3, 96, 152, 2, -1, 204, 148, 78, 35, 70, -1, -1, 15, 176, 238, 4, 18, 147, -1, 114, 26, 200, 176, -1, -1, -1, -1, -1, -1, 98, -1, 197, -1, 166, -1, 112, -1, 30, -1, -1, -1, 163, 238, -1, -1, 114, 191, -1, 151, 15, -1, 19, -1, 238, 40, 18, -1, 199, 160, 135, 203, 21, 18, 57, 137, 197, 109, 199, -1, 238, 209, -1, -1, -1, 96, 135, -1, 258, -1, 191, -1, -1, 133, -1, 15, 69, 18, 25, -1, 35, -1, 200, 3, -1, 200, 200, 94, -1, -1, 192, -1, -1, -1, -1, -1, -1, -1, -1, -1, 40, -1, -1, 25, -1, -1, 3, -1, -1, 104, -1, 15, -1, 148, 140, -1, -1, 176, -1, -1, -1, -1, -1, 204, 114, -1, -1, 19, -1, -1, 98, -1, 15, 151, -1, -1, -1, 200, -1, 18, 23, -1, 3, 29, -1, -1, 3, -1, 196, 238, 69, 40, -1, 97, -1, -1, 133, 133, -1, -1, -1, 204, -1, 72, 2, 258, 195, 25, 237, -1, 196, 258, 192, -1, 175, -1, 78, -1, -1, -1]\n",
            "0.5095011876484561\n",
            "[30, 237, 137, 50, 104, 203, 149, 166, 108, 15, 200, 161, 191, 156, 205, 176, 96, 156, 239, 97, 197, 241, 133, 161, 98, 3, 97, 191, 21, 196, 193, 19, 204, 64, 2, 30, 15, 114, 96, 135, 96, 69, 58, 115, 114, 191, 69, 151, 161, 202, 135, 78, 22, 175, 96, 196, 264, 125, 258, 238, 153, 104, 166, 109, 15, 25, 108, 18, 202, 64, 153, 194, 173, 25, 98, 96, 241, 161, 23, 135, 204, 76, 3, 25, 173, 238, 239, 78, 80, 71, 121, 203, 26, 121, 25, 50, 104, 202, 200, 25, 175, 153, 74, 133, 72, 259, 173, 156, 204, 21, 3, 239, 50, 26, 3, 240, 135, 153, 160, 50, 196, 197, 175, 50, 109, 237, 149, 107, 204, 106, 235, 21, 200, 118, 258, 208, 15, 18, 150, 94, 15, 22, 200, 237, 25, 153, 191, 135, 196, 258, 104, 23, 137, 175, 156, 0, 149, 200, 191, 104, 239, 191, 156, 156, 78, 50, 133, 193, 160, 176, 135, 163, 2, 107, 195, 235, 153, 237, 78, 175, 104, 50, 237, 137, 202, 191, 203, 258, 78, 10, 26, 204, 3, 10, 239, 238, 74, 78, 57, 237, 137, 239, 18, 95, 200, 196, 215, 237, 125, 192, 191, 98, 114, 165, 19, 58, 109, 193, 117, 97, 238, 238, 196, 98, 30, 238, 149, 97, 155, 114, 96, 58, 241, 25, 193, 94, 238, 15, 3, 135, 64, 213, 208, 193, 10, 10, 175, 204, 23, 241, 98, 115, 150, 29, 235, 95, 30, 25, 241, 115, 197, 165, 69, 135, 237, 133, 25, 202, 197, 191, 96, 237, 209, 94, 175, 19, 221, 98, 104, 147, 156, 78, 97, 200, 19, 26, 146, 78, 25, 135, 96, 175, 153, 50, 3, 104, 30, 165, 22, 140, 237, 22, 109, 64, 3, 218, 161, 147, 165, 78, 175, 29, 207, 165, 21, 15, 23, 200, 241, 140, 241, 133, 161, 155, 19, 238, 25, 202, 102, 19, 176, 163, 50, 26, 25, 152, 15, 200, 98, 218, 22, 207, 173, 139, 21, 3, 133, 21, 115, 195, 62, 151, 135, 238, 40, 121, 25, 19, 264, 204, 16, 78, 137, 25, 153, 196, 158, 135, 140, 109, 237, 23, 109, 109, 133, 135, 166, 96, 194, 25, 3, 193, 200, 200, 50, 237, 193, 15, 78, 40, 156, 191, 215, 176, 239, 200, 203, 191, 203, 57, 238, 102, 30, 151, 166, 96, 149, 118, 106, 133, 195, 147, 26, 135, 26, 264, 10, 175, 163, 149, 200, 25, 3, 78, 125, 22, 156, 209, 200, 18, 94, 150, 96, 165, 193, 207, 204, 156, 193, 149, 25, 102, 161, 170, 258, 19, 96, 191, 104, 238, 191, 204, 191, 101, 78, 57, 195, 30, 241, 166, 200, 193, 176, 40, 161, 18, 115, 149, 163, 166, 78, 22, 135, 135, 25, 153, 21, 196, 203, 30, 264, 172, 196, 160, 96, 241, 215, 133, 218, 239, 29, 153, 156, 163, 113, 19, 149, 30, 108, 98, 2, 200, 154, 137, 26, 50, 161, 18, 239, 21, 137, 140, 78, 3, 50, 57, 133, 192, 237, 135, 112, 163, 133, 135, 153, 238, 22, 21, 175, 58, 18, 237, 192, 98, 204, 191, 25, 208, 26, 161, 200, 166, 18, 2, 15, 26, 238, 238, 196, 4, 204, 193, 101, 15, 204, 239, 203, 112, 165, 161, 173, 106, 238, 237, 196, 80, 195, 196, 241, 58, 96, 18, 137, 203, 64, 202, 15, 18, 209, 202, 238, 193, 22, 15, 121, 104, 165, 204, 78, 155, 155, 173, 241, 155, 23, 196, 196, 192, 30, 200, 74, 26, 140, 96, 238, 72, 196, 18, 203, 114, 135, 237, 235, 2, 19, 200, 196, 237, 237, 133, 211, 238, 78, 153, 22, 104, 192, 204, 113, 133, 22, 153, 147, 80, 192, 239, 191, 239, 15, 29, 137, 26, 202, 71, 221, 15, 30, 204, 26, 213, 149, 153, 200, 235, 96, 151, 2, 104, 204, 148, 79, 155, 205, 78, 78, 23, 176, 221, 74, 18, 147, 161, 152, 26, 200, 238, 155, 191, 40, 209, 133, 191, 133, 113, 196, 194, 166, 200, 112, 238, 30, 175, 3, 258, 163, 3, 15, 23, 114, 191, 176, 151, 15, 97, 19, 221, 208, 40, 18, 96, 199, 160, 135, 203, 21, 18, 57, 137, 194, 108, 199, 200, 215, 209, 149, 176, 23, 96, 137, 203, 235, 200, 191, 112, 156, 133, 193, 15, 158, 18, 25, 203, 26, 112, 200, 3, 80, 200, 200, 94, 191, 69, 192, 133, 202, 175, 218, 23, 208, 10, 130, 101, 40, 72, 161, 25, 200, 125, 3, 21, 241, 104, 153, 15, 30, 205, 121, 35, 156, 176, 25, 195, 200, 58, 80, 204, 114, 18, 173, 19, 213, 104, 98, 79, 15, 151, 218, 191, 10, 200, 194, 18, 23, 96, 3, 139, 196, 96, 3, 196, 196, 160, 35, 22, 118, 97, 258, 2, 133, 133, 28, 23, 78, 204, 0, 30, 2, 237, 195, 23, 237, 80, 196, 258, 215, 26, 2, 78, 78, 196, 94, 160]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "final_pred_0=[]\n",
        "\n",
        "print(new_val_fnum_0)\n",
        "print(new_val_fnum_1)\n",
        "print(new_val_fnum_2)\n",
        "print(new_val_fnum_3)\n",
        "print(new_val_fnum_4)\n",
        "print(new_val_fnum_5)\n",
        "\n",
        "num_correct=0\n",
        "model_0_0=load_model(loss,accuracy,optimizer,'0-0')\n",
        "mod_labels_0=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_0:\n",
        "    pred_test_0_0=model_0_0.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_0_0 = pred_test_0_0.argmax(axis=1)\n",
        "    mod_labels_0.append(pred_labels_0_0[0])\n",
        "  else:\n",
        "    mod_labels_0.append(-1)\n",
        "\n",
        "model_1_0=load_model(loss,accuracy,optimizer,'1-0')\n",
        "mod_labels_1=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        " \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_1:\n",
        "    pred_test_1_0=model_1_0.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_1_0 = pred_test_1_0.argmax(axis=1)\n",
        "    mod_labels_1.append(pred_labels_1_0[0])\n",
        "  else:\n",
        "    mod_labels_1.append(-1)\n",
        "\n",
        "model_2_0=load_model(loss,accuracy,optimizer,'2-0')\n",
        "mod_labels_2=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_2:\n",
        "    pred_test_2_0=model_2_0.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_2_0 = pred_test_2_0.argmax(axis=1)\n",
        "    mod_labels_2.append(pred_labels_2_0[0])\n",
        "  else:\n",
        "    mod_labels_2.append(-1)\n",
        "\n",
        "model_3_0=load_model(loss,accuracy,optimizer,'3-0')\n",
        "mod_labels_3=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_3:\n",
        "    pred_test_3_0=model_3_0.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_3_0 = pred_test_3_0.argmax(axis=1)\n",
        "    mod_labels_3.append(pred_labels_3_0[0])\n",
        "  else:\n",
        "    mod_labels_3.append(-1)\n",
        "\n",
        "model_4_0=load_model(loss,accuracy,optimizer,'4-0')\n",
        "mod_labels_4=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_4:\n",
        "    pred_test_4_0=model_4_0.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_4_0 = pred_test_4_0.argmax(axis=1)\n",
        "    mod_labels_4.append(pred_labels_4_0[0])\n",
        "  else:\n",
        "    mod_labels_4.append(-1)\n",
        "\n",
        "model_5_0=load_model(loss,accuracy,optimizer,'5-0')\n",
        "mod_labels_5=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_5:\n",
        "    pred_test_5_0=model_5_0.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_5_0 = pred_test_5_0.argmax(axis=1)\n",
        "    mod_labels_5.append(pred_labels_5_0[0])\n",
        "  else:\n",
        "    mod_labels_5.append(-1)\n",
        "\n",
        "print(mod_labels_0)\n",
        "print(mod_labels_1)\n",
        "print(mod_labels_2)\n",
        "print(mod_labels_3)\n",
        "print(mod_labels_4)\n",
        "print(mod_labels_5)\n",
        "\n",
        "\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  fin_labels=[]\n",
        "  if mod_labels_0[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  if mod_labels_1[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  if mod_labels_2[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  if mod_labels_3[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  if mod_labels_4[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  if mod_labels_5[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  res=max(set(fin_labels), key = fin_labels.count)\n",
        "  final_pred_0.append(res)\n",
        "\n",
        "  if res==new_val_label_0[i]:\n",
        "    num_correct=num_correct+1\n",
        "\n",
        "accuracy=num_correct/len(new_val_label_0)\n",
        "print(accuracy)\n",
        "  \n",
        "print(final_pred_0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSrkJ8k6oBkG",
        "outputId": "9f040573-b6b0-4953-fe89-91ca0cb8d2eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.5095011876484561\n",
            "Weighted F1: 0.4570082423899872\n",
            "Micro F1: 0.5095011876484561\n",
            "Weighted Precision: 0.4435610668094691\n",
            "Micro Precision: 0.5095011876484561\n",
            "Weighted Recall: 0.5095011876484561\n",
            "Micro Recall: 0.5095011876484561\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "total_accuracy=0\n",
        "total_weighted_f1=0\n",
        "total_micro_f1=0\n",
        "total_weighted_precision=0\n",
        "total_micro_precision=0\n",
        "total_weighted_recall=0\n",
        "total_micro_recall=0\n",
        "\n",
        "accuracy=accuracy_score(new_val_label_0, final_pred_0)\n",
        "print(\"Accuracy: \"+str(accuracy))\n",
        "total_accuracy=total_accuracy+accuracy\n",
        "\n",
        "weighted_f1=f1_score(new_val_label_0,final_pred_0, average='weighted')\n",
        "print(\"Weighted F1: \"+ str(weighted_f1))\n",
        "total_weighted_f1=total_weighted_f1+weighted_f1\n",
        "micro_f1=f1_score(new_val_label_0,final_pred_0, average='micro')\n",
        "print(\"Micro F1: \"+ str(micro_f1))\n",
        "total_micro_f1=total_micro_f1+micro_f1\n",
        "\n",
        "weighted_precision=precision_score(new_val_label_0, final_pred_0, average='weighted')\n",
        "print(\"Weighted Precision: \" + str(weighted_precision))\n",
        "total_weighted_precision=total_weighted_precision+weighted_precision\n",
        "micro_precision=precision_score(new_val_label_0, final_pred_0, average='micro')\n",
        "print(\"Micro Precision: \" + str(micro_precision))\n",
        "total_micro_precision=total_micro_precision+micro_precision\n",
        "\n",
        "weighted_recall=recall_score(new_val_label_0, final_pred_0, average='weighted')\n",
        "print(\"Weighted Recall: \" + str(weighted_recall))\n",
        "total_weighted_recall=total_weighted_recall+weighted_recall\n",
        "micro_recall=recall_score(new_val_label_0, final_pred_0, average='micro')\n",
        "print(\"Micro Recall: \" + str(micro_recall))\n",
        "total_micro_recall=total_micro_recall+micro_recall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9SEXW7xUkmvH",
        "outputId": "b1c54999-2d79-4369-d1cd-e07caf4d6084"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_8\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_17 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_18 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_17[0][0]',               \n",
            " el)                            thPoolingAndCrossAt               'input_18[0][0]']               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_8 (Sl  (None, 768)         0           ['tf_roberta_model[8][0]']       \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_16 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_8[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_45 (Dropout)           (None, 512)          0           ['dense_16[0][0]']               \n",
            "                                                                                                  \n",
            " dense_17 (Dense)               (None, 279)          143127      ['dropout_45[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model: \"model_9\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_19 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_20 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_19[0][0]',               \n",
            " el)                            thPoolingAndCrossAt               'input_20[0][0]']               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_9 (Sl  (None, 768)         0           ['tf_roberta_model[9][0]']       \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_18 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_9[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_46 (Dropout)           (None, 512)          0           ['dense_18[0][0]']               \n",
            "                                                                                                  \n",
            " dense_19 (Dense)               (None, 279)          143127      ['dropout_46[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model: \"model_10\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_21 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_22 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_21[0][0]',               \n",
            " el)                            thPoolingAndCrossAt               'input_22[0][0]']               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_10 (S  (None, 768)         0           ['tf_roberta_model[10][0]']      \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " dense_20 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_10[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dropout_47 (Dropout)           (None, 512)          0           ['dense_20[0][0]']               \n",
            "                                                                                                  \n",
            " dense_21 (Dense)               (None, 279)          143127      ['dropout_47[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model: \"model_11\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_23 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_24 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_23[0][0]',               \n",
            " el)                            thPoolingAndCrossAt               'input_24[0][0]']               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_11 (S  (None, 768)         0           ['tf_roberta_model[11][0]']      \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " dense_22 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_11[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dropout_48 (Dropout)           (None, 512)          0           ['dense_22[0][0]']               \n",
            "                                                                                                  \n",
            " dense_23 (Dense)               (None, 279)          143127      ['dropout_48[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model: \"model_12\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_25 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_26 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_25[0][0]',               \n",
            " el)                            thPoolingAndCrossAt               'input_26[0][0]']               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_12 (S  (None, 768)         0           ['tf_roberta_model[12][0]']      \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " dense_24 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_12[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dropout_49 (Dropout)           (None, 512)          0           ['dense_24[0][0]']               \n",
            "                                                                                                  \n",
            " dense_25 (Dense)               (None, 279)          143127      ['dropout_49[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model: \"model_13\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_27 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_28 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_27[0][0]',               \n",
            " el)                            thPoolingAndCrossAt               'input_28[0][0]']               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_13 (S  (None, 768)         0           ['tf_roberta_model[13][0]']      \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " dense_26 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_13[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dropout_50 (Dropout)           (None, 512)          0           ['dense_26[0][0]']               \n",
            "                                                                                                  \n",
            " dense_27 (Dense)               (None, 279)          143127      ['dropout_50[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "[30, 237, 137, 16, 104, 203, 149, 166, 108, 17, 200, 161, 58, 156, 205, 176, 133, 156, 239, 97, 197, 241, 237, 161, 98, 3, 97, 191, 21, 196, 193, 19, 204, 70, 135, 30, 15, 133, 96, 135, 96, 69, 3, 155, 13, 191, 69, 151, 161, 202, 135, 78, 29, 175, 96, 194, 258, 125, 3, 98, 153, 104, 166, 109, 15, 40, 108, 18, 202, 10, 153, 194, 173, 25, 98, 96, 241, 161, 23, 135, 155, 76, 3, 25, 173, 238, 239, 78, 80, 71, 121, 191, 69, 121, 25, 62, 104, 3, 200, 25, 175, 153, 74, 133, 72, 238, 2, 161, 204, 21, 166, 110, 155, 71, 3, 195, 135, 153, 110, 50, 196, 197, 175, 62, 109, 237, 149, 107, 204, 106, 258, 21, 196, 147, 258, 208, 15, 18, 26, 94, 15, 23, 3, 195, 25, 241, 191, 135, 196, 258, 104, 23, 137, 175, 133, 202, 149, 200, 191, 104, 239, 191, 80, 202, 78, 50, 133, 193, 160, 176, 135, 163, 2, 3, 195, 3, 153, 237, 78, 176, 104, 50, 237, 133, 202, 191, 203, 258, 78, 10, 26, 204, 3, 10, 125, 238, 74, 78, 57, 237, 133, 239, 18, 95, 200, 196, 213, 237, 25, 191, 191, 98, 114, 264, 19, 58, 109, 193, 117, 97, 238, 175, 196, 98, 30, 176, 135, 97, 155, 114, 21, 58, 241, 25, 193, 94, 213, 15, 2, 135, 155, 213, 208, 193, 10, 10, 175, 204, 23, 129, 147, 115, 79, 22, 133, 95, 30, 155, 241, 111, 197, 165, 69, 135, 166, 133, 25, 202, 197, 191, 96, 98, 209, 94, 175, 19, 221, 98, 104, 147, 21, 78, 97, 78, 19, 26, 157, 78, 25, 135, 96, 175, 153, 50, 3, 104, 30, 165, 22, 140, 98, 22, 109, 64, 3, 221, 106, 147, 165, 78, 175, 40, 3, 165, 21, 15, 23, 200, 241, 140, 241, 133, 161, 133, 19, 238, 25, 202, 102, 19, 176, 163, 50, 26, 155, 152, 15, 200, 98, 221, 22, 207, 173, 22, 21, 3, 3, 21, 118, 195, 62, 150, 202, 238, 50, 121, 25, 19, 237, 204, 16, 78, 137, 25, 40, 196, 158, 140, 135, 109, 237, 25, 109, 109, 133, 135, 166, 239, 71, 25, 3, 193, 200, 200, 155, 237, 193, 15, 78, 40, 202, 191, 213, 176, 135, 200, 203, 191, 203, 69, 191, 102, 30, 151, 166, 237, 149, 147, 106, 133, 3, 147, 26, 135, 26, 133, 10, 175, 163, 155, 133, 15, 218, 78, 125, 22, 166, 209, 200, 18, 94, 150, 239, 29, 193, 207, 204, 155, 193, 149, 25, 102, 161, 170, 258, 19, 96, 191, 104, 3, 191, 204, 191, 101, 78, 57, 195, 30, 125, 166, 200, 193, 176, 40, 161, 18, 115, 149, 163, 166, 78, 22, 135, 135, 25, 153, 21, 196, 203, 30, 259, 140, 13, 115, 96, 241, 215, 133, 221, 21, 29, 153, 50, 163, 113, 19, 149, 58, 109, 238, 2, 200, 154, 137, 26, 50, 161, 18, 239, 135, 137, 135, 78, 3, 50, 57, 133, 192, 50, 135, 112, 163, 133, 135, 153, 238, 22, 50, 2, 58, 18, 239, 192, 98, 204, 191, 25, 191, 26, 161, 200, 166, 18, 2, 10, 26, 238, 238, 196, 4, 204, 193, 149, 15, 204, 239, 203, 3, 165, 161, 173, 106, 238, 237, 196, 80, 133, 238, 241, 53, 102, 19, 137, 203, 64, 202, 15, 18, 209, 160, 238, 193, 22, 15, 121, 104, 165, 204, 78, 158, 155, 173, 97, 57, 23, 196, 133, 192, 30, 200, 74, 26, 140, 38, 205, 72, 58, 18, 203, 114, 135, 237, 13, 239, 19, 200, 196, 237, 237, 133, 211, 238, 136, 153, 155, 104, 191, 161, 113, 133, 22, 40, 147, 80, 192, 239, 191, 235, 30, 25, 137, 26, 202, 71, 221, 117, 30, 204, 26, 213, 135, 153, 200, 3, 96, 150, 2, 104, 204, 148, 79, 155, 205, 78, 78, 23, 176, 221, 74, 18, 147, 161, 152, 26, 200, 176, 155, 191, 40, 209, 133, 191, 133, 113, 196, 194, 166, 3, 112, 57, 30, 199, 3, 258, 163, 191, 15, 23, 114, 191, 176, 151, 15, 97, 19, 221, 191, 40, 18, 202, 199, 160, 135, 203, 21, 18, 57, 137, 194, 109, 199, 200, 213, 209, 149, 176, 23, 121, 135, 203, 258, 200, 191, 238, 133, 133, 71, 15, 158, 18, 25, 238, 26, 112, 200, 3, 80, 200, 200, 94, 240, 69, 192, 133, 202, 175, 221, 23, 191, 10, 15, 153, 40, 72, 213, 25, 200, 239, 3, 21, 0, 104, 153, 15, 30, 148, 140, 35, 3, 176, 25, 195, 200, 70, 80, 204, 114, 18, 173, 19, 213, 238, 98, 78, 15, 150, 221, 237, 10, 200, 194, 18, 23, 96, 3, 139, 133, 71, 3, 118, 13, 196, 155, 22, 117, 97, 258, 2, 133, 175, 25, 23, 173, 204, 0, 72, 2, 237, 195, 23, 17, 80, 196, 160, 213, 241, 176, 78, 78, 196, 94, 70]\n",
            "[30, 237, -1, 50, 104, 203, 135, 166, 109, 15, 200, 161, 58, 156, 205, -1, 133, 156, 15, 97, 197, -1, -1, 161, 98, 3, 97, 191, 21, 196, 193, 19, 204, 70, -1, 25, 15, 114, 96, 135, 99, 69, 58, 110, 114, 191, 69, 151, 161, 202, 135, 80, 40, 176, 96, 237, 258, -1, 258, 238, -1, 104, 166, 109, 15, 25, 207, 18, 213, 155, 153, 194, 173, 25, 98, 96, -1, 161, 23, 135, 70, 76, 3, 25, 173, -1, -1, 78, 80, 71, 121, 191, 69, 121, 25, 62, 104, 3, 200, -1, 175, 153, 4, 133, 15, 258, 2, 156, 204, 21, -1, 110, 50, 71, 3, 240, 135, 153, 106, 50, 195, 195, 175, 50, 109, 237, 149, 107, 204, 106, 258, 21, 196, 147, 258, -1, 15, 18, 26, -1, 15, 23, 3, 237, 25, -1, 192, 135, 196, 238, 104, 15, 137, 175, 98, -1, -1, 196, 191, 104, 158, 191, 156, 160, -1, 50, 194, 193, 160, 238, 135, 163, 2, 107, 195, 235, -1, 237, 78, 238, 104, 50, 237, 160, -1, -1, 203, -1, 78, 10, 26, 3, 3, 10, -1, 238, 74, 78, 57, 237, 135, -1, 18, 135, 200, 196, 213, 237, -1, 191, 191, 107, 114, 237, 18, 58, -1, 193, 117, 97, 238, 238, 196, 98, 30, 176, 135, 97, 155, 114, -1, 58, -1, -1, 193, 94, 238, 15, 175, 135, 70, 213, 208, 193, 10, 10, 175, 204, 23, -1, 238, 115, 150, -1, 204, 135, 30, -1, -1, 111, 197, 165, 69, 135, 196, 196, 25, 202, 196, 191, 106, 237, 209, 94, 175, 19, 221, 98, 118, 147, 173, 78, 97, 200, 19, 26, 146, 78, 25, 135, -1, 175, -1, 50, 196, 104, 30, 165, 22, 140, 196, 22, 109, 238, -1, 221, 161, 106, 237, 78, 172, 29, 207, 165, 21, 15, 23, 200, -1, 140, -1, 133, 161, 18, 18, 195, 15, 202, 102, 18, 238, 163, 29, 26, 25, 152, 15, 200, 98, 221, 22, 207, 173, 139, 21, 3, 196, 21, 118, 195, 62, 150, 3, 213, 29, 121, 25, 19, 237, -1, -1, 78, 137, 25, -1, 196, 158, 146, 154, 109, -1, 23, 109, 109, 133, 135, 237, -1, 71, 25, 3, -1, 200, 200, 69, 237, 193, 15, 78, 40, -1, 191, -1, 176, 149, -1, 203, 191, 203, 57, 238, 135, 50, 152, 166, 97, -1, 118, 106, 133, 195, 147, 26, -1, 26, -1, 10, 172, 163, 149, 196, 15, 218, 78, -1, 22, 111, 209, 200, 18, 94, -1, -1, 29, 193, 207, 204, 25, 193, 149, 25, -1, 161, 170, 258, 19, -1, 192, 104, 238, 191, 204, 191, -1, -1, 57, 195, 30, -1, 166, 200, 193, 175, 40, 161, 18, 104, 135, 150, 166, -1, 22, 136, 135, 25, -1, -1, 196, 203, 30, -1, -1, 258, 258, 96, -1, 213, 258, 221, 21, 29, -1, 25, 163, 113, 19, 149, 70, 204, 98, 3, 200, 70, 208, 15, 50, 161, 18, 15, 135, 137, 140, 78, 3, 29, 69, 133, 192, -1, 135, 112, 163, 196, 135, -1, 238, 22, 50, 191, 58, -1, 237, 192, -1, 204, 191, 25, 208, -1, -1, -1, 166, 18, 2, -1, 26, 191, 238, 196, -1, 204, 193, 149, 15, 204, 258, 203, 111, 165, 161, 173, 106, 238, 237, 196, 80, 195, 3, -1, 64, -1, 18, 137, 203, 64, 213, 15, 18, 209, 160, 238, 193, 29, -1, 146, 104, 165, 204, 78, -1, 155, 173, 97, 57, 23, 196, -1, 192, 30, 200, -1, 26, 140, 96, 135, 72, 140, 18, 203, 114, 135, 237, -1, 3, 19, 200, 196, 237, 237, 133, 211, 238, 208, 153, 25, 104, 191, 161, -1, 156, 22, -1, 147, 80, 192, 146, -1, -1, 30, 15, 137, 15, 202, 110, 238, 117, 30, 204, 26, 213, 135, 153, 200, 3, 96, 152, 2, 104, 204, 148, 79, 155, 205, 78, 78, 15, 176, 221, 74, 18, 147, 161, 152, 26, 200, 238, 158, -1, 130, 209, 133, 191, 158, 113, 195, -1, 166, 200, 112, 238, 30, 199, -1, 240, 163, 238, -1, 23, 114, 191, 176, 150, 15, 97, 18, 221, 208, 29, 18, -1, 199, 160, 135, 203, 21, 18, 57, 137, 194, 110, 199, 200, 238, 209, -1, 176, 23, 121, 135, 203, 258, 200, 191, 112, 50, 133, 71, 15, 110, 18, 19, 203, 35, -1, 200, 3, 196, 200, 200, 94, 191, 69, 192, -1, 202, 175, 221, 23, -1, 10, -1, -1, 40, -1, 161, 25, -1, -1, 3, 21, -1, 104, -1, 15, 30, 205, 140, 96, 3, 171, 25, 194, 200, 58, 80, 204, 114, 18, -1, 19, 213, -1, 107, 79, 15, 150, -1, 237, 10, 200, -1, 18, 23, 96, 3, 29, 196, 71, 3, -1, 3, 196, 69, 22, 118, 97, 258, -1, 133, 135, 25, 23, -1, 204, -1, 72, 2, 237, 195, 23, 264, -1, 196, 235, 213, -1, 2, 78, 78, 196, 94, 70]\n",
            "[30, 197, -1, 50, 104, 203, 149, 166, 108, 15, 200, 161, 58, -1, 140, -1, 133, -1, 15, 97, 197, -1, -1, 161, 98, 202, 97, 191, -1, 196, 193, 19, 204, 155, -1, 155, 15, 114, 96, 135, 99, 69, 196, 113, 114, 191, 69, 151, -1, 202, 135, 78, 139, 175, 96, 194, 258, -1, 258, 146, -1, 104, 166, 109, 15, 40, 108, -1, 3, 18, 153, 194, 173, 25, 98, -1, -1, 161, 23, 135, 71, 76, 3, 25, 173, -1, -1, 78, 200, -1, 96, 191, -1, 121, 25, 62, 104, 135, 200, -1, 175, 153, -1, 133, 72, 237, 173, 156, 204, 258, -1, 258, 50, 71, 3, 197, -1, 153, 106, 50, 195, 197, 176, -1, 109, 237, 135, 107, 204, 106, 258, 21, 196, 147, 258, -1, 155, 18, 26, -1, 15, 22, 3, 195, -1, -1, 191, 135, 196, 258, 104, 15, -1, 195, 156, -1, -1, 200, -1, 104, 158, 191, 78, 156, -1, 50, 156, 258, 195, 175, 135, 163, -1, 98, -1, 258, -1, 237, -1, 238, 135, 50, 237, 133, -1, -1, 203, -1, 78, -1, 26, 204, 3, -1, -1, 213, 74, 78, 57, 237, 135, -1, 18, 95, 200, 196, 213, 237, -1, 191, 191, 98, 114, 264, 19, 56, -1, 193, 117, 97, 238, 238, 196, 98, 30, 176, 135, 97, 155, 114, -1, 58, -1, -1, 193, 94, 213, 15, 3, 135, 64, 213, 208, -1, 10, 10, 175, 204, 23, -1, 258, 115, 150, -1, 204, 95, 30, -1, -1, 111, 197, 165, 69, 135, 166, 196, 25, 202, 197, 191, 96, 98, 209, 94, 175, -1, 221, 98, -1, 147, 173, 78, 97, 200, 19, 40, 146, 78, -1, 135, -1, 175, -1, 50, 133, 104, 30, 165, 22, 140, 133, 22, 109, 64, -1, 221, 96, 147, 165, 78, 175, 10, 3, 165, 21, 15, 23, 200, -1, -1, -1, 133, 161, 155, 19, 197, 25, 202, 102, 18, 176, 163, 18, 26, 155, 152, 15, 200, 98, 221, -1, 207, -1, -1, 21, 3, 208, 21, 200, 195, 62, 150, 136, 197, 40, 121, 25, 18, 237, -1, -1, 78, 137, 25, -1, 196, 158, -1, 64, 109, -1, 25, 109, 109, 133, 135, 166, -1, 71, 25, 3, -1, 200, -1, -1, 237, 193, 15, 78, 40, -1, 191, -1, 176, 149, -1, 203, 191, 203, 57, 238, 96, 30, 152, 166, 97, -1, -1, 106, 133, 195, 147, 26, -1, 26, -1, 10, 175, 163, 149, 58, 15, 3, 78, -1, 22, -1, 209, 200, 18, 94, -1, -1, 165, 193, 207, 204, 155, 80, 149, 25, -1, 161, 170, 195, 19, -1, 191, 104, 238, 135, 204, 191, -1, -1, 57, 195, -1, -1, 166, 200, 193, 176, 10, 161, 18, 115, -1, -1, 166, -1, 22, 135, 135, 25, -1, -1, 196, 203, 30, -1, -1, 196, 258, 96, -1, 213, 156, -1, 235, 29, -1, 156, 163, -1, 19, 149, 18, 108, 98, 2, -1, 153, 208, -1, 50, 161, 18, 15, 196, 137, 135, 78, 3, 29, 57, 133, 192, -1, 135, 112, 163, -1, 135, -1, 238, 22, -1, 175, 58, -1, 196, 192, -1, 204, 191, 72, 191, -1, -1, -1, 166, 18, -1, -1, 26, 238, 238, 196, -1, 161, 193, 149, 15, 204, 114, 203, 112, 165, 161, 173, 106, 238, 237, 196, 80, 195, 195, -1, 53, -1, 18, 137, 203, 69, -1, 15, 18, 209, 160, 258, 193, 25, -1, 99, 104, 165, 204, 78, -1, 158, 173, 97, 57, 23, -1, -1, 192, 30, 200, -1, 26, 140, 71, 135, 72, 161, 18, 203, 114, 135, -1, -1, 3, 19, 200, 196, 237, 237, 133, 211, 258, 136, 153, 155, -1, 192, 161, -1, 156, 22, -1, 147, 80, 192, -1, -1, -1, 30, 25, -1, 26, 202, 71, 221, 117, 30, 75, 26, 213, 149, -1, 200, 235, 96, 152, 3, 104, 204, 148, 79, 155, 205, 78, -1, 23, 175, 221, 74, 18, 147, 161, 152, 26, 200, 176, 158, -1, -1, 209, 133, 191, 158, -1, 197, -1, 166, 200, 112, 57, 30, 199, -1, 258, 163, 238, -1, 23, 114, 191, 176, 151, 15, 97, 19, 221, 136, 29, 18, -1, 199, 156, 135, 203, 21, 18, 57, 137, 194, 109, 199, 200, 238, 209, -1, 176, -1, 96, 135, 203, 258, 200, 197, 112, -1, 133, -1, 15, 158, 18, 18, -1, 26, -1, 200, 3, 196, 200, 200, 94, -1, 69, 192, -1, 202, 175, 221, 23, -1, -1, -1, -1, 40, -1, 161, 25, -1, -1, 3, -1, -1, 104, -1, 15, 30, 148, 140, 96, -1, 176, 25, 194, 200, 58, 161, 204, 114, 18, -1, 19, 213, -1, 98, -1, 15, 151, -1, 237, 10, 200, -1, 18, 23, -1, 3, 139, -1, 35, 3, -1, 3, 196, 35, 22, 117, 97, 237, -1, 133, 133, 155, 23, -1, 204, -1, 30, 2, 258, 197, 23, 166, -1, 196, 258, 213, -1, 175, 78, 78, 196, 94, 70]\n",
            "[30, 197, -1, 16, 104, 203, 149, 166, 108, 15, -1, 161, 58, -1, 205, -1, 96, -1, 15, 97, 197, -1, -1, 161, 98, 3, 97, 191, -1, 196, 193, 19, 204, -1, -1, 25, 15, 114, 96, 135, -1, 69, 3, 13, 133, 191, -1, 151, -1, 202, 135, 78, 29, 175, 96, 196, 258, -1, 258, 98, -1, 104, 166, 109, 15, -1, 108, -1, 3, 58, 153, -1, 173, 25, 98, -1, -1, 3, 23, 135, 204, -1, 3, -1, 173, -1, -1, -1, 200, -1, 121, 191, -1, -1, -1, 78, 104, 3, 200, -1, -1, 153, -1, 133, 72, -1, 173, 156, 204, 21, -1, 110, 69, 71, 3, 195, -1, 153, 110, -1, 195, -1, 176, -1, 109, 71, 135, 107, 204, 106, 258, 21, 3, -1, 258, -1, 15, 18, 26, -1, 15, 23, 3, 195, -1, -1, 192, 135, -1, 258, 104, 15, -1, 175, 133, -1, -1, -1, -1, 104, 158, 191, 78, 3, -1, -1, 197, 258, 160, 176, 135, 163, -1, 3, -1, -1, -1, 237, -1, -1, 104, -1, 194, 196, -1, -1, 203, -1, 78, -1, -1, 204, 3, -1, -1, 238, -1, 78, 57, 237, 133, -1, 18, -1, 200, 196, 213, 258, -1, 191, -1, 98, 98, 258, 18, 3, -1, 193, 117, 97, 238, 238, 197, 98, 15, 176, 135, 97, 18, 114, -1, 58, -1, -1, 193, -1, 213, -1, 2, 135, 64, 213, -1, -1, 10, 10, 175, 204, 23, -1, 196, 115, 150, -1, 204, 95, 30, -1, -1, 115, 197, 165, 69, 135, 237, 196, 25, 3, -1, 191, 96, 98, 209, -1, 2, -1, 221, 98, -1, 147, 173, 78, 97, 78, 19, 26, 98, -1, -1, 135, -1, 175, -1, 50, 3, 104, 30, 165, 22, 140, 98, 22, 109, 64, -1, 221, 96, 147, 165, 78, 2, 29, 3, 165, 21, 15, 25, 200, -1, -1, -1, 158, 161, 18, 19, -1, 25, 202, 102, 18, -1, 163, 29, 26, 15, 152, -1, 200, 98, 221, -1, 207, -1, -1, -1, 3, 208, -1, 115, 195, 78, 151, 135, 213, 40, -1, 25, 19, 237, -1, -1, 78, 137, 25, -1, 196, 158, -1, -1, 109, -1, 25, -1, 109, 133, 135, 237, -1, 71, 25, 3, -1, -1, -1, -1, 258, 3, -1, 78, 40, -1, -1, -1, 173, 135, -1, 203, 209, 203, 69, 238, 102, 30, 151, 107, 97, -1, -1, 106, 133, 3, 147, 26, -1, 26, -1, 10, 176, 163, -1, 205, 15, 3, 78, -1, 22, -1, 209, 200, 18, 94, -1, -1, 165, 193, 207, 204, 15, 193, 149, 25, -1, 161, 170, 258, 19, -1, -1, 104, 238, 191, 204, 191, -1, -1, 57, 195, -1, -1, 166, -1, 193, 176, 40, 161, 18, 115, -1, -1, 166, -1, 22, 135, 135, 25, -1, -1, 196, -1, 30, -1, -1, 196, 121, 96, -1, 213, 258, -1, -1, 29, -1, 72, 163, -1, 19, 149, 58, 108, 98, 3, -1, 70, -1, -1, 50, -1, 18, 15, 84, 2, 98, 78, 3, 29, 57, 133, 192, -1, 135, 112, 163, -1, 135, -1, 238, 22, -1, 191, 58, -1, 196, -1, -1, 204, 191, 69, 208, -1, -1, -1, 166, 18, -1, -1, 26, 238, 238, 196, -1, 204, -1, 96, 15, 204, 237, 203, 3, 165, 161, 173, 106, 238, 237, 196, -1, 258, 238, -1, 53, -1, 18, 137, 203, 71, -1, 15, -1, 209, 3, 238, 193, 22, -1, 98, -1, 165, 204, 78, -1, 110, 173, 97, 57, 23, -1, -1, 192, 30, 200, -1, 26, -1, 71, 2, 72, 191, 18, 203, 114, 135, -1, -1, 2, 19, 200, 196, 237, 98, 133, 211, 238, 208, 153, 25, -1, 191, 204, -1, 258, 22, -1, -1, 80, 192, -1, -1, -1, 15, 25, -1, 15, -1, 71, 221, -1, 30, 204, 26, 213, 149, -1, 200, 3, 96, 152, 3, 104, 204, 148, 78, 50, 205, 78, -1, 15, 176, 221, 74, 18, 147, 161, 152, 26, 200, 2, -1, -1, -1, 209, 133, -1, 158, -1, 196, -1, 166, 200, 112, 57, 30, 199, -1, 258, 163, 238, -1, 23, 114, 191, -1, 151, 15, -1, 19, 221, 238, 29, 18, -1, 199, 160, 135, 203, 21, 18, 57, 137, 197, 108, 199, 200, 213, 209, -1, 176, -1, 121, 137, -1, 258, 200, 192, 112, -1, 133, -1, 15, 110, 18, 19, -1, 35, -1, 200, 3, 3, 200, 200, 94, -1, 69, 192, -1, 3, 175, 221, -1, -1, -1, -1, -1, 40, -1, 213, 25, -1, -1, 3, -1, -1, 104, -1, 15, 30, 148, 140, 71, -1, 176, -1, -1, 78, -1, -1, 204, 114, -1, -1, 19, 213, -1, 98, -1, 15, 151, -1, 191, -1, 200, -1, 18, 23, -1, 3, 139, -1, 71, 3, -1, 196, 196, 57, 22, 118, 97, -1, -1, 133, 2, 25, 23, -1, 204, -1, 72, 2, 161, 238, 23, 258, -1, 196, 258, 213, -1, 175, 78, 78, 196, -1, 64]\n",
            "[30, 237, -1, 16, 104, 203, 135, 166, -1, 26, -1, 161, 197, -1, 202, -1, 133, -1, 15, 97, -1, -1, -1, 161, 98, 202, 97, 191, -1, 196, 193, 19, 204, -1, -1, 155, 15, 114, 96, 135, -1, 69, 2, -1, 114, 191, -1, 151, -1, -1, 135, 78, 29, 175, 96, -1, -1, -1, 258, 98, -1, 104, 166, 109, 15, -1, 108, -1, -1, -1, 153, -1, -1, 25, 98, -1, -1, -1, 23, 135, 69, -1, 3, -1, 173, -1, -1, -1, -1, -1, 121, 203, -1, -1, -1, 78, -1, 3, 200, -1, -1, 153, -1, 133, 15, -1, -1, 156, 204, 21, -1, 258, 155, 71, 3, 237, -1, 135, 106, -1, 195, -1, -1, -1, 109, 237, 149, 107, 204, 106, 258, 21, 78, -1, 258, -1, 15, 18, 26, -1, -1, 23, 3, 237, -1, -1, 192, 135, -1, 237, 104, 23, -1, -1, -1, -1, -1, -1, -1, 104, -1, 191, 78, -1, -1, -1, 133, 193, 160, 176, 135, 163, -1, 199, -1, -1, -1, 237, -1, -1, 104, -1, 195, 160, -1, -1, 203, -1, -1, -1, -1, 204, 204, -1, -1, -1, -1, 78, 57, 237, 133, -1, 18, -1, -1, -1, 213, 237, -1, 191, -1, 107, 114, 237, 18, 197, -1, 193, 117, 97, 238, 238, 195, 98, 30, 176, 135, -1, -1, 114, -1, 58, -1, -1, 193, -1, 238, -1, 173, 135, 70, 213, -1, -1, 10, 10, 175, 204, 23, -1, 258, 115, 79, -1, 204, 95, -1, -1, -1, 112, 197, 22, 69, 95, 166, -1, 25, 202, -1, 191, -1, 237, 209, -1, 175, -1, 221, 98, -1, -1, 173, 78, 97, 78, 19, -1, 98, -1, -1, 135, -1, 175, -1, 30, 166, 104, 30, 165, 22, 135, -1, 22, 109, 64, -1, 221, -1, 147, 203, 78, 175, 155, -1, 165, 21, 15, -1, 200, -1, -1, -1, 158, 109, -1, 19, -1, -1, 202, 102, -1, -1, 163, 50, 26, -1, -1, -1, 200, 98, 221, -1, 207, -1, -1, -1, 3, 29, -1, 200, 195, 78, 151, 135, 238, 155, -1, 25, 19, 237, -1, -1, 78, 137, 25, -1, 196, -1, -1, -1, 109, -1, 25, -1, 109, 133, 135, 166, -1, 71, 25, 3, -1, -1, -1, -1, 237, 193, -1, 78, 40, -1, -1, -1, 173, 149, -1, 203, 191, 203, 69, 238, 102, 30, 150, 166, 97, -1, -1, 106, 133, 2, 147, 26, -1, 26, -1, 10, 175, 163, -1, 58, 15, 218, 78, -1, 22, -1, 209, 200, 18, 94, -1, -1, 29, 193, 207, 204, 155, 193, -1, 25, -1, 161, 170, 160, 19, -1, -1, -1, 238, 191, 204, 191, -1, -1, 57, -1, -1, -1, 166, -1, 193, 176, 40, 161, -1, 115, -1, -1, 166, -1, -1, 135, 135, 25, -1, -1, 196, -1, 30, -1, -1, 258, 160, 96, -1, 238, 133, -1, -1, 29, -1, 72, 163, -1, 19, 149, 70, 109, -1, 2, -1, 155, -1, -1, 50, -1, 18, -1, 135, 137, 135, 78, 3, 29, 69, 133, -1, -1, 135, 112, 163, -1, 135, -1, 238, 22, -1, 2, 58, -1, 237, -1, -1, 204, 191, -1, -1, -1, -1, -1, 166, 18, -1, -1, 26, 191, 238, 112, -1, 204, -1, 149, -1, 204, 114, 203, 112, 165, 204, 173, 106, 238, 237, 196, -1, -1, 238, -1, 64, -1, 18, 137, 203, 64, -1, 15, -1, 209, 160, 238, 193, 22, -1, 121, -1, 203, 204, 78, -1, 114, -1, 97, 57, 23, -1, -1, 192, 30, 78, -1, -1, -1, 35, 135, 72, 238, 18, 203, 114, 135, -1, -1, -1, -1, 200, 196, 237, 237, 133, 211, -1, 208, 135, 25, -1, 191, 161, -1, 50, -1, -1, -1, 3, 192, -1, -1, -1, 30, 15, -1, -1, -1, 71, 221, -1, 30, -1, 26, 213, 135, -1, 200, 3, 96, 152, 2, -1, 204, 148, 78, 155, 205, -1, -1, 15, 176, 221, 75, 18, 147, 161, 114, 26, 200, 176, -1, -1, -1, 209, 133, -1, 158, -1, 195, -1, 166, -1, 112, -1, 30, 199, -1, -1, 163, 221, -1, 23, 114, 191, -1, 200, 15, -1, 19, -1, 191, 155, 18, -1, 199, 160, 135, 203, 21, 18, 57, 137, 197, 106, 199, 200, 238, 209, -1, -1, -1, 121, 135, -1, 258, 200, 191, -1, -1, 133, -1, 15, 158, 18, 19, -1, 26, -1, 200, 3, 238, 200, 200, 94, -1, 69, 192, -1, 202, -1, 221, -1, -1, -1, -1, -1, 40, -1, -1, 25, -1, -1, 3, -1, -1, 104, -1, 15, -1, 148, 140, -1, -1, 176, -1, -1, 78, -1, -1, 204, 114, -1, -1, 19, -1, -1, 98, -1, 15, 151, -1, -1, -1, 200, -1, 18, 23, -1, 3, 29, -1, -1, 3, -1, 196, 197, 26, 35, -1, 97, -1, -1, 133, 133, 25, 23, -1, 161, -1, 72, 2, 258, 195, 23, 258, -1, 196, 258, 192, -1, 175, 78, 78, -1, -1, -1]\n",
            "[15, 165, -1, 16, 104, 3, 135, 166, -1, 15, -1, 161, 197, -1, 202, -1, 133, -1, 15, 97, -1, -1, -1, 161, 98, 202, 97, 191, -1, 196, 193, 19, 204, -1, -1, -1, -1, 114, 96, 135, -1, 69, 3, -1, -1, -1, -1, 151, -1, -1, 135, 78, 29, 175, 96, -1, -1, -1, 258, 98, -1, 104, -1, 109, 15, -1, 109, -1, -1, -1, 153, -1, -1, 25, 98, -1, -1, -1, -1, 135, 69, -1, -1, -1, -1, -1, -1, -1, -1, -1, 121, 203, -1, -1, -1, 165, -1, 3, 200, -1, -1, -1, -1, 133, 15, -1, -1, 156, 204, -1, -1, 110, 35, 35, 3, 197, -1, 135, -1, -1, 197, -1, -1, -1, 109, 258, 18, -1, 204, 106, 258, 21, -1, -1, 258, -1, 15, 18, 26, -1, -1, -1, 202, 197, -1, -1, 238, 136, -1, 258, 104, 15, -1, -1, -1, -1, -1, -1, -1, 104, -1, 191, -1, -1, -1, -1, 196, 193, 160, 171, 135, 163, -1, 98, -1, -1, -1, 258, -1, -1, 104, -1, 15, -1, -1, -1, 203, -1, -1, -1, -1, 204, 204, -1, -1, -1, -1, -1, 57, -1, 135, -1, 18, -1, -1, -1, -1, 258, -1, 191, -1, -1, 114, 258, 18, 202, -1, 193, -1, 97, 238, 238, 161, 98, -1, 176, 135, -1, -1, 114, -1, 58, -1, -1, 193, -1, -1, -1, -1, 135, 155, 213, -1, -1, 10, -1, 175, 204, 23, -1, 98, 258, 79, -1, -1, 95, -1, -1, -1, 258, 197, 22, 69, 95, 235, -1, 25, 202, -1, -1, -1, 98, 191, -1, -1, -1, 238, 98, -1, -1, -1, 156, 97, 200, 19, -1, 196, -1, -1, 135, -1, 135, -1, -1, 3, 104, 30, 165, -1, -1, -1, 22, 109, 238, -1, 238, -1, 133, 137, -1, -1, 15, -1, 165, 15, 15, -1, 200, -1, -1, -1, 158, 109, -1, 19, -1, -1, 202, 102, -1, -1, 163, 18, 26, -1, -1, -1, 200, 98, 238, -1, 207, -1, -1, -1, -1, 133, -1, -1, -1, -1, 151, 135, 196, 40, -1, 25, 19, 258, -1, -1, 78, 137, -1, -1, 196, -1, -1, -1, 109, -1, 25, -1, -1, -1, 135, 166, -1, 71, 25, 3, -1, -1, -1, -1, 237, 3, -1, 78, 40, -1, -1, -1, 173, -1, -1, 203, 191, -1, 69, 238, 102, -1, 152, 166, 97, -1, -1, 107, 196, 195, 118, 26, -1, 26, -1, 10, -1, 163, -1, 3, 25, 3, 78, -1, -1, -1, 209, 200, -1, 94, -1, -1, 165, -1, 207, 204, 258, 193, -1, 25, -1, 161, -1, -1, -1, -1, -1, -1, 238, 135, 204, 191, -1, -1, 57, -1, -1, -1, 166, -1, -1, 171, -1, 161, -1, 104, -1, -1, 166, -1, -1, 135, 135, 25, -1, -1, 196, -1, 30, -1, -1, 258, 258, 96, -1, -1, 258, -1, -1, 29, -1, 156, 163, -1, 19, -1, 18, 109, -1, 3, -1, 154, -1, -1, 30, -1, 18, -1, 238, 137, 22, 78, 3, 29, 69, 133, -1, -1, 135, 112, 163, -1, 135, -1, 238, 22, -1, 191, -1, -1, 196, -1, -1, 204, 191, -1, -1, -1, -1, -1, 114, 18, -1, -1, 26, 238, 238, 133, -1, 204, -1, 96, -1, -1, -1, 203, 3, 165, 161, 173, 106, 238, -1, 196, -1, -1, 196, -1, 57, -1, 18, 137, 203, 35, -1, 15, -1, 209, 160, 238, 193, 22, -1, 98, -1, 203, 204, 78, -1, 110, -1, 97, 57, 23, -1, -1, 192, 30, 200, -1, -1, -1, 35, 135, 72, 140, 18, 203, 114, 135, -1, -1, -1, -1, 200, -1, -1, -1, 133, 211, -1, 191, 135, -1, -1, 191, 161, -1, 15, -1, -1, -1, 3, 192, -1, -1, -1, 30, 25, -1, -1, -1, 50, 238, -1, 30, -1, 26, 213, 135, -1, 200, 258, 96, 152, 3, -1, 204, 148, 78, 50, 58, -1, -1, 15, 176, 238, 4, 18, 147, -1, 114, 26, 200, 98, -1, -1, -1, -1, -1, -1, 158, -1, 196, -1, 166, -1, 112, -1, 30, -1, -1, -1, 163, 238, -1, -1, 114, 191, -1, 152, 15, -1, 19, -1, 191, 40, 18, -1, 199, 160, 135, 203, 29, 18, 57, 137, 197, 106, 199, -1, 238, 209, -1, -1, -1, 121, 135, -1, 258, -1, 191, -1, -1, 133, -1, 15, 158, 18, 19, -1, 35, -1, 200, 3, -1, 200, 200, 94, -1, -1, 192, -1, -1, -1, -1, -1, -1, -1, -1, -1, 40, -1, -1, 25, -1, -1, 3, -1, -1, 104, -1, 15, -1, 148, 104, -1, -1, 173, -1, -1, -1, -1, -1, 204, 114, -1, -1, 19, -1, -1, 98, -1, 15, 151, -1, -1, -1, 200, -1, 18, 23, -1, 3, 29, -1, -1, 3, -1, 196, 196, 69, 40, -1, 97, -1, -1, 133, 156, -1, -1, -1, 204, -1, 72, 2, 258, 197, 23, 158, -1, 196, 258, 192, -1, 2, -1, 78, -1, -1, -1]\n",
            "0.5237529691211401\n",
            "[30, 237, 137, 16, 104, 203, 149, 166, 108, 17, 200, 161, 58, 156, 205, 176, 133, 156, 239, 97, 197, 241, 237, 161, 98, 3, 97, 191, 21, 196, 193, 19, 204, 70, 135, 30, 15, 133, 96, 135, 96, 69, 3, 155, 13, 191, 69, 151, 161, 202, 135, 78, 29, 175, 96, 194, 258, 125, 3, 98, 153, 104, 166, 109, 15, 40, 108, 18, 202, 10, 153, 194, 173, 25, 98, 96, 241, 161, 23, 135, 155, 76, 3, 25, 173, 238, 239, 78, 80, 71, 121, 191, 69, 121, 25, 62, 104, 3, 200, 25, 175, 153, 74, 133, 72, 238, 2, 161, 204, 21, 166, 110, 155, 71, 3, 195, 135, 153, 110, 50, 196, 197, 175, 62, 109, 237, 149, 107, 204, 106, 258, 21, 196, 147, 258, 208, 15, 18, 26, 94, 15, 23, 3, 195, 25, 241, 191, 135, 196, 258, 104, 23, 137, 175, 133, 202, 149, 200, 191, 104, 239, 191, 80, 202, 78, 50, 133, 193, 160, 176, 135, 163, 2, 3, 195, 3, 153, 237, 78, 176, 104, 50, 237, 133, 202, 191, 203, 258, 78, 10, 26, 204, 3, 10, 125, 238, 74, 78, 57, 237, 133, 239, 18, 95, 200, 196, 213, 237, 25, 191, 191, 98, 114, 264, 19, 58, 109, 193, 117, 97, 238, 175, 196, 98, 30, 176, 135, 97, 155, 114, 21, 58, 241, 25, 193, 94, 213, 15, 2, 135, 155, 213, 208, 193, 10, 10, 175, 204, 23, 129, 147, 115, 79, 22, 133, 95, 30, 155, 241, 111, 197, 165, 69, 135, 166, 133, 25, 202, 197, 191, 96, 98, 209, 94, 175, 19, 221, 98, 104, 147, 21, 78, 97, 78, 19, 26, 157, 78, 25, 135, 96, 175, 153, 50, 3, 104, 30, 165, 22, 140, 98, 22, 109, 64, 3, 221, 106, 147, 165, 78, 175, 40, 3, 165, 21, 15, 23, 200, 241, 140, 241, 133, 161, 133, 19, 238, 25, 202, 102, 19, 176, 163, 50, 26, 155, 152, 15, 200, 98, 221, 22, 207, 173, 22, 21, 3, 3, 21, 118, 195, 62, 150, 202, 238, 50, 121, 25, 19, 237, 204, 16, 78, 137, 25, 40, 196, 158, 140, 135, 109, 237, 25, 109, 109, 133, 135, 166, 239, 71, 25, 3, 193, 200, 200, 155, 237, 193, 15, 78, 40, 202, 191, 213, 176, 135, 200, 203, 191, 203, 69, 191, 102, 30, 151, 166, 237, 149, 147, 106, 133, 3, 147, 26, 135, 26, 133, 10, 175, 163, 155, 133, 15, 218, 78, 125, 22, 166, 209, 200, 18, 94, 150, 239, 29, 193, 207, 204, 155, 193, 149, 25, 102, 161, 170, 258, 19, 96, 191, 104, 3, 191, 204, 191, 101, 78, 57, 195, 30, 125, 166, 200, 193, 176, 40, 161, 18, 115, 149, 163, 166, 78, 22, 135, 135, 25, 153, 21, 196, 203, 30, 259, 140, 13, 115, 96, 241, 215, 133, 221, 21, 29, 153, 50, 163, 113, 19, 149, 58, 109, 238, 2, 200, 154, 137, 26, 50, 161, 18, 239, 135, 137, 135, 78, 3, 50, 57, 133, 192, 50, 135, 112, 163, 133, 135, 153, 238, 22, 50, 2, 58, 18, 239, 192, 98, 204, 191, 25, 191, 26, 161, 200, 166, 18, 2, 10, 26, 238, 238, 196, 4, 204, 193, 149, 15, 204, 239, 203, 3, 165, 161, 173, 106, 238, 237, 196, 80, 133, 238, 241, 53, 102, 19, 137, 203, 64, 202, 15, 18, 209, 160, 238, 193, 22, 15, 121, 104, 165, 204, 78, 158, 155, 173, 97, 57, 23, 196, 133, 192, 30, 200, 74, 26, 140, 38, 205, 72, 58, 18, 203, 114, 135, 237, 13, 239, 19, 200, 196, 237, 237, 133, 211, 238, 136, 153, 155, 104, 191, 161, 113, 133, 22, 40, 147, 80, 192, 239, 191, 235, 30, 25, 137, 26, 202, 71, 221, 117, 30, 204, 26, 213, 135, 153, 200, 3, 96, 150, 2, 104, 204, 148, 79, 155, 205, 78, 78, 23, 176, 221, 74, 18, 147, 161, 152, 26, 200, 176, 155, 191, 40, 209, 133, 191, 133, 113, 196, 194, 166, 3, 112, 57, 30, 199, 3, 258, 163, 191, 15, 23, 114, 191, 176, 151, 15, 97, 19, 221, 191, 40, 18, 202, 199, 160, 135, 203, 21, 18, 57, 137, 194, 109, 199, 200, 213, 209, 149, 176, 23, 121, 135, 203, 258, 200, 191, 238, 133, 133, 71, 15, 158, 18, 25, 238, 26, 112, 200, 3, 80, 200, 200, 94, 240, 69, 192, 133, 202, 175, 221, 23, 191, 10, 15, 153, 40, 72, 213, 25, 200, 239, 3, 21, 0, 104, 153, 15, 30, 148, 140, 35, 3, 176, 25, 195, 200, 70, 80, 204, 114, 18, 173, 19, 213, 238, 98, 78, 15, 150, 221, 237, 10, 200, 194, 18, 23, 96, 3, 139, 133, 71, 3, 118, 13, 196, 155, 22, 117, 97, 258, 2, 133, 175, 25, 23, 173, 204, 0, 72, 2, 237, 195, 23, 17, 80, 196, 160, 213, 241, 176, 78, 78, 196, 94, 70]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "final_pred_1=[]\n",
        "\n",
        "num_correct=0\n",
        "model_0_1=load_model(loss,accuracy,optimizer,'0-1')\n",
        "mod_labels_0=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_0:\n",
        "    pred_test_0_1=model_0_1.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_0_1 = pred_test_0_1.argmax(axis=1)\n",
        "    mod_labels_0.append(pred_labels_0_1[0])\n",
        "  else:\n",
        "    mod_labels_0.append(-1)\n",
        "\n",
        "model_1_1=load_model(loss,accuracy,optimizer,'1-1')\n",
        "mod_labels_1=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        " \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_1:\n",
        "    pred_test_1_1=model_1_1.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_1_1 = pred_test_1_1.argmax(axis=1)\n",
        "    mod_labels_1.append(pred_labels_1_1[0])\n",
        "  else:\n",
        "    mod_labels_1.append(-1)\n",
        "\n",
        "model_2_1=load_model(loss,accuracy,optimizer,'2-1')\n",
        "mod_labels_2=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_2:\n",
        "    pred_test_2_1=model_2_1.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_2_1 = pred_test_2_1.argmax(axis=1)\n",
        "    mod_labels_2.append(pred_labels_2_1[0])\n",
        "  else:\n",
        "    mod_labels_2.append(-1)\n",
        "\n",
        "model_3_1=load_model(loss,accuracy,optimizer,'3-1')\n",
        "mod_labels_3=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_3:\n",
        "    pred_test_3_1=model_3_1.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_3_1 = pred_test_3_1.argmax(axis=1)\n",
        "    mod_labels_3.append(pred_labels_3_1[0])\n",
        "  else:\n",
        "    mod_labels_3.append(-1)\n",
        "\n",
        "model_4_1=load_model(loss,accuracy,optimizer,'4-1')\n",
        "mod_labels_4=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_4:\n",
        "    pred_test_4_1=model_4_1.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_4_1 = pred_test_4_1.argmax(axis=1)\n",
        "    mod_labels_4.append(pred_labels_4_1[0])\n",
        "  else:\n",
        "    mod_labels_4.append(-1)\n",
        "\n",
        "model_5_1=load_model(loss,accuracy,optimizer,'5-1')\n",
        "mod_labels_5=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_5:\n",
        "    pred_test_5_1=model_5_1.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_5_1 = pred_test_5_1.argmax(axis=1)\n",
        "    mod_labels_5.append(pred_labels_5_1[0])\n",
        "  else:\n",
        "    mod_labels_5.append(-1)\n",
        "\n",
        "print(mod_labels_0)\n",
        "print(mod_labels_1)\n",
        "print(mod_labels_2)\n",
        "print(mod_labels_3)\n",
        "print(mod_labels_4)\n",
        "print(mod_labels_5)\n",
        "\n",
        "\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  fin_labels=[]\n",
        "  if mod_labels_0[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  if mod_labels_1[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  if mod_labels_2[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  if mod_labels_3[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  if mod_labels_4[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  if mod_labels_5[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  res=max(set(fin_labels), key = fin_labels.count)\n",
        "  final_pred_1.append(res)\n",
        "\n",
        "  if res==new_val_label_0[i]:\n",
        "    num_correct=num_correct+1\n",
        "\n",
        "accuracy=num_correct/len(new_val_label_0)\n",
        "print(accuracy)\n",
        "  \n",
        "print(final_pred_1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhhwGEPul5A4",
        "outputId": "208cdeb0-ba2e-4624-b59a-a5a98f92c39e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.5237529691211401\n",
            "Weighted F1: 0.47306065156975624\n",
            "Micro F1: 0.5237529691211401\n",
            "Weighted Precision: 0.4604739559716541\n",
            "Micro Precision: 0.5237529691211401\n",
            "Weighted Recall: 0.5237529691211401\n",
            "Micro Recall: 0.5237529691211401\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "\n",
        "accuracy=accuracy_score(new_val_label_0, final_pred_1)\n",
        "print(\"Accuracy: \"+str(accuracy))\n",
        "total_accuracy=total_accuracy+accuracy\n",
        "\n",
        "weighted_f1=f1_score(new_val_label_0,final_pred_1, average='weighted')\n",
        "print(\"Weighted F1: \"+ str(weighted_f1))\n",
        "total_weighted_f1=total_weighted_f1+weighted_f1\n",
        "micro_f1=f1_score(new_val_label_0,final_pred_1, average='micro')\n",
        "print(\"Micro F1: \"+ str(micro_f1))\n",
        "total_micro_f1=total_micro_f1+micro_f1\n",
        "\n",
        "weighted_precision=precision_score(new_val_label_0, final_pred_1, average='weighted')\n",
        "print(\"Weighted Precision: \" + str(weighted_precision))\n",
        "total_weighted_precision=total_weighted_precision+weighted_precision\n",
        "micro_precision=precision_score(new_val_label_0, final_pred_1, average='micro')\n",
        "print(\"Micro Precision: \" + str(micro_precision))\n",
        "total_micro_precision=total_micro_precision+micro_precision\n",
        "\n",
        "weighted_recall=recall_score(new_val_label_0, final_pred_1, average='weighted')\n",
        "print(\"Weighted Recall: \" + str(weighted_recall))\n",
        "total_weighted_recall=total_weighted_recall+weighted_recall\n",
        "micro_recall=recall_score(new_val_label_0, final_pred_1, average='micro')\n",
        "print(\"Micro Recall: \" + str(micro_recall))\n",
        "total_micro_recall=total_micro_recall+micro_recall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Pe7iq0ql_Vy",
        "outputId": "a908e237-3006-4548-a8ed-f69e9668c754"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_14\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_29 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_30 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_29[0][0]',               \n",
            " el)                            thPoolingAndCrossAt               'input_30[0][0]']               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_14 (S  (None, 768)         0           ['tf_roberta_model[14][0]']      \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " dense_28 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_14[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dropout_51 (Dropout)           (None, 512)          0           ['dense_28[0][0]']               \n",
            "                                                                                                  \n",
            " dense_29 (Dense)               (None, 279)          143127      ['dropout_51[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model: \"model_15\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_31 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_32 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_31[0][0]',               \n",
            " el)                            thPoolingAndCrossAt               'input_32[0][0]']               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_15 (S  (None, 768)         0           ['tf_roberta_model[15][0]']      \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " dense_30 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_15[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dropout_52 (Dropout)           (None, 512)          0           ['dense_30[0][0]']               \n",
            "                                                                                                  \n",
            " dense_31 (Dense)               (None, 279)          143127      ['dropout_52[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model: \"model_16\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_33 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_34 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_33[0][0]',               \n",
            " el)                            thPoolingAndCrossAt               'input_34[0][0]']               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_16 (S  (None, 768)         0           ['tf_roberta_model[16][0]']      \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " dense_32 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_16[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dropout_53 (Dropout)           (None, 512)          0           ['dense_32[0][0]']               \n",
            "                                                                                                  \n",
            " dense_33 (Dense)               (None, 279)          143127      ['dropout_53[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model: \"model_17\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_35 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_36 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_35[0][0]',               \n",
            " el)                            thPoolingAndCrossAt               'input_36[0][0]']               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_17 (S  (None, 768)         0           ['tf_roberta_model[17][0]']      \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " dense_34 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_17[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dropout_54 (Dropout)           (None, 512)          0           ['dense_34[0][0]']               \n",
            "                                                                                                  \n",
            " dense_35 (Dense)               (None, 279)          143127      ['dropout_54[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model: \"model_18\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_37 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_38 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_37[0][0]',               \n",
            " el)                            thPoolingAndCrossAt               'input_38[0][0]']               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_18 (S  (None, 768)         0           ['tf_roberta_model[18][0]']      \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " dense_36 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_18[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dropout_55 (Dropout)           (None, 512)          0           ['dense_36[0][0]']               \n",
            "                                                                                                  \n",
            " dense_37 (Dense)               (None, 279)          143127      ['dropout_55[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model: \"model_19\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_39 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_40 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_39[0][0]',               \n",
            " el)                            thPoolingAndCrossAt               'input_40[0][0]']               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_19 (S  (None, 768)         0           ['tf_roberta_model[19][0]']      \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " dense_38 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_19[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dropout_56 (Dropout)           (None, 512)          0           ['dense_38[0][0]']               \n",
            "                                                                                                  \n",
            " dense_39 (Dense)               (None, 279)          143127      ['dropout_56[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "[30, 29, 137, 16, 104, 203, 135, 166, 108, 15, 200, 161, 58, 156, 58, 176, 96, 156, 239, 97, 197, 241, 258, 161, 98, 3, 97, 191, 21, 196, 193, 19, 204, 30, 135, 30, 15, 114, 96, 135, 96, 69, 58, 115, 114, 191, 69, 151, 161, 202, 135, 80, 29, 175, 96, 195, 237, 125, 258, 238, 153, 104, 166, 109, 15, 155, 207, 18, 202, 64, 153, 194, 173, 25, 98, 96, 241, 161, 23, 135, 155, 160, 3, 25, 173, 238, 239, 78, 3, 71, 121, 191, 26, 121, 25, 62, 104, 3, 200, 25, 175, 153, 74, 133, 72, 259, 173, 156, 204, 21, 166, 158, 30, 71, 3, 258, 135, 135, 241, 50, 195, 194, 175, 62, 109, 237, 135, 107, 204, 106, 258, 21, 196, 240, 258, 208, 15, 18, 26, 94, 15, 23, 200, 195, 25, 241, 191, 135, 196, 196, 104, 23, 137, 175, 156, 0, 149, 200, 191, 104, 239, 191, 156, 156, 78, 50, 156, 193, 160, 176, 135, 163, 2, 3, 194, 258, 153, 237, 78, 176, 121, 50, 237, 137, 202, 191, 203, 258, 78, 10, 26, 204, 3, 10, 239, 238, 74, 78, 57, 237, 133, 239, 18, 135, 200, 196, 215, 237, 25, 192, 191, 107, 114, 264, 18, 58, 109, 193, 117, 97, 238, 238, 196, 98, 30, 176, 135, 97, 156, 114, 21, 58, 241, 25, 193, 94, 213, 15, 173, 135, 64, 213, 208, 193, 10, 10, 175, 204, 23, 237, 118, 115, 150, 50, 237, 135, 30, 155, 241, 111, 197, 165, 69, 135, 166, 133, 25, 202, 197, 191, 96, 98, 209, 94, 175, 19, 221, 98, 104, 118, 173, 78, 97, 78, 19, 26, 118, 78, 25, 135, 96, 175, 153, 50, 3, 121, 30, 165, 22, 140, 133, 22, 109, 64, 3, 221, 96, 147, 98, 78, 172, 40, 207, 165, 21, 15, 23, 200, 241, 139, 241, 133, 109, 155, 19, 176, 25, 202, 102, 19, 176, 163, 29, 26, 155, 152, 15, 200, 98, 221, 22, 207, 173, 139, 21, 3, 25, 21, 200, 195, 62, 150, 202, 213, 40, 121, 25, 19, 239, 204, 16, 78, 137, 25, 125, 196, 158, 150, 153, 109, 237, 25, 109, 109, 133, 135, 237, 96, 71, 25, 3, 193, 200, 200, 30, 237, 193, 15, 78, 40, 264, 191, 215, 176, 239, 200, 203, 209, 203, 57, 238, 96, 30, 151, 166, 97, 149, 147, 106, 133, 3, 147, 26, 135, 26, 133, 10, 175, 163, 149, 58, 25, 218, 78, 239, 22, 135, 209, 200, 18, 94, 150, 239, 165, 193, 264, 204, 155, 193, 149, 25, 102, 161, 170, 258, 19, 96, 191, 104, 238, 191, 204, 191, 161, 200, 57, 195, 30, 18, 166, 200, 193, 176, 10, 161, 18, 115, 133, 163, 135, 78, 22, 135, 135, 25, 153, 21, 196, 203, 30, 237, 176, 140, 258, 96, 241, 215, 133, 221, 21, 29, 153, 155, 163, 113, 19, 149, 30, 109, 173, 2, 200, 70, 137, 26, 50, 161, 18, 239, 135, 137, 139, 78, 3, 29, 30, 133, 192, 15, 135, 112, 163, 133, 135, 153, 238, 22, 50, 2, 58, 18, 237, 192, 133, 204, 191, 25, 191, 26, 135, 200, 166, 18, 2, 10, 26, 215, 238, 135, 4, 204, 193, 101, 15, 204, 239, 203, 111, 165, 161, 173, 106, 238, 237, 196, 80, 195, 3, 241, 64, 153, 18, 137, 203, 64, 202, 15, 18, 209, 160, 112, 193, 22, 15, 121, 104, 165, 204, 78, 155, 155, 173, 96, 57, 23, 196, 196, 192, 30, 200, 74, 26, 140, 96, 135, 72, 191, 18, 203, 114, 135, 237, 235, 239, 19, 200, 196, 237, 237, 133, 211, 238, 136, 153, 25, 104, 191, 204, 113, 133, 22, 239, 147, 80, 192, 239, 18, 125, 15, 25, 137, 26, 202, 155, 221, 117, 30, 204, 26, 215, 135, 153, 200, 3, 96, 150, 2, 104, 204, 148, 79, 155, 58, 78, 78, 15, 176, 221, 74, 18, 147, 161, 151, 26, 200, 176, 158, 191, 155, 209, 133, 191, 133, 113, 196, 194, 166, 200, 112, 57, 30, 175, 156, 258, 163, 221, 15, 23, 114, 191, 176, 151, 15, 97, 18, 221, 191, 22, 18, 125, 199, 160, 135, 203, 21, 18, 57, 137, 194, 161, 199, 200, 215, 209, 18, 176, 23, 96, 135, 203, 258, 200, 191, 112, 155, 133, 71, 15, 158, 18, 25, 192, 26, 112, 200, 3, 112, 200, 200, 94, 191, 57, 192, 133, 160, 175, 221, 23, 208, 10, 155, 101, 40, 72, 204, 25, 200, 239, 3, 21, 18, 104, 153, 15, 30, 148, 140, 35, 156, 176, 25, 194, 200, 30, 80, 204, 114, 18, 173, 19, 213, 104, 98, 78, 15, 151, 218, 237, 10, 200, 194, 18, 23, 96, 3, 139, 196, 71, 3, 118, 13, 80, 155, 22, 117, 97, 239, 2, 133, 133, 25, 23, 173, 204, 0, 72, 175, 237, 194, 23, 237, 80, 196, 258, 215, 241, 176, 78, 78, 196, 94, 202]\n",
            "[30, 237, -1, 16, 104, 237, 135, 166, 108, 15, 200, 161, 58, 156, 58, -1, 96, 156, 15, 97, 197, -1, -1, 161, 98, 202, 97, 191, 21, 196, 193, 19, 204, 70, -1, 30, 15, 114, 96, 135, 96, 69, 58, 115, 114, 191, 69, 150, 161, 202, 135, 80, 139, 175, 96, 195, 258, -1, 240, 238, -1, 104, 166, 109, 15, 25, 108, 18, 3, 155, 153, 194, 173, 25, 98, 96, -1, 161, 23, 135, 69, 76, 3, 25, 173, -1, -1, 78, 3, 25, 121, 191, 69, 104, 25, 62, 104, 202, 200, -1, 175, 153, 74, 133, 15, 240, 196, 156, 204, 21, -1, 110, 50, 71, 3, 240, 135, 153, 160, 50, 197, 195, 176, 62, 109, 237, 135, 107, 204, 106, 240, 21, 196, 117, 240, -1, 15, 18, 26, -1, 15, 22, 3, 195, 25, -1, 192, 135, 196, 238, 104, 15, 137, 175, 156, -1, -1, 200, 191, 104, 158, 191, 156, 156, -1, 50, 133, 193, 160, 176, 135, 163, 2, 107, 197, 235, -1, 237, 196, 176, 104, 50, 237, 160, -1, -1, 203, -1, 78, 10, 26, 204, 3, 10, -1, 176, 74, 78, 57, 237, 135, -1, 18, 95, 200, 196, 213, 237, -1, 192, 191, 107, 114, 237, 18, 56, -1, 193, 117, 97, 238, 238, 196, 98, 30, 176, 135, 97, 156, 114, -1, 58, -1, -1, 193, 94, 238, 15, 173, 135, 155, 213, 208, 193, 10, 10, 175, 204, 23, -1, 106, 115, 150, -1, 238, 95, 30, -1, -1, 111, 194, 165, 69, 135, 237, 196, 25, 202, 197, 191, 96, 237, 209, 94, 2, 19, 221, 98, 104, 147, 21, 78, 97, 200, 19, 26, 195, 78, 25, 135, -1, 175, -1, 50, 166, 104, 30, 165, 24, 135, 133, 22, 109, 64, -1, 221, 96, 147, 238, 78, 172, 15, 3, 165, 21, 15, 23, 200, -1, 135, -1, 133, 161, 18, 19, 195, 15, 202, 102, 18, 176, 163, 29, 26, 25, 152, 15, 200, 98, 221, 22, 207, 173, 139, 21, 3, 238, 21, 118, 195, 62, 150, 202, 238, 40, 121, 25, 19, 15, -1, -1, 78, 137, 25, -1, 196, 158, 93, 135, 109, -1, 25, 109, 109, 133, 135, 237, -1, 71, 25, 3, -1, 200, 200, 69, 237, 193, 15, 78, 40, -1, 191, -1, 176, 135, -1, 203, 209, 203, 69, 238, 102, 30, 150, 107, 97, -1, 147, 106, 133, 195, 147, 26, -1, 26, -1, 10, 175, 163, 156, 70, 15, 218, 78, -1, 22, 156, 209, 200, 18, 94, -1, -1, 165, 193, 207, 204, 155, 193, 149, 25, -1, 161, 170, 240, 19, -1, 191, 104, 238, 191, 204, 191, -1, -1, 57, 195, 30, -1, 118, 200, 193, 176, 40, 161, 18, 121, 149, 163, 135, -1, 22, 136, 135, 25, -1, -1, 196, 203, 30, -1, -1, 140, 99, 96, -1, 215, 133, 221, 21, 140, -1, 156, 163, 113, 19, 149, 70, 108, 98, 2, 200, 70, 208, 15, 50, 161, 18, 15, 135, 137, 135, 78, 3, 50, 69, 133, 192, -1, 135, 112, 163, 196, 135, -1, 238, 22, 15, 171, 58, -1, 195, 192, -1, 204, 191, 25, 208, -1, -1, -1, 114, 18, 2, -1, 26, 238, 238, 196, -1, 204, 193, 101, 15, 204, 114, 203, 112, 165, 161, 173, 106, 213, 237, 196, 80, 240, 3, -1, 58, -1, 18, 137, 203, 155, 202, 15, 18, 209, 160, 238, 193, 22, -1, 121, 104, 165, 204, 78, -1, 114, 173, 97, 57, 23, 196, -1, 192, 30, 200, -1, 26, 140, 36, 135, 72, 196, 18, 203, 114, 135, 237, -1, 2, 19, 200, 196, 237, 237, 133, 211, 238, 208, 153, 153, 104, 191, 204, -1, 133, 25, -1, 147, 80, 192, 118, -1, -1, 15, 25, 137, 15, 202, 25, 221, 15, 30, 161, 15, 213, 135, 153, 200, 3, 96, 152, 2, 104, 204, 148, 79, 155, 205, 78, 79, 15, 176, 221, 74, 18, 147, 161, 114, 26, 200, 238, 158, -1, 40, 209, 133, 191, 237, 113, 196, -1, 166, 200, 112, 156, 30, 199, -1, 258, 163, 238, -1, 23, 114, 191, 176, 150, 15, 97, 18, 221, 208, 40, 18, -1, 199, 160, 135, 203, 21, 18, 57, 137, 194, 106, 199, 200, 213, 209, -1, 176, 23, 121, 135, 203, 258, 200, 191, 112, 155, 133, 71, 15, 110, 18, 19, 192, 26, -1, 200, 3, 111, 200, 200, 94, 191, 69, 192, -1, 202, 175, 221, 23, -1, 10, -1, -1, 40, -1, 161, 25, -1, -1, 3, 21, -1, 104, -1, 15, 30, 148, 140, 36, 156, 176, 25, 194, 200, 56, 80, 204, 114, 18, -1, 19, 213, -1, 98, 78, 15, 150, -1, 237, 10, 200, -1, 18, 23, 96, 3, 139, 196, 96, 3, -1, 196, 258, 155, 22, 117, 97, 240, -1, 133, 133, 25, 23, -1, 204, -1, 72, 2, 237, 195, 23, 237, -1, 196, 160, 213, -1, 175, 78, 78, 196, 94, 160]\n",
            "[15, 237, -1, 16, 104, 203, 135, 166, 108, 15, 200, 161, 58, -1, 205, -1, 133, -1, 15, 97, 197, -1, -1, 161, 98, 3, 97, 191, -1, 196, 193, 19, 204, 58, -1, 40, 15, 114, 96, 135, 96, 69, 208, 155, 114, 191, 69, 151, -1, 191, 135, 78, 40, 175, 15, 237, 237, -1, 237, 98, -1, 104, 166, 109, 15, 155, 108, -1, 202, 25, 153, 194, 173, 25, 98, -1, -1, 161, 23, 135, 71, 76, 3, 25, 173, -1, -1, 78, 200, -1, 121, 191, -1, 121, 25, 29, 104, 3, 200, -1, 175, 153, -1, 133, 50, 237, 173, 156, 204, 21, -1, 237, 71, 71, 3, 195, -1, 135, 258, 25, 195, 197, 176, -1, 109, 71, 135, 107, 204, 106, 258, 21, 196, 117, 237, -1, 15, 18, 26, -1, 15, 23, 200, 195, -1, -1, 238, 136, 196, 258, 104, 15, -1, 175, 146, -1, -1, 200, -1, 104, 158, 191, 156, 202, -1, 25, 196, 193, 133, 172, 135, 163, -1, 3, -1, 235, -1, 237, -1, 238, 135, 50, 237, 137, -1, -1, 203, -1, 78, -1, 26, 204, 3, -1, -1, 238, 4, 78, 57, 237, 133, -1, 18, 95, 200, 196, 213, 237, -1, 192, 191, 98, 114, 165, 18, 197, -1, 193, 117, 97, 238, 238, 196, 98, 15, 176, 135, 97, 135, 114, -1, 58, -1, -1, 193, 94, 213, 15, 112, 135, 155, 213, 208, -1, 10, 10, 175, 204, 25, -1, 98, 99, 151, -1, 204, 95, 30, -1, -1, 111, 197, 165, 69, 95, 237, 133, 25, 3, 197, 191, 106, 98, 209, 94, 175, -1, 221, 98, -1, 147, 173, 165, 97, 78, 19, 26, 98, 78, -1, 135, -1, 175, -1, 50, 3, 104, 30, 165, 22, 135, 98, 22, 109, 64, -1, 221, 161, 147, 165, 78, 172, 26, 3, 165, 21, 15, 25, 200, -1, -1, -1, 237, 161, 155, 19, 238, 15, 202, 102, 19, 176, 163, 29, 26, 155, 152, 15, 200, 98, 221, -1, 207, -1, -1, 21, 3, 208, 29, 118, 195, 78, 150, 135, 238, 40, 121, 25, 19, 237, -1, -1, 78, 137, 25, -1, 196, 158, -1, 202, 109, -1, 25, 109, 108, 133, 135, 237, -1, 71, 25, 3, -1, 200, -1, -1, 237, 193, 15, 78, 40, -1, 191, -1, 173, 135, -1, 203, 209, 203, 69, 191, 96, 30, 150, 166, 97, -1, -1, 106, 196, 3, 147, 26, -1, 26, -1, 10, 175, 163, 19, 58, 25, 3, 78, -1, 22, -1, 209, 200, 18, 94, -1, -1, 165, 193, 207, 204, 50, 193, 149, 25, -1, 161, 170, 258, 19, -1, 238, 104, 238, 135, 204, 191, -1, -1, 57, 197, -1, -1, 166, 200, 193, 176, 40, 161, 18, 104, -1, -1, 167, -1, 22, 136, 135, 25, -1, -1, 196, 203, 30, -1, -1, 84, 258, 71, -1, 213, 237, -1, 258, 29, -1, 156, 163, -1, 19, 149, 58, 108, 238, 3, -1, 155, 137, -1, 50, 161, 18, 15, 102, 137, 135, 78, 3, 29, 57, 133, 192, -1, 135, 112, 163, -1, 135, -1, 238, 22, -1, 191, 58, -1, 237, 192, -1, 204, 191, 50, 191, -1, -1, -1, 166, 18, -1, -1, 26, 191, 238, 161, -1, 204, 193, 149, 15, 204, 114, 203, 112, 165, 161, 173, 106, 238, 237, 196, 80, 194, 238, -1, 53, -1, 18, 137, 203, 35, -1, 15, 18, 209, 258, 238, 193, 22, -1, 98, 104, 165, 204, 78, -1, 110, 173, 97, 57, 23, -1, -1, 192, 30, 200, -1, 26, 140, 71, 135, 72, 191, 18, 203, 114, 163, -1, -1, 2, 19, 200, 196, 237, 237, 133, 211, 258, 136, 153, 25, -1, 191, 161, -1, 237, 22, -1, 147, 80, 192, -1, -1, -1, 15, 25, -1, 26, 202, 71, 221, 117, 30, 204, 26, 213, 135, -1, 200, 3, 96, 152, 3, 104, 204, 148, 78, 71, 205, 78, -1, 15, 176, 221, 74, 18, 147, 161, 114, 26, 200, 176, 10, -1, -1, 209, 133, 191, 158, -1, 196, -1, 166, 78, 112, 57, 30, 199, -1, 258, 163, 221, -1, 25, 114, 191, 176, 151, 15, 97, 19, 221, 136, 40, 18, -1, 199, 160, 135, 203, 21, 18, 57, 137, 194, 109, 199, 200, 213, 209, -1, 176, -1, 121, 135, 203, 258, 200, 191, 112, -1, 133, -1, 15, 158, 18, 19, -1, 26, -1, 200, 3, 112, 200, 200, 94, -1, 69, 192, -1, 235, 175, 221, 23, -1, -1, -1, -1, 40, -1, 204, 25, -1, -1, 3, -1, -1, 104, -1, 15, 30, 148, 140, 71, -1, 172, 25, 194, 200, 58, 80, 204, 114, 18, -1, 19, 213, -1, 98, -1, 15, 150, -1, 237, 10, 200, -1, 18, 23, -1, 3, 139, -1, 71, 3, -1, 3, 196, 35, 22, 117, 97, 237, -1, 133, 40, 25, 23, -1, 204, -1, 50, 175, 235, 197, 23, 237, -1, 196, 258, 213, -1, 172, 78, 78, 196, 94, 155]\n",
            "[15, 196, -1, 16, 104, 203, 149, 166, 108, 15, -1, 161, 58, -1, 205, -1, 133, -1, 15, 97, 197, -1, -1, 161, 96, 3, 97, 191, -1, 196, 193, 19, 204, -1, -1, 30, 15, 114, 96, 135, -1, 69, 191, 110, 111, 191, -1, 151, -1, 202, 135, 78, 139, 175, 96, 195, 258, -1, 99, 238, -1, 104, 166, 109, 15, -1, 108, -1, 3, 18, 153, -1, 173, 25, 98, -1, -1, 161, 23, 135, 204, -1, 3, -1, 173, -1, -1, -1, 3, -1, 94, 191, -1, -1, -1, 78, 104, 3, 200, -1, -1, 153, -1, 133, 15, -1, 173, 156, 204, 21, -1, 110, 155, 71, 3, 195, -1, 153, 113, -1, 195, -1, 177, -1, 109, 237, 135, 107, 204, 106, 258, 21, 196, -1, 258, -1, 15, 18, 26, -1, 15, 23, 3, 195, -1, -1, 192, 136, -1, 238, 104, 15, -1, 175, 156, -1, -1, -1, -1, 104, 158, 191, 156, 3, -1, -1, 196, 258, 160, 176, 135, 163, -1, 3, -1, -1, -1, 237, -1, -1, 104, -1, 237, 133, -1, -1, 203, -1, 78, -1, -1, 204, 3, -1, -1, 213, -1, 78, 57, 237, 135, -1, 18, -1, 200, 196, 213, 237, -1, 191, -1, 107, 114, 29, 18, 58, -1, 193, 117, 99, 238, 238, 161, 98, 15, 176, 135, 97, 155, 114, -1, 58, -1, -1, 193, -1, 213, -1, 195, 135, 155, 213, -1, -1, 10, 10, 175, 204, 23, -1, 258, 115, 150, -1, 204, 95, 30, -1, -1, 111, 197, 165, 69, 95, 166, 196, 25, 202, -1, 191, 96, 133, 209, -1, 175, -1, 238, 98, -1, 147, 173, 78, 97, 200, 19, 26, 146, -1, -1, 135, -1, 175, -1, 72, 3, 104, 30, 165, 22, 140, 166, 22, 109, 64, -1, 221, 161, 147, 165, 78, 175, 15, 195, 165, 21, 15, 23, 200, -1, -1, -1, 158, 161, 18, 19, -1, 15, 202, 102, 18, -1, 163, 29, 26, 15, 152, -1, 200, 98, 238, -1, 207, -1, -1, -1, 213, 166, -1, 200, 195, 78, 151, 136, 238, 40, -1, 25, 19, 258, -1, -1, 78, 137, 25, -1, 196, 158, -1, -1, 109, -1, 23, -1, 109, 133, 136, 166, -1, 204, 25, 3, -1, -1, -1, -1, 237, 193, -1, 78, 40, -1, -1, -1, 177, 135, -1, 203, 191, 203, 69, 238, 102, 30, 151, 166, 97, -1, -1, 106, 133, 3, 147, 26, -1, 26, -1, 10, 175, 163, -1, 58, 15, 218, 78, -1, 22, -1, 209, 200, 18, 94, -1, -1, 29, 193, 207, 204, 15, 193, 149, 15, -1, 161, 170, 258, 19, -1, -1, 104, 238, 191, 204, 191, -1, -1, 57, 195, -1, -1, 166, -1, 193, 176, 40, 161, 18, 104, -1, -1, 166, -1, 22, 136, 135, 25, -1, -1, 196, -1, 30, -1, -1, 258, 258, 96, -1, 213, 258, -1, -1, 29, -1, 72, 163, -1, 19, 153, 18, 108, 98, 173, -1, 154, -1, -1, 237, -1, 18, 15, 135, 2, 155, 78, 3, 205, 57, 133, 192, -1, 135, 112, 163, -1, 135, -1, 238, 22, -1, 191, 58, -1, 195, -1, -1, 204, 191, 25, 208, -1, -1, -1, 114, 18, -1, -1, 26, 238, 238, 111, -1, 204, -1, 149, 15, 204, 258, 203, 112, 165, 161, 173, 106, 238, 237, 196, -1, 195, 238, -1, 58, -1, 18, 137, 203, 35, -1, 15, -1, 209, 202, 238, 193, 40, -1, 98, -1, 18, 204, 78, -1, 106, 173, 97, 57, 23, -1, -1, 192, 30, 200, -1, 26, -1, 35, 205, 72, 238, 18, 203, 114, 135, -1, -1, 2, 19, 200, 196, 237, 98, 133, 211, 258, 136, 154, 25, -1, 192, 161, -1, 133, 40, -1, -1, 80, 192, -1, -1, -1, 15, 15, -1, 15, -1, 71, 221, -1, 30, 204, 26, 213, 149, -1, 200, 3, 96, 151, 3, 104, 204, 148, 78, 155, 205, 78, -1, 15, 176, 221, 74, 18, 147, 161, 114, 15, 200, 98, -1, -1, -1, 209, 133, -1, 158, -1, 195, -1, 166, 200, 112, 238, 30, 199, -1, 258, 163, 238, -1, 23, 114, 191, -1, 151, 15, -1, 19, 238, 167, 50, 18, -1, 199, 3, 135, 203, 21, 18, 57, 137, 197, 108, 199, 200, 238, 209, -1, 176, -1, 121, 135, -1, 258, 200, 191, 112, -1, 3, -1, 15, 158, 18, 19, -1, 35, -1, 200, 3, 80, 200, 200, 94, -1, 69, 192, -1, 202, 176, 221, -1, -1, -1, -1, -1, 40, -1, 161, 25, -1, -1, 3, -1, -1, 104, -1, 15, 30, 148, 140, 71, -1, 177, -1, -1, 200, -1, -1, 204, 114, -1, -1, 19, 213, -1, 98, -1, 15, 151, -1, 191, -1, 200, -1, 18, 23, -1, 3, 139, -1, 71, 3, -1, 196, 196, 35, 40, 118, 97, -1, -1, 137, 2, 25, 23, -1, 204, -1, 72, 175, 161, 197, 23, 15, -1, 196, 258, 213, -1, 2, 78, 78, 196, -1, 155]\n",
            "[30, 237, -1, 16, 104, 3, 135, 166, -1, 15, -1, 161, 58, -1, 13, -1, 96, -1, 15, 97, -1, -1, -1, 161, 98, 202, 97, 191, -1, 196, 193, 19, 204, -1, -1, 156, 15, 114, 96, 135, -1, 69, 3, -1, 114, 191, -1, 151, -1, -1, 135, 78, 22, 176, 96, -1, -1, -1, 135, 173, -1, 104, 166, 109, 15, -1, 108, -1, -1, -1, 153, -1, -1, 25, 98, -1, -1, -1, 23, 135, 71, -1, 3, -1, 173, -1, -1, -1, -1, -1, 121, 203, -1, -1, -1, 78, -1, 3, 200, -1, -1, 153, -1, 133, 15, -1, -1, 156, 204, 21, -1, 258, 69, 71, 3, 258, -1, 135, 258, -1, 197, -1, -1, -1, 109, 71, 135, 107, 204, 106, 258, 21, 78, -1, 258, -1, 15, 18, 26, -1, -1, 23, 3, 197, -1, -1, 191, 135, -1, 258, 104, 15, -1, -1, -1, -1, -1, -1, -1, 104, -1, 191, 156, -1, -1, -1, 156, 193, 160, 176, 135, 163, -1, 199, -1, -1, -1, 237, -1, -1, 140, -1, 237, 137, -1, -1, 203, -1, -1, -1, -1, 204, 3, -1, -1, -1, -1, 78, 57, 258, 135, -1, 18, -1, -1, -1, 213, 258, -1, 191, -1, 107, 98, 258, 19, 202, -1, 193, 117, 99, 135, 238, 161, 98, 30, 176, 135, -1, -1, 114, -1, 58, -1, -1, 193, -1, 213, -1, 2, 135, 70, 213, -1, -1, 10, 10, 176, 204, 23, -1, 98, 115, 150, -1, 204, 95, -1, -1, -1, 112, 197, 29, 69, 135, 166, -1, 25, 3, -1, 191, -1, 98, 209, -1, 175, -1, 221, 98, -1, -1, 173, 78, 97, 200, 19, -1, 98, -1, -1, 135, -1, 175, -1, 15, 3, 104, 30, 165, 22, 135, -1, 22, 109, 64, -1, 221, -1, 106, 146, 78, 2, 40, -1, 165, 15, 15, -1, 200, -1, -1, -1, 158, 109, -1, 19, -1, -1, 202, 102, -1, -1, 163, 96, 26, -1, -1, -1, 200, 98, 221, -1, 207, -1, -1, -1, 3, 166, -1, 200, 197, 78, 150, 135, 213, 40, -1, 25, 19, 258, -1, -1, 78, 137, 25, -1, 196, -1, -1, -1, 109, -1, 25, -1, 109, 133, 135, 166, -1, 71, 25, 3, -1, -1, -1, -1, 237, 193, -1, 78, 40, -1, -1, -1, 176, 135, -1, 203, 191, 203, 69, 238, 102, 30, 151, 166, 97, -1, -1, 106, 133, 195, 118, 26, -1, 26, -1, 155, 175, 163, -1, 58, 25, 3, 78, -1, 22, -1, 209, 78, 19, 94, -1, -1, 29, 193, 207, 204, 156, 193, -1, 25, -1, 204, 170, 258, 19, -1, -1, -1, 238, 135, 204, 191, -1, -1, 57, -1, -1, -1, 166, -1, 193, 176, 40, 161, -1, 121, -1, -1, 166, -1, -1, 135, 135, 25, -1, -1, 196, -1, 30, -1, -1, 160, 258, 96, -1, 213, 160, -1, -1, 29, -1, 156, 163, -1, 19, 149, 118, 109, -1, 2, -1, 155, -1, -1, 15, -1, 18, -1, 135, 2, 135, 78, 3, 29, 57, 133, -1, -1, 135, 112, 163, -1, 135, -1, 173, 22, -1, 171, 58, -1, 238, -1, -1, 204, 191, -1, -1, -1, -1, -1, 166, 18, -1, -1, 26, 191, 203, 196, -1, 204, -1, 149, -1, 204, 15, 203, 112, 165, 161, 173, 106, 238, 237, 196, -1, -1, 238, -1, 35, -1, 19, 137, 203, 69, -1, 15, -1, 209, 160, 238, 193, 40, -1, 98, -1, 165, 204, 78, -1, 114, -1, 97, 57, 23, -1, -1, 192, 30, 200, -1, -1, -1, 26, 135, 72, 238, 18, 203, 114, 135, -1, -1, -1, -1, 200, 196, 237, 237, 133, 211, -1, 135, 135, 25, -1, 191, 161, -1, 156, -1, -1, -1, 3, 192, -1, -1, -1, 15, 25, -1, -1, -1, 71, 221, -1, 30, -1, 26, 213, 135, -1, 200, 3, 96, 151, 2, -1, 204, 148, 78, 156, 205, -1, -1, 15, 176, 221, 4, 18, 147, 161, 152, 26, 200, 176, -1, -1, -1, 209, 133, -1, 158, -1, 197, -1, 166, -1, 112, -1, 30, 199, -1, -1, 163, 221, -1, 23, 114, 191, -1, 151, 15, -1, 19, -1, 191, 40, 18, -1, 199, 160, 135, 203, 21, 18, 57, 137, 197, 108, 199, 200, 238, 209, -1, -1, -1, 96, 135, -1, 258, 200, 191, -1, -1, 133, -1, 15, 158, 18, 19, -1, 35, -1, 200, 3, 112, 200, 200, 94, -1, 69, 192, -1, 202, -1, 221, -1, -1, -1, -1, -1, 40, -1, -1, 25, -1, -1, 3, -1, -1, 104, -1, 15, -1, 135, 121, -1, -1, 176, -1, -1, 78, -1, -1, 204, 114, -1, -1, 19, -1, -1, 98, -1, 15, 150, -1, -1, -1, 200, -1, 18, 23, -1, 3, 135, -1, -1, 3, -1, 196, 196, 69, 40, -1, 97, -1, -1, 133, 175, 25, 23, -1, 204, -1, 30, 2, 161, 195, 23, 237, -1, 196, 160, 213, -1, 176, 78, 78, -1, -1, -1]\n",
            "[15, 237, -1, 15, 104, 203, 135, 166, -1, 15, -1, 161, 203, -1, 205, -1, 133, -1, 15, 97, -1, -1, -1, 161, 98, 3, 97, 191, -1, 196, 193, 19, 204, -1, -1, -1, -1, 133, 99, 135, -1, 69, 196, -1, -1, -1, -1, 151, -1, -1, 135, 78, 22, 175, 96, -1, -1, -1, 258, 98, -1, 140, -1, 109, 15, -1, 195, -1, -1, -1, 153, -1, -1, 25, 98, -1, -1, -1, -1, 135, 69, -1, -1, -1, -1, -1, -1, -1, -1, -1, 121, 191, -1, -1, -1, 78, -1, 3, 200, -1, -1, -1, -1, 133, 72, -1, -1, 158, 204, -1, -1, 258, 30, 35, 3, 258, -1, 135, -1, -1, 195, -1, -1, -1, 109, 72, 135, -1, 204, 107, 258, 21, -1, -1, 258, -1, 15, 18, 26, -1, -1, -1, 200, 197, -1, -1, 192, 135, -1, 258, 104, 15, -1, -1, -1, -1, -1, -1, -1, 104, -1, 191, -1, -1, -1, -1, 133, 193, 133, 176, 135, 163, -1, 107, -1, -1, -1, 237, -1, -1, 104, -1, 50, -1, -1, -1, 203, -1, -1, -1, -1, 204, 204, -1, -1, -1, -1, -1, 57, -1, 135, -1, 18, -1, -1, -1, -1, 237, -1, 191, -1, -1, 114, 258, 18, 57, -1, 193, -1, 99, 238, 238, 196, 98, -1, 176, 135, -1, -1, 114, -1, 70, -1, -1, 193, -1, -1, -1, -1, 135, 70, 238, -1, -1, 10, -1, 175, 204, 25, -1, 107, 99, 78, -1, -1, 95, -1, -1, -1, 111, 197, 165, 69, 95, 166, -1, 25, 202, -1, -1, -1, 98, 209, -1, -1, -1, 238, 98, -1, -1, -1, 78, 97, 200, 19, -1, 98, -1, -1, 135, -1, 175, -1, -1, 196, 104, 30, 165, -1, -1, -1, 22, 109, 64, -1, 238, -1, 118, 133, -1, -1, 15, -1, 165, 21, 15, -1, 200, -1, -1, -1, 133, 161, -1, 19, -1, -1, 202, 102, -1, -1, 163, 29, 26, -1, -1, -1, 200, 98, 238, -1, 207, -1, -1, -1, -1, 196, -1, -1, -1, -1, 151, 135, 238, 40, -1, 25, 19, 258, -1, -1, 78, 137, -1, -1, 196, -1, -1, -1, 109, -1, 23, -1, -1, -1, 135, 166, -1, 71, 25, 3, -1, -1, -1, -1, 191, 193, -1, 78, 40, -1, -1, -1, 173, -1, -1, 203, 191, -1, 69, 238, 96, -1, 152, 166, 97, -1, -1, 106, 133, 196, 118, 26, -1, 26, -1, 10, -1, 163, -1, 196, 25, 3, 78, -1, -1, -1, 209, 200, -1, 94, -1, -1, 165, -1, 207, 204, 155, 193, -1, 25, -1, 204, -1, -1, -1, -1, -1, -1, 238, 191, 204, 191, -1, -1, 69, -1, -1, -1, 166, -1, -1, 176, -1, 161, -1, 121, -1, -1, 166, -1, -1, 136, 135, 25, -1, -1, 196, -1, 30, -1, -1, 258, 258, 96, -1, -1, 133, -1, -1, 29, -1, 155, 163, -1, 19, -1, 57, 109, -1, 173, -1, 197, -1, -1, 35, -1, 18, -1, 102, 137, 135, 78, 3, 29, 69, 133, -1, -1, 135, 112, 163, -1, 135, -1, 238, 22, -1, 176, -1, -1, 196, -1, -1, 204, 191, -1, -1, -1, -1, -1, 166, 18, -1, -1, 26, 238, 238, 197, -1, 204, -1, 133, -1, -1, -1, 203, 112, 165, 161, 173, 106, 238, -1, 196, -1, -1, 3, -1, 70, -1, 18, 137, 203, 35, -1, 15, -1, 209, 258, 238, 193, 40, -1, 98, -1, 165, 204, 78, -1, 158, -1, 97, 202, 23, -1, -1, 192, 26, 200, -1, -1, -1, 96, 196, 72, 191, 18, 203, 114, 135, -1, -1, -1, -1, 200, -1, -1, -1, 133, 211, -1, 3, 135, -1, -1, 191, 161, -1, 72, -1, -1, -1, 3, 192, -1, -1, -1, 15, 25, -1, -1, -1, 30, 238, -1, 30, -1, 26, 238, 135, -1, 200, 3, 96, 152, 2, -1, 204, 148, 78, 155, 205, -1, -1, 15, 176, 238, 258, 18, 147, -1, 152, 26, 200, 176, -1, -1, -1, -1, -1, -1, 133, -1, 197, -1, 166, -1, 112, -1, 30, -1, -1, -1, 163, 238, -1, -1, 114, 191, -1, 151, 15, -1, 19, -1, 191, 40, 18, -1, 199, 133, 135, 203, 29, 18, 57, 137, 197, 108, 199, -1, 238, 209, -1, -1, -1, 96, 135, -1, 258, -1, 191, -1, -1, 133, -1, 15, 158, 18, 25, -1, 26, -1, 200, 3, -1, 200, 200, 94, -1, -1, 192, -1, -1, -1, -1, -1, -1, -1, -1, -1, 40, -1, -1, 25, -1, -1, 3, -1, -1, 104, -1, 15, -1, 148, 121, -1, -1, 176, -1, -1, -1, -1, -1, 204, 114, -1, -1, 19, -1, -1, 98, -1, 15, 151, -1, -1, -1, 200, -1, 18, 23, -1, 3, 29, -1, -1, 3, -1, 196, 196, 30, 22, -1, 97, -1, -1, 133, 133, -1, -1, -1, 161, -1, 72, 2, 258, 173, 23, 166, -1, 196, 258, 238, -1, 175, -1, 78, -1, -1, -1]\n",
            "0.5356294536817102\n",
            "[30, 29, 137, 16, 104, 203, 135, 166, 108, 15, 200, 161, 58, 156, 58, 176, 96, 156, 239, 97, 197, 241, 258, 161, 98, 3, 97, 191, 21, 196, 193, 19, 204, 30, 135, 30, 15, 114, 96, 135, 96, 69, 58, 115, 114, 191, 69, 151, 161, 202, 135, 80, 29, 175, 96, 195, 237, 125, 258, 238, 153, 104, 166, 109, 15, 155, 207, 18, 202, 64, 153, 194, 173, 25, 98, 96, 241, 161, 23, 135, 155, 160, 3, 25, 173, 238, 239, 78, 3, 71, 121, 191, 26, 121, 25, 62, 104, 3, 200, 25, 175, 153, 74, 133, 72, 259, 173, 156, 204, 21, 166, 158, 30, 71, 3, 258, 135, 135, 241, 50, 195, 194, 175, 62, 109, 237, 135, 107, 204, 106, 258, 21, 196, 240, 258, 208, 15, 18, 26, 94, 15, 23, 200, 195, 25, 241, 191, 135, 196, 196, 104, 23, 137, 175, 156, 0, 149, 200, 191, 104, 239, 191, 156, 156, 78, 50, 156, 193, 160, 176, 135, 163, 2, 3, 194, 258, 153, 237, 78, 176, 121, 50, 237, 137, 202, 191, 203, 258, 78, 10, 26, 204, 3, 10, 239, 238, 74, 78, 57, 237, 133, 239, 18, 135, 200, 196, 215, 237, 25, 192, 191, 107, 114, 264, 18, 58, 109, 193, 117, 97, 238, 238, 196, 98, 30, 176, 135, 97, 156, 114, 21, 58, 241, 25, 193, 94, 213, 15, 173, 135, 64, 213, 208, 193, 10, 10, 175, 204, 23, 237, 118, 115, 150, 50, 237, 135, 30, 155, 241, 111, 197, 165, 69, 135, 166, 133, 25, 202, 197, 191, 96, 98, 209, 94, 175, 19, 221, 98, 104, 118, 173, 78, 97, 78, 19, 26, 118, 78, 25, 135, 96, 175, 153, 50, 3, 121, 30, 165, 22, 140, 133, 22, 109, 64, 3, 221, 96, 147, 98, 78, 172, 40, 207, 165, 21, 15, 23, 200, 241, 139, 241, 133, 109, 155, 19, 176, 25, 202, 102, 19, 176, 163, 29, 26, 155, 152, 15, 200, 98, 221, 22, 207, 173, 139, 21, 3, 25, 21, 200, 195, 62, 150, 202, 213, 40, 121, 25, 19, 239, 204, 16, 78, 137, 25, 125, 196, 158, 150, 153, 109, 237, 25, 109, 109, 133, 135, 237, 96, 71, 25, 3, 193, 200, 200, 30, 237, 193, 15, 78, 40, 264, 191, 215, 176, 239, 200, 203, 209, 203, 57, 238, 96, 30, 151, 166, 97, 149, 147, 106, 133, 3, 147, 26, 135, 26, 133, 10, 175, 163, 149, 58, 25, 218, 78, 239, 22, 135, 209, 200, 18, 94, 150, 239, 165, 193, 264, 204, 155, 193, 149, 25, 102, 161, 170, 258, 19, 96, 191, 104, 238, 191, 204, 191, 161, 200, 57, 195, 30, 18, 166, 200, 193, 176, 10, 161, 18, 115, 133, 163, 135, 78, 22, 135, 135, 25, 153, 21, 196, 203, 30, 237, 176, 140, 258, 96, 241, 215, 133, 221, 21, 29, 153, 155, 163, 113, 19, 149, 30, 109, 173, 2, 200, 70, 137, 26, 50, 161, 18, 239, 135, 137, 139, 78, 3, 29, 30, 133, 192, 15, 135, 112, 163, 133, 135, 153, 238, 22, 50, 2, 58, 18, 237, 192, 133, 204, 191, 25, 191, 26, 135, 200, 166, 18, 2, 10, 26, 215, 238, 135, 4, 204, 193, 101, 15, 204, 239, 203, 111, 165, 161, 173, 106, 238, 237, 196, 80, 195, 3, 241, 64, 153, 18, 137, 203, 64, 202, 15, 18, 209, 160, 112, 193, 22, 15, 121, 104, 165, 204, 78, 155, 155, 173, 96, 57, 23, 196, 196, 192, 30, 200, 74, 26, 140, 96, 135, 72, 191, 18, 203, 114, 135, 237, 235, 239, 19, 200, 196, 237, 237, 133, 211, 238, 136, 153, 25, 104, 191, 204, 113, 133, 22, 239, 147, 80, 192, 239, 18, 125, 15, 25, 137, 26, 202, 155, 221, 117, 30, 204, 26, 215, 135, 153, 200, 3, 96, 150, 2, 104, 204, 148, 79, 155, 58, 78, 78, 15, 176, 221, 74, 18, 147, 161, 151, 26, 200, 176, 158, 191, 155, 209, 133, 191, 133, 113, 196, 194, 166, 200, 112, 57, 30, 175, 156, 258, 163, 221, 15, 23, 114, 191, 176, 151, 15, 97, 18, 221, 191, 22, 18, 125, 199, 160, 135, 203, 21, 18, 57, 137, 194, 161, 199, 200, 215, 209, 18, 176, 23, 96, 135, 203, 258, 200, 191, 112, 155, 133, 71, 15, 158, 18, 25, 192, 26, 112, 200, 3, 112, 200, 200, 94, 191, 57, 192, 133, 160, 175, 221, 23, 208, 10, 155, 101, 40, 72, 204, 25, 200, 239, 3, 21, 18, 104, 153, 15, 30, 148, 140, 35, 156, 176, 25, 194, 200, 30, 80, 204, 114, 18, 173, 19, 213, 104, 98, 78, 15, 151, 218, 237, 10, 200, 194, 18, 23, 96, 3, 139, 196, 71, 3, 118, 13, 80, 155, 22, 117, 97, 239, 2, 133, 133, 25, 23, 173, 204, 0, 72, 175, 237, 194, 23, 237, 80, 196, 258, 215, 241, 176, 78, 78, 196, 94, 202]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "final_pred_2=[]\n",
        "\n",
        "num_correct=0\n",
        "model_0_2=load_model(loss,accuracy,optimizer,'0-2')\n",
        "mod_labels_0=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_0:\n",
        "    pred_test_0_2=model_0_2.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_0_2 = pred_test_0_2.argmax(axis=1)\n",
        "    mod_labels_0.append(pred_labels_0_2[0])\n",
        "  else:\n",
        "    mod_labels_0.append(-1)\n",
        "\n",
        "model_1_2=load_model(loss,accuracy,optimizer,'1-2')\n",
        "mod_labels_1=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        " \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_1:\n",
        "    pred_test_1_2=model_1_2.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_1_2 = pred_test_1_2.argmax(axis=1)\n",
        "    mod_labels_1.append(pred_labels_1_2[0])\n",
        "  else:\n",
        "    mod_labels_1.append(-1)\n",
        "\n",
        "model_2_2=load_model(loss,accuracy,optimizer,'2-2')\n",
        "mod_labels_2=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_2:\n",
        "    pred_test_2_2=model_2_2.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_2_2 = pred_test_2_2.argmax(axis=1)\n",
        "    mod_labels_2.append(pred_labels_2_2[0])\n",
        "  else:\n",
        "    mod_labels_2.append(-1)\n",
        "\n",
        "model_3_2=load_model(loss,accuracy,optimizer,'3-2')\n",
        "mod_labels_3=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_3:\n",
        "    pred_test_3_2=model_3_2.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_3_2 = pred_test_3_2.argmax(axis=1)\n",
        "    mod_labels_3.append(pred_labels_3_2[0])\n",
        "  else:\n",
        "    mod_labels_3.append(-1)\n",
        "\n",
        "model_4_2=load_model(loss,accuracy,optimizer,'4-2')\n",
        "mod_labels_4=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_4:\n",
        "    pred_test_4_2=model_4_2.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_4_2 = pred_test_4_2.argmax(axis=1)\n",
        "    mod_labels_4.append(pred_labels_4_2[0])\n",
        "  else:\n",
        "    mod_labels_4.append(-1)\n",
        "\n",
        "model_5_2=load_model(loss,accuracy,optimizer,'5-2')\n",
        "mod_labels_5=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_5:\n",
        "    pred_test_5_2=model_5_2.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_5_2 = pred_test_5_2.argmax(axis=1)\n",
        "    mod_labels_5.append(pred_labels_5_2[0])\n",
        "  else:\n",
        "    mod_labels_5.append(-1)\n",
        "\n",
        "print(mod_labels_0)\n",
        "print(mod_labels_1)\n",
        "print(mod_labels_2)\n",
        "print(mod_labels_3)\n",
        "print(mod_labels_4)\n",
        "print(mod_labels_5)\n",
        "\n",
        "\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  fin_labels=[]\n",
        "  if mod_labels_0[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  if mod_labels_1[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  if mod_labels_2[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  if mod_labels_3[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  if mod_labels_4[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  if mod_labels_5[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  res=max(set(fin_labels), key = fin_labels.count)\n",
        "  final_pred_2.append(res)\n",
        "\n",
        "  if res==new_val_label_0[i]:\n",
        "    num_correct=num_correct+1\n",
        "\n",
        "accuracy=num_correct/len(new_val_label_0)\n",
        "print(accuracy)\n",
        "  \n",
        "print(final_pred_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snuZ1OsZn3cC",
        "outputId": "cc7a27d2-e3b7-460e-9f7b-85a323bc2dd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.5356294536817102\n",
            "Weighted F1: 0.4800959826708471\n",
            "Micro F1: 0.5356294536817102\n",
            "Weighted Precision: 0.47053681863906016\n",
            "Micro Precision: 0.5356294536817102\n",
            "Weighted Recall: 0.5356294536817102\n",
            "Micro Recall: 0.5356294536817102\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "\n",
        "accuracy=accuracy_score(new_val_label_0, final_pred_2)\n",
        "print(\"Accuracy: \"+str(accuracy))\n",
        "total_accuracy=total_accuracy+accuracy\n",
        "\n",
        "weighted_f1=f1_score(new_val_label_0,final_pred_2, average='weighted')\n",
        "print(\"Weighted F1: \"+ str(weighted_f1))\n",
        "total_weighted_f1=total_weighted_f1+weighted_f1\n",
        "micro_f1=f1_score(new_val_label_0,final_pred_2, average='micro')\n",
        "print(\"Micro F1: \"+ str(micro_f1))\n",
        "total_micro_f1=total_micro_f1+micro_f1\n",
        "\n",
        "weighted_precision=precision_score(new_val_label_0, final_pred_2, average='weighted')\n",
        "print(\"Weighted Precision: \" + str(weighted_precision))\n",
        "total_weighted_precision=total_weighted_precision+weighted_precision\n",
        "micro_precision=precision_score(new_val_label_0, final_pred_2, average='micro')\n",
        "print(\"Micro Precision: \" + str(micro_precision))\n",
        "total_micro_precision=total_micro_precision+micro_precision\n",
        "\n",
        "weighted_recall=recall_score(new_val_label_0, final_pred_2, average='weighted')\n",
        "print(\"Weighted Recall: \" + str(weighted_recall))\n",
        "total_weighted_recall=total_weighted_recall+weighted_recall\n",
        "micro_recall=recall_score(new_val_label_0, final_pred_2, average='micro')\n",
        "print(\"Micro Recall: \" + str(micro_recall))\n",
        "total_micro_recall=total_micro_recall+micro_recall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Z-lAlHvoBj6",
        "outputId": "ac82dd52-0c17-4815-d907-fc1ee53d7758"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_20\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_41 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_42 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_41[0][0]',               \n",
            " el)                            thPoolingAndCrossAt               'input_42[0][0]']               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_20 (S  (None, 768)         0           ['tf_roberta_model[20][0]']      \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " dense_40 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_20[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dropout_57 (Dropout)           (None, 512)          0           ['dense_40[0][0]']               \n",
            "                                                                                                  \n",
            " dense_41 (Dense)               (None, 279)          143127      ['dropout_57[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model: \"model_21\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_43 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_44 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_43[0][0]',               \n",
            " el)                            thPoolingAndCrossAt               'input_44[0][0]']               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_21 (S  (None, 768)         0           ['tf_roberta_model[21][0]']      \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " dense_42 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_21[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dropout_58 (Dropout)           (None, 512)          0           ['dense_42[0][0]']               \n",
            "                                                                                                  \n",
            " dense_43 (Dense)               (None, 279)          143127      ['dropout_58[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model: \"model_22\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_45 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_46 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_45[0][0]',               \n",
            " el)                            thPoolingAndCrossAt               'input_46[0][0]']               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_22 (S  (None, 768)         0           ['tf_roberta_model[22][0]']      \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " dense_44 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_22[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dropout_59 (Dropout)           (None, 512)          0           ['dense_44[0][0]']               \n",
            "                                                                                                  \n",
            " dense_45 (Dense)               (None, 279)          143127      ['dropout_59[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model: \"model_23\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_47 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_48 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_47[0][0]',               \n",
            " el)                            thPoolingAndCrossAt               'input_48[0][0]']               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_23 (S  (None, 768)         0           ['tf_roberta_model[23][0]']      \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " dense_46 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_23[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dropout_60 (Dropout)           (None, 512)          0           ['dense_46[0][0]']               \n",
            "                                                                                                  \n",
            " dense_47 (Dense)               (None, 279)          143127      ['dropout_60[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model: \"model_24\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_49 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_50 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_49[0][0]',               \n",
            " el)                            thPoolingAndCrossAt               'input_50[0][0]']               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_24 (S  (None, 768)         0           ['tf_roberta_model[24][0]']      \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " dense_48 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_24[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dropout_61 (Dropout)           (None, 512)          0           ['dense_48[0][0]']               \n",
            "                                                                                                  \n",
            " dense_49 (Dense)               (None, 279)          143127      ['dropout_61[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model: \"model_25\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_51 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_52 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_51[0][0]',               \n",
            " el)                            thPoolingAndCrossAt               'input_52[0][0]']               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_25 (S  (None, 768)         0           ['tf_roberta_model[25][0]']      \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " dense_50 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_25[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dropout_62 (Dropout)           (None, 512)          0           ['dense_50[0][0]']               \n",
            "                                                                                                  \n",
            " dense_51 (Dense)               (None, 279)          143127      ['dropout_62[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "[30, 196, 137, 50, 104, 237, 149, 166, 108, 15, 200, 161, 58, 156, 58, 176, 133, 156, 15, 97, 197, 241, 240, 161, 98, 202, 97, 191, 21, 196, 193, 19, 204, 56, 191, 30, 15, 114, 96, 135, 99, 69, 58, 156, 13, 191, 69, 151, 161, 202, 135, 80, 40, 176, 96, 237, 264, 25, 258, 238, 153, 104, 166, 109, 15, 25, 108, 18, 3, 62, 153, 194, 173, 25, 98, 101, 241, 202, 23, 135, 204, 160, 3, 25, 173, 238, 0, 78, 200, 71, 115, 203, 158, 121, 25, 62, 104, 3, 200, 25, 175, 153, 74, 133, 72, 237, 173, 156, 204, 21, 166, 110, 70, 71, 3, 240, 135, 135, 160, 50, 195, 195, 175, 62, 109, 237, 149, 107, 204, 106, 258, 21, 78, 147, 237, 208, 15, 18, 26, 93, 15, 23, 202, 237, 25, 153, 192, 135, 196, 240, 104, 23, 137, 175, 156, 0, 149, 200, 191, 104, 133, 191, 156, 156, 78, 50, 156, 193, 160, 176, 135, 163, 2, 3, 195, 240, 153, 237, 196, 176, 104, 50, 237, 137, 202, 191, 203, 258, 78, 10, 26, 202, 3, 10, 239, 238, 74, 78, 57, 237, 135, 97, 18, 135, 200, 196, 213, 237, 125, 191, 191, 98, 114, 264, 19, 202, 109, 193, 117, 99, 238, 238, 161, 98, 30, 176, 149, 97, 155, 114, 21, 58, 241, 25, 193, 94, 238, 15, 98, 135, 64, 213, 208, 80, 10, 10, 175, 204, 25, 237, 258, 115, 79, 30, 196, 95, 30, 25, 241, 111, 197, 165, 69, 135, 237, 133, 25, 202, 197, 191, 96, 237, 191, 94, 175, 19, 221, 98, 104, 147, 173, 78, 97, 78, 19, 26, 195, 78, 25, 135, 96, 175, 153, 50, 196, 104, 30, 165, 22, 135, 133, 22, 109, 64, 3, 221, 202, 147, 165, 78, 175, 29, 207, 165, 21, 15, 23, 200, 241, 140, 241, 133, 74, 133, 19, 2, 25, 202, 102, 19, 176, 163, 22, 26, 25, 152, 15, 200, 98, 221, 22, 207, 173, 29, 21, 258, 208, 21, 200, 195, 62, 150, 202, 238, 29, 121, 25, 19, 237, 204, 16, 78, 137, 25, 40, 196, 158, 140, 203, 109, 237, 25, 109, 109, 133, 135, 237, 97, 71, 25, 3, 193, 200, 200, 30, 237, 193, 15, 78, 40, 160, 191, 215, 176, 135, 200, 203, 191, 203, 69, 238, 102, 30, 151, 166, 97, 149, 118, 106, 133, 3, 147, 26, 135, 26, 258, 10, 2, 163, 155, 58, 25, 218, 78, 40, 22, 156, 209, 200, 18, 94, 150, 98, 29, 193, 264, 204, 72, 193, 149, 25, 102, 161, 170, 258, 19, 96, 191, 104, 3, 135, 204, 191, 161, 200, 57, 195, 30, 40, 166, 200, 193, 176, 40, 161, 18, 115, 149, 163, 166, 78, 22, 135, 135, 25, 153, 21, 196, 203, 30, 237, 238, 258, 115, 96, 241, 215, 156, 221, 21, 29, 153, 155, 163, 113, 19, 149, 62, 108, 175, 3, 200, 70, 137, 26, 50, 161, 18, 15, 135, 137, 135, 78, 3, 50, 57, 133, 192, 264, 135, 112, 163, 133, 135, 153, 238, 22, 50, 171, 58, 18, 237, 192, 98, 204, 191, 25, 208, 26, 161, 200, 166, 18, 2, 15, 26, 191, 203, 196, 74, 204, 193, 149, 15, 204, 239, 203, 3, 165, 161, 173, 106, 238, 237, 196, 80, 195, 3, 241, 56, 93, 18, 137, 203, 64, 3, 15, 18, 209, 160, 258, 193, 22, 15, 115, 104, 165, 204, 78, 125, 155, 173, 97, 57, 23, 196, 133, 192, 30, 200, 74, 26, 140, 38, 135, 72, 191, 18, 203, 114, 135, 237, 258, 2, 19, 200, 196, 237, 237, 133, 211, 258, 208, 153, 25, 104, 192, 161, 113, 258, 22, 125, 147, 80, 192, 146, 191, 130, 30, 25, 137, 26, 202, 71, 221, 117, 30, 204, 26, 213, 149, 153, 200, 258, 96, 151, 3, 104, 204, 148, 79, 155, 58, 78, 78, 23, 176, 221, 74, 18, 147, 161, 152, 26, 200, 176, 158, 237, 130, 209, 133, 191, 237, 113, 196, 194, 166, 200, 112, 238, 30, 199, 3, 258, 163, 238, 15, 23, 114, 191, 176, 151, 15, 97, 19, 221, 208, 40, 18, 125, 199, 202, 135, 203, 21, 18, 57, 137, 194, 108, 199, 200, 202, 209, 18, 176, 23, 96, 135, 203, 258, 200, 191, 112, 156, 258, 71, 26, 158, 18, 25, 203, 26, 111, 200, 3, 80, 200, 200, 94, 191, 69, 192, 133, 202, 176, 221, 23, 208, 10, 15, 153, 40, 72, 204, 25, 200, 125, 3, 21, 40, 104, 153, 15, 30, 148, 140, 71, 156, 176, 25, 195, 200, 58, 80, 204, 114, 18, 173, 19, 213, 237, 98, 78, 15, 151, 221, 237, 10, 200, 194, 18, 23, 96, 3, 139, 238, 71, 3, 118, 133, 196, 35, 22, 117, 97, 237, 2, 133, 133, 25, 23, 3, 204, 0, 30, 2, 237, 195, 23, 130, 80, 196, 258, 213, 26, 176, 78, 78, 196, 94, 240]\n",
            "[15, 237, -1, 16, 104, 203, 135, 166, 109, 15, 200, 161, 58, 156, 3, -1, 96, 156, 15, 97, 197, -1, -1, 161, 98, 3, 97, 191, 21, 196, 193, 19, 204, 70, -1, 26, 15, 114, 96, 135, 96, 69, 58, 115, 114, 191, 69, 150, 161, 202, 135, 78, 29, 176, 96, 237, 259, -1, 235, 98, -1, 104, 166, 109, 15, 50, 108, 18, 3, 18, 153, 71, 173, 25, 98, 96, -1, 200, 23, 135, 69, 160, 3, 25, 173, -1, -1, 78, 200, 71, 121, 191, 69, 121, 25, 62, 104, 136, 200, -1, 175, 153, 4, 71, 15, 237, 235, 156, 204, 21, -1, 110, 50, 71, 3, 237, 135, 153, 235, 50, 195, 194, 2, 50, 109, 71, 149, 107, 204, 106, 235, 21, 235, 135, 235, -1, 15, 18, 26, -1, 15, 22, 13, 237, 25, -1, 192, 136, 196, 238, 104, 15, 137, 175, 156, -1, -1, 200, 191, 104, 158, 191, 80, 156, -1, 50, 133, 193, 160, 171, 135, 163, 2, 107, 194, 235, -1, 237, 78, 176, 135, 50, 237, 240, -1, -1, 203, -1, 78, 10, 26, 204, 218, 10, -1, 238, 4, 78, 57, 237, 135, -1, 18, 95, 200, 196, 213, 237, -1, 191, 191, 107, 114, 264, 19, 202, -1, 193, 117, 97, 238, 238, 161, 98, 30, 176, 135, 97, 155, 114, -1, 58, -1, -1, 193, 94, 238, 15, 3, 135, 155, 213, 208, 193, 10, 10, 175, 204, 25, -1, 106, 115, 151, -1, 204, 95, 30, -1, -1, 115, 197, 165, 69, 95, 237, 196, 25, 3, 197, 191, 96, 237, 209, 94, 175, 19, 221, 98, 104, 147, 21, 78, 97, 78, 19, 26, 146, 78, 15, 135, -1, 175, -1, 50, 166, 104, 15, 165, 22, 140, 237, 22, 109, 64, -1, 221, 106, 106, 165, 78, 175, 26, 170, 165, 21, 15, 23, 200, -1, 156, -1, 237, 108, 18, 19, 2, 15, 202, 102, 19, 238, 163, 50, 26, 15, 152, 15, 200, 98, 221, 22, 170, 173, 139, 21, 3, 208, 21, 118, 194, 62, 150, 136, 238, 40, 121, 25, 19, 237, -1, -1, 78, 137, 25, -1, 196, 158, 150, 202, 109, -1, 25, 109, 109, 196, 135, 166, -1, 71, 25, 3, -1, 200, 200, 69, 237, 193, 15, 78, 50, -1, 191, -1, 173, 117, -1, 203, 209, 203, 69, 238, 4, 30, 152, 106, 97, -1, 118, 106, 196, 195, 147, 26, -1, 26, -1, 10, 2, 163, 155, 58, 15, 3, 78, -1, 22, 166, 209, 200, 18, 94, -1, -1, 29, 193, 207, 204, 71, 193, 149, 25, -1, 161, 170, 160, 19, -1, 192, 104, 238, 135, 204, 191, -1, -1, 69, 195, 30, -1, 166, 200, 193, 176, 40, 161, 18, 121, 149, 150, 166, -1, 22, 135, 135, 25, -1, -1, 196, 203, 30, -1, -1, 258, 115, 96, -1, 213, 71, 221, 21, 50, -1, 155, 163, 113, 19, 149, 58, 108, 98, 2, 200, 154, 156, 26, 50, 161, 18, 15, 84, 137, 22, 78, 3, 50, 69, 133, 192, -1, 135, 112, 163, 196, 135, -1, 238, 22, 50, 171, 58, -1, 237, 192, -1, 204, 191, 25, 191, -1, -1, -1, 166, 18, 2, -1, 26, 191, 238, 111, -1, 204, 193, 96, 15, 204, 15, 203, 112, 165, 161, 173, 106, 238, 237, 196, 80, 160, 204, -1, 70, -1, 18, 137, 203, 50, 3, 15, 18, 209, 160, 238, 193, 40, -1, 121, 104, 165, 204, 78, -1, 114, 173, 97, 57, 23, 196, -1, 192, 30, 200, -1, 26, 140, 38, 135, 72, 238, 18, 203, 114, 135, 237, -1, 2, 19, 200, 196, 237, 237, 133, 211, 238, 136, 153, 25, 104, 191, 204, -1, 235, 22, -1, 147, 80, 192, 147, -1, -1, 15, 15, 137, 15, 202, 71, 221, 15, 30, 204, 26, 213, 149, 153, 200, 3, 96, 150, 2, 104, 204, 148, 78, 71, 58, 78, 78, 15, 176, 221, 4, 18, 147, 161, 97, 26, 200, 238, 158, -1, 158, 209, 133, 191, 237, 113, 196, -1, 166, 200, 112, 235, 30, 199, -1, 235, 163, 238, -1, 155, 114, 191, 176, 150, 15, 97, 19, 221, 136, 50, 18, -1, 199, 160, 135, 203, 21, 18, 57, 137, 194, 109, 199, 200, 238, 209, -1, 176, 23, 121, 135, 203, 235, 200, 191, 112, 15, 133, 71, 15, 110, 18, 25, 238, 26, -1, 200, 3, 111, 200, 200, 94, 191, 69, 192, -1, 160, 176, 221, 23, -1, 10, -1, -1, 40, -1, 204, 25, -1, -1, 3, 71, -1, 104, -1, 15, 30, 148, 104, 96, 156, 171, 25, 194, 200, 155, 80, 204, 114, 18, -1, 19, 213, -1, 98, 78, 15, 151, -1, 237, 10, 200, -1, 18, 23, 96, 3, 29, 196, 96, 3, -1, 235, 196, 69, 40, 117, 97, 237, -1, 133, 156, 25, 23, -1, 204, -1, 72, 2, 237, 195, 23, 15, -1, 196, 258, 213, -1, 175, 78, 78, 196, 94, 239]\n",
            "[30, 237, -1, 16, 104, 237, 135, 166, 108, 15, 200, 161, 58, -1, 58, -1, 133, -1, 15, 97, 197, -1, -1, 161, 98, 3, 97, 191, -1, 196, 193, 19, 204, 58, -1, 30, 15, 114, 96, 135, 96, 69, 58, 156, 114, 191, 50, 151, -1, 202, 135, 80, 22, 175, 96, 237, 237, -1, 97, 98, -1, 104, 166, 109, 15, 50, 170, -1, 3, 118, 153, 194, 173, 25, 98, -1, -1, 202, 23, 135, 35, 76, 3, 25, 173, -1, -1, 78, 200, -1, 121, 191, -1, 121, 25, 78, 104, 3, 200, -1, 175, 153, -1, 133, 72, 237, 173, 156, 204, 21, -1, 110, 50, 71, 3, 237, -1, 135, 113, 50, 197, 197, 238, -1, 109, 237, 149, 107, 204, 106, 258, 21, 196, 117, 258, -1, 15, 18, 26, -1, 15, 23, 200, 237, -1, -1, 191, 135, 196, 258, 104, 25, -1, 133, 156, -1, -1, 200, -1, 104, 158, 191, 156, 156, -1, 50, 197, 193, 160, 238, 135, 163, -1, 3, -1, 235, -1, 237, -1, 238, 135, 50, 237, 137, -1, -1, 203, -1, 78, -1, 26, 204, 3, -1, -1, 213, 74, 78, 57, 237, 133, -1, 18, 95, 200, 196, 213, 237, -1, 192, 191, 107, 114, 237, 18, 58, -1, 193, 117, 97, 238, 238, 161, 98, 30, 176, 135, 97, 156, 114, -1, 58, -1, -1, 193, 94, 238, 15, 112, 135, 70, 213, 208, -1, 10, 10, 175, 204, 23, -1, 258, 115, 79, -1, 204, 95, 30, -1, -1, 111, 197, 165, 69, 135, 237, 133, 25, 202, 197, 191, 96, 98, 191, 94, 2, -1, 221, 98, -1, 147, 173, 165, 97, 200, 19, 26, 195, 78, -1, 135, -1, 175, -1, 50, 161, 121, 30, 165, 22, 139, 98, 22, 109, 238, -1, 221, 155, 147, 165, 78, 3, 50, 170, 165, 21, 15, 25, 200, -1, -1, -1, 237, 161, 18, 19, 238, 25, 202, 102, 19, 238, 163, 29, 26, 25, 152, 15, 200, 98, 221, -1, 207, -1, -1, 21, 3, 25, 165, 200, 197, 78, 150, 135, 238, 40, 121, 25, 19, 237, -1, -1, 78, 137, 25, -1, 196, 158, -1, 135, 109, -1, 25, 109, 109, 133, 135, 237, -1, 71, 25, 3, -1, 200, -1, -1, 237, 193, 15, 78, 40, -1, 191, -1, 176, 149, -1, 203, 209, 203, 57, 238, 102, 30, 152, 166, 97, -1, -1, 106, 133, 195, 147, 26, -1, 26, -1, 10, 184, 163, 155, 58, 15, 218, 78, -1, 22, -1, 209, 200, 18, 94, -1, -1, 165, 193, 207, 204, 30, 193, 149, 25, -1, 204, 170, 258, 19, -1, 191, 104, 238, 191, 204, 191, -1, -1, 57, 195, -1, -1, 166, 200, 193, 177, 10, 161, 18, 115, -1, -1, 166, -1, 22, 135, 135, 25, -1, -1, 196, 203, 30, -1, -1, 258, 258, 71, -1, 213, 237, -1, 235, 50, -1, 72, 163, -1, 19, 149, 58, 109, 98, 3, -1, 154, 137, -1, 50, 161, 18, 15, 258, 137, 135, 78, 3, 29, 57, 133, 192, -1, 135, 112, 163, -1, 135, -1, 238, 22, -1, 191, 58, -1, 237, 192, -1, 204, 191, 25, 191, -1, -1, -1, 114, 18, -1, -1, 26, 191, 238, 161, -1, 204, 193, 149, 15, 204, 235, 203, 112, 165, 161, 173, 106, 238, 237, 196, 80, 195, 3, -1, 58, -1, 19, 137, 203, 35, -1, 15, 18, 209, 160, 258, 193, 22, -1, 121, 104, 165, 204, 78, -1, 110, 173, 97, 57, 23, -1, -1, 192, 30, 200, -1, 26, 140, 35, 193, 50, 161, 18, 203, 114, 135, -1, -1, 2, 19, 200, 196, 237, 237, 133, 211, 238, 136, 153, 25, -1, 191, 161, -1, 235, 22, -1, 147, 80, 192, -1, -1, -1, 30, 25, -1, 26, 202, 71, 221, 117, 30, 204, 26, 213, 149, -1, 200, 258, 96, 152, 3, 104, 204, 148, 78, 155, 205, 78, -1, 15, 176, 221, 74, 18, 147, 161, 152, 26, 200, 238, 158, -1, -1, 209, 133, 191, 237, -1, 195, -1, 166, 200, 112, 235, 30, 199, -1, 258, 163, 238, -1, 10, 114, 191, 176, 150, 15, 97, 19, 221, 191, 40, 18, -1, 199, 160, 135, 203, 22, 18, 57, 137, 194, 109, 199, 200, 213, 209, -1, 176, -1, 121, 135, 203, 258, 200, 191, 111, -1, 133, -1, 26, 110, 18, 19, -1, 35, -1, 200, 3, 112, 200, 200, 102, -1, 69, 192, -1, 160, 175, 221, 23, -1, -1, -1, -1, 40, -1, 204, 25, -1, -1, 3, -1, -1, 104, -1, 15, 30, 148, 140, 35, -1, 238, 25, 194, 200, 58, 80, 204, 114, 18, -1, 19, 213, -1, 98, -1, 15, 150, -1, 237, 10, 200, -1, 18, 23, -1, 3, 139, -1, 71, 3, -1, 196, 161, 35, 22, 147, 97, 237, -1, 133, 25, 25, 23, -1, 204, -1, 72, 2, 237, 197, 23, 237, -1, 196, 258, 213, -1, 175, 78, 78, 196, 94, 50]\n",
            "[15, 196, -1, 16, 104, 237, 149, 166, 108, 15, -1, 161, 58, -1, 205, -1, 133, -1, 15, 97, 195, -1, -1, 161, 98, 3, 97, 191, -1, 196, 193, 19, 204, -1, -1, 15, 15, 114, 96, 135, -1, 69, 3, 115, 13, 191, -1, 151, -1, 202, 135, 78, 40, 175, 96, 196, 258, -1, 156, 238, -1, 104, 166, 109, 15, -1, 108, -1, 3, 155, 153, -1, 173, 25, 98, -1, -1, 115, 23, 135, 204, -1, 3, -1, 173, -1, -1, -1, 200, -1, 121, 191, -1, -1, -1, 78, 104, 3, 200, -1, -1, 153, -1, 133, 72, -1, 173, 161, 204, 258, -1, 158, 50, 71, 3, 240, -1, 153, 76, -1, 195, -1, 176, -1, 109, 258, 149, 107, 204, 106, 258, 21, 3, -1, 258, -1, 15, 18, 26, -1, 15, 23, 3, 195, -1, -1, 191, 135, -1, 258, 104, 15, -1, 170, 133, -1, -1, -1, -1, 104, 158, 191, 80, 202, -1, -1, 196, 258, 160, 176, 135, 163, -1, 3, -1, -1, -1, 237, -1, -1, 104, -1, 237, 160, -1, -1, 203, -1, 78, -1, -1, 204, 3, -1, -1, 238, -1, 78, 57, 237, 135, -1, 18, -1, 200, 196, 213, 237, -1, 192, -1, 107, 114, 237, 18, 58, -1, 193, 117, 99, 238, 238, 161, 98, 30, 176, 135, 97, 115, 114, -1, 58, -1, -1, 193, -1, 213, -1, 2, 135, 70, 213, -1, -1, 10, 10, 175, 204, 23, -1, 258, 115, 150, -1, 204, 95, 30, -1, -1, 115, 197, 29, 69, 95, 166, 133, 25, 202, -1, 191, 106, 133, 209, -1, 175, -1, 221, 98, -1, 147, 173, 78, 97, 200, 19, 26, 98, -1, -1, 135, -1, 175, -1, 50, 3, 104, 30, 165, 22, 140, 98, 22, 108, 18, -1, 221, 96, 104, 205, 78, 2, 15, 207, 165, 21, 15, 23, 200, -1, -1, -1, 158, 108, 133, 19, -1, 25, 202, 102, 18, -1, 163, 29, 26, 15, 151, -1, 200, 98, 221, -1, 207, -1, -1, -1, 3, 166, -1, 115, 195, 78, 150, 136, 213, 40, -1, 25, 19, 258, -1, -1, 78, 137, 25, -1, 196, 158, -1, -1, 109, -1, 25, -1, 109, 133, 148, 166, -1, 71, 25, 3, -1, -1, -1, -1, 237, 3, -1, 78, 40, -1, -1, -1, 177, 149, -1, 203, 191, 203, 69, 238, 96, 72, 151, 107, 97, -1, -1, 106, 133, 3, 147, 26, -1, 26, -1, 10, 172, 163, -1, 133, 15, 3, 78, -1, 22, -1, 209, 200, 18, 94, -1, -1, 18, 193, 207, 204, 50, 80, 149, 25, -1, 204, 170, 240, 19, -1, -1, 104, 3, 191, 204, 191, -1, -1, 57, 195, -1, -1, 166, -1, 193, 176, 40, 161, 18, 115, -1, -1, 166, -1, 22, 136, 135, 25, -1, -1, 196, -1, 30, -1, -1, 258, 160, 96, -1, 238, 258, -1, -1, 29, -1, 72, 163, -1, 19, 149, 58, 108, 98, 173, -1, 155, -1, -1, 50, -1, 18, 15, 135, 2, 135, 78, 3, 25, 57, 133, 192, -1, 135, 112, 163, -1, 135, -1, 238, 22, -1, 171, 58, -1, 196, -1, -1, 204, 191, 25, 191, -1, -1, -1, 166, 18, -1, -1, 26, 238, 238, 196, -1, 204, -1, 149, 15, 204, 15, 203, 3, 165, 161, 173, 106, 238, 237, 196, -1, 195, 3, -1, 191, -1, 18, 137, 203, 35, -1, 15, -1, 209, 160, 238, 193, 40, -1, 104, -1, 18, 204, 78, -1, 110, 173, 97, 57, 23, -1, -1, 192, 30, 200, -1, 26, -1, 38, 135, 72, 191, 18, 203, 114, 150, -1, -1, 2, 19, 200, 196, 237, 237, 133, 211, 258, 136, 153, 25, -1, 192, 161, -1, 15, 22, -1, -1, 80, 192, -1, -1, -1, 15, 25, -1, 15, -1, 71, 221, -1, 30, 204, 26, 213, 149, -1, 200, 3, 96, 151, 2, 104, 204, 148, 78, 155, 205, 78, -1, 15, 176, 221, 74, 18, 147, 161, 150, 26, 200, 176, -1, -1, -1, 209, 133, -1, 158, -1, 196, -1, 166, 200, 112, 238, 30, 199, -1, 258, 163, 238, -1, 23, 114, 191, -1, 151, 15, -1, 19, 221, 191, 50, 18, -1, 199, 160, 135, 203, 21, 18, 57, 137, 194, 108, 199, 200, 238, 209, -1, 176, -1, 115, 135, -1, 258, 200, 191, 111, -1, 133, -1, 15, 158, 18, 25, -1, 26, -1, 200, 3, 3, 200, 200, 94, -1, 69, 192, -1, 160, 175, 221, -1, -1, -1, -1, -1, 40, -1, 204, 25, -1, -1, 3, -1, -1, 104, -1, 15, 64, 148, 104, 38, -1, 176, -1, -1, 200, -1, -1, 204, 114, -1, -1, 19, 213, -1, 98, -1, 15, 151, -1, 191, -1, 200, -1, 18, 23, -1, 3, 140, -1, 71, 3, -1, 13, 196, 35, 35, 118, 97, -1, -1, 133, 2, 25, 23, -1, 204, -1, 72, 2, 161, 195, 23, 258, -1, 196, 258, 213, -1, 175, 78, 78, 196, -1, 160]\n",
            "[30, 197, -1, 16, 104, 203, 135, 166, -1, 15, -1, 161, 196, -1, 135, -1, 133, -1, 158, 97, -1, -1, -1, 161, 98, 3, 97, 191, -1, 196, 193, 19, 204, -1, -1, 15, 15, 114, 96, 135, -1, 69, 196, -1, 13, 191, -1, 151, -1, -1, 135, 78, 22, 175, 96, -1, -1, -1, 135, 98, -1, 104, 166, 109, 15, -1, 207, -1, -1, -1, 153, -1, -1, 25, 98, -1, -1, -1, 10, 135, 204, -1, 3, -1, 173, -1, -1, -1, -1, -1, 121, 191, -1, -1, -1, 78, -1, 3, 200, -1, -1, 153, -1, 133, 72, -1, -1, 156, 204, 258, -1, 110, 155, 71, 207, 258, -1, 135, 106, -1, 197, -1, -1, -1, 109, 237, 135, 107, 204, 106, 258, 21, 78, -1, 258, -1, 15, 18, 26, -1, -1, 23, 102, 197, -1, -1, 191, 135, -1, 258, 104, 15, -1, -1, -1, -1, -1, -1, -1, 104, -1, 191, 79, -1, -1, -1, 196, 193, 160, 176, 135, 163, -1, 112, -1, -1, -1, 237, -1, -1, 135, -1, 197, 137, -1, -1, 203, -1, -1, -1, -1, 204, 3, -1, -1, -1, -1, 78, 57, 237, 135, -1, 18, -1, -1, -1, 213, 237, -1, 191, -1, 107, 114, 237, 18, 3, -1, 193, 117, 99, 238, 238, 161, 107, 30, 176, 135, -1, -1, 114, -1, 58, -1, -1, 193, -1, 213, -1, 176, 135, 70, 213, -1, -1, 10, 10, 175, 204, 23, -1, 258, 115, 151, -1, 204, 95, -1, -1, -1, 115, 197, 29, 69, 135, 166, -1, 25, 204, -1, 191, -1, 237, 209, -1, 175, -1, 221, 98, -1, -1, 173, 78, 97, 78, 19, -1, 146, -1, -1, 135, -1, 175, -1, 69, 196, 104, 30, 165, 22, 135, -1, 22, 109, 13, -1, 221, -1, 135, 21, 78, 175, 26, -1, 165, 21, 15, -1, 200, -1, -1, -1, 15, 161, -1, 19, -1, -1, 202, 102, -1, -1, 163, 96, 26, -1, -1, -1, 200, 98, 221, -1, 207, -1, -1, -1, 161, 137, -1, 118, 197, 78, 151, 135, 238, 155, -1, 25, 19, 237, -1, -1, 78, 137, 25, -1, 196, -1, -1, -1, 109, -1, 25, -1, 109, 133, 135, 166, -1, 133, 25, 3, -1, -1, -1, -1, 258, 193, -1, 78, 40, -1, -1, -1, 173, 135, -1, 203, 191, 203, 57, 238, 102, 30, 151, 166, 97, -1, -1, 106, 133, 195, 118, 26, -1, 26, -1, 10, 2, 163, -1, 58, 15, 218, 78, -1, 22, -1, 209, 200, 18, 94, -1, -1, 165, 193, 207, 204, 137, 193, -1, 25, -1, 161, 170, 160, 19, -1, -1, -1, 238, 191, 204, 191, -1, -1, 57, -1, -1, -1, 166, -1, 193, 176, 40, 161, -1, 115, -1, -1, 166, -1, -1, 136, 135, 25, -1, -1, 196, -1, 30, -1, -1, 196, 160, 96, -1, 213, 133, -1, -1, 140, -1, 70, 163, -1, 19, 149, 70, 109, -1, 3, -1, 133, -1, -1, 50, -1, 18, -1, 102, 137, 135, 78, 3, 50, 57, 133, -1, -1, 135, 112, 163, -1, 135, -1, 238, 22, -1, 191, 58, -1, 237, -1, -1, 204, 191, -1, -1, -1, -1, -1, 166, 18, -1, -1, 26, 191, 238, 111, -1, 204, -1, 149, -1, 204, 114, 203, 112, 165, 161, 173, 106, 238, 237, 196, -1, -1, 3, -1, 140, -1, 18, 137, 203, 155, -1, 15, -1, 209, 160, 238, 193, 22, -1, 121, -1, 165, 204, 78, -1, 114, -1, 97, 57, 23, -1, -1, 192, 30, 200, -1, -1, -1, 26, 135, 72, 238, 18, 203, 114, 135, -1, -1, -1, -1, 200, 196, 237, 237, 133, 211, -1, 135, 135, 25, -1, 191, 161, -1, 133, -1, -1, -1, 78, 192, -1, -1, -1, 30, 25, -1, -1, -1, 71, 221, -1, 30, -1, 26, 213, 149, -1, 200, 3, 96, 152, 3, -1, 204, 148, 78, 155, 196, -1, -1, 15, 176, 221, 74, 18, 150, 161, 114, 26, 200, 175, -1, -1, -1, 209, 133, -1, 158, -1, 197, -1, 166, -1, 112, -1, 30, 199, -1, -1, 163, 202, -1, 23, 114, 191, -1, 151, 15, -1, 19, -1, 136, 22, 18, -1, 199, 160, 135, 203, 21, 18, 57, 137, 197, 106, 199, 200, 238, 209, -1, -1, -1, 115, 137, -1, 258, 200, 191, -1, -1, 133, -1, 15, 158, 18, 18, -1, 26, -1, 200, 3, 200, 200, 200, 94, -1, 69, 192, -1, 202, -1, 221, -1, -1, -1, -1, -1, 40, -1, -1, 25, -1, -1, 3, -1, -1, 104, -1, 15, -1, 148, 140, -1, -1, 176, -1, -1, 78, -1, -1, 204, 114, -1, -1, 19, -1, -1, 98, -1, 15, 151, -1, -1, -1, 200, -1, 18, 23, -1, 3, 139, -1, -1, 208, -1, 196, 258, 69, 22, -1, 97, -1, -1, 133, 133, 25, 23, -1, 204, -1, 72, 2, 161, 133, 23, 237, -1, 196, 160, 213, -1, 175, 78, 78, -1, -1, -1]\n",
            "[15, 22, -1, 16, 104, 203, 135, 166, -1, 15, -1, 161, 203, -1, 202, -1, 133, -1, 15, 97, -1, -1, -1, 161, 98, 3, 97, 191, -1, 196, 193, 19, 204, -1, -1, -1, -1, 114, 96, 135, -1, 69, 58, -1, -1, -1, -1, 151, -1, -1, 135, 78, 22, 175, 96, -1, -1, -1, 106, 173, -1, 104, -1, 109, 15, -1, 108, -1, -1, -1, 153, -1, -1, 25, 98, -1, -1, -1, -1, 135, 69, -1, -1, -1, -1, -1, -1, -1, -1, -1, 121, 203, -1, -1, -1, 29, -1, 136, 200, -1, -1, -1, -1, 133, 15, -1, -1, 156, 204, -1, -1, 258, 35, 94, 3, 258, -1, 135, -1, -1, 197, -1, -1, -1, 109, 258, 135, -1, 204, 106, 258, 21, -1, -1, 258, -1, 15, 18, 26, -1, -1, -1, 202, 197, -1, -1, 192, 135, -1, 258, 104, 15, -1, -1, -1, -1, -1, -1, -1, 104, -1, 191, -1, -1, -1, -1, 156, 193, 258, 176, 135, 163, -1, 107, -1, -1, -1, 237, -1, -1, 104, -1, 237, -1, -1, -1, 203, -1, -1, -1, -1, 204, 204, -1, -1, -1, -1, -1, 57, -1, 135, -1, 18, -1, -1, -1, -1, 237, -1, 191, -1, -1, 114, 237, 19, 197, -1, 193, -1, 97, 238, 238, 196, 98, -1, 176, 135, -1, -1, 114, -1, 58, -1, -1, 193, -1, -1, -1, -1, 135, 70, 213, -1, -1, 10, -1, 175, 204, 25, -1, 258, 115, 150, -1, -1, 95, -1, -1, -1, 112, 197, 165, 69, 135, 166, -1, 25, 202, -1, -1, -1, 96, 209, -1, -1, -1, 238, 98, -1, -1, -1, 238, 97, 200, 19, -1, 146, -1, -1, 135, -1, 175, -1, -1, 3, 104, 30, 165, -1, -1, -1, 22, 109, 64, -1, 238, -1, 147, 203, -1, -1, 26, -1, 165, 21, 15, -1, 200, -1, -1, -1, 133, 108, -1, 19, -1, -1, 202, 102, -1, -1, 163, 22, 26, -1, -1, -1, 200, 98, 238, -1, 207, -1, -1, -1, -1, 166, -1, -1, -1, -1, 150, 136, 238, 40, -1, 25, 18, 238, -1, -1, 78, 137, -1, -1, 196, -1, -1, -1, 109, -1, 25, -1, -1, -1, 135, 166, -1, 161, 25, 3, -1, -1, -1, -1, 258, 193, -1, 78, 40, -1, -1, -1, 173, -1, -1, 203, 209, -1, 69, 238, 96, -1, 152, 166, 97, -1, -1, 106, 196, 197, 117, 26, -1, 26, -1, 10, -1, 163, -1, 203, 15, 3, 78, -1, -1, -1, 209, 200, -1, 94, -1, -1, 165, -1, 207, 204, 155, 193, -1, 25, -1, 204, -1, -1, -1, -1, -1, -1, 238, 191, 204, 191, -1, -1, 69, -1, -1, -1, 166, -1, -1, 176, -1, 161, -1, 104, -1, -1, 166, -1, -1, 136, 135, 25, -1, -1, 196, -1, 26, -1, -1, 258, 200, 96, -1, -1, 133, -1, -1, 29, -1, 156, 163, -1, 19, -1, 58, 109, -1, 3, -1, 70, -1, -1, 15, -1, 18, -1, 95, 137, 135, 78, 3, 137, 69, 133, -1, -1, 135, 112, 163, -1, 135, -1, 238, 22, -1, 191, -1, -1, 196, -1, -1, 204, 191, -1, -1, -1, -1, -1, 166, 18, -1, -1, 26, 191, 238, 196, -1, 204, -1, 133, -1, -1, -1, 203, 3, 165, 161, 173, 106, 238, -1, 196, -1, -1, 3, -1, 64, -1, 18, 137, 203, 35, -1, 15, -1, 209, 258, 238, 193, 40, -1, 104, -1, 165, 204, 78, -1, 158, -1, 97, 57, 23, -1, -1, 192, 26, 200, -1, -1, -1, 94, 137, 25, 191, 18, 203, 114, 135, -1, -1, -1, -1, 200, -1, -1, -1, 133, 211, -1, 136, 135, -1, -1, 191, 161, -1, 133, -1, -1, -1, 3, 192, -1, -1, -1, 15, 25, -1, -1, -1, 155, 238, -1, 30, -1, 26, 213, 135, -1, 200, 3, 96, 151, 3, -1, 204, 148, 78, 155, 196, -1, -1, 15, 176, 238, 4, 18, 147, -1, 152, 26, 200, 176, -1, -1, -1, -1, -1, -1, 133, -1, 197, -1, 166, -1, 112, -1, 30, -1, -1, -1, 163, 238, -1, -1, 114, 191, -1, 151, 15, -1, 19, -1, 136, 29, 18, -1, 199, 258, 135, 203, 21, 18, 57, 137, 195, 109, 199, -1, 238, 209, -1, -1, -1, 121, 137, -1, 258, -1, 258, -1, -1, 133, -1, 15, 158, 18, 25, -1, 35, -1, 200, 3, -1, 200, 200, 94, -1, -1, 192, -1, -1, -1, -1, -1, -1, -1, -1, -1, 40, -1, -1, 25, -1, -1, 3, -1, -1, 104, -1, 15, -1, 148, 104, -1, -1, 176, -1, -1, -1, -1, -1, 204, 114, -1, -1, 19, -1, -1, 98, -1, 15, 150, -1, -1, -1, 200, -1, 18, 23, -1, 3, 139, -1, -1, 3, -1, 196, 196, 69, 22, -1, 97, -1, -1, 133, 133, -1, -1, -1, 204, -1, 155, 2, 258, 173, 23, 15, -1, 196, 258, 192, -1, 175, -1, 78, -1, -1, -1]\n",
            "0.5273159144893111\n",
            "[30, 196, 137, 50, 104, 237, 149, 166, 108, 15, 200, 161, 58, 156, 58, 176, 133, 156, 15, 97, 197, 241, 240, 161, 98, 202, 97, 191, 21, 196, 193, 19, 204, 56, 191, 30, 15, 114, 96, 135, 99, 69, 58, 156, 13, 191, 69, 151, 161, 202, 135, 80, 40, 176, 96, 237, 264, 25, 258, 238, 153, 104, 166, 109, 15, 25, 108, 18, 3, 62, 153, 194, 173, 25, 98, 101, 241, 202, 23, 135, 204, 160, 3, 25, 173, 238, 0, 78, 200, 71, 115, 203, 158, 121, 25, 62, 104, 3, 200, 25, 175, 153, 74, 133, 72, 237, 173, 156, 204, 21, 166, 110, 70, 71, 3, 240, 135, 135, 160, 50, 195, 195, 175, 62, 109, 237, 149, 107, 204, 106, 258, 21, 78, 147, 237, 208, 15, 18, 26, 93, 15, 23, 202, 237, 25, 153, 192, 135, 196, 240, 104, 23, 137, 175, 156, 0, 149, 200, 191, 104, 133, 191, 156, 156, 78, 50, 156, 193, 160, 176, 135, 163, 2, 3, 195, 240, 153, 237, 196, 176, 104, 50, 237, 137, 202, 191, 203, 258, 78, 10, 26, 202, 3, 10, 239, 238, 74, 78, 57, 237, 135, 97, 18, 135, 200, 196, 213, 237, 125, 191, 191, 98, 114, 264, 19, 202, 109, 193, 117, 99, 238, 238, 161, 98, 30, 176, 149, 97, 155, 114, 21, 58, 241, 25, 193, 94, 238, 15, 98, 135, 64, 213, 208, 80, 10, 10, 175, 204, 25, 237, 258, 115, 79, 30, 196, 95, 30, 25, 241, 111, 197, 165, 69, 135, 237, 133, 25, 202, 197, 191, 96, 237, 191, 94, 175, 19, 221, 98, 104, 147, 173, 78, 97, 78, 19, 26, 195, 78, 25, 135, 96, 175, 153, 50, 196, 104, 30, 165, 22, 135, 133, 22, 109, 64, 3, 221, 202, 147, 165, 78, 175, 29, 207, 165, 21, 15, 23, 200, 241, 140, 241, 133, 74, 133, 19, 2, 25, 202, 102, 19, 176, 163, 22, 26, 25, 152, 15, 200, 98, 221, 22, 207, 173, 29, 21, 258, 208, 21, 200, 195, 62, 150, 202, 238, 29, 121, 25, 19, 237, 204, 16, 78, 137, 25, 40, 196, 158, 140, 203, 109, 237, 25, 109, 109, 133, 135, 237, 97, 71, 25, 3, 193, 200, 200, 30, 237, 193, 15, 78, 40, 160, 191, 215, 176, 135, 200, 203, 191, 203, 69, 238, 102, 30, 151, 166, 97, 149, 118, 106, 133, 3, 147, 26, 135, 26, 258, 10, 2, 163, 155, 58, 25, 218, 78, 40, 22, 156, 209, 200, 18, 94, 150, 98, 29, 193, 264, 204, 72, 193, 149, 25, 102, 161, 170, 258, 19, 96, 191, 104, 3, 135, 204, 191, 161, 200, 57, 195, 30, 40, 166, 200, 193, 176, 40, 161, 18, 115, 149, 163, 166, 78, 22, 135, 135, 25, 153, 21, 196, 203, 30, 237, 238, 258, 115, 96, 241, 215, 156, 221, 21, 29, 153, 155, 163, 113, 19, 149, 62, 108, 175, 3, 200, 70, 137, 26, 50, 161, 18, 15, 135, 137, 135, 78, 3, 50, 57, 133, 192, 264, 135, 112, 163, 133, 135, 153, 238, 22, 50, 171, 58, 18, 237, 192, 98, 204, 191, 25, 208, 26, 161, 200, 166, 18, 2, 15, 26, 191, 203, 196, 74, 204, 193, 149, 15, 204, 239, 203, 3, 165, 161, 173, 106, 238, 237, 196, 80, 195, 3, 241, 56, 93, 18, 137, 203, 64, 3, 15, 18, 209, 160, 258, 193, 22, 15, 115, 104, 165, 204, 78, 125, 155, 173, 97, 57, 23, 196, 133, 192, 30, 200, 74, 26, 140, 38, 135, 72, 191, 18, 203, 114, 135, 237, 258, 2, 19, 200, 196, 237, 237, 133, 211, 258, 208, 153, 25, 104, 192, 161, 113, 258, 22, 125, 147, 80, 192, 146, 191, 130, 30, 25, 137, 26, 202, 71, 221, 117, 30, 204, 26, 213, 149, 153, 200, 258, 96, 151, 3, 104, 204, 148, 79, 155, 58, 78, 78, 23, 176, 221, 74, 18, 147, 161, 152, 26, 200, 176, 158, 237, 130, 209, 133, 191, 237, 113, 196, 194, 166, 200, 112, 238, 30, 199, 3, 258, 163, 238, 15, 23, 114, 191, 176, 151, 15, 97, 19, 221, 208, 40, 18, 125, 199, 202, 135, 203, 21, 18, 57, 137, 194, 108, 199, 200, 202, 209, 18, 176, 23, 96, 135, 203, 258, 200, 191, 112, 156, 258, 71, 26, 158, 18, 25, 203, 26, 111, 200, 3, 80, 200, 200, 94, 191, 69, 192, 133, 202, 176, 221, 23, 208, 10, 15, 153, 40, 72, 204, 25, 200, 125, 3, 21, 40, 104, 153, 15, 30, 148, 140, 71, 156, 176, 25, 195, 200, 58, 80, 204, 114, 18, 173, 19, 213, 237, 98, 78, 15, 151, 221, 237, 10, 200, 194, 18, 23, 96, 3, 139, 238, 71, 3, 118, 133, 196, 35, 22, 117, 97, 237, 2, 133, 133, 25, 23, 3, 204, 0, 30, 2, 237, 195, 23, 130, 80, 196, 258, 213, 26, 176, 78, 78, 196, 94, 240]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "final_pred_3=[]\n",
        "\n",
        "num_correct=0\n",
        "model_0_3=load_model(loss,accuracy,optimizer,'0-3')\n",
        "mod_labels_0=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_0:\n",
        "    pred_test_0_3=model_0_3.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_0_3 = pred_test_0_3.argmax(axis=1)\n",
        "    mod_labels_0.append(pred_labels_0_3[0])\n",
        "  else:\n",
        "    mod_labels_0.append(-1)\n",
        "\n",
        "model_1_3=load_model(loss,accuracy,optimizer,'1-3')\n",
        "mod_labels_1=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        " \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_1:\n",
        "    pred_test_1_3=model_1_3.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_1_3 = pred_test_1_3.argmax(axis=1)\n",
        "    mod_labels_1.append(pred_labels_1_3[0])\n",
        "  else:\n",
        "    mod_labels_1.append(-1)\n",
        "\n",
        "model_2_3=load_model(loss,accuracy,optimizer,'2-3')\n",
        "mod_labels_2=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_2:\n",
        "    pred_test_2_3=model_2_3.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_2_3 = pred_test_2_3.argmax(axis=1)\n",
        "    mod_labels_2.append(pred_labels_2_3[0])\n",
        "  else:\n",
        "    mod_labels_2.append(-1)\n",
        "\n",
        "model_3_3=load_model(loss,accuracy,optimizer,'3-3')\n",
        "mod_labels_3=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_3:\n",
        "    pred_test_3_3=model_3_3.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_3_3 = pred_test_3_3.argmax(axis=1)\n",
        "    mod_labels_3.append(pred_labels_3_3[0])\n",
        "  else:\n",
        "    mod_labels_3.append(-1)\n",
        "\n",
        "model_4_3=load_model(loss,accuracy,optimizer,'4-3')\n",
        "mod_labels_4=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_4:\n",
        "    pred_test_4_3=model_4_3.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_4_3 = pred_test_4_3.argmax(axis=1)\n",
        "    mod_labels_4.append(pred_labels_4_3[0])\n",
        "  else:\n",
        "    mod_labels_4.append(-1)\n",
        "\n",
        "model_5_3=load_model(loss,accuracy,optimizer,'5-3')\n",
        "mod_labels_5=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_5:\n",
        "    pred_test_5_3=model_5_3.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_5_3 = pred_test_5_3.argmax(axis=1)\n",
        "    mod_labels_5.append(pred_labels_5_3[0])\n",
        "  else:\n",
        "    mod_labels_5.append(-1)\n",
        "\n",
        "print(mod_labels_0)\n",
        "print(mod_labels_1)\n",
        "print(mod_labels_2)\n",
        "print(mod_labels_3)\n",
        "print(mod_labels_4)\n",
        "print(mod_labels_5)\n",
        "\n",
        "\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  fin_labels=[]\n",
        "  if mod_labels_0[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  if mod_labels_1[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  if mod_labels_2[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  if mod_labels_3[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  if mod_labels_4[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  if mod_labels_5[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  res=max(set(fin_labels), key = fin_labels.count)\n",
        "  final_pred_3.append(res)\n",
        "\n",
        "  if res==new_val_label_0[i]:\n",
        "    num_correct=num_correct+1\n",
        "\n",
        "accuracy=num_correct/len(new_val_label_0)\n",
        "print(accuracy)\n",
        "  \n",
        "print(final_pred_3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGmPqjsrofw9",
        "outputId": "14b2acb5-4d91-4993-a965-40b102983d77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.5273159144893111\n",
            "Weighted F1: 0.47601398108124565\n",
            "Micro F1: 0.5273159144893111\n",
            "Weighted Precision: 0.4693709243456175\n",
            "Micro Precision: 0.5273159144893111\n",
            "Weighted Recall: 0.5273159144893111\n",
            "Micro Recall: 0.5273159144893111\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "\n",
        "accuracy=accuracy_score(new_val_label_0, final_pred_3)\n",
        "print(\"Accuracy: \"+str(accuracy))\n",
        "total_accuracy=total_accuracy+accuracy\n",
        "\n",
        "weighted_f1=f1_score(new_val_label_0,final_pred_3, average='weighted')\n",
        "print(\"Weighted F1: \"+ str(weighted_f1))\n",
        "total_weighted_f1=total_weighted_f1+weighted_f1\n",
        "micro_f1=f1_score(new_val_label_0,final_pred_3, average='micro')\n",
        "print(\"Micro F1: \"+ str(micro_f1))\n",
        "total_micro_f1=total_micro_f1+micro_f1\n",
        "\n",
        "weighted_precision=precision_score(new_val_label_0, final_pred_3, average='weighted')\n",
        "print(\"Weighted Precision: \" + str(weighted_precision))\n",
        "total_weighted_precision=total_weighted_precision+weighted_precision\n",
        "micro_precision=precision_score(new_val_label_0, final_pred_3, average='micro')\n",
        "print(\"Micro Precision: \" + str(micro_precision))\n",
        "total_micro_precision=total_micro_precision+micro_precision\n",
        "\n",
        "weighted_recall=recall_score(new_val_label_0, final_pred_3, average='weighted')\n",
        "print(\"Weighted Recall: \" + str(weighted_recall))\n",
        "total_weighted_recall=total_weighted_recall+weighted_recall\n",
        "micro_recall=recall_score(new_val_label_0, final_pred_3, average='micro')\n",
        "print(\"Micro Recall: \" + str(micro_recall))\n",
        "total_micro_recall=total_micro_recall+micro_recall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcnIY-AfoqM9",
        "outputId": "b4b440b9-96b1-4e70-c19f-4e1e4d5a3b2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_26\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_53 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_54 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_53[0][0]',               \n",
            " el)                            thPoolingAndCrossAt               'input_54[0][0]']               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_26 (S  (None, 768)         0           ['tf_roberta_model[26][0]']      \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " dense_52 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_26[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dropout_63 (Dropout)           (None, 512)          0           ['dense_52[0][0]']               \n",
            "                                                                                                  \n",
            " dense_53 (Dense)               (None, 279)          143127      ['dropout_63[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model: \"model_27\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_55 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_56 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_55[0][0]',               \n",
            " el)                            thPoolingAndCrossAt               'input_56[0][0]']               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_27 (S  (None, 768)         0           ['tf_roberta_model[27][0]']      \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " dense_54 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_27[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dropout_64 (Dropout)           (None, 512)          0           ['dense_54[0][0]']               \n",
            "                                                                                                  \n",
            " dense_55 (Dense)               (None, 279)          143127      ['dropout_64[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model: \"model_28\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_57 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_58 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_57[0][0]',               \n",
            " el)                            thPoolingAndCrossAt               'input_58[0][0]']               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_28 (S  (None, 768)         0           ['tf_roberta_model[28][0]']      \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " dense_56 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_28[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dropout_65 (Dropout)           (None, 512)          0           ['dense_56[0][0]']               \n",
            "                                                                                                  \n",
            " dense_57 (Dense)               (None, 279)          143127      ['dropout_65[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model: \"model_29\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_59 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_60 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_59[0][0]',               \n",
            " el)                            thPoolingAndCrossAt               'input_60[0][0]']               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_29 (S  (None, 768)         0           ['tf_roberta_model[29][0]']      \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " dense_58 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_29[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dropout_66 (Dropout)           (None, 512)          0           ['dense_58[0][0]']               \n",
            "                                                                                                  \n",
            " dense_59 (Dense)               (None, 279)          143127      ['dropout_66[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model: \"model_30\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_61 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_62 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_61[0][0]',               \n",
            " el)                            thPoolingAndCrossAt               'input_62[0][0]']               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_30 (S  (None, 768)         0           ['tf_roberta_model[30][0]']      \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " dense_60 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_30[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dropout_67 (Dropout)           (None, 512)          0           ['dense_60[0][0]']               \n",
            "                                                                                                  \n",
            " dense_61 (Dense)               (None, 279)          143127      ['dropout_67[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model: \"model_31\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_63 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_64 (InputLayer)          [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_63[0][0]',               \n",
            " el)                            thPoolingAndCrossAt               'input_64[0][0]']               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_31 (S  (None, 768)         0           ['tf_roberta_model[31][0]']      \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " dense_62 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_31[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dropout_68 (Dropout)           (None, 512)          0           ['dense_62[0][0]']               \n",
            "                                                                                                  \n",
            " dense_63 (Dense)               (None, 279)          143127      ['dropout_68[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "[30, 29, 137, 16, 104, 237, 149, 166, 108, 15, 200, 161, 58, 156, 205, 176, 96, 156, 15, 97, 195, 241, 258, 161, 98, 3, 97, 191, 21, 196, 193, 19, 204, 58, 135, 30, 15, 114, 96, 135, 96, 69, 64, 133, 114, 191, 69, 151, 161, 202, 135, 80, 22, 175, 96, 237, 258, 125, 258, 98, 153, 104, 166, 109, 15, 25, 207, 18, 202, 10, 153, 194, 173, 25, 98, 96, 241, 202, 23, 135, 71, 160, 3, 25, 173, 238, 239, 78, 3, 71, 121, 203, 258, 121, 25, 62, 104, 3, 200, 25, 175, 153, 74, 133, 72, 258, 173, 156, 204, 258, 166, 239, 50, 71, 3, 195, 135, 153, 125, 29, 196, 195, 175, 50, 109, 237, 149, 107, 204, 106, 258, 21, 78, 133, 258, 208, 15, 18, 26, 94, 15, 22, 3, 237, 25, 162, 192, 136, 196, 258, 104, 15, 137, 175, 133, 241, 149, 3, 191, 104, 158, 191, 156, 156, 78, 50, 156, 193, 160, 176, 135, 163, 2, 3, 195, 258, 153, 237, 78, 176, 104, 50, 237, 137, 202, 191, 203, 258, 78, 10, 26, 204, 3, 10, 129, 238, 4, 78, 64, 237, 133, 97, 18, 135, 200, 196, 215, 237, 25, 191, 191, 107, 114, 165, 18, 58, 109, 193, 117, 99, 238, 238, 161, 98, 30, 176, 135, 97, 156, 114, 21, 58, 241, 25, 193, 94, 213, 15, 111, 135, 64, 213, 208, 193, 10, 10, 175, 204, 23, 237, 106, 115, 78, 50, 258, 95, 30, 155, 241, 115, 197, 29, 69, 135, 166, 196, 25, 202, 197, 191, 96, 133, 209, 94, 175, 19, 221, 98, 104, 147, 21, 78, 97, 200, 19, 26, 258, 78, 25, 135, 96, 175, 153, 50, 3, 104, 30, 165, 24, 139, 133, 22, 109, 64, 3, 221, 115, 135, 165, 78, 175, 40, 3, 165, 21, 15, 23, 200, 241, 140, 241, 158, 109, 155, 19, 195, 25, 202, 102, 19, 176, 163, 50, 26, 15, 152, 15, 200, 98, 221, 22, 207, 173, 22, 21, 3, 166, 21, 200, 195, 62, 150, 202, 213, 40, 121, 25, 19, 258, 204, 16, 78, 137, 25, 40, 196, 158, 143, 64, 109, 258, 23, 109, 109, 133, 135, 166, 96, 71, 25, 3, 193, 200, 200, 30, 237, 193, 15, 78, 40, 3, 191, 215, 176, 135, 200, 203, 191, 203, 155, 238, 102, 30, 151, 166, 97, 149, 117, 107, 133, 3, 147, 26, 135, 26, 133, 10, 175, 163, 19, 58, 15, 218, 78, 239, 22, 156, 209, 200, 18, 94, 150, 96, 29, 193, 207, 204, 155, 193, 149, 25, 102, 161, 170, 258, 19, 96, 191, 104, 238, 191, 204, 191, 241, 78, 57, 195, 30, 239, 166, 200, 193, 176, 40, 161, 18, 121, 149, 163, 166, 78, 22, 135, 135, 25, 153, 21, 196, 203, 30, 258, 238, 258, 115, 96, 241, 215, 258, 221, 21, 29, 153, 50, 163, 113, 19, 149, 58, 109, 176, 173, 200, 154, 156, 26, 50, 161, 18, 15, 135, 137, 22, 78, 3, 50, 155, 133, 192, 50, 135, 112, 163, 196, 135, 153, 238, 22, 22, 191, 58, 18, 237, 203, 98, 204, 191, 25, 208, 26, 202, 200, 166, 18, 258, 15, 26, 238, 238, 196, 4, 204, 193, 101, 15, 204, 239, 203, 112, 165, 204, 173, 106, 238, 237, 196, 80, 195, 238, 241, 53, 102, 133, 137, 203, 50, 3, 15, 18, 209, 202, 112, 193, 40, 15, 121, 104, 203, 204, 78, 239, 114, 173, 101, 57, 23, 196, 196, 192, 30, 200, 74, 26, 140, 96, 135, 72, 64, 18, 203, 114, 135, 237, 3, 2, 19, 200, 196, 237, 237, 133, 211, 258, 136, 153, 153, 104, 191, 204, 113, 156, 22, 18, 147, 80, 192, 147, 238, 239, 30, 25, 137, 15, 202, 71, 221, 15, 30, 204, 26, 213, 149, 153, 200, 3, 96, 150, 2, 104, 204, 148, 78, 155, 58, 78, 78, 23, 176, 221, 74, 18, 147, 161, 152, 26, 200, 176, 158, 264, 130, 209, 133, 191, 158, 113, 196, 194, 166, 3, 112, 57, 30, 199, 3, 258, 163, 238, 15, 23, 114, 191, 176, 150, 15, 96, 19, 221, 136, 22, 18, 125, 199, 160, 135, 203, 21, 18, 57, 137, 194, 109, 199, 200, 238, 209, 18, 176, 23, 121, 135, 203, 258, 200, 191, 112, 156, 133, 71, 15, 158, 18, 25, 203, 26, 111, 200, 3, 112, 200, 200, 94, 191, 155, 192, 133, 160, 175, 221, 23, 208, 10, 15, 153, 40, 72, 213, 25, 200, 101, 3, 21, 18, 104, 153, 15, 30, 148, 140, 96, 3, 176, 125, 195, 200, 58, 80, 204, 114, 18, 173, 19, 213, 104, 98, 78, 15, 151, 221, 237, 10, 200, 194, 18, 23, 96, 3, 22, 196, 71, 3, 80, 196, 258, 71, 35, 117, 97, 258, 2, 133, 156, 25, 23, 156, 204, 0, 30, 2, 237, 195, 23, 258, 80, 196, 258, 215, 26, 175, 78, 78, 196, 94, 260]\n",
            "[30, 237, -1, 50, 104, 203, 149, 166, 109, 15, 200, 161, 58, 156, 58, -1, 133, 156, 15, 97, 197, -1, -1, 161, 98, 3, 97, 191, 21, 196, 193, 19, 204, 70, -1, 155, 15, 114, 96, 135, 96, 69, 58, 111, 114, 191, 69, 150, 161, 202, 135, 80, 139, 176, 96, 196, 258, -1, 115, 238, -1, 140, 166, 109, 15, 50, 196, 18, 3, 18, 153, 194, 173, 25, 98, 96, -1, 161, 23, 135, 50, 76, 3, 25, 173, -1, -1, 78, 193, 30, 121, 203, 26, 121, 25, 62, 104, 136, 200, -1, 175, 153, 4, 133, 50, 240, 196, 156, 204, 21, -1, 258, 50, 71, 3, 258, 135, 135, 113, 50, 197, 195, 2, 50, 109, 237, 149, 107, 204, 106, 258, 21, 80, 117, 258, -1, 15, 18, 26, -1, 15, 22, 3, 237, 25, -1, 192, 135, 196, 258, 104, 15, 137, 175, 156, -1, -1, 200, 191, 104, 158, 191, 156, 156, -1, 50, 196, 193, 160, 238, 135, 163, 2, 3, 197, 235, -1, 237, 196, 238, 135, 50, 237, 196, -1, -1, 203, -1, 78, 10, 26, 204, 204, 10, -1, 238, 74, 78, 57, 237, 135, -1, 18, 135, 200, 196, 213, 237, -1, 191, 191, 107, 114, 237, 19, 58, -1, 193, 117, 106, 218, 238, 196, 98, 30, 238, 135, 97, 155, 114, -1, 58, -1, -1, 193, 94, 213, 15, 112, 135, 70, 213, 208, 193, 10, 10, 175, 204, 23, -1, 258, 115, 150, -1, 238, 95, 30, -1, -1, 111, 197, 165, 69, 135, 237, 196, 25, 202, 197, 191, 96, 237, 209, 94, 2, 19, 221, 98, 104, 147, 21, 78, 97, 78, 19, 26, 258, 78, 25, 135, -1, 2, -1, 50, 196, 104, 30, 165, 22, 135, 133, 22, 109, 64, -1, 221, 106, 118, 165, 78, 2, 29, 3, 165, 21, 15, 23, 200, -1, 140, -1, 133, 108, 18, 3, 2, 15, 202, 102, 18, 176, 163, 50, 26, 15, 150, 15, 200, 98, 221, 22, 207, 173, 139, 21, 213, 166, 21, 118, 197, 62, 150, 202, 213, 155, 121, 25, 19, 240, -1, -1, 78, 137, 25, -1, 196, 158, 135, 136, 109, -1, 23, 109, 109, 133, 135, 237, -1, 71, 25, 3, -1, 200, 200, 69, 237, 193, 15, 78, 40, -1, 191, -1, 176, 196, -1, 203, 191, 203, 69, 238, 102, 30, 150, 107, 97, -1, 118, 106, 196, 195, 147, 26, -1, 26, -1, 10, 2, 163, 19, 58, 15, 3, 78, -1, 22, 156, 209, 200, 18, 94, -1, -1, 165, 193, 207, 204, 155, 193, 149, 25, -1, 161, 170, 258, 19, -1, 191, 104, 238, 135, 204, 191, -1, -1, 57, 197, 30, -1, 166, 200, 193, 176, 40, 161, 18, 121, 149, 150, 166, -1, 22, 135, 135, 25, -1, -1, 196, 203, 30, -1, -1, 258, 258, 96, -1, 213, 156, 221, 21, 29, -1, 155, 163, 113, 19, 149, 70, 204, 98, 3, 200, 154, 208, 15, 50, 161, 18, 15, 135, 137, 135, 78, 3, 50, 57, 133, 192, -1, 135, 112, 163, 196, 135, -1, 238, 22, 50, 3, 58, -1, 237, 192, -1, 204, 191, 30, 208, -1, -1, -1, 166, 18, 258, -1, 26, 238, 238, 196, -1, 204, 193, 96, 15, 204, 237, 203, 111, 165, 161, 173, 106, 238, 237, 196, 80, 258, 195, -1, 58, -1, 133, 137, 203, 64, 3, 15, 18, 209, 258, 258, 193, 29, -1, 115, 104, 58, 204, 78, -1, 114, 173, 97, 57, 23, 196, -1, 192, 30, 200, -1, 26, 140, 96, 238, 72, 238, 18, 203, 114, 135, 237, -1, 2, 19, 200, 196, 237, 237, 196, 211, 238, 135, 153, 25, 104, 191, 204, -1, 235, 40, -1, 147, 80, 192, 118, -1, -1, 30, 15, 137, 15, 202, 71, 221, 15, 30, 204, 26, 213, 149, 153, 200, 3, 96, 150, 3, 104, 204, 95, 78, 155, 58, 78, 78, 15, 176, 221, 4, 18, 147, 161, 114, 26, 200, 176, 158, -1, 155, 209, 133, 191, 156, 113, 196, -1, 166, 200, 112, 238, 30, 199, -1, 258, 163, 238, -1, 23, 114, 191, 176, 150, 15, 96, 18, 221, 208, 40, 18, -1, 199, 3, 135, 203, 21, 18, 57, 137, 194, 109, 199, 200, 238, 209, -1, 2, 23, 99, 135, 203, 258, 200, 191, 112, 156, 133, 71, 15, 110, 18, 25, 203, 26, -1, 200, 3, 112, 200, 200, 94, 240, 69, 192, -1, 160, 2, 221, 23, -1, 10, -1, -1, 40, -1, 204, 25, -1, -1, 3, 21, -1, 104, -1, 15, 30, 148, 140, 71, 156, 2, 25, 194, 200, 58, 80, 204, 114, 18, -1, 19, 213, -1, 98, 118, 15, 150, -1, 237, 10, 200, -1, 18, 23, 96, 3, 139, 196, 71, 3, -1, 196, 196, 30, 40, 118, 97, 240, -1, 133, 156, 25, 23, -1, 204, -1, 72, 2, 258, 197, 23, 237, -1, 196, 258, 213, -1, 176, 78, 78, 196, 94, 70]\n",
            "[30, 237, -1, 50, 104, 203, 149, 166, 109, 15, 200, 161, 196, -1, 135, -1, 96, -1, 15, 97, 197, -1, -1, 161, 98, 3, 97, 191, -1, 196, 193, 19, 204, 58, -1, 30, 15, 114, 96, 135, 96, 69, 58, 155, 114, 191, 69, 152, -1, 202, 135, 78, 22, 175, 96, 196, 258, -1, 258, 238, -1, 104, 166, 109, 15, 155, 108, -1, 3, 18, 153, 194, 173, 25, 98, -1, -1, 200, 23, 135, 69, 3, 3, 25, 173, -1, -1, 78, 200, -1, 121, 191, -1, 121, 25, 79, 104, 136, 200, -1, 175, 153, -1, 133, 15, 191, 173, 156, 204, 21, -1, 258, 50, 71, 3, 194, -1, 153, 106, 50, 197, 197, 2, -1, 109, 237, 135, 107, 204, 106, 258, 21, 3, 147, 258, -1, 15, 18, 26, -1, 15, 23, 200, 237, -1, -1, 191, 136, 196, 238, 104, 15, -1, 197, 156, -1, -1, 200, -1, 104, 158, 191, 156, 156, -1, 50, 198, 193, 3, 2, 135, 163, -1, 98, -1, 235, -1, 237, -1, 238, 104, 50, 237, 137, -1, -1, 203, -1, 78, -1, 26, 204, 3, -1, -1, 238, 4, 78, 57, 237, 135, -1, 18, 95, 200, 196, 213, 237, -1, 191, 191, 98, 98, 237, 18, 202, -1, 193, 117, 97, 238, 238, 161, 98, 30, 176, 135, 97, 155, 114, -1, 58, -1, -1, 193, 94, 238, 15, 3, 135, 154, 213, 208, -1, 10, 10, 175, 204, 23, -1, 147, 115, 150, -1, 238, 135, 30, -1, -1, 111, 197, 165, 69, 135, 237, 196, 25, 202, 197, 191, 96, 237, 209, 94, 2, -1, 221, 98, -1, 147, 173, 78, 97, 200, 19, 26, 238, 78, -1, 135, -1, 175, -1, 50, 238, 104, 30, 165, 22, 135, 98, 22, 109, 64, -1, 221, 96, 118, 98, 78, 2, 26, 3, 165, 21, 15, 25, 200, -1, -1, -1, 237, 161, 155, 19, 238, 15, 202, 102, 19, 176, 163, 50, 26, 155, 152, 15, 200, 98, 221, -1, 207, -1, -1, 21, 3, 156, 21, 118, 197, 79, 150, 136, 238, 40, 121, 25, 19, 237, -1, -1, 78, 137, 25, -1, 196, 158, -1, 202, 109, -1, 25, 109, 109, 133, 135, 237, -1, 71, 25, 3, -1, 200, -1, -1, 237, 193, 15, 78, 40, -1, 191, -1, 176, 149, -1, 203, 191, 203, 69, 238, 96, 30, 152, 166, 97, -1, -1, 106, 196, 195, 147, 26, -1, 26, -1, 10, 2, 163, 19, 58, 15, 3, 78, -1, 22, -1, 209, 200, 18, 94, -1, -1, 165, 193, 207, 204, 155, 193, 149, 25, -1, 161, 170, 258, 19, -1, 191, 104, 238, 135, 204, 191, -1, -1, 57, 197, -1, -1, 166, 200, 193, 2, 40, 161, 18, 121, -1, -1, 166, -1, 22, 136, 135, 25, -1, -1, 196, 203, 30, -1, -1, 196, 191, 96, -1, 238, 133, -1, 21, 40, -1, 156, 163, -1, 19, 135, 58, 109, 98, 2, -1, 154, 238, -1, 50, 161, 18, 15, 238, 137, 135, 78, 3, 50, 57, 133, 192, -1, 135, 112, 163, -1, 135, -1, 238, 22, -1, 191, 58, -1, 237, 192, -1, 204, 191, 25, 208, -1, -1, -1, 166, 18, -1, -1, 26, 238, 238, 196, -1, 204, 193, 149, 15, 204, 114, 203, 3, 165, 161, 173, 106, 238, 237, 196, 80, 195, 197, -1, 135, -1, 18, 137, 203, 64, -1, 15, 18, 209, 155, 238, 193, 40, -1, 98, 104, 78, 204, 78, -1, 114, 173, 97, 57, 23, -1, -1, 192, 30, 200, -1, 26, 135, 26, 135, 50, 196, 18, 203, 114, 135, -1, -1, 2, 19, 200, 196, 237, 237, 133, 211, 238, 136, 153, 25, -1, 191, 204, -1, 133, 22, -1, 147, 80, 192, -1, -1, -1, 15, 15, -1, 26, 202, 155, 221, 117, 30, 204, 26, 213, 149, -1, 200, 3, 96, 152, 2, 104, 204, 148, 78, 155, 22, 78, -1, 15, 175, 221, 74, 18, 147, 161, 152, 26, 200, 176, 158, -1, -1, 209, 133, 191, 237, -1, 196, -1, 166, 200, 112, 238, 30, 199, -1, 258, 163, 238, -1, 10, 114, 191, 175, 152, 15, 97, 19, 221, 136, 40, 18, -1, 199, 3, 135, 203, 21, 18, 57, 137, 194, 106, 199, 200, 238, 209, -1, 175, -1, 96, 135, 203, 258, 200, 191, 112, -1, 133, -1, 15, 158, 18, 19, -1, 26, -1, 200, 3, 3, 200, 200, 94, -1, 69, 192, -1, 202, 175, 221, 23, -1, -1, -1, -1, 40, -1, 213, 25, -1, -1, 3, -1, -1, 104, -1, 15, 30, 148, 140, 155, -1, 2, 25, 197, 200, 58, 80, 204, 114, 18, -1, 19, 213, -1, 98, -1, 15, 151, -1, 191, 10, 200, -1, 18, 23, -1, 3, 22, -1, 155, 3, -1, 196, 196, 30, 22, 118, 97, 258, -1, 133, 156, 10, 23, -1, 204, -1, 72, 2, 237, 238, 23, 15, -1, 196, 258, 238, -1, 2, 78, 78, 196, 94, 155]\n",
            "[15, 29, -1, 16, 104, 203, 135, 166, 108, 15, -1, 161, 58, -1, 58, -1, 133, -1, 15, 97, 195, -1, -1, 161, 98, 3, 97, 191, -1, 196, 193, 19, 204, -1, -1, 15, 15, 133, 96, 135, -1, 69, 58, 113, 114, 191, -1, 151, -1, 202, 151, 78, 29, 175, 96, 196, 264, -1, 166, 146, -1, 104, 166, 109, 15, -1, 108, -1, 3, 58, 153, -1, 173, 25, 98, -1, -1, 200, 23, 135, 204, -1, 3, -1, 173, -1, -1, -1, 200, -1, 121, 191, -1, -1, -1, 78, 104, 3, 200, -1, -1, 153, -1, 133, 15, -1, 173, 156, 204, 258, -1, 133, 155, 50, 3, 195, -1, 153, 76, -1, 196, -1, 238, -1, 109, 237, 149, 107, 204, 106, 235, 21, 196, -1, 258, -1, 15, 18, 26, -1, 15, 23, 3, 195, -1, -1, 192, 136, -1, 238, 104, 15, -1, 175, 133, -1, -1, -1, -1, 104, 158, 191, 156, 3, -1, -1, 133, 258, 160, 238, 135, 163, -1, 3, -1, -1, -1, 237, -1, -1, 104, -1, 194, 160, -1, -1, 203, -1, 78, -1, -1, 204, 3, -1, -1, 238, -1, 78, 57, 237, 133, -1, 18, -1, 200, 196, 213, 258, -1, 192, -1, 98, 114, 264, 18, 58, -1, 193, 117, 99, 238, 238, 161, 98, 30, 176, 135, 97, 133, 114, -1, 58, -1, -1, 193, -1, 213, -1, 2, 135, 70, 213, -1, -1, 10, 10, 175, 204, 23, -1, 98, 115, 151, -1, 204, 95, 30, -1, -1, 111, 197, 29, 69, 95, 166, 133, 25, 202, -1, 191, 106, 98, 209, -1, 172, -1, 221, 98, -1, 147, 173, 78, 97, 200, 19, 26, 146, -1, -1, 135, -1, 175, -1, 50, 3, 104, 30, 165, 22, 140, 98, 22, 109, 64, -1, 221, 96, 147, 238, 78, 172, 155, 207, 165, 21, 15, 23, 200, -1, -1, -1, 15, 108, 133, 19, -1, 25, 202, 102, 19, -1, 163, 29, 26, 15, 152, -1, 200, 98, 221, -1, 207, -1, -1, -1, 3, 3, -1, 115, 195, 78, 151, 136, 213, 155, -1, 25, 19, 264, -1, -1, 78, 137, 25, -1, 196, 158, -1, -1, 109, -1, 23, -1, 109, 133, 135, 166, -1, 133, 25, 3, -1, -1, -1, -1, 237, 195, -1, 78, 40, -1, -1, -1, 173, 196, -1, 203, 191, 203, 69, 238, 102, 50, 151, 107, 97, -1, -1, 106, 133, 3, 147, 26, -1, 26, -1, 10, 172, 163, -1, 58, 15, 218, 78, -1, 22, -1, 209, 200, 18, 94, -1, -1, 29, 193, 207, 204, 155, 193, 149, 25, -1, 161, 170, 196, 19, -1, -1, 104, 238, 191, 204, 191, -1, -1, 57, 195, -1, -1, 166, -1, 193, 176, 10, 161, 18, 113, -1, -1, 166, -1, 22, 136, 135, 25, -1, -1, 196, -1, 30, -1, -1, 196, 258, 96, -1, 213, 133, -1, -1, 140, -1, 155, 163, -1, 19, 149, 58, 108, 98, 2, -1, 155, -1, -1, 30, -1, 18, 15, 196, 2, 135, 78, 3, 29, 57, 133, 192, -1, 135, 112, 163, -1, 135, -1, 238, 22, -1, 3, 58, -1, 238, -1, -1, 204, 191, 25, 238, -1, -1, -1, 166, 18, -1, -1, 26, 238, 238, 96, -1, 204, -1, 149, 35, 204, 114, 203, 3, 165, 161, 173, 106, 238, 258, 196, -1, 195, 238, -1, 53, -1, 19, 137, 203, 64, -1, 15, -1, 209, 160, 238, 193, 29, -1, 121, -1, 165, 204, 78, -1, 106, 173, 97, 57, 23, -1, -1, 192, 30, 200, -1, 26, -1, 38, 135, 72, 191, 18, 203, 114, 135, -1, -1, 2, 19, 200, 196, 237, 98, 133, 211, 196, 136, 153, 153, -1, 192, 161, -1, 133, 22, -1, -1, 80, 192, -1, -1, -1, 30, 15, -1, 26, -1, 110, 221, -1, 30, 204, 26, 213, 149, -1, 200, 3, 96, 151, 3, 104, 204, 148, 78, 155, 58, 78, -1, 15, 176, 221, 74, 18, 147, 161, 152, 26, 200, 176, -1, -1, -1, 209, 133, -1, 158, -1, 196, -1, 166, 200, 112, 57, 30, 199, -1, 258, 163, 221, -1, 23, 114, 191, -1, 151, 15, -1, 19, 221, 238, 155, 18, -1, 199, 160, 135, 203, 21, 18, 57, 137, 194, 109, 199, 200, 213, 209, -1, 176, -1, 121, 135, -1, 235, 200, 191, 112, -1, 133, -1, 15, 158, 18, 19, -1, 26, -1, 200, 3, 80, 200, 200, 94, -1, 69, 192, -1, 202, 175, 221, -1, -1, -1, -1, -1, 40, -1, 161, 25, -1, -1, 3, -1, -1, 104, -1, 15, 30, 148, 140, 35, -1, 2, -1, -1, 200, -1, -1, 204, 114, -1, -1, 19, 213, -1, 98, -1, 15, 151, -1, 203, -1, 200, -1, 18, 23, -1, 3, 140, -1, 35, 196, -1, 196, 196, 57, 40, 118, 97, -1, -1, 133, 133, 25, 23, -1, 204, -1, 72, 172, 196, 238, 23, 237, -1, 196, 235, 213, -1, 2, 78, 78, 196, -1, 258]\n",
            "[30, 165, -1, 16, 104, 203, 135, 166, -1, 15, -1, 161, 58, -1, 58, -1, 133, -1, 15, 97, -1, -1, -1, 161, 98, 218, 97, 191, -1, 196, 193, 19, 204, -1, -1, 40, 15, 114, 96, 135, -1, 69, 58, -1, 114, 191, -1, 151, -1, -1, 135, 78, 22, 175, 96, -1, -1, -1, 237, 238, -1, 104, 166, 109, 15, -1, 207, -1, -1, -1, 153, -1, -1, 25, 98, -1, -1, -1, 23, 135, 71, -1, 3, -1, 173, -1, -1, -1, -1, -1, 121, 203, -1, -1, -1, 78, -1, 136, 200, -1, -1, 153, -1, 133, 72, -1, -1, 156, 204, 237, -1, 237, 71, 71, 207, 237, -1, 153, 106, -1, 197, -1, -1, -1, 109, 71, 135, 107, 204, 106, 258, 21, 78, -1, 259, -1, 15, 18, 26, -1, -1, 23, 200, 197, -1, -1, 192, 135, -1, 237, 104, 15, -1, -1, -1, -1, -1, -1, -1, 104, -1, 191, 78, -1, -1, -1, 161, 193, 160, 176, 135, 163, -1, 199, -1, -1, -1, 237, -1, -1, 135, -1, 237, 137, -1, -1, 203, -1, -1, -1, -1, 204, 204, -1, -1, -1, -1, 78, 57, 237, 135, -1, 18, -1, -1, -1, 213, 258, -1, 192, -1, 107, 114, 237, 19, 58, -1, 193, 117, 99, 238, 238, 161, 133, 30, 176, 135, -1, -1, 114, -1, 58, -1, -1, 193, -1, 213, -1, 195, 135, 70, 213, -1, -1, 10, 10, 175, 204, 23, -1, 118, 115, 78, -1, 204, 135, -1, -1, -1, 112, 197, 165, 69, 135, 237, -1, 25, 202, -1, 191, -1, 98, 209, -1, 175, -1, 221, 98, -1, -1, 173, 78, 97, 78, 19, -1, 195, -1, -1, 135, -1, 175, -1, 72, 166, 104, 30, 165, 22, 135, -1, 22, 109, 64, -1, 221, -1, 147, 137, 78, 175, 155, -1, 165, 21, 15, -1, 200, -1, -1, -1, 158, 108, -1, 19, -1, -1, 202, 102, -1, -1, 163, 22, 26, -1, -1, -1, 200, 98, 221, -1, 207, -1, -1, -1, 3, 25, -1, 200, 197, 78, 150, 136, 197, 40, -1, 25, 19, 237, -1, -1, 78, 137, 25, -1, 196, -1, -1, -1, 109, -1, 23, -1, 109, 133, 135, 166, -1, 71, 25, 3, -1, -1, -1, -1, 237, 193, -1, 78, 40, -1, -1, -1, 173, 149, -1, 203, 209, 203, 69, 238, 102, 30, 152, 166, 97, -1, -1, 106, 133, 195, 147, 26, -1, 26, -1, 10, 175, 163, -1, 58, 25, 218, 78, -1, 22, -1, 209, 200, 18, 94, -1, -1, 22, 193, 207, 204, 21, 193, -1, 25, -1, 161, 170, 160, 19, -1, -1, -1, 238, 191, 204, 191, -1, -1, 57, -1, -1, -1, 166, -1, 193, 176, 40, 161, -1, 115, -1, -1, 166, -1, -1, 135, 135, 25, -1, -1, 196, -1, 30, -1, -1, 237, 99, 96, -1, 213, 237, -1, -1, 40, -1, 72, 163, -1, 19, 149, 56, 109, -1, 3, -1, 70, -1, -1, 30, -1, 18, -1, 196, 137, 135, 78, 238, 29, 70, 133, -1, -1, 135, 112, 163, -1, 135, -1, 238, 22, -1, 191, 58, -1, 237, -1, -1, 204, 191, -1, -1, -1, -1, -1, 166, 18, -1, -1, 26, 191, 203, 196, -1, 204, -1, 149, -1, 204, 237, 203, 112, 165, 161, 173, 106, 213, 237, 196, -1, -1, 197, -1, 64, -1, 19, 137, 203, 71, -1, 15, -1, 209, 160, 238, 193, 22, -1, 121, -1, 18, 204, 78, -1, 155, -1, 97, 57, 23, -1, -1, 192, 30, 200, -1, -1, -1, 35, 135, 72, 191, 18, 203, 114, 135, -1, -1, -1, -1, 200, 202, 237, 98, 133, 211, -1, 136, 135, 25, -1, 191, 161, -1, 237, -1, -1, -1, 78, 192, -1, -1, -1, 30, 25, -1, -1, -1, 71, 221, -1, 30, -1, 26, 213, 135, -1, 200, 202, 96, 151, 2, -1, 204, 148, 78, 155, 58, -1, -1, 15, 176, 221, 74, 18, 147, 161, 114, 155, 200, 176, -1, -1, -1, 209, 133, -1, 133, -1, 197, -1, 166, -1, 112, -1, 30, 199, -1, -1, 163, 238, -1, 23, 114, 191, -1, 150, 15, -1, 19, -1, 191, 40, 19, -1, 199, 160, 135, 203, 22, 18, 57, 137, 197, 106, 199, 200, 213, 209, -1, -1, -1, 121, 137, -1, 258, 200, 191, -1, -1, 133, -1, 15, 158, 18, 25, -1, 35, -1, 200, 3, 202, 200, 200, 94, -1, 69, 192, -1, 135, -1, 221, -1, -1, -1, -1, -1, 40, -1, -1, 25, -1, -1, 3, -1, -1, 104, -1, 15, -1, 148, 121, -1, -1, 176, -1, -1, 78, -1, -1, 204, 114, -1, -1, 19, -1, -1, 98, -1, 15, 151, -1, -1, -1, 200, -1, 18, 23, -1, 3, 135, -1, -1, 3, -1, 196, 196, 71, 22, -1, 97, -1, -1, 133, 135, 25, 23, -1, 161, -1, 72, 175, 237, 197, 23, 237, -1, 196, 258, 213, -1, 175, 78, 78, -1, -1, -1]\n",
            "[15, 237, -1, 16, 104, 203, 135, 166, -1, 15, -1, 161, 197, -1, 205, -1, 133, -1, 15, 97, -1, -1, -1, 161, 98, 3, 97, 191, -1, 196, 193, 19, 204, -1, -1, -1, -1, 114, 96, 135, -1, 69, 3, -1, -1, -1, -1, 151, -1, -1, 135, 78, 22, 176, 96, -1, -1, -1, 258, 238, -1, 140, -1, 109, 15, -1, 207, -1, -1, -1, 153, -1, -1, 25, 98, -1, -1, -1, -1, 135, 35, -1, -1, -1, -1, -1, -1, -1, -1, -1, 121, 191, -1, -1, -1, 78, -1, 135, 200, -1, -1, -1, -1, 133, 15, -1, -1, 156, 204, -1, -1, 110, 69, 35, 3, 258, -1, 135, -1, -1, 197, -1, -1, -1, 109, 237, 135, -1, 204, 106, 258, 21, -1, -1, 258, -1, 15, 18, 26, -1, -1, -1, 202, 197, -1, -1, 191, 135, -1, 258, 104, 15, -1, -1, -1, -1, -1, -1, -1, 104, -1, 191, -1, -1, -1, -1, 161, 193, 160, 176, 135, 163, -1, 107, -1, -1, -1, 237, -1, -1, 104, -1, 237, -1, -1, -1, 203, -1, -1, -1, -1, 204, 204, -1, -1, -1, -1, -1, 57, -1, 135, -1, 18, -1, -1, -1, -1, 258, -1, 191, -1, -1, 114, 237, 18, 202, -1, 193, -1, 99, 225, 238, 196, 133, -1, 176, 135, -1, -1, 114, -1, 58, -1, -1, 193, -1, -1, -1, -1, 135, 155, 213, -1, -1, 10, -1, 175, 204, 25, -1, 98, 99, 78, -1, -1, 95, -1, -1, -1, 111, 197, 29, 69, 95, 166, -1, 25, 202, -1, -1, -1, 98, 209, -1, -1, -1, 218, 98, -1, -1, -1, 78, 97, 200, 19, -1, 238, -1, -1, 135, -1, 176, -1, -1, 3, 104, 30, 165, -1, -1, -1, 22, 109, 64, -1, 218, -1, 18, 196, -1, -1, 26, -1, 165, 21, 15, -1, 200, -1, -1, -1, 15, 204, -1, 19, -1, -1, 202, 102, -1, -1, 163, 22, 26, -1, -1, -1, 200, 98, 218, -1, 207, -1, -1, -1, -1, 166, -1, -1, -1, -1, 151, 136, 238, 40, -1, 25, 19, 258, -1, -1, 78, 137, -1, -1, 196, -1, -1, -1, 109, -1, 25, -1, -1, -1, 135, 166, -1, 133, 25, 3, -1, -1, -1, -1, 237, 193, -1, 78, 40, -1, -1, -1, 173, -1, -1, 203, 191, -1, 69, 238, 102, -1, 151, 166, 97, -1, -1, 106, 196, 195, 117, 26, -1, 26, -1, 10, -1, 163, -1, 3, 15, 3, 78, -1, -1, -1, 209, 200, -1, 94, -1, -1, 78, -1, 207, 204, 155, 193, -1, 25, -1, 161, -1, -1, -1, -1, -1, -1, 238, 191, 204, 191, -1, -1, 57, -1, -1, -1, 166, -1, -1, 176, -1, 161, -1, 99, -1, -1, 166, -1, -1, 136, 158, 25, -1, -1, 196, -1, 30, -1, -1, 258, 258, 96, -1, -1, 156, -1, -1, 29, -1, 156, 163, -1, 19, -1, 58, 109, -1, 2, -1, 154, -1, -1, 15, -1, 18, -1, 102, 137, 140, 78, 3, 166, 69, 133, -1, -1, 135, 112, 163, -1, 135, -1, 238, 22, -1, 191, -1, -1, 237, -1, -1, 204, 191, -1, -1, -1, -1, -1, 166, 18, -1, -1, 26, 238, 238, 161, -1, 204, -1, 96, -1, -1, -1, 203, 3, 165, 161, 173, 106, 238, -1, 196, -1, -1, 238, -1, 64, -1, 18, 137, 203, 35, -1, 15, -1, 209, 160, 238, 193, 40, -1, 99, -1, 78, 204, 78, -1, 155, -1, 97, 57, 23, -1, -1, 192, 30, 200, -1, -1, -1, 38, 135, 72, 191, 18, 203, 114, 135, -1, -1, -1, -1, 200, -1, -1, -1, 133, 211, -1, 238, 135, -1, -1, 191, 161, -1, 21, -1, -1, -1, 3, 192, -1, -1, -1, 30, 25, -1, -1, -1, 155, 218, -1, 30, -1, 26, 213, 149, -1, 200, 3, 96, 152, 176, -1, 204, 148, 78, 35, 205, -1, -1, 15, 176, 218, 75, 18, 147, -1, 114, 26, 200, 176, -1, -1, -1, -1, -1, -1, 158, -1, 197, -1, 166, -1, 112, -1, 30, -1, -1, -1, 163, 238, -1, -1, 114, 191, -1, 151, 15, -1, 19, -1, 3, 40, 18, -1, 199, 160, 135, 203, 21, 18, 57, 137, 197, 106, 199, -1, 238, 209, -1, -1, -1, 160, 135, -1, 258, -1, 191, -1, -1, 133, -1, 15, 158, 18, 25, -1, 26, -1, 200, 3, -1, 200, 200, 94, -1, -1, 192, -1, -1, -1, -1, -1, -1, -1, -1, -1, 40, -1, -1, 25, -1, -1, 3, -1, -1, 104, -1, 15, -1, 148, 121, -1, -1, 176, -1, -1, -1, -1, -1, 204, 114, -1, -1, 19, -1, -1, 98, -1, 15, 151, -1, -1, -1, 200, -1, 18, 23, -1, 3, 205, -1, -1, 3, -1, 196, 196, 35, 22, -1, 97, -1, -1, 133, 156, -1, -1, -1, 204, -1, 72, 2, 258, 197, 25, 15, -1, 196, 258, 213, -1, 176, -1, 78, -1, -1, -1]\n",
            "0.504750593824228\n",
            "[30, 29, 137, 16, 104, 237, 149, 166, 108, 15, 200, 161, 58, 156, 205, 176, 96, 156, 15, 97, 195, 241, 258, 161, 98, 3, 97, 191, 21, 196, 193, 19, 204, 58, 135, 30, 15, 114, 96, 135, 96, 69, 64, 133, 114, 191, 69, 151, 161, 202, 135, 80, 22, 175, 96, 237, 258, 125, 258, 98, 153, 104, 166, 109, 15, 25, 207, 18, 202, 10, 153, 194, 173, 25, 98, 96, 241, 202, 23, 135, 71, 160, 3, 25, 173, 238, 239, 78, 3, 71, 121, 203, 258, 121, 25, 62, 104, 3, 200, 25, 175, 153, 74, 133, 72, 258, 173, 156, 204, 258, 166, 239, 50, 71, 3, 195, 135, 153, 125, 29, 196, 195, 175, 50, 109, 237, 149, 107, 204, 106, 258, 21, 78, 133, 258, 208, 15, 18, 26, 94, 15, 22, 3, 237, 25, 162, 192, 136, 196, 258, 104, 15, 137, 175, 133, 241, 149, 3, 191, 104, 158, 191, 156, 156, 78, 50, 156, 193, 160, 176, 135, 163, 2, 3, 195, 258, 153, 237, 78, 176, 104, 50, 237, 137, 202, 191, 203, 258, 78, 10, 26, 204, 3, 10, 129, 238, 4, 78, 64, 237, 133, 97, 18, 135, 200, 196, 215, 237, 25, 191, 191, 107, 114, 165, 18, 58, 109, 193, 117, 99, 238, 238, 161, 98, 30, 176, 135, 97, 156, 114, 21, 58, 241, 25, 193, 94, 213, 15, 111, 135, 64, 213, 208, 193, 10, 10, 175, 204, 23, 237, 106, 115, 78, 50, 258, 95, 30, 155, 241, 115, 197, 29, 69, 135, 166, 196, 25, 202, 197, 191, 96, 133, 209, 94, 175, 19, 221, 98, 104, 147, 21, 78, 97, 200, 19, 26, 258, 78, 25, 135, 96, 175, 153, 50, 3, 104, 30, 165, 24, 139, 133, 22, 109, 64, 3, 221, 115, 135, 165, 78, 175, 40, 3, 165, 21, 15, 23, 200, 241, 140, 241, 158, 109, 155, 19, 195, 25, 202, 102, 19, 176, 163, 50, 26, 15, 152, 15, 200, 98, 221, 22, 207, 173, 22, 21, 3, 166, 21, 200, 195, 62, 150, 202, 213, 40, 121, 25, 19, 258, 204, 16, 78, 137, 25, 40, 196, 158, 143, 64, 109, 258, 23, 109, 109, 133, 135, 166, 96, 71, 25, 3, 193, 200, 200, 30, 237, 193, 15, 78, 40, 3, 191, 215, 176, 135, 200, 203, 191, 203, 155, 238, 102, 30, 151, 166, 97, 149, 117, 107, 133, 3, 147, 26, 135, 26, 133, 10, 175, 163, 19, 58, 15, 218, 78, 239, 22, 156, 209, 200, 18, 94, 150, 96, 29, 193, 207, 204, 155, 193, 149, 25, 102, 161, 170, 258, 19, 96, 191, 104, 238, 191, 204, 191, 241, 78, 57, 195, 30, 239, 166, 200, 193, 176, 40, 161, 18, 121, 149, 163, 166, 78, 22, 135, 135, 25, 153, 21, 196, 203, 30, 258, 238, 258, 115, 96, 241, 215, 258, 221, 21, 29, 153, 50, 163, 113, 19, 149, 58, 109, 176, 173, 200, 154, 156, 26, 50, 161, 18, 15, 135, 137, 22, 78, 3, 50, 155, 133, 192, 50, 135, 112, 163, 196, 135, 153, 238, 22, 22, 191, 58, 18, 237, 203, 98, 204, 191, 25, 208, 26, 202, 200, 166, 18, 258, 15, 26, 238, 238, 196, 4, 204, 193, 101, 15, 204, 239, 203, 112, 165, 204, 173, 106, 238, 237, 196, 80, 195, 238, 241, 53, 102, 133, 137, 203, 50, 3, 15, 18, 209, 202, 112, 193, 40, 15, 121, 104, 203, 204, 78, 239, 114, 173, 101, 57, 23, 196, 196, 192, 30, 200, 74, 26, 140, 96, 135, 72, 64, 18, 203, 114, 135, 237, 3, 2, 19, 200, 196, 237, 237, 133, 211, 258, 136, 153, 153, 104, 191, 204, 113, 156, 22, 18, 147, 80, 192, 147, 238, 239, 30, 25, 137, 15, 202, 71, 221, 15, 30, 204, 26, 213, 149, 153, 200, 3, 96, 150, 2, 104, 204, 148, 78, 155, 58, 78, 78, 23, 176, 221, 74, 18, 147, 161, 152, 26, 200, 176, 158, 264, 130, 209, 133, 191, 158, 113, 196, 194, 166, 3, 112, 57, 30, 199, 3, 258, 163, 238, 15, 23, 114, 191, 176, 150, 15, 96, 19, 221, 136, 22, 18, 125, 199, 160, 135, 203, 21, 18, 57, 137, 194, 109, 199, 200, 238, 209, 18, 176, 23, 121, 135, 203, 258, 200, 191, 112, 156, 133, 71, 15, 158, 18, 25, 203, 26, 111, 200, 3, 112, 200, 200, 94, 191, 155, 192, 133, 160, 175, 221, 23, 208, 10, 15, 153, 40, 72, 213, 25, 200, 101, 3, 21, 18, 104, 153, 15, 30, 148, 140, 96, 3, 176, 125, 195, 200, 58, 80, 204, 114, 18, 173, 19, 213, 104, 98, 78, 15, 151, 221, 237, 10, 200, 194, 18, 23, 96, 3, 22, 196, 71, 3, 80, 196, 258, 71, 35, 117, 97, 258, 2, 133, 156, 25, 23, 156, 204, 0, 30, 2, 237, 195, 23, 258, 80, 196, 258, 215, 26, 175, 78, 78, 196, 94, 260]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "final_pred_4=[]\n",
        "\n",
        "num_correct=0\n",
        "model_0_4=load_model(loss,accuracy,optimizer,'0-4')\n",
        "mod_labels_0=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_0:\n",
        "    pred_test_0_4=model_0_4.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_0_4 = pred_test_0_4.argmax(axis=1)\n",
        "    mod_labels_0.append(pred_labels_0_4[0])\n",
        "  else:\n",
        "    mod_labels_0.append(-1)\n",
        "\n",
        "model_1_4=load_model(loss,accuracy,optimizer,'1-4')\n",
        "mod_labels_1=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        " \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_1:\n",
        "    pred_test_1_4=model_1_4.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_1_4 = pred_test_1_4.argmax(axis=1)\n",
        "    mod_labels_1.append(pred_labels_1_4[0])\n",
        "  else:\n",
        "    mod_labels_1.append(-1)\n",
        "\n",
        "model_2_4=load_model(loss,accuracy,optimizer,'2-4')\n",
        "mod_labels_2=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_2:\n",
        "    pred_test_2_4=model_2_4.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_2_4 = pred_test_2_4.argmax(axis=1)\n",
        "    mod_labels_2.append(pred_labels_2_4[0])\n",
        "  else:\n",
        "    mod_labels_2.append(-1)\n",
        "\n",
        "model_3_4=load_model(loss,accuracy,optimizer,'3-4')\n",
        "mod_labels_3=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_3:\n",
        "    pred_test_3_4=model_3_4.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_3_4 = pred_test_3_4.argmax(axis=1)\n",
        "    mod_labels_3.append(pred_labels_3_4[0])\n",
        "  else:\n",
        "    mod_labels_3.append(-1)\n",
        "\n",
        "model_4_4=load_model(loss,accuracy,optimizer,'4-4')\n",
        "mod_labels_4=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_4:\n",
        "    pred_test_4_4=model_4_4.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_4_4 = pred_test_4_4.argmax(axis=1)\n",
        "    mod_labels_4.append(pred_labels_4_4[0])\n",
        "  else:\n",
        "    mod_labels_4.append(-1)\n",
        "\n",
        "model_5_4=load_model(loss,accuracy,optimizer,'5-4')\n",
        "mod_labels_5=[]\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  \n",
        "  #print(i)\n",
        "  if new_val_fnum_0[i] in new_val_fnum_5:\n",
        "    pred_test_5_4=model_5_4.predict([new_val_inp_0[i].reshape(1,512),new_val_mask_0[i].reshape(1,512)])\n",
        "    pred_labels_5_4 = pred_test_5_4.argmax(axis=1)\n",
        "    mod_labels_5.append(pred_labels_5_4[0])\n",
        "  else:\n",
        "    mod_labels_5.append(-1)\n",
        "\n",
        "print(mod_labels_0)\n",
        "print(mod_labels_1)\n",
        "print(mod_labels_2)\n",
        "print(mod_labels_3)\n",
        "print(mod_labels_4)\n",
        "print(mod_labels_5)\n",
        "\n",
        "\n",
        "for i in range(len(new_val_fnum_0)):\n",
        "  fin_labels=[]\n",
        "  if mod_labels_0[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  if mod_labels_1[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  if mod_labels_2[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  if mod_labels_3[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  if mod_labels_4[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  if mod_labels_5[i] != -1:\n",
        "    fin_labels.append(mod_labels_0[i])\n",
        "  res=max(set(fin_labels), key = fin_labels.count)\n",
        "  final_pred_4.append(res)\n",
        "\n",
        "  if res==new_val_label_0[i]:\n",
        "    num_correct=num_correct+1\n",
        "\n",
        "accuracy=num_correct/len(new_val_label_0)\n",
        "print(accuracy)\n",
        "  \n",
        "print(final_pred_4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhJg0uGdpPP6",
        "outputId": "c25e3e18-b719-43f5-fed9-e3b75c94996e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.504750593824228\n",
            "Weighted F1: 0.45106782174320464\n",
            "Micro F1: 0.504750593824228\n",
            "Weighted Precision: 0.43315295331472864\n",
            "Micro Precision: 0.504750593824228\n",
            "Weighted Recall: 0.504750593824228\n",
            "Micro Recall: 0.504750593824228\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "\n",
        "accuracy=accuracy_score(new_val_label_0, final_pred_4)\n",
        "print(\"Accuracy: \"+str(accuracy))\n",
        "total_accuracy=total_accuracy+accuracy\n",
        "\n",
        "weighted_f1=f1_score(new_val_label_0,final_pred_4, average='weighted')\n",
        "print(\"Weighted F1: \"+ str(weighted_f1))\n",
        "total_weighted_f1=total_weighted_f1+weighted_f1\n",
        "micro_f1=f1_score(new_val_label_0,final_pred_4, average='micro')\n",
        "print(\"Micro F1: \"+ str(micro_f1))\n",
        "total_micro_f1=total_micro_f1+micro_f1\n",
        "\n",
        "weighted_precision=precision_score(new_val_label_0, final_pred_4, average='weighted')\n",
        "print(\"Weighted Precision: \" + str(weighted_precision))\n",
        "total_weighted_precision=total_weighted_precision+weighted_precision\n",
        "micro_precision=precision_score(new_val_label_0, final_pred_4, average='micro')\n",
        "print(\"Micro Precision: \" + str(micro_precision))\n",
        "total_micro_precision=total_micro_precision+micro_precision\n",
        "\n",
        "weighted_recall=recall_score(new_val_label_0, final_pred_4, average='weighted')\n",
        "print(\"Weighted Recall: \" + str(weighted_recall))\n",
        "total_weighted_recall=total_weighted_recall+weighted_recall\n",
        "micro_recall=recall_score(new_val_label_0, final_pred_4, average='micro')\n",
        "print(\"Micro Recall: \" + str(micro_recall))\n",
        "total_micro_recall=total_micro_recall+micro_recall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UnV81NZ-paGM",
        "outputId": "541d46aa-e644-4b7e-f2c1-6c77fd8750fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Accuracy: 0.5201900237529691\n",
            "Average Weighted F1: 0.4674493358910081\n",
            "Average Micro F1: 0.5201900237529691\n",
            "Average Weighted Precision: 0.4554191438161059\n",
            "Average Micro Precision: 0.5201900237529691\n",
            "Average Weighted Recall: 0.5201900237529691\n",
            "Average Micro Recall: 0.5201900237529691\n"
          ]
        }
      ],
      "source": [
        "print(\"Average Accuracy: \"+str(total_accuracy/5))\n",
        "print(\"Average Weighted F1: \"+str(total_weighted_f1/5))\n",
        "print(\"Average Micro F1: \"+str(total_micro_f1/5))\n",
        "print(\"Average Weighted Precision: \"+str(total_weighted_precision/5))\n",
        "print(\"Average Micro Precision: \"+str(total_micro_precision/5))\n",
        "print(\"Average Weighted Recall: \"+str(total_weighted_recall/5))\n",
        "print(\"Average Micro Recall: \"+str(total_micro_recall/5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wl_ZrlhOsMJz"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "RoBERTa_Ensemble_279labels.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}