{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mCIR7rd5h9fv",
    "outputId": "53830c68-0d4f-4ea9-9978-d1684a586900"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "#drive.flush_and_unmount()\n",
    "drive.mount('/content/drive')\n",
    "#drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0k8FHrSIiqDo",
    "outputId": "18763450-95ef-4c99-f202-053db938b0ff"
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install sentencepiece\n",
    "!pip install tensorflow==2.7.0\n",
    "!pip install stanza\n",
    "!pip install transformers\n",
    "!pip install tensorflow-addons\n",
    "!pip install nltk\n",
    "!pip install textacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K0rs0NoritMk",
    "outputId": "92b77bac-3521-4e3b-cf37-6f33a0d5c9f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "wYwcFK5gixXz"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from textacy.datasets.supreme_court import SupremeCourt\n",
    "import numpy as np\n",
    "import re\n",
    "import unicodedata\n",
    "import nltk\n",
    "#from transformers import pipeline\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense,Dropout, Input, BatchNormalization\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from sklearn.metrics import confusion_matrix,f1_score,classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras import regularizers\n",
    "#from transformers import *\n",
    "from transformers import BertTokenizer, TFBertModel, BertConfig,TFDistilBertModel,DistilBertTokenizer,DistilBertConfig\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "import numpy as np\n",
    "import gc\n",
    "import math\n",
    "import json\n",
    "import stanza\n",
    "from tensorflow.keras import *\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import *\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import TFRobertaModel,RobertaTokenizer,LongformerTokenizer,TFLongformerModel\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.initializers import RandomUniform\n",
    "\n",
    "from numpy.random import seed\n",
    "import random as python_random\n",
    "import os\n",
    "import sys\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "np.random.seed(1)\n",
    "python_random.seed(1)\n",
    "tf.random.set_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bffI5vYTUeFW"
   },
   "outputs": [],
   "source": [
    "!cp \"/content/drive/My Drive/labels_sc.txt\" \"./labels_sc.txt\"\n",
    "!cp \"/content/drive/My Drive/labels_sc_279.txt\" \"./labels_sc_279.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "Number of devices: 2\n"
     ]
    }
   ],
   "source": [
    "# Create a MirroredStrategy.\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_model():\n",
    "    inps = Input(shape = (max_len,), dtype='int64')\n",
    "    masks= Input(shape = (max_len,), dtype='int64')\n",
    "    dbert_layer = dbert_model(inps, attention_mask=masks)[0][:,0,:]\n",
    "    dense_0 = Dense(512,activation='relu',kernel_regularizer=regularizers.l2(0.01))(dbert_layer)\n",
    "    dropout_0= Dropout(0.5)(dense_0)\n",
    "    pred = Dense(279, activation='softmax',kernel_regularizer=regularizers.l2(0.01))(dropout_0)\n",
    "    model = tf.keras.Model(inputs=[inps,masks], outputs=pred)\n",
    "    print(model.summary())\n",
    "    return model   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_accuracy=0\n",
    "total_weighted_f1=0\n",
    "total_micro_f1=0\n",
    "total_weighted_precision=0\n",
    "total_micro_precision=0\n",
    "total_weighted_recall=0\n",
    "total_micro_recall=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'supreme_court', 'site_url': 'http://caselaw.findlaw.com/court/us-supreme-court', 'description': 'Collection of ~8.4k decisions issued by the U.S. Supreme Court between November 1946 and June 2016.'}\n",
      "[-1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "{'-1': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7, '8': 8, '9': 9, '10': 10, '11': 11, '12': 12, '13': 13, '14': 14}\n",
      "8419\n",
      "Average Length 2920.3854377004395\n",
      "Found 8419 texts.\n",
      "Found 15 labels.\n",
      "                                                   text  label\n",
      "0     [ Halliburton Oil Well Cementing Co. v. Walker...    209\n",
      "1     Rehearing Denied Dec. 16, 1946. See . Mr.Claud...     63\n",
      "2     Rehearing Denied Dec. 16, 1946 See . Appeal fr...    216\n",
      "3     Mr. Walter J. Cummings, Jr., of Washington, D....    108\n",
      "4     Mr.A. Devitt Vaneck, of Washington, D.C., for ...    196\n",
      "...                                                 ...    ...\n",
      "8414  Opinion reported: Ante, p. 88. DECREE 1. It is...     74\n",
      "8415  In this dispute between Utah and the United St...      1\n",
      "8416  The United States, to the exclusion of defenda...      4\n",
      "8417  Louisiana's exception to the portion of the Sp...     74\n",
      "8418  To resolve a dispute over the ownership of cer...    263\n",
      "\n",
      "[8419 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-14 19:40:48.619662: I tensorflow/stream_executor/cuda/cuda_blas.cc:1774] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "Some layers from the model checkpoint at allenai/longformer-base-4096 were not used when initializing TFLongformerModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFLongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFLongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFLongformerModel were initialized from the model checkpoint at allenai/longformer-base-4096.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFLongformerModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 4096)]       0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 4096)]       0           []                               \n",
      "                                                                                                  \n",
      " tf_longformer_model (TFLongfor  TFLongformerBaseMod  148659456  ['input_1[0][0]',                \n",
      " merModel)                      elOutputWithPooling               'input_2[0][0]']                \n",
      "                                (last_hidden_state=                                               \n",
      "                                (None, 4096, 768),                                                \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 hidden_states=None                                               \n",
      "                                , attentions=None,                                                \n",
      "                                global_attentions=N                                               \n",
      "                                one)                                                              \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_longformer_model[0][0]']    \n",
      " ingOpLambda)                                                                                     \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dropout_49 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 279)          143127      ['dropout_49[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 149,196,311\n",
      "Trainable params: 149,196,311\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7577, 4096)\n",
      "(7577, 4096)\n",
      "(7577,)\n",
      "(842, 4096)\n",
      "(842, 4096)\n",
      "(842,)\n",
      "(7576, 4096)\n",
      "(7576, 4096)\n",
      "(7576,)\n",
      "(840, 4096)\n",
      "(840, 4096)\n",
      "(840,)\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "\n",
    "    sc = SupremeCourt()\n",
    "    print(sc.info)\n",
    "    sc.download()\n",
    "\n",
    "    texts = []  # list of text samples\n",
    "    labels_index = {}  # dictionary mapping label name to numeric id\n",
    "    labels = []  # list of label ids\n",
    "\n",
    "    issue_codes = list(sc.issue_area_codes.keys()) # 15 labels\n",
    "    print(issue_codes)\n",
    "    issue_codes.sort()\n",
    "    issue_codes = [str(ic) for ic in issue_codes]\n",
    "\n",
    "    labels_index = dict(zip(issue_codes, np.arange(len(issue_codes))))\n",
    "    print(labels_index)\n",
    "    count=0\n",
    "\n",
    "    for record in sc.records():\n",
    "\n",
    "            count=count+1\n",
    "            if record[1]['issue'] == None: # some cases have None as an issue\n",
    "                labels.append(labels_index['-1'])\n",
    "            else:\n",
    "                labels.append(labels_index[record[1]['issue'][:-4]])\n",
    "\n",
    "            new_sen=record[0].split(\"Footnotes\")[0]\n",
    "            new_new_sen=new_sen.split()\n",
    "            if len(new_new_sen) >= 4096:\n",
    "              new_new_sen=new_new_sen[0:4096]\n",
    "\n",
    "            elif len(new_new_sen) < 4096:\n",
    "              new_new_sen=new_new_sen[0:len(new_new_sen)]\n",
    "\n",
    "            new_new_sen=' '.join(new_new_sen)\n",
    "\n",
    "            texts.append(new_new_sen)\n",
    "\n",
    "    len_list = [len(ele.split()) for ele in texts]\n",
    "\n",
    "    #print(labels)\n",
    "    print(len(labels))\n",
    "\n",
    "    res = 0 if len(len_list) == 0 else (float(sum(len_list)) / len(len_list))\n",
    "    print(\"Average Length %s\" % res) \n",
    "    print('Found %s texts.' % len(texts))\n",
    "    print('Found %s labels.' % len(labels_index))\n",
    "    temp_file = open(\"labels_sc_279.txt\", \"r\")\n",
    "\n",
    "    data = temp_file.read()\n",
    "    label_list = data.split(\"\\n\")\n",
    "    #print(label_list)\n",
    "    label_list = label_list[0:-1]\n",
    "    #print(label_list)\n",
    "    label_list = [int(i) for i in label_list]\n",
    "    summarized_data = pd.DataFrame(texts,\n",
    "                   columns =['text'])\n",
    "    summarized_data['label'] = label_list\n",
    "    print(summarized_data)\n",
    "    #gc.collect()\n",
    "    #tf.keras.backend.clear_session()\n",
    "    dbert_tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
    "    dbert_model = TFLongformerModel.from_pretrained('allenai/longformer-base-4096')\n",
    "    max_len=4096\n",
    "    sentences=summarized_data['text']\n",
    "    labels=summarized_data['label']\n",
    "    len(sentences),len(labels)\n",
    "    model_0=create_model()\n",
    "    input_ids=[]\n",
    "    attention_masks=[]\n",
    "\n",
    "    for sent in sentences:\n",
    "      dbert_inps=dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n",
    "      input_ids.append(dbert_inps['input_ids'])\n",
    "      attention_masks.append(dbert_inps['attention_mask'])\n",
    "    input_ids=np.asarray(input_ids)\n",
    "\n",
    "    attention_masks=np.array(attention_masks)\n",
    "    labels=np.array(labels)\n",
    "    train_inp,val_inp,train_label,val_label,train_mask,val_mask=train_test_split(input_ids,labels,attention_masks,test_size=0.1,random_state=42)\n",
    "    print(train_inp.shape)\n",
    "    print(train_mask.shape)\n",
    "    print(train_label.shape)\n",
    "    print(val_inp.shape)\n",
    "    print(val_mask.shape)\n",
    "    print(val_label.shape)\n",
    "    train_inp=train_inp[0:7576]\n",
    "    train_mask=train_mask[0:7576]\n",
    "    train_label=train_label[0:7576]\n",
    "\n",
    "    val_inp=val_inp[0:840]\n",
    "    val_label=val_label[0:840]\n",
    "    val_mask=val_mask[0:840]\n",
    "    print(train_inp.shape)\n",
    "    print(train_mask.shape)\n",
    "    print(train_label.shape)\n",
    "    print(val_inp.shape)\n",
    "    print(val_mask.shape)\n",
    "    print(val_label.shape)\n",
    "    \n",
    "    log_dir='dbert_model'\n",
    "\n",
    "    model_save_path='LSM/longformer-0-279labels.h5'\n",
    "\n",
    "\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    accuracy = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "    callbacks= [tf.keras.callbacks.ModelCheckpoint(filepath=model_save_path,monitor='val_accuracy',mode='max',save_best_only=True,save_weights_only=True),keras.callbacks.TensorBoard(log_dir=log_dir)]\n",
    "    model_0.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])\n",
    "          \n",
    "\n",
    "     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Sep 16 01:29:36 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM...  On   | 00000000:31:00.0 Off |                    0 |\n",
      "| N/A   26C    P0    71W / 500W |  79810MiB / 81920MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100-SXM...  On   | 00000000:CA:00.0 Off |                    0 |\n",
      "| N/A   25C    P0    69W / 500W |  79536MiB / 81920MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A    184456      C   python                          79805MiB |\n",
      "|    1   N/A  N/A    184456      C   python                          79531MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "    print('Not connected to a GPU')\n",
    "else:\n",
    "    print(gpu_info)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-14 16:24:34.398940: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:766] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_9\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"PrefetchDataset/_8\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_slice_batch_indices_50122\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\020FlatMapDataset:4\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 8\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT64\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_longformer_model/longformer/pooler/dense/kernel:0', 'tf_longformer_model/longformer/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_longformer_model/longformer/pooler/dense/kernel:0', 'tf_longformer_model/longformer/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:tensorflow:batch_all_reduce: 270 all-reduces with algorithm = nccl, num_packs = 1\n",
      "WARNING:tensorflow:Efficient allreduce is not supported for 3 IndexedSlices\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_longformer_model/longformer/pooler/dense/kernel:0', 'tf_longformer_model/longformer/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_longformer_model/longformer/pooler/dense/kernel:0', 'tf_longformer_model/longformer/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:tensorflow:batch_all_reduce: 270 all-reduces with algorithm = nccl, num_packs = 1\n",
      "WARNING:tensorflow:Efficient allreduce is not supported for 3 IndexedSlices\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
      "156/947 [===>..........................] - ETA: 11:42 - loss: 14.7583 - accuracy: 0.0184"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "tf.keras.backend.clear_session()\n",
    "history=model_0.fit([train_inp,train_mask],train_label,batch_size=8,epochs=5,validation_data=([val_inp,val_mask],val_label),callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_12\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_25 (InputLayer)          [(None, 4096)]       0           []                               \n",
      "                                                                                                  \n",
      " input_26 (InputLayer)          [(None, 4096)]       0           []                               \n",
      "                                                                                                  \n",
      " tf_longformer_model (TFLongfor  TFLongformerBaseMod  148659456  ['input_25[0][0]',               \n",
      " merModel)                      elOutputWithPooling               'input_26[0][0]']               \n",
      "                                (last_hidden_state=                                               \n",
      "                                (None, 4096, 768),                                                \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 hidden_states=None                                               \n",
      "                                , attentions=None,                                                \n",
      "                                global_attentions=N                                               \n",
      "                                one)                                                              \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_12 (S  (None, 768)         0           ['tf_longformer_model[12][0]']   \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " dense_24 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_12[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " dropout_61 (Dropout)           (None, 512)          0           ['dense_24[0][0]']               \n",
      "                                                                                                  \n",
      " dense_25 (Dense)               (None, 279)          143127      ['dropout_61[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 149,196,311\n",
      "Trainable params: 149,196,311\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "(840, 4096)\n",
      "(840, 4096)\n",
      "(840,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - 83s 649ms/step - loss: 6.2562 - accuracy: 0.5560\n",
      "0.5559523701667786\n",
      "Accuracy: 0.555952380952381\n",
      "Weighted F1: 0.5111982187649761\n",
      "Micro F1: 0.555952380952381\n",
      "Weighted Precision: 0.5060268429903294\n",
      "Micro Precision: 0.555952380952381\n",
      "Weighted Recall: 0.555952380952381\n",
      "Micro Recall: 0.555952380952381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "pred_labels=[]\n",
    "\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "accuracy = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "\n",
    "model_saved= create_model()\n",
    "model_saved.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])\n",
    "model_saved.load_weights('LSM/longformer-0-279labels.h5')\n",
    "\n",
    "\n",
    "print(val_inp.shape)\n",
    "print(val_mask.shape)\n",
    "print(val_label.shape)\n",
    "eval_loss, eval_acc=model_saved.evaluate([val_inp,val_mask],val_label,batch_size=8)\n",
    "print(eval_acc)\n",
    "\n",
    "\n",
    "for i in range(0,len(val_inp)):\n",
    "    pred=model_saved.predict([val_inp[i].reshape(1,4096),val_mask[i].reshape(1,4096)],batch_size=1)\n",
    "    pred_label = pred.argmax(axis=1)\n",
    "    pred_labels.append(pred_label)\n",
    "accuracy=accuracy_score(val_label, pred_labels)\n",
    "print(\"Accuracy: \"+str(accuracy))\n",
    "total_accuracy=total_accuracy+accuracy\n",
    "\n",
    "weighted_f1=f1_score(val_label,pred_labels, average='weighted')\n",
    "print(\"Weighted F1: \"+ str(weighted_f1))\n",
    "total_weighted_f1=total_weighted_f1+weighted_f1\n",
    "micro_f1=f1_score(val_label,pred_labels, average='micro')\n",
    "print(\"Micro F1: \"+ str(micro_f1))\n",
    "total_micro_f1=total_micro_f1+micro_f1\n",
    "\n",
    "weighted_precision=precision_score(val_label, pred_labels, average='weighted')\n",
    "print(\"Weighted Precision: \" + str(weighted_precision))\n",
    "total_weighted_precision=total_weighted_precision+weighted_precision\n",
    "micro_precision=precision_score(val_label, pred_labels, average='micro')\n",
    "print(\"Micro Precision: \" + str(micro_precision))\n",
    "total_micro_precision=total_micro_precision+micro_precision\n",
    "\n",
    "weighted_recall=recall_score(val_label, pred_labels, average='weighted')\n",
    "print(\"Weighted Recall: \" + str(weighted_recall))\n",
    "total_weighted_recall=total_weighted_recall+weighted_recall\n",
    "micro_recall=recall_score(val_label, pred_labels, average='micro')\n",
    "print(\"Micro Recall: \" + str(micro_recall))\n",
    "total_micro_recall=total_micro_recall+micro_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'supreme_court', 'site_url': 'http://caselaw.findlaw.com/court/us-supreme-court', 'description': 'Collection of ~8.4k decisions issued by the U.S. Supreme Court between November 1946 and June 2016.'}\n",
      "[-1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "{'-1': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7, '8': 8, '9': 9, '10': 10, '11': 11, '12': 12, '13': 13, '14': 14}\n",
      "8419\n",
      "Average Length 2920.3854377004395\n",
      "Found 8419 texts.\n",
      "Found 15 labels.\n",
      "                                                   text  label\n",
      "0     [ Halliburton Oil Well Cementing Co. v. Walker...    209\n",
      "1     Rehearing Denied Dec. 16, 1946. See . Mr.Claud...     63\n",
      "2     Rehearing Denied Dec. 16, 1946 See . Appeal fr...    216\n",
      "3     Mr. Walter J. Cummings, Jr., of Washington, D....    108\n",
      "4     Mr.A. Devitt Vaneck, of Washington, D.C., for ...    196\n",
      "...                                                 ...    ...\n",
      "8414  Opinion reported: Ante, p. 88. DECREE 1. It is...     74\n",
      "8415  In this dispute between Utah and the United St...      1\n",
      "8416  The United States, to the exclusion of defenda...      4\n",
      "8417  Louisiana's exception to the portion of the Sp...     74\n",
      "8418  To resolve a dispute over the ownership of cer...    263\n",
      "\n",
      "[8419 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at allenai/longformer-base-4096 were not used when initializing TFLongformerModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFLongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFLongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFLongformerModel were initialized from the model checkpoint at allenai/longformer-base-4096.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFLongformerModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)           [(None, 4096)]       0           []                               \n",
      "                                                                                                  \n",
      " input_6 (InputLayer)           [(None, 4096)]       0           []                               \n",
      "                                                                                                  \n",
      " tf_longformer_model_1 (TFLongf  TFLongformerBaseMod  148659456  ['input_5[0][0]',                \n",
      " ormerModel)                    elOutputWithPooling               'input_6[0][0]']                \n",
      "                                (last_hidden_state=                                               \n",
      "                                (None, 4096, 768),                                                \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 hidden_states=None                                               \n",
      "                                , attentions=None,                                                \n",
      "                                global_attentions=N                                               \n",
      "                                one)                                                              \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_2 (Sl  (None, 768)         0           ['tf_longformer_model_1[0][0]']  \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_2[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dropout_100 (Dropout)          (None, 512)          0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 279)          143127      ['dropout_100[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 149,196,311\n",
      "Trainable params: 149,196,311\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7577, 4096)\n",
      "(7577, 4096)\n",
      "(7577,)\n",
      "(842, 4096)\n",
      "(842, 4096)\n",
      "(842,)\n",
      "(7576, 4096)\n",
      "(7576, 4096)\n",
      "(7576,)\n",
      "(840, 4096)\n",
      "(840, 4096)\n",
      "(840,)\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "    sc = SupremeCourt()\n",
    "    print(sc.info)\n",
    "    sc.download()\n",
    "\n",
    "    texts = []  # list of text samples\n",
    "    labels_index = {}  # dictionary mapping label name to numeric id\n",
    "    labels = []  # list of label ids\n",
    "\n",
    "    issue_codes = list(sc.issue_area_codes.keys()) # 15 labels\n",
    "    print(issue_codes)\n",
    "    issue_codes.sort()\n",
    "    issue_codes = [str(ic) for ic in issue_codes]\n",
    "\n",
    "    labels_index = dict(zip(issue_codes, np.arange(len(issue_codes))))\n",
    "    print(labels_index)\n",
    "    count=0\n",
    "\n",
    "    for record in sc.records():\n",
    "\n",
    "            count=count+1\n",
    "            if record[1]['issue'] == None: # some cases have None as an issue\n",
    "                labels.append(labels_index['-1'])\n",
    "            else:\n",
    "                labels.append(labels_index[record[1]['issue'][:-4]])\n",
    "\n",
    "            new_sen=record[0].split(\"Footnotes\")[0]\n",
    "            new_new_sen=new_sen.split()\n",
    "            if len(new_new_sen) >= 4096:\n",
    "              new_new_sen=new_new_sen[0:4096]\n",
    "\n",
    "            elif len(new_new_sen) < 4096:\n",
    "              new_new_sen=new_new_sen[0:len(new_new_sen)]\n",
    "\n",
    "            new_new_sen=' '.join(new_new_sen)\n",
    "\n",
    "            texts.append(new_new_sen)\n",
    "\n",
    "    len_list = [len(ele.split()) for ele in texts]\n",
    "\n",
    "    #print(labels)\n",
    "    print(len(labels))\n",
    "\n",
    "    res = 0 if len(len_list) == 0 else (float(sum(len_list)) / len(len_list))\n",
    "    print(\"Average Length %s\" % res) \n",
    "    print('Found %s texts.' % len(texts))\n",
    "    print('Found %s labels.' % len(labels_index))\n",
    "    temp_file = open(\"labels_sc_279.txt\", \"r\")\n",
    "\n",
    "    data = temp_file.read()\n",
    "    label_list = data.split(\"\\n\")\n",
    "    #print(label_list)\n",
    "    label_list = label_list[0:-1]\n",
    "    #print(label_list)\n",
    "    label_list = [int(i) for i in label_list]\n",
    "    summarized_data = pd.DataFrame(texts,\n",
    "                   columns =['text'])\n",
    "    summarized_data['label'] = label_list\n",
    "    print(summarized_data)\n",
    "    #gc.collect()\n",
    "    #tf.keras.backend.clear_session()\n",
    "    dbert_tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
    "    dbert_model = TFLongformerModel.from_pretrained('allenai/longformer-base-4096')\n",
    "    max_len=4096\n",
    "    sentences=summarized_data['text']\n",
    "    labels=summarized_data['label']\n",
    "    len(sentences),len(labels)\n",
    "    model_0=create_model()\n",
    "    input_ids=[]\n",
    "    attention_masks=[]\n",
    "\n",
    "    for sent in sentences:\n",
    "      dbert_inps=dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n",
    "      input_ids.append(dbert_inps['input_ids'])\n",
    "      attention_masks.append(dbert_inps['attention_mask'])\n",
    "    input_ids=np.asarray(input_ids)\n",
    "\n",
    "    attention_masks=np.array(attention_masks)\n",
    "    labels=np.array(labels)\n",
    "    train_inp,val_inp,train_label,val_label,train_mask,val_mask=train_test_split(input_ids,labels,attention_masks,test_size=0.1,random_state=42)\n",
    "    print(train_inp.shape)\n",
    "    print(train_mask.shape)\n",
    "    print(train_label.shape)\n",
    "    print(val_inp.shape)\n",
    "    print(val_mask.shape)\n",
    "    print(val_label.shape)\n",
    "    train_inp=train_inp[0:7576]\n",
    "    train_mask=train_mask[0:7576]\n",
    "    train_label=train_label[0:7576]\n",
    "\n",
    "    val_inp=val_inp[0:840]\n",
    "    val_label=val_label[0:840]\n",
    "    val_mask=val_mask[0:840]\n",
    "    print(train_inp.shape)\n",
    "    print(train_mask.shape)\n",
    "    print(train_label.shape)\n",
    "    print(val_inp.shape)\n",
    "    print(val_mask.shape)\n",
    "    print(val_label.shape)\n",
    "    log_dir='dbert_model'\n",
    "\n",
    "    model_save_path='LSM/longformer-1-279labels.h5'\n",
    "\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    accuracy = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "    callbacks= [tf.keras.callbacks.ModelCheckpoint(filepath=model_save_path,monitor='val_accuracy',mode='max',save_best_only=True,save_weights_only=True),keras.callbacks.TensorBoard(log_dir=log_dir)]\n",
    "    model_0.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-14 19:49:36.838216: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:766] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_9\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"PrefetchDataset/_8\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_slice_batch_indices_204489\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\023FlatMapDataset:9255\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 8\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT64\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_longformer_model_1/longformer/pooler/dense/kernel:0', 'tf_longformer_model_1/longformer/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_longformer_model_1/longformer/pooler/dense/kernel:0', 'tf_longformer_model_1/longformer/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:tensorflow:batch_all_reduce: 270 all-reduces with algorithm = nccl, num_packs = 1\n",
      "WARNING:tensorflow:Efficient allreduce is not supported for 3 IndexedSlices\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_longformer_model_1/longformer/pooler/dense/kernel:0', 'tf_longformer_model_1/longformer/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_longformer_model_1/longformer/pooler/dense/kernel:0', 'tf_longformer_model_1/longformer/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:tensorflow:batch_all_reduce: 270 all-reduces with algorithm = nccl, num_packs = 1\n",
      "WARNING:tensorflow:Efficient allreduce is not supported for 3 IndexedSlices\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
      "947/947 [==============================] - ETA: 0s - loss: 12.4583 - accuracy: 0.1381"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-14 20:05:59.875192: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:766] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_9\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"PrefetchDataset/_8\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_slice_batch_indices_343633\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\023FlatMapDataset:9280\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 8\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT64\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "947/947 [==============================] - 1046s 952ms/step - loss: 12.4583 - accuracy: 0.1381 - val_loss: 10.2271 - val_accuracy: 0.3024\n",
      "Epoch 2/5\n",
      "947/947 [==============================] - 870s 919ms/step - loss: 9.3453 - accuracy: 0.3532 - val_loss: 8.4847 - val_accuracy: 0.4119\n",
      "Epoch 3/5\n",
      "947/947 [==============================] - 870s 919ms/step - loss: 7.8966 - accuracy: 0.4554 - val_loss: 7.4781 - val_accuracy: 0.4786\n",
      "Epoch 4/5\n",
      "947/947 [==============================] - 875s 924ms/step - loss: 6.8728 - accuracy: 0.5296 - val_loss: 6.7749 - val_accuracy: 0.4976\n",
      "Epoch 5/5\n",
      "947/947 [==============================] - 874s 923ms/step - loss: 6.0282 - accuracy: 0.5990 - val_loss: 6.1458 - val_accuracy: 0.5250\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "tf.keras.backend.clear_session()\n",
    "history=model_0.fit([train_inp,train_mask],train_label,batch_size=8,epochs=5,validation_data=([val_inp,val_mask],val_label),callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_13\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_27 (InputLayer)          [(None, 4096)]       0           []                               \n",
      "                                                                                                  \n",
      " input_28 (InputLayer)          [(None, 4096)]       0           []                               \n",
      "                                                                                                  \n",
      " tf_longformer_model (TFLongfor  TFLongformerBaseMod  148659456  ['input_27[0][0]',               \n",
      " merModel)                      elOutputWithPooling               'input_28[0][0]']               \n",
      "                                (last_hidden_state=                                               \n",
      "                                (None, 4096, 768),                                                \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 hidden_states=None                                               \n",
      "                                , attentions=None,                                                \n",
      "                                global_attentions=N                                               \n",
      "                                one)                                                              \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_13 (S  (None, 768)         0           ['tf_longformer_model[13][0]']   \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " dense_26 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_13[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " dropout_62 (Dropout)           (None, 512)          0           ['dense_26[0][0]']               \n",
      "                                                                                                  \n",
      " dense_27 (Dense)               (None, 279)          143127      ['dropout_62[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 149,196,311\n",
      "Trainable params: 149,196,311\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Accuracy: 0.5238095238095238\n",
      "Weighted F1: 0.47789785803621926\n",
      "Micro F1: 0.5238095238095238\n",
      "Weighted Precision: 0.4767311365505379\n",
      "Micro Precision: 0.5238095238095238\n",
      "Weighted Recall: 0.5238095238095238\n",
      "Micro Recall: 0.5238095238095238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "pred_labels=[]\n",
    "\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "accuracy = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "\n",
    "model_saved= create_model()\n",
    "model_saved.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])\n",
    "model_saved.load_weights('LSM/longformer-1-279labels.h5')\n",
    "\n",
    "for i in range(0,len(val_inp)):\n",
    "    pred=model_saved.predict([val_inp[i].reshape(1,4096),val_mask[i].reshape(1,4096)],batch_size=8)\n",
    "    pred_label = pred.argmax(axis=1)\n",
    "    pred_labels.append(pred_label)\n",
    "accuracy=accuracy_score(val_label, pred_labels)\n",
    "print(\"Accuracy: \"+str(accuracy))\n",
    "total_accuracy=total_accuracy+accuracy\n",
    "\n",
    "weighted_f1=f1_score(val_label,pred_labels, average='weighted')\n",
    "print(\"Weighted F1: \"+ str(weighted_f1))\n",
    "total_weighted_f1=total_weighted_f1+weighted_f1\n",
    "micro_f1=f1_score(val_label,pred_labels, average='micro')\n",
    "print(\"Micro F1: \"+ str(micro_f1))\n",
    "total_micro_f1=total_micro_f1+micro_f1\n",
    "\n",
    "weighted_precision=precision_score(val_label, pred_labels, average='weighted')\n",
    "print(\"Weighted Precision: \" + str(weighted_precision))\n",
    "total_weighted_precision=total_weighted_precision+weighted_precision\n",
    "micro_precision=precision_score(val_label, pred_labels, average='micro')\n",
    "print(\"Micro Precision: \" + str(micro_precision))\n",
    "total_micro_precision=total_micro_precision+micro_precision\n",
    "\n",
    "weighted_recall=recall_score(val_label, pred_labels, average='weighted')\n",
    "print(\"Weighted Recall: \" + str(weighted_recall))\n",
    "total_weighted_recall=total_weighted_recall+weighted_recall\n",
    "micro_recall=recall_score(val_label, pred_labels, average='micro')\n",
    "print(\"Micro Recall: \" + str(micro_recall))\n",
    "total_micro_recall=total_micro_recall+micro_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'supreme_court', 'site_url': 'http://caselaw.findlaw.com/court/us-supreme-court', 'description': 'Collection of ~8.4k decisions issued by the U.S. Supreme Court between November 1946 and June 2016.'}\n",
      "[-1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "{'-1': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7, '8': 8, '9': 9, '10': 10, '11': 11, '12': 12, '13': 13, '14': 14}\n",
      "8419\n",
      "Average Length 2920.3854377004395\n",
      "Found 8419 texts.\n",
      "Found 15 labels.\n",
      "                                                   text  label\n",
      "0     [ Halliburton Oil Well Cementing Co. v. Walker...    209\n",
      "1     Rehearing Denied Dec. 16, 1946. See . Mr.Claud...     63\n",
      "2     Rehearing Denied Dec. 16, 1946 See . Appeal fr...    216\n",
      "3     Mr. Walter J. Cummings, Jr., of Washington, D....    108\n",
      "4     Mr.A. Devitt Vaneck, of Washington, D.C., for ...    196\n",
      "...                                                 ...    ...\n",
      "8414  Opinion reported: Ante, p. 88. DECREE 1. It is...     74\n",
      "8415  In this dispute between Utah and the United St...      1\n",
      "8416  The United States, to the exclusion of defenda...      4\n",
      "8417  Louisiana's exception to the portion of the Sp...     74\n",
      "8418  To resolve a dispute over the ownership of cer...    263\n",
      "\n",
      "[8419 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at allenai/longformer-base-4096 were not used when initializing TFLongformerModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFLongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFLongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFLongformerModel were initialized from the model checkpoint at allenai/longformer-base-4096.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFLongformerModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 4096)]       0           []                               \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)           [(None, 4096)]       0           []                               \n",
      "                                                                                                  \n",
      " tf_longformer_model (TFLongfor  TFLongformerBaseMod  148659456  ['input_3[0][0]',                \n",
      " merModel)                      elOutputWithPooling               'input_4[0][0]']                \n",
      "                                (last_hidden_state=                                               \n",
      "                                (None, 4096, 768),                                                \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 hidden_states=None                                               \n",
      "                                , attentions=None,                                                \n",
      "                                global_attentions=N                                               \n",
      "                                one)                                                              \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_longformer_model[0][0]']    \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dropout_50 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 279)          143127      ['dropout_50[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 149,196,311\n",
      "Trainable params: 149,196,311\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7577, 4096)\n",
      "(7577, 4096)\n",
      "(7577,)\n",
      "(842, 4096)\n",
      "(842, 4096)\n",
      "(842,)\n",
      "(7576, 4096)\n",
      "(7576, 4096)\n",
      "(7576,)\n",
      "(840, 4096)\n",
      "(840, 4096)\n",
      "(840,)\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "    sc = SupremeCourt()\n",
    "    print(sc.info)\n",
    "    sc.download()\n",
    "\n",
    "    texts = []  # list of text samples\n",
    "    labels_index = {}  # dictionary mapping label name to numeric id\n",
    "    labels = []  # list of label ids\n",
    "\n",
    "    issue_codes = list(sc.issue_area_codes.keys()) # 15 labels\n",
    "    print(issue_codes)\n",
    "    issue_codes.sort()\n",
    "    issue_codes = [str(ic) for ic in issue_codes]\n",
    "\n",
    "    labels_index = dict(zip(issue_codes, np.arange(len(issue_codes))))\n",
    "    print(labels_index)\n",
    "    count=0\n",
    "\n",
    "    for record in sc.records():\n",
    "\n",
    "            count=count+1\n",
    "            if record[1]['issue'] == None: # some cases have None as an issue\n",
    "                labels.append(labels_index['-1'])\n",
    "            else:\n",
    "                labels.append(labels_index[record[1]['issue'][:-4]])\n",
    "\n",
    "            new_sen=record[0].split(\"Footnotes\")[0]\n",
    "            new_new_sen=new_sen.split()\n",
    "            if len(new_new_sen) >= 4096:\n",
    "              new_new_sen=new_new_sen[0:4096]\n",
    "\n",
    "            elif len(new_new_sen) < 4096:\n",
    "              new_new_sen=new_new_sen[0:len(new_new_sen)]\n",
    "\n",
    "            new_new_sen=' '.join(new_new_sen)\n",
    "\n",
    "            texts.append(new_new_sen)\n",
    "\n",
    "    len_list = [len(ele.split()) for ele in texts]\n",
    "\n",
    "    #print(labels)\n",
    "    print(len(labels))\n",
    "\n",
    "    res = 0 if len(len_list) == 0 else (float(sum(len_list)) / len(len_list))\n",
    "    print(\"Average Length %s\" % res) \n",
    "    print('Found %s texts.' % len(texts))\n",
    "    print('Found %s labels.' % len(labels_index))\n",
    "    temp_file = open(\"labels_sc_279.txt\", \"r\")\n",
    "\n",
    "    data = temp_file.read()\n",
    "    label_list = data.split(\"\\n\")\n",
    "    #print(label_list)\n",
    "    label_list = label_list[0:-1]\n",
    "    #print(label_list)\n",
    "    label_list = [int(i) for i in label_list]\n",
    "    summarized_data = pd.DataFrame(texts,\n",
    "                   columns =['text'])\n",
    "    summarized_data['label'] = label_list\n",
    "    print(summarized_data)\n",
    "    #gc.collect()\n",
    "    #tf.keras.backend.clear_session()\n",
    "    dbert_tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
    "    dbert_model = TFLongformerModel.from_pretrained('allenai/longformer-base-4096')\n",
    "    max_len=4096\n",
    "    sentences=summarized_data['text']\n",
    "    labels=summarized_data['label']\n",
    "    len(sentences),len(labels)\n",
    "    model_0=create_model()\n",
    "    input_ids=[]\n",
    "    attention_masks=[]\n",
    "\n",
    "    for sent in sentences:\n",
    "      dbert_inps=dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n",
    "      input_ids.append(dbert_inps['input_ids'])\n",
    "      attention_masks.append(dbert_inps['attention_mask'])\n",
    "    input_ids=np.asarray(input_ids)\n",
    "\n",
    "    attention_masks=np.array(attention_masks)\n",
    "    labels=np.array(labels)\n",
    "    train_inp,val_inp,train_label,val_label,train_mask,val_mask=train_test_split(input_ids,labels,attention_masks,test_size=0.1,random_state=42)\n",
    "    print(train_inp.shape)\n",
    "    print(train_mask.shape)\n",
    "    print(train_label.shape)\n",
    "    print(val_inp.shape)\n",
    "    print(val_mask.shape)\n",
    "    print(val_label.shape)\n",
    "    train_inp=train_inp[0:7576]\n",
    "    train_mask=train_mask[0:7576]\n",
    "    train_label=train_label[0:7576]\n",
    "\n",
    "    val_inp=val_inp[0:840]\n",
    "    val_label=val_label[0:840]\n",
    "    val_mask=val_mask[0:840]\n",
    "    print(train_inp.shape)\n",
    "    print(train_mask.shape)\n",
    "    print(train_label.shape)\n",
    "    print(val_inp.shape)\n",
    "    print(val_mask.shape)\n",
    "    print(val_label.shape)\n",
    "    log_dir='dbert_model'\n",
    "\n",
    "    model_save_path='LSM/longformer-2-279labels.h5'\n",
    "\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    accuracy = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "    callbacks= [tf.keras.callbacks.ModelCheckpoint(filepath=model_save_path,monitor='val_accuracy',mode='max',save_best_only=True,save_weights_only=True),keras.callbacks.TensorBoard(log_dir=log_dir)]\n",
    "    model_0.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-14 21:10:38.250494: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:766] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_9\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"PrefetchDataset/_8\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_slice_batch_indices_548875\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\024FlatMapDataset:20259\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 8\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT64\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_longformer_model/longformer/pooler/dense/kernel:0', 'tf_longformer_model/longformer/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_longformer_model/longformer/pooler/dense/kernel:0', 'tf_longformer_model/longformer/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_longformer_model/longformer/pooler/dense/kernel:0', 'tf_longformer_model/longformer/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_longformer_model/longformer/pooler/dense/kernel:0', 'tf_longformer_model/longformer/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "947/947 [==============================] - ETA: 0s - loss: 12.4578 - accuracy: 0.1497"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-14 21:27:11.368015: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:766] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_9\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"PrefetchDataset/_8\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_slice_batch_indices_688015\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\024FlatMapDataset:20284\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 8\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT64\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "947/947 [==============================] - 1053s 957ms/step - loss: 12.4578 - accuracy: 0.1497 - val_loss: 10.1372 - val_accuracy: 0.3202\n",
      "Epoch 2/5\n",
      "947/947 [==============================] - 880s 929ms/step - loss: 9.2949 - accuracy: 0.3763 - val_loss: 8.4106 - val_accuracy: 0.4476\n",
      "Epoch 3/5\n",
      "947/947 [==============================] - 880s 929ms/step - loss: 7.8659 - accuracy: 0.4739 - val_loss: 7.5196 - val_accuracy: 0.4869\n",
      "Epoch 4/5\n",
      "947/947 [==============================] - 880s 929ms/step - loss: 6.8785 - accuracy: 0.5454 - val_loss: 6.7931 - val_accuracy: 0.5095\n",
      "Epoch 5/5\n",
      "947/947 [==============================] - 880s 929ms/step - loss: 6.0720 - accuracy: 0.6043 - val_loss: 6.2147 - val_accuracy: 0.5179\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "tf.keras.backend.clear_session()\n",
    "history=model_0.fit([train_inp,train_mask],train_label,batch_size=8,epochs=5,validation_data=([val_inp,val_mask],val_label),callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_14\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_29 (InputLayer)          [(None, 4096)]       0           []                               \n",
      "                                                                                                  \n",
      " input_30 (InputLayer)          [(None, 4096)]       0           []                               \n",
      "                                                                                                  \n",
      " tf_longformer_model (TFLongfor  TFLongformerBaseMod  148659456  ['input_29[0][0]',               \n",
      " merModel)                      elOutputWithPooling               'input_30[0][0]']               \n",
      "                                (last_hidden_state=                                               \n",
      "                                (None, 4096, 768),                                                \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 hidden_states=None                                               \n",
      "                                , attentions=None,                                                \n",
      "                                global_attentions=N                                               \n",
      "                                one)                                                              \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_14 (S  (None, 768)         0           ['tf_longformer_model[14][0]']   \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " dense_28 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_14[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " dropout_63 (Dropout)           (None, 512)          0           ['dense_28[0][0]']               \n",
      "                                                                                                  \n",
      " dense_29 (Dense)               (None, 279)          143127      ['dropout_63[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 149,196,311\n",
      "Trainable params: 149,196,311\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Accuracy: 0.5178571428571429\n",
      "Weighted F1: 0.46924888299706635\n",
      "Micro F1: 0.5178571428571429\n",
      "Weighted Precision: 0.463799479787545\n",
      "Micro Precision: 0.5178571428571429\n",
      "Weighted Recall: 0.5178571428571429\n",
      "Micro Recall: 0.5178571428571429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "pred_labels=[]\n",
    "\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "accuracy = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "\n",
    "model_saved= create_model()\n",
    "model_saved.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])\n",
    "model_saved.load_weights('LSM/longformer-2-279labels.h5')\n",
    "\n",
    "for i in range(0,len(val_inp)):\n",
    "    pred=model_saved.predict([val_inp[i].reshape(1,4096),val_mask[i].reshape(1,4096)],batch_size=8)\n",
    "    pred_label = pred.argmax(axis=1)\n",
    "    pred_labels.append(pred_label)\n",
    "accuracy=accuracy_score(val_label, pred_labels)\n",
    "print(\"Accuracy: \"+str(accuracy))\n",
    "total_accuracy=total_accuracy+accuracy\n",
    "\n",
    "weighted_f1=f1_score(val_label,pred_labels, average='weighted')\n",
    "print(\"Weighted F1: \"+ str(weighted_f1))\n",
    "total_weighted_f1=total_weighted_f1+weighted_f1\n",
    "micro_f1=f1_score(val_label,pred_labels, average='micro')\n",
    "print(\"Micro F1: \"+ str(micro_f1))\n",
    "total_micro_f1=total_micro_f1+micro_f1\n",
    "\n",
    "weighted_precision=precision_score(val_label, pred_labels, average='weighted')\n",
    "print(\"Weighted Precision: \" + str(weighted_precision))\n",
    "total_weighted_precision=total_weighted_precision+weighted_precision\n",
    "micro_precision=precision_score(val_label, pred_labels, average='micro')\n",
    "print(\"Micro Precision: \" + str(micro_precision))\n",
    "total_micro_precision=total_micro_precision+micro_precision\n",
    "\n",
    "weighted_recall=recall_score(val_label, pred_labels, average='weighted')\n",
    "print(\"Weighted Recall: \" + str(weighted_recall))\n",
    "total_weighted_recall=total_weighted_recall+weighted_recall\n",
    "micro_recall=recall_score(val_label, pred_labels, average='micro')\n",
    "print(\"Micro Recall: \" + str(micro_recall))\n",
    "total_micro_recall=total_micro_recall+micro_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'supreme_court', 'site_url': 'http://caselaw.findlaw.com/court/us-supreme-court', 'description': 'Collection of ~8.4k decisions issued by the U.S. Supreme Court between November 1946 and June 2016.'}\n",
      "[-1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "{'-1': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7, '8': 8, '9': 9, '10': 10, '11': 11, '12': 12, '13': 13, '14': 14}\n",
      "8419\n",
      "Average Length 2920.3854377004395\n",
      "Found 8419 texts.\n",
      "Found 15 labels.\n",
      "                                                   text  label\n",
      "0     [ Halliburton Oil Well Cementing Co. v. Walker...    209\n",
      "1     Rehearing Denied Dec. 16, 1946. See . Mr.Claud...     63\n",
      "2     Rehearing Denied Dec. 16, 1946 See . Appeal fr...    216\n",
      "3     Mr. Walter J. Cummings, Jr., of Washington, D....    108\n",
      "4     Mr.A. Devitt Vaneck, of Washington, D.C., for ...    196\n",
      "...                                                 ...    ...\n",
      "8414  Opinion reported: Ante, p. 88. DECREE 1. It is...     74\n",
      "8415  In this dispute between Utah and the United St...      1\n",
      "8416  The United States, to the exclusion of defenda...      4\n",
      "8417  Louisiana's exception to the portion of the Sp...     74\n",
      "8418  To resolve a dispute over the ownership of cer...    263\n",
      "\n",
      "[8419 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at allenai/longformer-base-4096 were not used when initializing TFLongformerModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFLongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFLongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFLongformerModel were initialized from the model checkpoint at allenai/longformer-base-4096.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFLongformerModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 4096)]       0           []                               \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)           [(None, 4096)]       0           []                               \n",
      "                                                                                                  \n",
      " tf_longformer_model (TFLongfor  TFLongformerBaseMod  148659456  ['input_3[0][0]',                \n",
      " merModel)                      elOutputWithPooling               'input_4[0][0]']                \n",
      "                                (last_hidden_state=                                               \n",
      "                                (None, 4096, 768),                                                \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 hidden_states=None                                               \n",
      "                                , attentions=None,                                                \n",
      "                                global_attentions=N                                               \n",
      "                                one)                                                              \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_longformer_model[0][0]']    \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dropout_50 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 279)          143127      ['dropout_50[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 149,196,311\n",
      "Trainable params: 149,196,311\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7577, 4096)\n",
      "(7577, 4096)\n",
      "(7577,)\n",
      "(842, 4096)\n",
      "(842, 4096)\n",
      "(842,)\n",
      "(7576, 4096)\n",
      "(7576, 4096)\n",
      "(7576,)\n",
      "(840, 4096)\n",
      "(840, 4096)\n",
      "(840,)\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "    sc = SupremeCourt()\n",
    "    print(sc.info)\n",
    "    sc.download()\n",
    "\n",
    "    texts = []  # list of text samples\n",
    "    labels_index = {}  # dictionary mapping label name to numeric id\n",
    "    labels = []  # list of label ids\n",
    "\n",
    "    issue_codes = list(sc.issue_area_codes.keys()) # 15 labels\n",
    "    print(issue_codes)\n",
    "    issue_codes.sort()\n",
    "    issue_codes = [str(ic) for ic in issue_codes]\n",
    "\n",
    "    labels_index = dict(zip(issue_codes, np.arange(len(issue_codes))))\n",
    "    print(labels_index)\n",
    "    count=0\n",
    "\n",
    "    for record in sc.records():\n",
    "\n",
    "            count=count+1\n",
    "            if record[1]['issue'] == None: # some cases have None as an issue\n",
    "                labels.append(labels_index['-1'])\n",
    "            else:\n",
    "                labels.append(labels_index[record[1]['issue'][:-4]])\n",
    "\n",
    "            new_sen=record[0].split(\"Footnotes\")[0]\n",
    "            new_new_sen=new_sen.split()\n",
    "            if len(new_new_sen) >= 4096:\n",
    "              new_new_sen=new_new_sen[0:4096]\n",
    "\n",
    "            elif len(new_new_sen) < 4096:\n",
    "              new_new_sen=new_new_sen[0:len(new_new_sen)]\n",
    "\n",
    "            new_new_sen=' '.join(new_new_sen)\n",
    "\n",
    "            texts.append(new_new_sen)\n",
    "\n",
    "    len_list = [len(ele.split()) for ele in texts]\n",
    "\n",
    "    #print(labels)\n",
    "    print(len(labels))\n",
    "\n",
    "    res = 0 if len(len_list) == 0 else (float(sum(len_list)) / len(len_list))\n",
    "    print(\"Average Length %s\" % res) \n",
    "    print('Found %s texts.' % len(texts))\n",
    "    print('Found %s labels.' % len(labels_index))\n",
    "    temp_file = open(\"labels_sc_279.txt\", \"r\")\n",
    "\n",
    "    data = temp_file.read()\n",
    "    label_list = data.split(\"\\n\")\n",
    "    #print(label_list)\n",
    "    label_list = label_list[0:-1]\n",
    "    #print(label_list)\n",
    "    label_list = [int(i) for i in label_list]\n",
    "    summarized_data = pd.DataFrame(texts,\n",
    "                   columns =['text'])\n",
    "    summarized_data['label'] = label_list\n",
    "    print(summarized_data)\n",
    "    #gc.collect()\n",
    "    #tf.keras.backend.clear_session()\n",
    "    dbert_tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
    "    dbert_model = TFLongformerModel.from_pretrained('allenai/longformer-base-4096')\n",
    "    max_len=4096\n",
    "    sentences=summarized_data['text']\n",
    "    labels=summarized_data['label']\n",
    "    len(sentences),len(labels)\n",
    "    model_0=create_model()\n",
    "    input_ids=[]\n",
    "    attention_masks=[]\n",
    "\n",
    "    for sent in sentences:\n",
    "      dbert_inps=dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n",
    "      input_ids.append(dbert_inps['input_ids'])\n",
    "      attention_masks.append(dbert_inps['attention_mask'])\n",
    "    input_ids=np.asarray(input_ids)\n",
    "\n",
    "    attention_masks=np.array(attention_masks)\n",
    "    labels=np.array(labels)\n",
    "    train_inp,val_inp,train_label,val_label,train_mask,val_mask=train_test_split(input_ids,labels,attention_masks,test_size=0.1,random_state=42)\n",
    "    print(train_inp.shape)\n",
    "    print(train_mask.shape)\n",
    "    print(train_label.shape)\n",
    "    print(val_inp.shape)\n",
    "    print(val_mask.shape)\n",
    "    print(val_label.shape)\n",
    "    train_inp=train_inp[0:7576]\n",
    "    train_mask=train_mask[0:7576]\n",
    "    train_label=train_label[0:7576]\n",
    "\n",
    "    val_inp=val_inp[0:840]\n",
    "    val_label=val_label[0:840]\n",
    "    val_mask=val_mask[0:840]\n",
    "    print(train_inp.shape)\n",
    "    print(train_mask.shape)\n",
    "    print(train_label.shape)\n",
    "    print(val_inp.shape)\n",
    "    print(val_mask.shape)\n",
    "    print(val_label.shape)\n",
    "    log_dir='dbert_model'\n",
    "\n",
    "    model_save_path='LSM/longformer-3-279labels.h5'\n",
    "\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    accuracy = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "    callbacks= [tf.keras.callbacks.ModelCheckpoint(filepath=model_save_path,monitor='val_accuracy',mode='max',save_best_only=True,save_weights_only=True),keras.callbacks.TensorBoard(log_dir=log_dir)]\n",
    "    model_0.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-14 22:32:19.179814: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:766] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_9\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"PrefetchDataset/_8\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_slice_batch_indices_893257\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\024FlatMapDataset:31263\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 8\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT64\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_longformer_model/longformer/pooler/dense/kernel:0', 'tf_longformer_model/longformer/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_longformer_model/longformer/pooler/dense/kernel:0', 'tf_longformer_model/longformer/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_longformer_model/longformer/pooler/dense/kernel:0', 'tf_longformer_model/longformer/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_longformer_model/longformer/pooler/dense/kernel:0', 'tf_longformer_model/longformer/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "947/947 [==============================] - ETA: 0s - loss: 12.5271 - accuracy: 0.1177"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-14 22:48:41.691827: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:766] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_9\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"PrefetchDataset/_8\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_slice_batch_indices_1032397\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\024FlatMapDataset:31288\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 8\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT64\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "947/947 [==============================] - 1044s 949ms/step - loss: 12.5271 - accuracy: 0.1177 - val_loss: 10.0536 - val_accuracy: 0.3179\n",
      "Epoch 2/5\n",
      "947/947 [==============================] - 871s 920ms/step - loss: 9.2537 - accuracy: 0.3420 - val_loss: 8.3708 - val_accuracy: 0.4143\n",
      "Epoch 3/5\n",
      "947/947 [==============================] - 872s 920ms/step - loss: 7.8377 - accuracy: 0.4339 - val_loss: 7.3594 - val_accuracy: 0.4714\n",
      "Epoch 4/5\n",
      "947/947 [==============================] - 871s 920ms/step - loss: 6.8063 - accuracy: 0.5194 - val_loss: 6.6795 - val_accuracy: 0.4940\n",
      "Epoch 5/5\n",
      "729/947 [======================>.......] - ETA: 3:12 - loss: 6.0348 - accuracy: 0.5850"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "tf.keras.backend.clear_session()\n",
    "history=model_0.fit([train_inp,train_mask],train_label,batch_size=8,epochs=5,validation_data=([val_inp,val_mask],val_label),callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_15\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_31 (InputLayer)          [(None, 4096)]       0           []                               \n",
      "                                                                                                  \n",
      " input_32 (InputLayer)          [(None, 4096)]       0           []                               \n",
      "                                                                                                  \n",
      " tf_longformer_model (TFLongfor  TFLongformerBaseMod  148659456  ['input_31[0][0]',               \n",
      " merModel)                      elOutputWithPooling               'input_32[0][0]']               \n",
      "                                (last_hidden_state=                                               \n",
      "                                (None, 4096, 768),                                                \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 hidden_states=None                                               \n",
      "                                , attentions=None,                                                \n",
      "                                global_attentions=N                                               \n",
      "                                one)                                                              \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_15 (S  (None, 768)         0           ['tf_longformer_model[15][0]']   \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " dense_30 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_15[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " dropout_64 (Dropout)           (None, 512)          0           ['dense_30[0][0]']               \n",
      "                                                                                                  \n",
      " dense_31 (Dense)               (None, 279)          143127      ['dropout_64[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 149,196,311\n",
      "Trainable params: 149,196,311\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Accuracy: 0.5416666666666666\n",
      "Weighted F1: 0.4923234373100098\n",
      "Micro F1: 0.5416666666666666\n",
      "Weighted Precision: 0.4828388024883081\n",
      "Micro Precision: 0.5416666666666666\n",
      "Weighted Recall: 0.5416666666666666\n",
      "Micro Recall: 0.5416666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "pred_labels=[]\n",
    "\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "accuracy = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "\n",
    "model_saved= create_model()\n",
    "model_saved.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])\n",
    "model_saved.load_weights('LSM/longformer-3-279labels.h5')\n",
    "\n",
    "for i in range(0,len(val_inp)):\n",
    "    pred=model_saved.predict([val_inp[i].reshape(1,4096),val_mask[i].reshape(1,4096)],batch_size=8)\n",
    "    pred_label = pred.argmax(axis=1)\n",
    "    pred_labels.append(pred_label)\n",
    "accuracy=accuracy_score(val_label, pred_labels)\n",
    "print(\"Accuracy: \"+str(accuracy))\n",
    "total_accuracy=total_accuracy+accuracy\n",
    "\n",
    "weighted_f1=f1_score(val_label,pred_labels, average='weighted')\n",
    "print(\"Weighted F1: \"+ str(weighted_f1))\n",
    "total_weighted_f1=total_weighted_f1+weighted_f1\n",
    "micro_f1=f1_score(val_label,pred_labels, average='micro')\n",
    "print(\"Micro F1: \"+ str(micro_f1))\n",
    "total_micro_f1=total_micro_f1+micro_f1\n",
    "\n",
    "weighted_precision=precision_score(val_label, pred_labels, average='weighted')\n",
    "print(\"Weighted Precision: \" + str(weighted_precision))\n",
    "total_weighted_precision=total_weighted_precision+weighted_precision\n",
    "micro_precision=precision_score(val_label, pred_labels, average='micro')\n",
    "print(\"Micro Precision: \" + str(micro_precision))\n",
    "total_micro_precision=total_micro_precision+micro_precision\n",
    "\n",
    "weighted_recall=recall_score(val_label, pred_labels, average='weighted')\n",
    "print(\"Weighted Recall: \" + str(weighted_recall))\n",
    "total_weighted_recall=total_weighted_recall+weighted_recall\n",
    "micro_recall=recall_score(val_label, pred_labels, average='micro')\n",
    "print(\"Micro Recall: \" + str(micro_recall))\n",
    "total_micro_recall=total_micro_recall+micro_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    sc = SupremeCourt()\n",
    "    print(sc.info)\n",
    "    sc.download()\n",
    "\n",
    "    texts = []  # list of text samples\n",
    "    labels_index = {}  # dictionary mapping label name to numeric id\n",
    "    labels = []  # list of label ids\n",
    "\n",
    "    issue_codes = list(sc.issue_area_codes.keys()) # 15 labels\n",
    "    print(issue_codes)\n",
    "    issue_codes.sort()\n",
    "    issue_codes = [str(ic) for ic in issue_codes]\n",
    "\n",
    "    labels_index = dict(zip(issue_codes, np.arange(len(issue_codes))))\n",
    "    print(labels_index)\n",
    "    count=0\n",
    "\n",
    "    for record in sc.records():\n",
    "\n",
    "            count=count+1\n",
    "            if record[1]['issue'] == None: # some cases have None as an issue\n",
    "                labels.append(labels_index['-1'])\n",
    "            else:\n",
    "                labels.append(labels_index[record[1]['issue'][:-4]])\n",
    "\n",
    "            new_sen=record[0].split(\"Footnotes\")[0]\n",
    "            new_new_sen=new_sen.split()\n",
    "            if len(new_new_sen) >= 4096:\n",
    "              new_new_sen=new_new_sen[0:4096]\n",
    "\n",
    "            elif len(new_new_sen) < 4096:\n",
    "              new_new_sen=new_new_sen[0:len(new_new_sen)]\n",
    "\n",
    "            new_new_sen=' '.join(new_new_sen)\n",
    "\n",
    "            texts.append(new_new_sen)\n",
    "\n",
    "    len_list = [len(ele.split()) for ele in texts]\n",
    "\n",
    "    #print(labels)\n",
    "    print(len(labels))\n",
    "\n",
    "    res = 0 if len(len_list) == 0 else (float(sum(len_list)) / len(len_list))\n",
    "    print(\"Average Length %s\" % res) \n",
    "    print('Found %s texts.' % len(texts))\n",
    "    print('Found %s labels.' % len(labels_index))\n",
    "    temp_file = open(\"labels_sc_279.txt\", \"r\")\n",
    "\n",
    "    data = temp_file.read()\n",
    "    label_list = data.split(\"\\n\")\n",
    "    #print(label_list)\n",
    "    label_list = label_list[0:-1]\n",
    "    #print(label_list)\n",
    "    label_list = [int(i) for i in label_list]\n",
    "    summarized_data = pd.DataFrame(texts,\n",
    "                   columns =['text'])\n",
    "    summarized_data['label'] = label_list\n",
    "    print(summarized_data)\n",
    "    #gc.collect()\n",
    "    #tf.keras.backend.clear_session()\n",
    "    dbert_tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
    "    dbert_model = TFLongformerModel.from_pretrained('allenai/longformer-base-4096')\n",
    "    max_len=4096\n",
    "    sentences=summarized_data['text']\n",
    "    labels=summarized_data['label']\n",
    "    len(sentences),len(labels)\n",
    "    model_0=create_model()\n",
    "    input_ids=[]\n",
    "    attention_masks=[]\n",
    "\n",
    "    for sent in sentences:\n",
    "      dbert_inps=dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n",
    "      input_ids.append(dbert_inps['input_ids'])\n",
    "      attention_masks.append(dbert_inps['attention_mask'])\n",
    "    input_ids=np.asarray(input_ids)\n",
    "\n",
    "    attention_masks=np.array(attention_masks)\n",
    "    labels=np.array(labels)\n",
    "    train_inp,val_inp,train_label,val_label,train_mask,val_mask=train_test_split(input_ids,labels,attention_masks,test_size=0.1,random_state=42)\n",
    "    print(train_inp.shape)\n",
    "    print(train_mask.shape)\n",
    "    print(train_label.shape)\n",
    "    print(val_inp.shape)\n",
    "    print(val_mask.shape)\n",
    "    print(val_label.shape)\n",
    "    train_inp=train_inp[0:7576]\n",
    "    train_mask=train_mask[0:7576]\n",
    "    train_label=train_label[0:7576]\n",
    "\n",
    "    val_inp=val_inp[0:840]\n",
    "    val_label=val_label[0:840]\n",
    "    val_mask=val_mask[0:840]\n",
    "    print(train_inp.shape)\n",
    "    print(train_mask.shape)\n",
    "    print(train_label.shape)\n",
    "    print(val_inp.shape)\n",
    "    print(val_mask.shape)\n",
    "    print(val_label.shape)\n",
    "    log_dir='dbert_model'\n",
    "\n",
    "    model_save_path='LSM/longformer-4-279labels.h5'\n",
    "\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    accuracy = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "    callbacks= [tf.keras.callbacks.ModelCheckpoint(filepath=model_save_path,monitor='val_accuracy',mode='max',save_best_only=True,save_weights_only=True),keras.callbacks.TensorBoard(log_dir=log_dir)]\n",
    "    model_0.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "tf.keras.backend.clear_session()\n",
    "history=model_0.fit([train_inp,train_mask],train_label,batch_size=8,epochs=5,validation_data=([val_inp,val_mask],val_label),callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_16\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_33 (InputLayer)          [(None, 4096)]       0           []                               \n",
      "                                                                                                  \n",
      " input_34 (InputLayer)          [(None, 4096)]       0           []                               \n",
      "                                                                                                  \n",
      " tf_longformer_model (TFLongfor  TFLongformerBaseMod  148659456  ['input_33[0][0]',               \n",
      " merModel)                      elOutputWithPooling               'input_34[0][0]']               \n",
      "                                (last_hidden_state=                                               \n",
      "                                (None, 4096, 768),                                                \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 hidden_states=None                                               \n",
      "                                , attentions=None,                                                \n",
      "                                global_attentions=N                                               \n",
      "                                one)                                                              \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_16 (S  (None, 768)         0           ['tf_longformer_model[16][0]']   \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " dense_32 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_16[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " dropout_65 (Dropout)           (None, 512)          0           ['dense_32[0][0]']               \n",
      "                                                                                                  \n",
      " dense_33 (Dense)               (None, 279)          143127      ['dropout_65[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 149,196,311\n",
      "Trainable params: 149,196,311\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Accuracy: 0.5345238095238095\n",
      "Weighted F1: 0.4869099025046913\n",
      "Micro F1: 0.5345238095238095\n",
      "Weighted Precision: 0.48020082646844986\n",
      "Micro Precision: 0.5345238095238095\n",
      "Weighted Recall: 0.5345238095238095\n",
      "Micro Recall: 0.5345238095238095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "pred_labels=[]\n",
    "\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "accuracy = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "\n",
    "model_saved= create_model()\n",
    "model_saved.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])\n",
    "model_saved.load_weights('LSM/longformer-4-279labels.h5')\n",
    "\n",
    "for i in range(0,len(val_inp)):\n",
    "    pred=model_saved.predict([val_inp[i].reshape(1,4096),val_mask[i].reshape(1,4096)],batch_size=8)\n",
    "    pred_label = pred.argmax(axis=1)\n",
    "    pred_labels.append(pred_label)\n",
    "accuracy=accuracy_score(val_label, pred_labels)\n",
    "print(\"Accuracy: \"+str(accuracy))\n",
    "total_accuracy=total_accuracy+accuracy\n",
    "\n",
    "weighted_f1=f1_score(val_label,pred_labels, average='weighted')\n",
    "print(\"Weighted F1: \"+ str(weighted_f1))\n",
    "total_weighted_f1=total_weighted_f1+weighted_f1\n",
    "micro_f1=f1_score(val_label,pred_labels, average='micro')\n",
    "print(\"Micro F1: \"+ str(micro_f1))\n",
    "total_micro_f1=total_micro_f1+micro_f1\n",
    "\n",
    "weighted_precision=precision_score(val_label, pred_labels, average='weighted')\n",
    "print(\"Weighted Precision: \" + str(weighted_precision))\n",
    "total_weighted_precision=total_weighted_precision+weighted_precision\n",
    "micro_precision=precision_score(val_label, pred_labels, average='micro')\n",
    "print(\"Micro Precision: \" + str(micro_precision))\n",
    "total_micro_precision=total_micro_precision+micro_precision\n",
    "\n",
    "weighted_recall=recall_score(val_label, pred_labels, average='weighted')\n",
    "print(\"Weighted Recall: \" + str(weighted_recall))\n",
    "total_weighted_recall=total_weighted_recall+weighted_recall\n",
    "micro_recall=recall_score(val_label, pred_labels, average='micro')\n",
    "print(\"Micro Recall: \" + str(micro_recall))\n",
    "total_micro_recall=total_micro_recall+micro_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy: 0.5347619047619048\n",
      "Average Weighted F1: 0.48751565992259255\n",
      "Average Micro F1: 0.5347619047619048\n",
      "Average Weighted Precision: 0.481919417657034\n",
      "Average Micro Precision: 0.5347619047619048\n",
      "Average Weighted Recall: 0.5347619047619048\n",
      "Average Micro Recall: 0.5347619047619048\n"
     ]
    }
   ],
   "source": [
    "print(\"Average Accuracy: \"+str(total_accuracy/5))\n",
    "print(\"Average Weighted F1: \"+str(total_weighted_f1/5))\n",
    "print(\"Average Micro F1: \"+str(total_micro_f1/5))\n",
    "print(\"Average Weighted Precision: \"+str(total_weighted_precision/5))\n",
    "print(\"Average Micro Precision: \"+str(total_micro_precision/5))\n",
    "print(\"Average Weighted Recall: \"+str(total_weighted_recall/5))\n",
    "print(\"Average Micro Recall: \"+str(total_micro_recall/5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "background_execution": "on",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Best-512_0:512_15labels.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "WOL_ENV",
   "language": "python",
   "name": "wol_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
