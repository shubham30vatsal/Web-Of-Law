{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mCIR7rd5h9fv",
    "outputId": "53830c68-0d4f-4ea9-9978-d1684a586900"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "#drive.flush_and_unmount()\n",
    "drive.mount('/content/drive')\n",
    "#drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0k8FHrSIiqDo",
    "outputId": "18763450-95ef-4c99-f202-053db938b0ff",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (4.19.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from transformers) (1.21.6)\n",
      "Requirement already satisfied: filelock in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from transformers) (3.7.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: importlib-metadata in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from transformers) (4.11.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from transformers) (2022.4.24)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: requests in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: zipp>=0.5 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.8.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from requests->transformers) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from requests->transformers) (2022.5.18.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: sentencepiece in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (0.1.96)\n",
      "Requirement already satisfied: tensorflow==2.7.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (2.7.0)\n",
      "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorflow==2.7.0) (0.4.0)\n",
      "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorflow==2.7.0) (2.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorflow==2.7.0) (1.6.3)\n",
      "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorflow==2.7.0) (2.7.0)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorflow==2.7.0) (1.0.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorflow==2.7.0) (1.1.2)\n",
      "Requirement already satisfied: numpy>=1.14.5 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorflow==2.7.0) (1.21.6)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorflow==2.7.0) (0.2.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorflow==2.7.0) (0.26.0)\n",
      "Requirement already satisfied: libclang>=9.0.1 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorflow==2.7.0) (14.0.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorflow==2.7.0) (1.46.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.32.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorflow==2.7.0) (0.37.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorflow==2.7.0) (1.1.0)\n",
      "Requirement already satisfied: tensorboard~=2.6 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorflow==2.7.0) (2.9.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorflow==2.7.0) (3.3.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorflow==2.7.0) (1.16.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorflow==2.7.0) (1.14.1)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorflow==2.7.0) (3.19.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorflow==2.7.0) (3.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorflow==2.7.0) (3.10.0.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorflow==2.7.0) (2.7.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow==2.7.0) (3.3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow==2.7.0) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow==2.7.0) (1.8.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow==2.7.0) (2.6.6)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow==2.7.0) (61.2.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow==2.7.0) (2.1.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow==2.7.0) (2.27.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow==2.7.0) (0.4.6)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0) (5.2.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0) (4.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.7.0) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow==2.7.0) (4.11.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow==2.7.0) (3.8.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0) (0.4.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0) (2022.5.18.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0) (3.3)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.7.0) (3.2.0)\n",
      "Requirement already satisfied: stanza in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (1.4.0)\n",
      "Requirement already satisfied: protobuf in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from stanza) (3.19.0)\n",
      "Requirement already satisfied: requests in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from stanza) (2.27.1)\n",
      "Requirement already satisfied: tqdm in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from stanza) (4.64.0)\n",
      "Requirement already satisfied: six in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from stanza) (1.16.0)\n",
      "Requirement already satisfied: torch>=1.3.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from stanza) (1.11.0)\n",
      "Requirement already satisfied: numpy in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from stanza) (1.21.6)\n",
      "Requirement already satisfied: emoji in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from stanza) (1.7.0)\n",
      "Requirement already satisfied: transformers in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from stanza) (4.19.2)\n",
      "Requirement already satisfied: typing-extensions in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from torch>=1.3.0->stanza) (3.10.0.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from requests->stanza) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from requests->stanza) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from requests->stanza) (2022.5.18.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from requests->stanza) (2.0.12)\n",
      "Requirement already satisfied: packaging>=20.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from transformers->stanza) (21.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from transformers->stanza) (0.12.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from transformers->stanza) (2022.4.24)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from transformers->stanza) (0.7.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from transformers->stanza) (6.0)\n",
      "Requirement already satisfied: importlib-metadata in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from transformers->stanza) (4.11.4)\n",
      "Requirement already satisfied: filelock in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from transformers->stanza) (3.7.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from packaging>=20.0->transformers->stanza) (3.0.9)\n",
      "Requirement already satisfied: zipp>=0.5 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from importlib-metadata->transformers->stanza) (3.8.0)\n",
      "Requirement already satisfied: transformers in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (4.19.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from transformers) (1.21.6)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from transformers) (2022.4.24)\n",
      "Requirement already satisfied: importlib-metadata in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from transformers) (4.11.4)\n",
      "Requirement already satisfied: filelock in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from transformers) (3.7.0)\n",
      "Requirement already satisfied: requests in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: zipp>=0.5 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.8.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from requests->transformers) (2022.5.18.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from requests->transformers) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: tensorflow-addons in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (0.17.0)\n",
      "Requirement already satisfied: packaging in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorflow-addons) (21.3)\n",
      "Requirement already satisfied: typeguard>=2.7 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorflow-addons) (2.13.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from packaging->tensorflow-addons) (3.0.9)\n",
      "Requirement already satisfied: nltk in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (3.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from nltk) (2022.4.24)\n",
      "Requirement already satisfied: click in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: tqdm in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from nltk) (4.64.0)\n",
      "Requirement already satisfied: importlib-metadata in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from click->nltk) (4.11.4)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from importlib-metadata->click->nltk) (3.10.0.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from importlib-metadata->click->nltk) (3.8.0)\n",
      "Requirement already satisfied: textacy in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (0.11.0)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from textacy) (1.7.3)\n",
      "Requirement already satisfied: scikit-learn>=0.19.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from textacy) (1.0.2)\n",
      "Requirement already satisfied: jellyfish>=0.8.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from textacy) (0.9.0)\n",
      "Requirement already satisfied: joblib>=0.13.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from textacy) (1.1.0)\n",
      "Requirement already satisfied: requests>=2.10.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from textacy) (2.27.1)\n",
      "Requirement already satisfied: spacy>=3.0.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from textacy) (3.3.0)\n",
      "Requirement already satisfied: networkx>=2.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from textacy) (2.6.3)\n",
      "Requirement already satisfied: cachetools>=4.0.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from textacy) (5.2.0)\n",
      "Requirement already satisfied: tqdm>=4.19.6 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from textacy) (4.64.0)\n",
      "Requirement already satisfied: pyphen>=0.10.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from textacy) (0.12.0)\n",
      "Requirement already satisfied: cytoolz>=0.10.1 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from textacy) (0.11.2)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from textacy) (1.21.6)\n",
      "Requirement already satisfied: toolz>=0.8.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from cytoolz>=0.10.1->textacy) (0.11.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from requests>=2.10.0->textacy) (2022.5.18.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from requests>=2.10.0->textacy) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from requests>=2.10.0->textacy) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from requests>=2.10.0->textacy) (3.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from scikit-learn>=0.19.0->textacy) (3.1.0)\n",
      "Requirement already satisfied: jinja2 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (3.1.2)\n",
      "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (3.10.0.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (2.0.6)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (8.0.16)\n",
      "Requirement already satisfied: setuptools in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (61.2.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (3.0.9)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (2.4.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (3.0.6)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (0.6.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (1.8.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (0.7.7)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (0.4.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (0.9.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (21.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (3.3.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (1.0.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (1.0.7)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (2.0.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from catalogue<2.1.0,>=2.0.6->spacy>=3.0.0->textacy) (3.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from packaging>=20.0->spacy>=3.0.0->textacy) (3.0.9)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from pathy>=0.3.5->spacy>=3.0.0->textacy) (5.2.1)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from typer<0.5.0,>=0.3.0->spacy>=3.0.0->textacy) (8.1.3)\n",
      "Requirement already satisfied: importlib-metadata in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from click<9.0.0,>=7.1.1->typer<0.5.0,>=0.3.0->spacy>=3.0.0->textacy) (4.11.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from jinja2->spacy>=3.0.0->textacy) (2.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install sentencepiece\n",
    "!pip install tensorflow==2.7.0\n",
    "!pip install stanza\n",
    "!pip install transformers\n",
    "!pip install tensorflow-addons\n",
    "!pip install nltk\n",
    "!pip install textacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K0rs0NoritMk",
    "outputId": "92b77bac-3521-4e3b-cf37-6f33a0d5c9f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "wYwcFK5gixXz"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from textacy.datasets.supreme_court import SupremeCourt\n",
    "import numpy as np\n",
    "import re\n",
    "import unicodedata\n",
    "import nltk\n",
    "#from transformers import pipeline\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense,Dropout, Input, BatchNormalization\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from sklearn.metrics import confusion_matrix,f1_score,classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras import regularizers\n",
    "#from transformers import *\n",
    "from transformers import BertTokenizer, TFBertModel, BertConfig,TFDistilBertModel,DistilBertTokenizer,DistilBertConfig\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "import numpy as np\n",
    "import gc\n",
    "import math\n",
    "import json\n",
    "import stanza\n",
    "from tensorflow.keras import *\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import *\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import TFRobertaModel,RobertaTokenizer,LongformerTokenizer,TFLongformerModel\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.initializers import RandomUniform\n",
    "\n",
    "from numpy.random import seed\n",
    "import random as python_random\n",
    "import os\n",
    "import sys\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "np.random.seed(1)\n",
    "python_random.seed(1)\n",
    "tf.random.set_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bffI5vYTUeFW"
   },
   "outputs": [],
   "source": [
    "!cp \"/content/drive/My Drive/labels_sc.txt\" \"./labels_sc.txt\"\n",
    "!cp \"/content/drive/My Drive/labels_sc_279.txt\" \"./labels_sc_279.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "Number of devices: 2\n"
     ]
    }
   ],
   "source": [
    "# Create a MirroredStrategy.\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_model():\n",
    "    inps = Input(shape = (max_len,), dtype='int64')\n",
    "    masks= Input(shape = (max_len,), dtype='int64')\n",
    "    dbert_layer = dbert_model(inps, attention_mask=masks)[0][:,0,:]\n",
    "    dense_0 = Dense(512,activation='relu',kernel_regularizer=regularizers.l2(0.01))(dbert_layer)\n",
    "    dropout_0= Dropout(0.5)(dense_0)\n",
    "    pred = Dense(279, activation='softmax',kernel_regularizer=regularizers.l2(0.01))(dropout_0)\n",
    "    model = tf.keras.Model(inputs=[inps,masks], outputs=pred)\n",
    "    print(model.summary())\n",
    "    return model   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'supreme_court', 'site_url': 'http://caselaw.findlaw.com/court/us-supreme-court', 'description': 'Collection of ~8.4k decisions issued by the U.S. Supreme Court between November 1946 and June 2016.'}\n",
      "[-1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "{'-1': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7, '8': 8, '9': 9, '10': 10, '11': 11, '12': 12, '13': 13, '14': 14}\n",
      "8419\n",
      "Average Length 2920.3854377004395\n",
      "Found 8419 texts.\n",
      "Found 15 labels.\n",
      "                                                   text  label\n",
      "0     [ Halliburton Oil Well Cementing Co. v. Walker...    209\n",
      "1     Rehearing Denied Dec. 16, 1946. See . Mr.Claud...     63\n",
      "2     Rehearing Denied Dec. 16, 1946 See . Appeal fr...    216\n",
      "3     Mr. Walter J. Cummings, Jr., of Washington, D....    108\n",
      "4     Mr.A. Devitt Vaneck, of Washington, D.C., for ...    196\n",
      "...                                                 ...    ...\n",
      "8414  Opinion reported: Ante, p. 88. DECREE 1. It is...     74\n",
      "8415  In this dispute between Utah and the United St...      1\n",
      "8416  The United States, to the exclusion of defenda...      4\n",
      "8417  Louisiana's exception to the portion of the Sp...     74\n",
      "8418  To resolve a dispute over the ownership of cer...    263\n",
      "\n",
      "[8419 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-16 00:48:41.075072: I tensorflow/stream_executor/cuda/cuda_blas.cc:1774] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "Some layers from the model checkpoint at saibo/legal-longformer-base-4096 were not used when initializing TFLongformerModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFLongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFLongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFLongformerModel were initialized from the model checkpoint at saibo/legal-longformer-base-4096.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFLongformerModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 4096)]       0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 4096)]       0           []                               \n",
      "                                                                                                  \n",
      " tf_longformer_model (TFLongfor  TFLongformerBaseMod  148659456  ['input_1[0][0]',                \n",
      " merModel)                      elOutputWithPooling               'input_2[0][0]']                \n",
      "                                (last_hidden_state=                                               \n",
      "                                (None, 4096, 768),                                                \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 hidden_states=None                                               \n",
      "                                , attentions=None,                                                \n",
      "                                global_attentions=N                                               \n",
      "                                one)                                                              \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_longformer_model[0][0]']    \n",
      " ingOpLambda)                                                                                     \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dropout_49 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 279)          143127      ['dropout_49[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 149,196,311\n",
      "Trainable params: 149,196,311\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7577, 4096)\n",
      "(7577, 4096)\n",
      "(7577,)\n",
      "(842, 4096)\n",
      "(842, 4096)\n",
      "(842,)\n",
      "(7576, 4096)\n",
      "(7576, 4096)\n",
      "(7576,)\n",
      "(840, 4096)\n",
      "(840, 4096)\n",
      "(840,)\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "    total_accuracy=0\n",
    "    total_weighted_f1=0\n",
    "    total_micro_f1=0\n",
    "    total_weighted_precision=0\n",
    "    total_micro_precision=0\n",
    "    total_weighted_recall=0\n",
    "    total_micro_recall=0\n",
    "    sc = SupremeCourt()\n",
    "    print(sc.info)\n",
    "    sc.download()\n",
    "\n",
    "    texts = []  # list of text samples\n",
    "    labels_index = {}  # dictionary mapping label name to numeric id\n",
    "    labels = []  # list of label ids\n",
    "\n",
    "    issue_codes = list(sc.issue_area_codes.keys()) # 15 labels\n",
    "    print(issue_codes)\n",
    "    issue_codes.sort()\n",
    "    issue_codes = [str(ic) for ic in issue_codes]\n",
    "\n",
    "    labels_index = dict(zip(issue_codes, np.arange(len(issue_codes))))\n",
    "    print(labels_index)\n",
    "    count=0\n",
    "\n",
    "    for record in sc.records():\n",
    "\n",
    "            count=count+1\n",
    "            if record[1]['issue'] == None: # some cases have None as an issue\n",
    "                labels.append(labels_index['-1'])\n",
    "            else:\n",
    "                labels.append(labels_index[record[1]['issue'][:-4]])\n",
    "\n",
    "            new_sen=record[0].split(\"Footnotes\")[0]\n",
    "            new_new_sen=new_sen.split()\n",
    "            if len(new_new_sen) >= 4096:\n",
    "              new_new_sen=new_new_sen[0:4096]\n",
    "\n",
    "            elif len(new_new_sen) < 4096:\n",
    "              new_new_sen=new_new_sen[0:len(new_new_sen)]\n",
    "\n",
    "            new_new_sen=' '.join(new_new_sen)\n",
    "\n",
    "            texts.append(new_new_sen)\n",
    "\n",
    "    len_list = [len(ele.split()) for ele in texts]\n",
    "\n",
    "    #print(labels)\n",
    "    print(len(labels))\n",
    "\n",
    "    res = 0 if len(len_list) == 0 else (float(sum(len_list)) / len(len_list))\n",
    "    print(\"Average Length %s\" % res) \n",
    "    print('Found %s texts.' % len(texts))\n",
    "    print('Found %s labels.' % len(labels_index))\n",
    "    temp_file = open(\"labels_sc_279.txt\", \"r\")\n",
    "\n",
    "    data = temp_file.read()\n",
    "    label_list = data.split(\"\\n\")\n",
    "    #print(label_list)\n",
    "    label_list = label_list[0:-1]\n",
    "    #print(label_list)\n",
    "    label_list = [int(i) for i in label_list]\n",
    "    summarized_data = pd.DataFrame(texts,\n",
    "                   columns =['text'])\n",
    "    summarized_data['label'] = label_list\n",
    "    print(summarized_data)\n",
    "    #gc.collect()\n",
    "    #tf.keras.backend.clear_session()\n",
    "    dbert_tokenizer = LongformerTokenizer.from_pretrained('saibo/legal-longformer-base-4096')\n",
    "    dbert_model = TFLongformerModel.from_pretrained('saibo/legal-longformer-base-4096')\n",
    "    max_len=4096\n",
    "    sentences=summarized_data['text']\n",
    "    labels=summarized_data['label']\n",
    "    len(sentences),len(labels)\n",
    "    model_0=create_model()\n",
    "    input_ids=[]\n",
    "    attention_masks=[]\n",
    "\n",
    "    for sent in sentences:\n",
    "      dbert_inps=dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n",
    "      input_ids.append(dbert_inps['input_ids'])\n",
    "      attention_masks.append(dbert_inps['attention_mask'])\n",
    "    input_ids=np.asarray(input_ids)\n",
    "\n",
    "    attention_masks=np.array(attention_masks)\n",
    "    labels=np.array(labels)\n",
    "    train_inp,val_inp,train_label,val_label,train_mask,val_mask=train_test_split(input_ids,labels,attention_masks,test_size=0.1,random_state=42)\n",
    "    print(train_inp.shape)\n",
    "    print(train_mask.shape)\n",
    "    print(train_label.shape)\n",
    "    print(val_inp.shape)\n",
    "    print(val_mask.shape)\n",
    "    print(val_label.shape)\n",
    "    train_inp=train_inp[0:7576]\n",
    "    train_mask=train_mask[0:7576]\n",
    "    train_label=train_label[0:7576]\n",
    "\n",
    "    val_inp=val_inp[0:840]\n",
    "    val_label=val_label[0:840]\n",
    "    val_mask=val_mask[0:840]\n",
    "    print(train_inp.shape)\n",
    "    print(train_mask.shape)\n",
    "    print(train_label.shape)\n",
    "    print(val_inp.shape)\n",
    "    print(val_mask.shape)\n",
    "    print(val_label.shape)\n",
    "    \n",
    "    log_dir='dbert_model'\n",
    "\n",
    "    model_save_path='LSM/legallongformer-0-279labels.h5'\n",
    "\n",
    "\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    accuracy = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "    callbacks= [tf.keras.callbacks.ModelCheckpoint(filepath=model_save_path,monitor='val_accuracy',mode='max',save_best_only=True,save_weights_only=True),keras.callbacks.TensorBoard(log_dir=log_dir)]\n",
    "    model_0.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])\n",
    "          \n",
    "\n",
    "     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Sep 16 00:55:01 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM...  On   | 00000000:31:00.0 Off |                    0 |\n",
      "| N/A   27C    P0    71W / 500W |  79810MiB / 81920MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100-SXM...  On   | 00000000:CA:00.0 Off |                    0 |\n",
      "| N/A   25C    P0    70W / 500W |  79536MiB / 81920MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A    184456      C   python                          79805MiB |\n",
      "|    1   N/A  N/A    184456      C   python                          79531MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "    print('Not connected to a GPU')\n",
    "else:\n",
    "    print(gpu_info)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-15 02:03:34.542831: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:766] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_9\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"PrefetchDataset/_8\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_slice_batch_indices_100388\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\021FlatMapDataset:36\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 8\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT64\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_longformer_model/longformer/pooler/dense/kernel:0', 'tf_longformer_model/longformer/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_longformer_model/longformer/pooler/dense/kernel:0', 'tf_longformer_model/longformer/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:tensorflow:batch_all_reduce: 270 all-reduces with algorithm = nccl, num_packs = 1\n",
      "WARNING:tensorflow:Efficient allreduce is not supported for 3 IndexedSlices\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_longformer_model/longformer/pooler/dense/kernel:0', 'tf_longformer_model/longformer/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_longformer_model/longformer/pooler/dense/kernel:0', 'tf_longformer_model/longformer/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:tensorflow:batch_all_reduce: 270 all-reduces with algorithm = nccl, num_packs = 1\n",
      "WARNING:tensorflow:Efficient allreduce is not supported for 3 IndexedSlices\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
      "947/947 [==============================] - ETA: 0s - loss: 12.4965 - accuracy: 0.2051"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-15 02:19:59.806305: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:766] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_9\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"PrefetchDataset/_8\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_slice_batch_indices_239532\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\021FlatMapDataset:61\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 8\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT64\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "947/947 [==============================] - 1049s 953ms/step - loss: 12.4965 - accuracy: 0.2051 - val_loss: 10.3824 - val_accuracy: 0.3774\n",
      "Epoch 2/5\n",
      "947/947 [==============================] - 873s 922ms/step - loss: 9.6101 - accuracy: 0.4219 - val_loss: 8.7146 - val_accuracy: 0.4857\n",
      "Epoch 3/5\n",
      "947/947 [==============================] - 873s 922ms/step - loss: 8.1805 - accuracy: 0.5149 - val_loss: 7.7446 - val_accuracy: 0.5238\n",
      "Epoch 4/5\n",
      "947/947 [==============================] - 874s 923ms/step - loss: 7.0716 - accuracy: 0.5948 - val_loss: 7.0143 - val_accuracy: 0.5464\n",
      "Epoch 5/5\n",
      "947/947 [==============================] - 873s 922ms/step - loss: 6.1665 - accuracy: 0.6639 - val_loss: 6.5217 - val_accuracy: 0.5524\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "tf.keras.backend.clear_session()\n",
    "history=model_0.fit([train_inp,train_mask],train_label,batch_size=8,epochs=5,validation_data=([val_inp,val_mask],val_label),callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)           [(None, 4096)]       0           []                               \n",
      "                                                                                                  \n",
      " input_6 (InputLayer)           [(None, 4096)]       0           []                               \n",
      "                                                                                                  \n",
      " tf_longformer_model (TFLongfor  TFLongformerBaseMod  148659456  ['input_5[0][0]',                \n",
      " merModel)                      elOutputWithPooling               'input_6[0][0]']                \n",
      "                                (last_hidden_state=                                               \n",
      "                                (None, 4096, 768),                                                \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 hidden_states=None                                               \n",
      "                                , attentions=None,                                                \n",
      "                                global_attentions=N                                               \n",
      "                                one)                                                              \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_2 (Sl  (None, 768)         0           ['tf_longformer_model[2][0]']    \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_2[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dropout_51 (Dropout)           (None, 512)          0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 279)          143127      ['dropout_51[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 149,196,311\n",
      "Trainable params: 149,196,311\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "(840, 4096)\n",
      "(840, 4096)\n",
      "(840,)\n",
      "105/105 [==============================] - 82s 650ms/step - loss: 6.5217 - accuracy: 0.5524\n",
      "0.5523809790611267\n",
      "Accuracy: 0.5523809523809524\n",
      "Weighted F1: 0.5075775342127925\n",
      "Micro F1: 0.5523809523809524\n",
      "Weighted Precision: 0.5019251818030682\n",
      "Micro Precision: 0.5523809523809524\n",
      "Weighted Recall: 0.5523809523809524\n",
      "Micro Recall: 0.5523809523809524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "pred_labels=[]\n",
    "\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "accuracy = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "\n",
    "model_saved= create_model()\n",
    "model_saved.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])\n",
    "model_saved.load_weights('LSM/legallongformer-0-279labels.h5')\n",
    "\n",
    "\n",
    "print(val_inp.shape)\n",
    "print(val_mask.shape)\n",
    "print(val_label.shape)\n",
    "eval_loss, eval_acc=model_saved.evaluate([val_inp,val_mask],val_label,batch_size=8)\n",
    "print(eval_acc)\n",
    "\n",
    "\n",
    "for i in range(0,len(val_inp)):\n",
    "    pred=model_saved.predict([val_inp[i].reshape(1,4096),val_mask[i].reshape(1,4096)],batch_size=1)\n",
    "    pred_label = pred.argmax(axis=1)\n",
    "    pred_labels.append(pred_label)\n",
    "accuracy=accuracy_score(val_label, pred_labels)\n",
    "print(\"Accuracy: \"+str(accuracy))\n",
    "total_accuracy=total_accuracy+accuracy\n",
    "\n",
    "weighted_f1=f1_score(val_label,pred_labels, average='weighted')\n",
    "print(\"Weighted F1: \"+ str(weighted_f1))\n",
    "total_weighted_f1=total_weighted_f1+weighted_f1\n",
    "micro_f1=f1_score(val_label,pred_labels, average='micro')\n",
    "print(\"Micro F1: \"+ str(micro_f1))\n",
    "total_micro_f1=total_micro_f1+micro_f1\n",
    "\n",
    "weighted_precision=precision_score(val_label, pred_labels, average='weighted')\n",
    "print(\"Weighted Precision: \" + str(weighted_precision))\n",
    "total_weighted_precision=total_weighted_precision+weighted_precision\n",
    "micro_precision=precision_score(val_label, pred_labels, average='micro')\n",
    "print(\"Micro Precision: \" + str(micro_precision))\n",
    "total_micro_precision=total_micro_precision+micro_precision\n",
    "\n",
    "weighted_recall=recall_score(val_label, pred_labels, average='weighted')\n",
    "print(\"Weighted Recall: \" + str(weighted_recall))\n",
    "total_weighted_recall=total_weighted_recall+weighted_recall\n",
    "micro_recall=recall_score(val_label, pred_labels, average='micro')\n",
    "print(\"Micro Recall: \" + str(micro_recall))\n",
    "total_micro_recall=total_micro_recall+micro_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    sc = SupremeCourt()\n",
    "    print(sc.info)\n",
    "    sc.download()\n",
    "\n",
    "    texts = []  # list of text samples\n",
    "    labels_index = {}  # dictionary mapping label name to numeric id\n",
    "    labels = []  # list of label ids\n",
    "\n",
    "    issue_codes = list(sc.issue_area_codes.keys()) # 15 labels\n",
    "    print(issue_codes)\n",
    "    issue_codes.sort()\n",
    "    issue_codes = [str(ic) for ic in issue_codes]\n",
    "\n",
    "    labels_index = dict(zip(issue_codes, np.arange(len(issue_codes))))\n",
    "    print(labels_index)\n",
    "    count=0\n",
    "\n",
    "    for record in sc.records():\n",
    "\n",
    "            count=count+1\n",
    "            if record[1]['issue'] == None: # some cases have None as an issue\n",
    "                labels.append(labels_index['-1'])\n",
    "            else:\n",
    "                labels.append(labels_index[record[1]['issue'][:-4]])\n",
    "\n",
    "            new_sen=record[0].split(\"Footnotes\")[0]\n",
    "            new_new_sen=new_sen.split()\n",
    "            if len(new_new_sen) >= 4096:\n",
    "              new_new_sen=new_new_sen[0:4096]\n",
    "\n",
    "            elif len(new_new_sen) < 4096:\n",
    "              new_new_sen=new_new_sen[0:len(new_new_sen)]\n",
    "\n",
    "            new_new_sen=' '.join(new_new_sen)\n",
    "\n",
    "            texts.append(new_new_sen)\n",
    "\n",
    "    len_list = [len(ele.split()) for ele in texts]\n",
    "\n",
    "    #print(labels)\n",
    "    print(len(labels))\n",
    "\n",
    "    res = 0 if len(len_list) == 0 else (float(sum(len_list)) / len(len_list))\n",
    "    print(\"Average Length %s\" % res) \n",
    "    print('Found %s texts.' % len(texts))\n",
    "    print('Found %s labels.' % len(labels_index))\n",
    "    temp_file = open(\"labels_sc_279.txt\", \"r\")\n",
    "\n",
    "    data = temp_file.read()\n",
    "    label_list = data.split(\"\\n\")\n",
    "    #print(label_list)\n",
    "    label_list = label_list[0:-1]\n",
    "    #print(label_list)\n",
    "    label_list = [int(i) for i in label_list]\n",
    "    summarized_data = pd.DataFrame(texts,\n",
    "                   columns =['text'])\n",
    "    summarized_data['label'] = label_list\n",
    "    print(summarized_data)\n",
    "    #gc.collect()\n",
    "    #tf.keras.backend.clear_session()\n",
    "    dbert_tokenizer = LongformerTokenizer.from_pretrained('saibo/legal-longformer-base-4096')\n",
    "    dbert_model = TFLongformerModel.from_pretrained('saibo/legal-longformer-base-4096')\n",
    "    max_len=4096\n",
    "    sentences=summarized_data['text']\n",
    "    labels=summarized_data['label']\n",
    "    len(sentences),len(labels)\n",
    "    model_0=create_model()\n",
    "    input_ids=[]\n",
    "    attention_masks=[]\n",
    "\n",
    "    for sent in sentences:\n",
    "      dbert_inps=dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n",
    "      input_ids.append(dbert_inps['input_ids'])\n",
    "      attention_masks.append(dbert_inps['attention_mask'])\n",
    "    input_ids=np.asarray(input_ids)\n",
    "\n",
    "    attention_masks=np.array(attention_masks)\n",
    "    labels=np.array(labels)\n",
    "    train_inp,val_inp,train_label,val_label,train_mask,val_mask=train_test_split(input_ids,labels,attention_masks,test_size=0.1,random_state=42)\n",
    "    print(train_inp.shape)\n",
    "    print(train_mask.shape)\n",
    "    print(train_label.shape)\n",
    "    print(val_inp.shape)\n",
    "    print(val_mask.shape)\n",
    "    print(val_label.shape)\n",
    "    train_inp=train_inp[0:7576]\n",
    "    train_mask=train_mask[0:7576]\n",
    "    train_label=train_label[0:7576]\n",
    "\n",
    "    val_inp=val_inp[0:840]\n",
    "    val_label=val_label[0:840]\n",
    "    val_mask=val_mask[0:840]\n",
    "    print(train_inp.shape)\n",
    "    print(train_mask.shape)\n",
    "    print(train_label.shape)\n",
    "    print(val_inp.shape)\n",
    "    print(val_mask.shape)\n",
    "    print(val_label.shape)\n",
    "    log_dir='dbert_model'\n",
    "\n",
    "    model_save_path='LSM/legallongformer-1-279labels.h5'\n",
    "\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    accuracy = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "    callbacks= [tf.keras.callbacks.ModelCheckpoint(filepath=model_save_path,monitor='val_accuracy',mode='max',save_best_only=True,save_weights_only=True),keras.callbacks.TensorBoard(log_dir=log_dir)]\n",
    "    model_0.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-15 03:25:23.968603: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:766] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_9\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"PrefetchDataset/_8\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_slice_batch_indices_449347\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\023FlatMapDataset:9371\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 8\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT64\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_longformer_model/longformer/pooler/dense/kernel:0', 'tf_longformer_model/longformer/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_longformer_model/longformer/pooler/dense/kernel:0', 'tf_longformer_model/longformer/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_longformer_model/longformer/pooler/dense/kernel:0', 'tf_longformer_model/longformer/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_longformer_model/longformer/pooler/dense/kernel:0', 'tf_longformer_model/longformer/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "691/947 [====================>.........] - ETA: 3:45 - loss: 13.0110 - accuracy: 0.1702"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "tf.keras.backend.clear_session()\n",
    "history=model_0.fit([train_inp,train_mask],train_label,batch_size=8,epochs=5,validation_data=([val_inp,val_mask],val_label),callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_7 (InputLayer)           [(None, 4096)]       0           []                               \n",
      "                                                                                                  \n",
      " input_8 (InputLayer)           [(None, 4096)]       0           []                               \n",
      "                                                                                                  \n",
      " tf_longformer_model (TFLongfor  TFLongformerBaseMod  148659456  ['input_7[0][0]',                \n",
      " merModel)                      elOutputWithPooling               'input_8[0][0]']                \n",
      "                                (last_hidden_state=                                               \n",
      "                                (None, 4096, 768),                                                \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 hidden_states=None                                               \n",
      "                                , attentions=None,                                                \n",
      "                                global_attentions=N                                               \n",
      "                                one)                                                              \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_3 (Sl  (None, 768)         0           ['tf_longformer_model[3][0]']    \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_3[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dropout_52 (Dropout)           (None, 512)          0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 279)          143127      ['dropout_52[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 149,196,311\n",
      "Trainable params: 149,196,311\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Accuracy: 0.5654761904761905\n",
      "Weighted F1: 0.5257634957527921\n",
      "Micro F1: 0.5654761904761905\n",
      "Weighted Precision: 0.5281423044040064\n",
      "Micro Precision: 0.5654761904761905\n",
      "Weighted Recall: 0.5654761904761905\n",
      "Micro Recall: 0.5654761904761905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "pred_labels=[]\n",
    "\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "accuracy = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "\n",
    "model_saved= create_model()\n",
    "model_saved.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])\n",
    "model_saved.load_weights('LSM/legallongformer-1-279labels.h5')\n",
    "\n",
    "for i in range(0,len(val_inp)):\n",
    "    pred=model_saved.predict([val_inp[i].reshape(1,4096),val_mask[i].reshape(1,4096)],batch_size=8)\n",
    "    pred_label = pred.argmax(axis=1)\n",
    "    pred_labels.append(pred_label)\n",
    "accuracy=accuracy_score(val_label, pred_labels)\n",
    "print(\"Accuracy: \"+str(accuracy))\n",
    "total_accuracy=total_accuracy+accuracy\n",
    "\n",
    "weighted_f1=f1_score(val_label,pred_labels, average='weighted')\n",
    "print(\"Weighted F1: \"+ str(weighted_f1))\n",
    "total_weighted_f1=total_weighted_f1+weighted_f1\n",
    "micro_f1=f1_score(val_label,pred_labels, average='micro')\n",
    "print(\"Micro F1: \"+ str(micro_f1))\n",
    "total_micro_f1=total_micro_f1+micro_f1\n",
    "\n",
    "weighted_precision=precision_score(val_label, pred_labels, average='weighted')\n",
    "print(\"Weighted Precision: \" + str(weighted_precision))\n",
    "total_weighted_precision=total_weighted_precision+weighted_precision\n",
    "micro_precision=precision_score(val_label, pred_labels, average='micro')\n",
    "print(\"Micro Precision: \" + str(micro_precision))\n",
    "total_micro_precision=total_micro_precision+micro_precision\n",
    "\n",
    "weighted_recall=recall_score(val_label, pred_labels, average='weighted')\n",
    "print(\"Weighted Recall: \" + str(weighted_recall))\n",
    "total_weighted_recall=total_weighted_recall+weighted_recall\n",
    "micro_recall=recall_score(val_label, pred_labels, average='micro')\n",
    "print(\"Micro Recall: \" + str(micro_recall))\n",
    "total_micro_recall=total_micro_recall+micro_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    sc = SupremeCourt()\n",
    "    print(sc.info)\n",
    "    sc.download()\n",
    "\n",
    "    texts = []  # list of text samples\n",
    "    labels_index = {}  # dictionary mapping label name to numeric id\n",
    "    labels = []  # list of label ids\n",
    "\n",
    "    issue_codes = list(sc.issue_area_codes.keys()) # 15 labels\n",
    "    print(issue_codes)\n",
    "    issue_codes.sort()\n",
    "    issue_codes = [str(ic) for ic in issue_codes]\n",
    "\n",
    "    labels_index = dict(zip(issue_codes, np.arange(len(issue_codes))))\n",
    "    print(labels_index)\n",
    "    count=0\n",
    "\n",
    "    for record in sc.records():\n",
    "\n",
    "            count=count+1\n",
    "            if record[1]['issue'] == None: # some cases have None as an issue\n",
    "                labels.append(labels_index['-1'])\n",
    "            else:\n",
    "                labels.append(labels_index[record[1]['issue'][:-4]])\n",
    "\n",
    "            new_sen=record[0].split(\"Footnotes\")[0]\n",
    "            new_new_sen=new_sen.split()\n",
    "            if len(new_new_sen) >= 4096:\n",
    "              new_new_sen=new_new_sen[0:4096]\n",
    "\n",
    "            elif len(new_new_sen) < 4096:\n",
    "              new_new_sen=new_new_sen[0:len(new_new_sen)]\n",
    "\n",
    "            new_new_sen=' '.join(new_new_sen)\n",
    "\n",
    "            texts.append(new_new_sen)\n",
    "\n",
    "    len_list = [len(ele.split()) for ele in texts]\n",
    "\n",
    "    #print(labels)\n",
    "    print(len(labels))\n",
    "\n",
    "    res = 0 if len(len_list) == 0 else (float(sum(len_list)) / len(len_list))\n",
    "    print(\"Average Length %s\" % res) \n",
    "    print('Found %s texts.' % len(texts))\n",
    "    print('Found %s labels.' % len(labels_index))\n",
    "    temp_file = open(\"labels_sc_279.txt\", \"r\")\n",
    "\n",
    "    data = temp_file.read()\n",
    "    label_list = data.split(\"\\n\")\n",
    "    #print(label_list)\n",
    "    label_list = label_list[0:-1]\n",
    "    #print(label_list)\n",
    "    label_list = [int(i) for i in label_list]\n",
    "    summarized_data = pd.DataFrame(texts,\n",
    "                   columns =['text'])\n",
    "    summarized_data['label'] = label_list\n",
    "    print(summarized_data)\n",
    "    #gc.collect()\n",
    "    #tf.keras.backend.clear_session()\n",
    "    dbert_tokenizer = LongformerTokenizer.from_pretrained('saibo/legal-longformer-base-4096')\n",
    "    dbert_model = TFLongformerModel.from_pretrained('saibo/legal-longformer-base-4096')\n",
    "    max_len=4096\n",
    "    sentences=summarized_data['text']\n",
    "    labels=summarized_data['label']\n",
    "    len(sentences),len(labels)\n",
    "    model_0=create_model()\n",
    "    input_ids=[]\n",
    "    attention_masks=[]\n",
    "\n",
    "    for sent in sentences:\n",
    "      dbert_inps=dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n",
    "      input_ids.append(dbert_inps['input_ids'])\n",
    "      attention_masks.append(dbert_inps['attention_mask'])\n",
    "    input_ids=np.asarray(input_ids)\n",
    "\n",
    "    attention_masks=np.array(attention_masks)\n",
    "    labels=np.array(labels)\n",
    "    train_inp,val_inp,train_label,val_label,train_mask,val_mask=train_test_split(input_ids,labels,attention_masks,test_size=0.1,random_state=42)\n",
    "    print(train_inp.shape)\n",
    "    print(train_mask.shape)\n",
    "    print(train_label.shape)\n",
    "    print(val_inp.shape)\n",
    "    print(val_mask.shape)\n",
    "    print(val_label.shape)\n",
    "    train_inp=train_inp[0:7576]\n",
    "    train_mask=train_mask[0:7576]\n",
    "    train_label=train_label[0:7576]\n",
    "\n",
    "    val_inp=val_inp[0:840]\n",
    "    val_label=val_label[0:840]\n",
    "    val_mask=val_mask[0:840]\n",
    "    print(train_inp.shape)\n",
    "    print(train_mask.shape)\n",
    "    print(train_label.shape)\n",
    "    print(val_inp.shape)\n",
    "    print(val_mask.shape)\n",
    "    print(val_label.shape)\n",
    "    log_dir='dbert_model'\n",
    "\n",
    "    model_save_path='LSM/legallongformer-2-279labels.h5'\n",
    "\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    accuracy = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "    callbacks= [tf.keras.callbacks.ModelCheckpoint(filepath=model_save_path,monitor='val_accuracy',mode='max',save_best_only=True,save_weights_only=True),keras.callbacks.TensorBoard(log_dir=log_dir)]\n",
    "    model_0.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "tf.keras.backend.clear_session()\n",
    "history=model_0.fit([train_inp,train_mask],train_label,batch_size=8,epochs=5,validation_data=([val_inp,val_mask],val_label),callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_9 (InputLayer)           [(None, 4096)]       0           []                               \n",
      "                                                                                                  \n",
      " input_10 (InputLayer)          [(None, 4096)]       0           []                               \n",
      "                                                                                                  \n",
      " tf_longformer_model (TFLongfor  TFLongformerBaseMod  148659456  ['input_9[0][0]',                \n",
      " merModel)                      elOutputWithPooling               'input_10[0][0]']               \n",
      "                                (last_hidden_state=                                               \n",
      "                                (None, 4096, 768),                                                \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 hidden_states=None                                               \n",
      "                                , attentions=None,                                                \n",
      "                                global_attentions=N                                               \n",
      "                                one)                                                              \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_4 (Sl  (None, 768)         0           ['tf_longformer_model[4][0]']    \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_4[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dropout_53 (Dropout)           (None, 512)          0           ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 279)          143127      ['dropout_53[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 149,196,311\n",
      "Trainable params: 149,196,311\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Accuracy: 0.5547619047619048\n",
      "Weighted F1: 0.5058243590068798\n",
      "Micro F1: 0.5547619047619048\n",
      "Weighted Precision: 0.4964899168440858\n",
      "Micro Precision: 0.5547619047619048\n",
      "Weighted Recall: 0.5547619047619048\n",
      "Micro Recall: 0.5547619047619048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "pred_labels=[]\n",
    "\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "accuracy = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "\n",
    "model_saved= create_model()\n",
    "model_saved.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])\n",
    "model_saved.load_weights('LSM/legallongformer-2-279labels.h5')\n",
    "\n",
    "for i in range(0,len(val_inp)):\n",
    "    pred=model_saved.predict([val_inp[i].reshape(1,4096),val_mask[i].reshape(1,4096)],batch_size=8)\n",
    "    pred_label = pred.argmax(axis=1)\n",
    "    pred_labels.append(pred_label)\n",
    "accuracy=accuracy_score(val_label, pred_labels)\n",
    "print(\"Accuracy: \"+str(accuracy))\n",
    "total_accuracy=total_accuracy+accuracy\n",
    "\n",
    "weighted_f1=f1_score(val_label,pred_labels, average='weighted')\n",
    "print(\"Weighted F1: \"+ str(weighted_f1))\n",
    "total_weighted_f1=total_weighted_f1+weighted_f1\n",
    "micro_f1=f1_score(val_label,pred_labels, average='micro')\n",
    "print(\"Micro F1: \"+ str(micro_f1))\n",
    "total_micro_f1=total_micro_f1+micro_f1\n",
    "\n",
    "weighted_precision=precision_score(val_label, pred_labels, average='weighted')\n",
    "print(\"Weighted Precision: \" + str(weighted_precision))\n",
    "total_weighted_precision=total_weighted_precision+weighted_precision\n",
    "micro_precision=precision_score(val_label, pred_labels, average='micro')\n",
    "print(\"Micro Precision: \" + str(micro_precision))\n",
    "total_micro_precision=total_micro_precision+micro_precision\n",
    "\n",
    "weighted_recall=recall_score(val_label, pred_labels, average='weighted')\n",
    "print(\"Weighted Recall: \" + str(weighted_recall))\n",
    "total_weighted_recall=total_weighted_recall+weighted_recall\n",
    "micro_recall=recall_score(val_label, pred_labels, average='micro')\n",
    "print(\"Micro Recall: \" + str(micro_recall))\n",
    "total_micro_recall=total_micro_recall+micro_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    sc = SupremeCourt()\n",
    "    print(sc.info)\n",
    "    sc.download()\n",
    "\n",
    "    texts = []  # list of text samples\n",
    "    labels_index = {}  # dictionary mapping label name to numeric id\n",
    "    labels = []  # list of label ids\n",
    "\n",
    "    issue_codes = list(sc.issue_area_codes.keys()) # 15 labels\n",
    "    print(issue_codes)\n",
    "    issue_codes.sort()\n",
    "    issue_codes = [str(ic) for ic in issue_codes]\n",
    "\n",
    "    labels_index = dict(zip(issue_codes, np.arange(len(issue_codes))))\n",
    "    print(labels_index)\n",
    "    count=0\n",
    "\n",
    "    for record in sc.records():\n",
    "\n",
    "            count=count+1\n",
    "            if record[1]['issue'] == None: # some cases have None as an issue\n",
    "                labels.append(labels_index['-1'])\n",
    "            else:\n",
    "                labels.append(labels_index[record[1]['issue'][:-4]])\n",
    "\n",
    "            new_sen=record[0].split(\"Footnotes\")[0]\n",
    "            new_new_sen=new_sen.split()\n",
    "            if len(new_new_sen) >= 4096:\n",
    "              new_new_sen=new_new_sen[0:4096]\n",
    "\n",
    "            elif len(new_new_sen) < 4096:\n",
    "              new_new_sen=new_new_sen[0:len(new_new_sen)]\n",
    "\n",
    "            new_new_sen=' '.join(new_new_sen)\n",
    "\n",
    "            texts.append(new_new_sen)\n",
    "\n",
    "    len_list = [len(ele.split()) for ele in texts]\n",
    "\n",
    "    #print(labels)\n",
    "    print(len(labels))\n",
    "\n",
    "    res = 0 if len(len_list) == 0 else (float(sum(len_list)) / len(len_list))\n",
    "    print(\"Average Length %s\" % res) \n",
    "    print('Found %s texts.' % len(texts))\n",
    "    print('Found %s labels.' % len(labels_index))\n",
    "    temp_file = open(\"labels_sc_279.txt\", \"r\")\n",
    "\n",
    "    data = temp_file.read()\n",
    "    label_list = data.split(\"\\n\")\n",
    "    #print(label_list)\n",
    "    label_list = label_list[0:-1]\n",
    "    #print(label_list)\n",
    "    label_list = [int(i) for i in label_list]\n",
    "    summarized_data = pd.DataFrame(texts,\n",
    "                   columns =['text'])\n",
    "    summarized_data['label'] = label_list\n",
    "    print(summarized_data)\n",
    "    #gc.collect()\n",
    "    #tf.keras.backend.clear_session()\n",
    "    dbert_tokenizer = LongformerTokenizer.from_pretrained('saibo/legal-longformer-base-4096')\n",
    "    dbert_model = TFLongformerModel.from_pretrained('saibo/legal-longformer-base-4096')\n",
    "    max_len=4096\n",
    "    sentences=summarized_data['text']\n",
    "    labels=summarized_data['label']\n",
    "    len(sentences),len(labels)\n",
    "    model_0=create_model()\n",
    "    input_ids=[]\n",
    "    attention_masks=[]\n",
    "\n",
    "    for sent in sentences:\n",
    "      dbert_inps=dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n",
    "      input_ids.append(dbert_inps['input_ids'])\n",
    "      attention_masks.append(dbert_inps['attention_mask'])\n",
    "    input_ids=np.asarray(input_ids)\n",
    "\n",
    "    attention_masks=np.array(attention_masks)\n",
    "    labels=np.array(labels)\n",
    "    train_inp,val_inp,train_label,val_label,train_mask,val_mask=train_test_split(input_ids,labels,attention_masks,test_size=0.1,random_state=42)\n",
    "    print(train_inp.shape)\n",
    "    print(train_mask.shape)\n",
    "    print(train_label.shape)\n",
    "    print(val_inp.shape)\n",
    "    print(val_mask.shape)\n",
    "    print(val_label.shape)\n",
    "    train_inp=train_inp[0:7576]\n",
    "    train_mask=train_mask[0:7576]\n",
    "    train_label=train_label[0:7576]\n",
    "\n",
    "    val_inp=val_inp[0:840]\n",
    "    val_label=val_label[0:840]\n",
    "    val_mask=val_mask[0:840]\n",
    "    print(train_inp.shape)\n",
    "    print(train_mask.shape)\n",
    "    print(train_label.shape)\n",
    "    print(val_inp.shape)\n",
    "    print(val_mask.shape)\n",
    "    print(val_label.shape)\n",
    "    log_dir='dbert_model'\n",
    "\n",
    "    model_save_path='LSM/legallongformer-3-279labels.h5'\n",
    "\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    accuracy = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "    callbacks= [tf.keras.callbacks.ModelCheckpoint(filepath=model_save_path,monitor='val_accuracy',mode='max',save_best_only=True,save_weights_only=True),keras.callbacks.TensorBoard(log_dir=log_dir)]\n",
    "    model_0.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-15 14:11:13.239681: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:766] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_9\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"PrefetchDataset/_8\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_slice_batch_indices_735406\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\024FlatMapDataset:31162\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 8\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT64\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_longformer_model_3/longformer/pooler/dense/kernel:0', 'tf_longformer_model_3/longformer/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_longformer_model_3/longformer/pooler/dense/kernel:0', 'tf_longformer_model_3/longformer/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_longformer_model_3/longformer/pooler/dense/kernel:0', 'tf_longformer_model_3/longformer/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_longformer_model_3/longformer/pooler/dense/kernel:0', 'tf_longformer_model_3/longformer/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "947/947 [==============================] - ETA: 0s - loss: 12.6649 - accuracy: 0.1587"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-15 14:27:45.046694: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:766] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_9\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"PrefetchDataset/_8\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_slice_batch_indices_874546\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\024FlatMapDataset:31187\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 8\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT64\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "947/947 [==============================] - 1052s 949ms/step - loss: 12.6649 - accuracy: 0.1587 - val_loss: 10.2974 - val_accuracy: 0.3643\n",
      "Epoch 2/5\n",
      "947/947 [==============================] - 874s 923ms/step - loss: 9.4930 - accuracy: 0.4011 - val_loss: 8.6128 - val_accuracy: 0.4857\n",
      "Epoch 3/5\n",
      "947/947 [==============================] - 874s 923ms/step - loss: 7.9735 - accuracy: 0.5174 - val_loss: 7.6501 - val_accuracy: 0.5083\n",
      "Epoch 4/5\n",
      "947/947 [==============================] - 874s 923ms/step - loss: 6.8458 - accuracy: 0.6113 - val_loss: 6.8636 - val_accuracy: 0.5417\n",
      "Epoch 5/5\n",
      "947/947 [==============================] - 874s 923ms/step - loss: 5.9304 - accuracy: 0.6843 - val_loss: 6.2744 - val_accuracy: 0.5762\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "tf.keras.backend.clear_session()\n",
    "history=model_0.fit([train_inp,train_mask],train_label,batch_size=8,epochs=5,validation_data=([val_inp,val_mask],val_label),callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_11 (InputLayer)          [(None, 4096)]       0           []                               \n",
      "                                                                                                  \n",
      " input_12 (InputLayer)          [(None, 4096)]       0           []                               \n",
      "                                                                                                  \n",
      " tf_longformer_model (TFLongfor  TFLongformerBaseMod  148659456  ['input_11[0][0]',               \n",
      " merModel)                      elOutputWithPooling               'input_12[0][0]']               \n",
      "                                (last_hidden_state=                                               \n",
      "                                (None, 4096, 768),                                                \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 hidden_states=None                                               \n",
      "                                , attentions=None,                                                \n",
      "                                global_attentions=N                                               \n",
      "                                one)                                                              \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_5 (Sl  (None, 768)         0           ['tf_longformer_model[5][0]']    \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_5[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dropout_54 (Dropout)           (None, 512)          0           ['dense_10[0][0]']               \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 279)          143127      ['dropout_54[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 149,196,311\n",
      "Trainable params: 149,196,311\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Accuracy: 0.5761904761904761\n",
      "Weighted F1: 0.537353443299116\n",
      "Micro F1: 0.5761904761904761\n",
      "Weighted Precision: 0.5303080579195522\n",
      "Micro Precision: 0.5761904761904761\n",
      "Weighted Recall: 0.5761904761904761\n",
      "Micro Recall: 0.5761904761904761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "pred_labels=[]\n",
    "\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "accuracy = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "\n",
    "model_saved= create_model()\n",
    "model_saved.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])\n",
    "model_saved.load_weights('LSM/legallongformer-3-279labels.h5')\n",
    "\n",
    "for i in range(0,len(val_inp)):\n",
    "    pred=model_saved.predict([val_inp[i].reshape(1,4096),val_mask[i].reshape(1,4096)],batch_size=8)\n",
    "    pred_label = pred.argmax(axis=1)\n",
    "    pred_labels.append(pred_label)\n",
    "accuracy=accuracy_score(val_label, pred_labels)\n",
    "print(\"Accuracy: \"+str(accuracy))\n",
    "total_accuracy=total_accuracy+accuracy\n",
    "\n",
    "weighted_f1=f1_score(val_label,pred_labels, average='weighted')\n",
    "print(\"Weighted F1: \"+ str(weighted_f1))\n",
    "total_weighted_f1=total_weighted_f1+weighted_f1\n",
    "micro_f1=f1_score(val_label,pred_labels, average='micro')\n",
    "print(\"Micro F1: \"+ str(micro_f1))\n",
    "total_micro_f1=total_micro_f1+micro_f1\n",
    "\n",
    "weighted_precision=precision_score(val_label, pred_labels, average='weighted')\n",
    "print(\"Weighted Precision: \" + str(weighted_precision))\n",
    "total_weighted_precision=total_weighted_precision+weighted_precision\n",
    "micro_precision=precision_score(val_label, pred_labels, average='micro')\n",
    "print(\"Micro Precision: \" + str(micro_precision))\n",
    "total_micro_precision=total_micro_precision+micro_precision\n",
    "\n",
    "weighted_recall=recall_score(val_label, pred_labels, average='weighted')\n",
    "print(\"Weighted Recall: \" + str(weighted_recall))\n",
    "total_weighted_recall=total_weighted_recall+weighted_recall\n",
    "micro_recall=recall_score(val_label, pred_labels, average='micro')\n",
    "print(\"Micro Recall: \" + str(micro_recall))\n",
    "total_micro_recall=total_micro_recall+micro_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    sc = SupremeCourt()\n",
    "    print(sc.info)\n",
    "    sc.download()\n",
    "\n",
    "    texts = []  # list of text samples\n",
    "    labels_index = {}  # dictionary mapping label name to numeric id\n",
    "    labels = []  # list of label ids\n",
    "\n",
    "    issue_codes = list(sc.issue_area_codes.keys()) # 15 labels\n",
    "    print(issue_codes)\n",
    "    issue_codes.sort()\n",
    "    issue_codes = [str(ic) for ic in issue_codes]\n",
    "\n",
    "    labels_index = dict(zip(issue_codes, np.arange(len(issue_codes))))\n",
    "    print(labels_index)\n",
    "    count=0\n",
    "\n",
    "    for record in sc.records():\n",
    "\n",
    "            count=count+1\n",
    "            if record[1]['issue'] == None: # some cases have None as an issue\n",
    "                labels.append(labels_index['-1'])\n",
    "            else:\n",
    "                labels.append(labels_index[record[1]['issue'][:-4]])\n",
    "\n",
    "            new_sen=record[0].split(\"Footnotes\")[0]\n",
    "            new_new_sen=new_sen.split()\n",
    "            if len(new_new_sen) >= 4096:\n",
    "              new_new_sen=new_new_sen[0:4096]\n",
    "\n",
    "            elif len(new_new_sen) < 4096:\n",
    "              new_new_sen=new_new_sen[0:len(new_new_sen)]\n",
    "\n",
    "            new_new_sen=' '.join(new_new_sen)\n",
    "\n",
    "            texts.append(new_new_sen)\n",
    "\n",
    "    len_list = [len(ele.split()) for ele in texts]\n",
    "\n",
    "    #print(labels)\n",
    "    print(len(labels))\n",
    "\n",
    "    res = 0 if len(len_list) == 0 else (float(sum(len_list)) / len(len_list))\n",
    "    print(\"Average Length %s\" % res) \n",
    "    print('Found %s texts.' % len(texts))\n",
    "    print('Found %s labels.' % len(labels_index))\n",
    "    temp_file = open(\"labels_sc_279.txt\", \"r\")\n",
    "\n",
    "    data = temp_file.read()\n",
    "    label_list = data.split(\"\\n\")\n",
    "    #print(label_list)\n",
    "    label_list = label_list[0:-1]\n",
    "    #print(label_list)\n",
    "    label_list = [int(i) for i in label_list]\n",
    "    summarized_data = pd.DataFrame(texts,\n",
    "                   columns =['text'])\n",
    "    summarized_data['label'] = label_list\n",
    "    print(summarized_data)\n",
    "    #gc.collect()\n",
    "    #tf.keras.backend.clear_session()\n",
    "    dbert_tokenizer = LongformerTokenizer.from_pretrained('saibo/legal-longformer-base-4096')\n",
    "    dbert_model = TFLongformerModel.from_pretrained('saibo/legal-longformer-base-4096')\n",
    "    max_len=4096\n",
    "    sentences=summarized_data['text']\n",
    "    labels=summarized_data['label']\n",
    "    len(sentences),len(labels)\n",
    "    model_0=create_model()\n",
    "    input_ids=[]\n",
    "    attention_masks=[]\n",
    "\n",
    "    for sent in sentences:\n",
    "      dbert_inps=dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n",
    "      input_ids.append(dbert_inps['input_ids'])\n",
    "      attention_masks.append(dbert_inps['attention_mask'])\n",
    "    input_ids=np.asarray(input_ids)\n",
    "\n",
    "    attention_masks=np.array(attention_masks)\n",
    "    labels=np.array(labels)\n",
    "    train_inp,val_inp,train_label,val_label,train_mask,val_mask=train_test_split(input_ids,labels,attention_masks,test_size=0.1,random_state=42)\n",
    "    print(train_inp.shape)\n",
    "    print(train_mask.shape)\n",
    "    print(train_label.shape)\n",
    "    print(val_inp.shape)\n",
    "    print(val_mask.shape)\n",
    "    print(val_label.shape)\n",
    "    train_inp=train_inp[0:7576]\n",
    "    train_mask=train_mask[0:7576]\n",
    "    train_label=train_label[0:7576]\n",
    "\n",
    "    val_inp=val_inp[0:840]\n",
    "    val_label=val_label[0:840]\n",
    "    val_mask=val_mask[0:840]\n",
    "    print(train_inp.shape)\n",
    "    print(train_mask.shape)\n",
    "    print(train_label.shape)\n",
    "    print(val_inp.shape)\n",
    "    print(val_mask.shape)\n",
    "    print(val_label.shape)\n",
    "    log_dir='dbert_model'\n",
    "\n",
    "    model_save_path='LSM/legallongformer-4-279labels.h5'\n",
    "\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    accuracy = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "    callbacks= [tf.keras.callbacks.ModelCheckpoint(filepath=model_save_path,monitor='val_accuracy',mode='max',save_best_only=True,save_weights_only=True),keras.callbacks.TensorBoard(log_dir=log_dir)]\n",
    "    model_0.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-15 15:32:30.468403: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:766] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_9\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"PrefetchDataset/_8\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_slice_batch_indices_1079788\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\024FlatMapDataset:42166\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 8\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT64\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_longformer_model/longformer/pooler/dense/kernel:0', 'tf_longformer_model/longformer/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_longformer_model/longformer/pooler/dense/kernel:0', 'tf_longformer_model/longformer/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_longformer_model/longformer/pooler/dense/kernel:0', 'tf_longformer_model/longformer/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_longformer_model/longformer/pooler/dense/kernel:0', 'tf_longformer_model/longformer/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "947/947 [==============================] - ETA: 0s - loss: 12.6415 - accuracy: 0.1521"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-15 15:49:01.542147: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:766] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_9\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"PrefetchDataset/_8\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_slice_batch_indices_1218928\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\024FlatMapDataset:42191\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 8\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT64\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "947/947 [==============================] - 1053s 950ms/step - loss: 12.6415 - accuracy: 0.1521 - val_loss: 10.3385 - val_accuracy: 0.3417\n",
      "Epoch 2/5\n",
      "808/947 [========================>.....] - ETA: 2:02 - loss: 9.5539 - accuracy: 0.3874"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "tf.keras.backend.clear_session()\n",
    "history=model_0.fit([train_inp,train_mask],train_label,batch_size=8,epochs=5,validation_data=([val_inp,val_mask],val_label),callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_13 (InputLayer)          [(None, 4096)]       0           []                               \n",
      "                                                                                                  \n",
      " input_14 (InputLayer)          [(None, 4096)]       0           []                               \n",
      "                                                                                                  \n",
      " tf_longformer_model (TFLongfor  TFLongformerBaseMod  148659456  ['input_13[0][0]',               \n",
      " merModel)                      elOutputWithPooling               'input_14[0][0]']               \n",
      "                                (last_hidden_state=                                               \n",
      "                                (None, 4096, 768),                                                \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 hidden_states=None                                               \n",
      "                                , attentions=None,                                                \n",
      "                                global_attentions=N                                               \n",
      "                                one)                                                              \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_6 (Sl  (None, 768)         0           ['tf_longformer_model[6][0]']    \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_6[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dropout_55 (Dropout)           (None, 512)          0           ['dense_12[0][0]']               \n",
      "                                                                                                  \n",
      " dense_13 (Dense)               (None, 279)          143127      ['dropout_55[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 149,196,311\n",
      "Trainable params: 149,196,311\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Accuracy: 0.5642857142857143\n",
      "Weighted F1: 0.5220586064157966\n",
      "Micro F1: 0.5642857142857143\n",
      "Weighted Precision: 0.5222534925791991\n",
      "Micro Precision: 0.5642857142857143\n",
      "Weighted Recall: 0.5642857142857143\n",
      "Micro Recall: 0.5642857142857143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "pred_labels=[]\n",
    "\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "accuracy = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "\n",
    "model_saved= create_model()\n",
    "model_saved.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])\n",
    "model_saved.load_weights('LSM/legallongformer-4-279labels.h5')\n",
    "\n",
    "for i in range(0,len(val_inp)):\n",
    "    pred=model_saved.predict([val_inp[i].reshape(1,4096),val_mask[i].reshape(1,4096)],batch_size=8)\n",
    "    pred_label = pred.argmax(axis=1)\n",
    "    pred_labels.append(pred_label)\n",
    "accuracy=accuracy_score(val_label, pred_labels)\n",
    "print(\"Accuracy: \"+str(accuracy))\n",
    "total_accuracy=total_accuracy+accuracy\n",
    "\n",
    "weighted_f1=f1_score(val_label,pred_labels, average='weighted')\n",
    "print(\"Weighted F1: \"+ str(weighted_f1))\n",
    "total_weighted_f1=total_weighted_f1+weighted_f1\n",
    "micro_f1=f1_score(val_label,pred_labels, average='micro')\n",
    "print(\"Micro F1: \"+ str(micro_f1))\n",
    "total_micro_f1=total_micro_f1+micro_f1\n",
    "\n",
    "weighted_precision=precision_score(val_label, pred_labels, average='weighted')\n",
    "print(\"Weighted Precision: \" + str(weighted_precision))\n",
    "total_weighted_precision=total_weighted_precision+weighted_precision\n",
    "micro_precision=precision_score(val_label, pred_labels, average='micro')\n",
    "print(\"Micro Precision: \" + str(micro_precision))\n",
    "total_micro_precision=total_micro_precision+micro_precision\n",
    "\n",
    "weighted_recall=recall_score(val_label, pred_labels, average='weighted')\n",
    "print(\"Weighted Recall: \" + str(weighted_recall))\n",
    "total_weighted_recall=total_weighted_recall+weighted_recall\n",
    "micro_recall=recall_score(val_label, pred_labels, average='micro')\n",
    "print(\"Micro Recall: \" + str(micro_recall))\n",
    "total_micro_recall=total_micro_recall+micro_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy: 0.5626190476190477\n",
      "Average Weighted F1: 0.5197154877374754\n",
      "Average Micro F1: 0.5626190476190477\n",
      "Average Weighted Precision: 0.5158237907099823\n",
      "Average Micro Precision: 0.5626190476190477\n",
      "Average Weighted Recall: 0.5626190476190477\n",
      "Average Micro Recall: 0.5626190476190477\n"
     ]
    }
   ],
   "source": [
    "print(\"Average Accuracy: \"+str(total_accuracy/5))\n",
    "print(\"Average Weighted F1: \"+str(total_weighted_f1/5))\n",
    "print(\"Average Micro F1: \"+str(total_micro_f1/5))\n",
    "print(\"Average Weighted Precision: \"+str(total_weighted_precision/5))\n",
    "print(\"Average Micro Precision: \"+str(total_micro_precision/5))\n",
    "print(\"Average Weighted Recall: \"+str(total_weighted_recall/5))\n",
    "print(\"Average Micro Recall: \"+str(total_micro_recall/5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "background_execution": "on",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Best-512_0:512_15labels.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "WOL_ENV",
   "language": "python",
   "name": "wol_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
