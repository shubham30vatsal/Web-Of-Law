{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mCIR7rd5h9fv",
    "outputId": "53830c68-0d4f-4ea9-9978-d1684a586900"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "#drive.flush_and_unmount()\n",
    "drive.mount('/content/drive')\n",
    "#drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0k8FHrSIiqDo",
    "outputId": "18763450-95ef-4c99-f202-053db938b0ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (4.19.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from transformers) (1.21.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from transformers) (2022.4.24)\n",
      "Requirement already satisfied: importlib-metadata in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from transformers) (4.11.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: filelock in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from transformers) (3.7.0)\n",
      "Requirement already satisfied: requests in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: zipp>=0.5 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.8.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from requests->transformers) (2022.5.18.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from requests->transformers) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: sentencepiece in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (0.1.96)\n",
      "Requirement already satisfied: tensorflow==2.7.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (2.7.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorflow==2.7.0) (1.6.3)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorflow==2.7.0) (1.0.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorflow==2.7.0) (0.2.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorflow==2.7.0) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorflow==2.7.0) (2.7.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorflow==2.7.0) (1.16.0)\n",
      "Requirement already satisfied: numpy>=1.14.5 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorflow==2.7.0) (1.21.6)\n",
      "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorflow==2.7.0) (0.4.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorflow==2.7.0) (3.7.0)\n",
      "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorflow==2.7.0) (2.7.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorflow==2.7.0) (3.3.0)\n",
      "Requirement already satisfied: libclang>=9.0.1 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorflow==2.7.0) (14.0.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorflow==2.7.0) (1.1.2)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorflow==2.7.0) (3.10.0.2)\n",
      "Requirement already satisfied: tensorboard~=2.6 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorflow==2.7.0) (2.9.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorflow==2.7.0) (1.1.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorflow==2.7.0) (1.46.3)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorflow==2.7.0) (0.26.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.32.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorflow==2.7.0) (0.37.1)\n",
      "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorflow==2.7.0) (2.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorflow==2.7.0) (3.19.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow==2.7.0) (61.2.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow==2.7.0) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow==2.7.0) (0.6.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow==2.7.0) (2.6.6)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow==2.7.0) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow==2.7.0) (2.1.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow==2.7.0) (3.3.7)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow==2.7.0) (2.27.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0) (5.2.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0) (4.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.7.0) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow==2.7.0) (4.11.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow==2.7.0) (3.8.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0) (0.4.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0) (2022.5.18.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0) (1.26.9)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.7.0) (3.2.0)\n",
      "Requirement already satisfied: stanza in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (1.4.0)\n",
      "Requirement already satisfied: emoji in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from stanza) (1.7.0)\n",
      "Requirement already satisfied: protobuf in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from stanza) (3.19.0)\n",
      "Requirement already satisfied: numpy in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from stanza) (1.21.6)\n",
      "Requirement already satisfied: tqdm in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from stanza) (4.64.0)\n",
      "Requirement already satisfied: requests in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from stanza) (2.27.1)\n",
      "Requirement already satisfied: torch>=1.3.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from stanza) (1.11.0)\n",
      "Requirement already satisfied: six in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from stanza) (1.16.0)\n",
      "Requirement already satisfied: transformers in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from stanza) (4.19.2)\n",
      "Requirement already satisfied: typing-extensions in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from torch>=1.3.0->stanza) (3.10.0.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from requests->stanza) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from requests->stanza) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from requests->stanza) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from requests->stanza) (2022.5.18.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from transformers->stanza) (0.12.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from transformers->stanza) (6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from transformers->stanza) (0.7.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from transformers->stanza) (21.3)\n",
      "Requirement already satisfied: importlib-metadata in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from transformers->stanza) (4.11.4)\n",
      "Requirement already satisfied: filelock in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from transformers->stanza) (3.7.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from transformers->stanza) (2022.4.24)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from packaging>=20.0->transformers->stanza) (3.0.9)\n",
      "Requirement already satisfied: zipp>=0.5 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from importlib-metadata->transformers->stanza) (3.8.0)\n",
      "Requirement already satisfied: transformers in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (4.19.2)\n",
      "Requirement already satisfied: filelock in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from transformers) (3.7.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from transformers) (2022.4.24)\n",
      "Requirement already satisfied: packaging>=20.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from transformers) (1.21.6)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: requests in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: importlib-metadata in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from transformers) (4.11.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: zipp>=0.5 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.8.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from requests->transformers) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from requests->transformers) (2022.5.18.1)\n",
      "Requirement already satisfied: tensorflow-addons in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (0.17.0)\n",
      "Requirement already satisfied: typeguard>=2.7 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorflow-addons) (2.13.3)\n",
      "Requirement already satisfied: packaging in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorflow-addons) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from packaging->tensorflow-addons) (3.0.9)\n",
      "Requirement already satisfied: nltk in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (3.7)\n",
      "Requirement already satisfied: joblib in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: tqdm in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from nltk) (4.64.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from nltk) (2022.4.24)\n",
      "Requirement already satisfied: click in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: importlib-metadata in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from click->nltk) (4.11.4)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from importlib-metadata->click->nltk) (3.10.0.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from importlib-metadata->click->nltk) (3.8.0)\n",
      "Requirement already satisfied: textacy in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (0.11.0)\n",
      "Requirement already satisfied: tqdm>=4.19.6 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from textacy) (4.64.0)\n",
      "Requirement already satisfied: jellyfish>=0.8.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from textacy) (0.9.0)\n",
      "Requirement already satisfied: spacy>=3.0.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from textacy) (3.3.0)\n",
      "Requirement already satisfied: cytoolz>=0.10.1 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from textacy) (0.11.2)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from textacy) (1.21.6)\n",
      "Requirement already satisfied: networkx>=2.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from textacy) (2.6.3)\n",
      "Requirement already satisfied: pyphen>=0.10.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from textacy) (0.12.0)\n",
      "Requirement already satisfied: requests>=2.10.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from textacy) (2.27.1)\n",
      "Requirement already satisfied: joblib>=0.13.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from textacy) (1.1.0)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from textacy) (1.7.3)\n",
      "Requirement already satisfied: cachetools>=4.0.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from textacy) (5.2.0)\n",
      "Requirement already satisfied: scikit-learn>=0.19.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from textacy) (1.0.2)\n",
      "Requirement already satisfied: toolz>=0.8.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from cytoolz>=0.10.1->textacy) (0.11.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from requests>=2.10.0->textacy) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from requests>=2.10.0->textacy) (2022.5.18.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from requests>=2.10.0->textacy) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from requests>=2.10.0->textacy) (2.0.12)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from scikit-learn>=0.19.0->textacy) (3.1.0)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (0.4.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (2.0.7)\n",
      "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (3.10.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (21.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (3.0.9)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (3.3.0)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (0.6.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (2.0.6)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (8.0.16)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (0.9.1)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (0.7.7)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (2.4.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (3.0.6)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (1.0.2)\n",
      "Requirement already satisfied: setuptools in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (61.2.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (1.0.7)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (1.8.2)\n",
      "Requirement already satisfied: jinja2 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (3.1.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from catalogue<2.1.0,>=2.0.6->spacy>=3.0.0->textacy) (3.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from packaging>=20.0->spacy>=3.0.0->textacy) (3.0.9)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from pathy>=0.3.5->spacy>=3.0.0->textacy) (5.2.1)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from typer<0.5.0,>=0.3.0->spacy>=3.0.0->textacy) (8.1.3)\n",
      "Requirement already satisfied: importlib-metadata in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from click<9.0.0,>=7.1.1->typer<0.5.0,>=0.3.0->spacy>=3.0.0->textacy) (4.11.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from jinja2->spacy>=3.0.0->textacy) (2.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install sentencepiece\n",
    "!pip install tensorflow==2.7.0\n",
    "!pip install stanza\n",
    "!pip install transformers\n",
    "!pip install tensorflow-addons\n",
    "!pip install nltk\n",
    "!pip install textacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K0rs0NoritMk",
    "outputId": "92b77bac-3521-4e3b-cf37-6f33a0d5c9f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "wYwcFK5gixXz"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2022-09-16 02:09:34.345080: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-16 02:09:36.355759: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 78949 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:31:00.0, compute capability: 8.0\n",
      "2022-09-16 02:09:36.361209: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 78949 MB memory:  -> device: 1, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:ca:00.0, compute capability: 8.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from textacy.datasets.supreme_court import SupremeCourt\n",
    "import numpy as np\n",
    "import re\n",
    "import unicodedata\n",
    "import nltk\n",
    "#from transformers import pipeline\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense,Dropout, Input, BatchNormalization\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from sklearn.metrics import confusion_matrix,f1_score,classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras import regularizers\n",
    "#from transformers import *\n",
    "from transformers import BertTokenizer, TFBertModel, BertConfig,TFDistilBertModel,DistilBertTokenizer,DistilBertConfig\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "import numpy as np\n",
    "import gc\n",
    "import math\n",
    "import json\n",
    "import stanza\n",
    "from tensorflow.keras import *\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import *\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import TFRobertaModel,RobertaTokenizer,LongformerTokenizer,TFLongformerModel\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.initializers import RandomUniform\n",
    "\n",
    "from numpy.random import seed\n",
    "import random as python_random\n",
    "import os\n",
    "import sys\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "np.random.seed(1)\n",
    "python_random.seed(1)\n",
    "tf.random.set_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bffI5vYTUeFW"
   },
   "outputs": [],
   "source": [
    "!cp \"/content/drive/My Drive/labels_sc.txt\" \"./labels_sc.txt\"\n",
    "!cp \"/content/drive/My Drive/labels_sc_279.txt\" \"./labels_sc_279.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "Number of devices: 2\n"
     ]
    }
   ],
   "source": [
    "# Create a MirroredStrategy.\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_model():\n",
    "    inps = Input(shape = (max_len,), dtype='int64')\n",
    "    masks= Input(shape = (max_len,), dtype='int64')\n",
    "    dbert_layer = dbert_model(inps, attention_mask=masks)[0][:,0,:]\n",
    "    dense_0 = Dense(512,activation='relu',kernel_regularizer=regularizers.l2(0.01))(dbert_layer)\n",
    "    dropout_0= Dropout(0.5)(dense_0)\n",
    "    pred = Dense(15, activation='softmax',kernel_regularizer=regularizers.l2(0.01))(dropout_0)\n",
    "    model = tf.keras.Model(inputs=[inps,masks], outputs=pred)\n",
    "    print(model.summary())\n",
    "    return model   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_accuracy=0\n",
    "total_weighted_f1=0\n",
    "total_micro_f1=0\n",
    "total_weighted_precision=0\n",
    "total_micro_precision=0\n",
    "total_weighted_recall=0\n",
    "total_micro_recall=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'supreme_court', 'site_url': 'http://caselaw.findlaw.com/court/us-supreme-court', 'description': 'Collection of ~8.4k decisions issued by the U.S. Supreme Court between November 1946 and June 2016.'}\n",
      "[-1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "{'-1': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7, '8': 8, '9': 9, '10': 10, '11': 11, '12': 12, '13': 13, '14': 14}\n",
      "8419\n",
      "Average Length 2920.3854377004395\n",
      "Found 8419 texts.\n",
      "Found 15 labels.\n",
      "                                                   text  label\n",
      "0     [ Halliburton Oil Well Cementing Co. v. Walker...      8\n",
      "1     Rehearing Denied Dec. 16, 1946. See . Mr.Claud...      1\n",
      "2     Rehearing Denied Dec. 16, 1946 See . Appeal fr...      8\n",
      "3     Mr. Walter J. Cummings, Jr., of Washington, D....      2\n",
      "4     Mr.A. Devitt Vaneck, of Washington, D.C., for ...      8\n",
      "...                                                 ...    ...\n",
      "8414  Opinion reported: Ante, p. 88. DECREE 1. It is...     11\n",
      "8415  In this dispute between Utah and the United St...     10\n",
      "8416  The United States, to the exclusion of defenda...     10\n",
      "8417  Louisiana's exception to the portion of the Sp...     11\n",
      "8418  To resolve a dispute over the ownership of cer...      9\n",
      "\n",
      "[8419 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-16 02:11:08.573312: I tensorflow/stream_executor/cuda/cuda_blas.cc:1774] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "Some layers from the model checkpoint at saibo/legal-longformer-base-4096 were not used when initializing TFLongformerModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFLongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFLongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFLongformerModel were initialized from the model checkpoint at saibo/legal-longformer-base-4096.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFLongformerModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 4096)]       0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 4096)]       0           []                               \n",
      "                                                                                                  \n",
      " tf_longformer_model (TFLongfor  TFLongformerBaseMod  148659456  ['input_1[0][0]',                \n",
      " merModel)                      elOutputWithPooling               'input_2[0][0]']                \n",
      "                                (last_hidden_state=                                               \n",
      "                                (None, 4096, 768),                                                \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 hidden_states=None                                               \n",
      "                                , attentions=None,                                                \n",
      "                                global_attentions=N                                               \n",
      "                                one)                                                              \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_longformer_model[0][0]']    \n",
      " ingOpLambda)                                                                                     \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dropout_49 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 15)           7695        ['dropout_49[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 149,060,879\n",
      "Trainable params: 149,060,879\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7577, 4096)\n",
      "(7577, 4096)\n",
      "(7577,)\n",
      "(842, 4096)\n",
      "(842, 4096)\n",
      "(842,)\n",
      "(7576, 4096)\n",
      "(7576, 4096)\n",
      "(7576,)\n",
      "(840, 4096)\n",
      "(840, 4096)\n",
      "(840,)\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "\n",
    "    sc = SupremeCourt()\n",
    "    print(sc.info)\n",
    "    sc.download()\n",
    "\n",
    "    texts = []  # list of text samples\n",
    "    labels_index = {}  # dictionary mapping label name to numeric id\n",
    "    labels = []  # list of label ids\n",
    "\n",
    "    issue_codes = list(sc.issue_area_codes.keys()) # 15 labels\n",
    "    print(issue_codes)\n",
    "    issue_codes.sort()\n",
    "    issue_codes = [str(ic) for ic in issue_codes]\n",
    "\n",
    "    labels_index = dict(zip(issue_codes, np.arange(len(issue_codes))))\n",
    "    print(labels_index)\n",
    "    count=0\n",
    "\n",
    "    for record in sc.records():\n",
    "\n",
    "            count=count+1\n",
    "            if record[1]['issue'] == None: # some cases have None as an issue\n",
    "                labels.append(labels_index['-1'])\n",
    "            else:\n",
    "                labels.append(labels_index[record[1]['issue'][:-4]])\n",
    "\n",
    "            new_sen=record[0].split(\"Footnotes\")[0]\n",
    "            new_new_sen=new_sen.split()\n",
    "            if len(new_new_sen) >= 4096:\n",
    "              new_new_sen=new_new_sen[0:4096]\n",
    "\n",
    "            elif len(new_new_sen) < 4096:\n",
    "              new_new_sen=new_new_sen[0:len(new_new_sen)]\n",
    "\n",
    "            new_new_sen=' '.join(new_new_sen)\n",
    "\n",
    "            texts.append(new_new_sen)\n",
    "\n",
    "    len_list = [len(ele.split()) for ele in texts]\n",
    "\n",
    "    #print(labels)\n",
    "    print(len(labels))\n",
    "\n",
    "    res = 0 if len(len_list) == 0 else (float(sum(len_list)) / len(len_list))\n",
    "    print(\"Average Length %s\" % res) \n",
    "    print('Found %s texts.' % len(texts))\n",
    "    print('Found %s labels.' % len(labels_index))\n",
    "    temp_file = open(\"labels_sc.txt\", \"r\")\n",
    "\n",
    "    data = temp_file.read()\n",
    "    label_list = data.split(\"\\n\")\n",
    "    #print(label_list)\n",
    "    label_list = label_list[0:-1]\n",
    "    #print(label_list)\n",
    "    label_list = [int(i) for i in label_list]\n",
    "    summarized_data = pd.DataFrame(texts,\n",
    "                   columns =['text'])\n",
    "    summarized_data['label'] = label_list\n",
    "    print(summarized_data)\n",
    "    #gc.collect()\n",
    "    #tf.keras.backend.clear_session()\n",
    "    dbert_tokenizer = LongformerTokenizer.from_pretrained('saibo/legal-longformer-base-4096')\n",
    "    dbert_model = TFLongformerModel.from_pretrained('saibo/legal-longformer-base-4096')\n",
    "    max_len=4096\n",
    "    sentences=summarized_data['text']\n",
    "    labels=summarized_data['label']\n",
    "    len(sentences),len(labels)\n",
    "    model_0=create_model()\n",
    "    input_ids=[]\n",
    "    attention_masks=[]\n",
    "\n",
    "    for sent in sentences:\n",
    "      dbert_inps=dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n",
    "      input_ids.append(dbert_inps['input_ids'])\n",
    "      attention_masks.append(dbert_inps['attention_mask'])\n",
    "    input_ids=np.asarray(input_ids)\n",
    "\n",
    "    attention_masks=np.array(attention_masks)\n",
    "    labels=np.array(labels)\n",
    "    train_inp,val_inp,train_label,val_label,train_mask,val_mask=train_test_split(input_ids,labels,attention_masks,test_size=0.1,random_state=42)\n",
    "    print(train_inp.shape)\n",
    "    print(train_mask.shape)\n",
    "    print(train_label.shape)\n",
    "    print(val_inp.shape)\n",
    "    print(val_mask.shape)\n",
    "    print(val_label.shape)\n",
    "    train_inp=train_inp[0:7576]\n",
    "    train_mask=train_mask[0:7576]\n",
    "    train_label=train_label[0:7576]\n",
    "\n",
    "    val_inp=val_inp[0:840]\n",
    "    val_label=val_label[0:840]\n",
    "    val_mask=val_mask[0:840]\n",
    "    print(train_inp.shape)\n",
    "    print(train_mask.shape)\n",
    "    print(train_label.shape)\n",
    "    print(val_inp.shape)\n",
    "    print(val_mask.shape)\n",
    "    print(val_label.shape)\n",
    "    \n",
    "    log_dir='dbert_model'\n",
    "\n",
    "    model_save_path='LSM/legallongformer-0-15labels.h5'\n",
    "\n",
    "\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    accuracy = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "    callbacks= [tf.keras.callbacks.ModelCheckpoint(filepath=model_save_path,monitor='val_accuracy',mode='max',save_best_only=True,save_weights_only=True),keras.callbacks.TensorBoard(log_dir=log_dir)]\n",
    "    model_0.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])\n",
    "          \n",
    "\n",
    "     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Sep 16 02:10:10 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM...  On   | 00000000:31:00.0 Off |                    0 |\n",
      "| N/A   26C    P0    69W / 500W |  79534MiB / 81920MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100-SXM...  On   | 00000000:CA:00.0 Off |                    0 |\n",
      "| N/A   25C    P0    69W / 500W |    584MiB / 81920MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A    507635      C   python                          79531MiB |\n",
      "|    1   N/A  N/A    507635      C   python                            581MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "    print('Not connected to a GPU')\n",
    "else:\n",
    "    print(gpu_info)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-14 00:59:40.659063: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:766] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_9\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"PrefetchDataset/_8\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_slice_batch_indices_642346\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\024FlatMapDataset:32848\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 8\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT64\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_longformer_model/longformer/pooler/dense/kernel:0', 'tf_longformer_model/longformer/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_longformer_model/longformer/pooler/dense/kernel:0', 'tf_longformer_model/longformer/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_longformer_model/longformer/pooler/dense/kernel:0', 'tf_longformer_model/longformer/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_longformer_model/longformer/pooler/dense/kernel:0', 'tf_longformer_model/longformer/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "947/947 [==============================] - ETA: 0s - loss: 6.7976 - accuracy: 0.6147"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-14 01:16:22.744033: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:766] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_9\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"PrefetchDataset/_8\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_slice_batch_indices_781486\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\024FlatMapDataset:32873\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 8\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT64\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "947/947 [==============================] - 1064s 970ms/step - loss: 6.7976 - accuracy: 0.6147 - val_loss: 5.7496 - val_accuracy: 0.7452\n",
      "Epoch 2/5\n",
      "947/947 [==============================] - 893s 943ms/step - loss: 5.2209 - accuracy: 0.7785 - val_loss: 4.8524 - val_accuracy: 0.7476\n",
      "Epoch 3/5\n",
      "947/947 [==============================] - 893s 943ms/step - loss: 4.2171 - accuracy: 0.8394 - val_loss: 4.0660 - val_accuracy: 0.7762\n",
      "Epoch 4/5\n",
      "947/947 [==============================] - 892s 942ms/step - loss: 3.3730 - accuracy: 0.8836 - val_loss: 3.5361 - val_accuracy: 0.7560\n",
      "Epoch 5/5\n",
      "947/947 [==============================] - 899s 949ms/step - loss: 2.6938 - accuracy: 0.9159 - val_loss: 3.1467 - val_accuracy: 0.7619\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "tf.keras.backend.clear_session()\n",
    "history=model_0.fit([train_inp,train_mask],train_label,batch_size=8,epochs=5,validation_data=([val_inp,val_mask],val_label),callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 4096)]       0           []                               \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)           [(None, 4096)]       0           []                               \n",
      "                                                                                                  \n",
      " tf_longformer_model (TFLongfor  TFLongformerBaseMod  148659456  ['input_3[0][0]',                \n",
      " merModel)                      elOutputWithPooling               'input_4[0][0]']                \n",
      "                                (last_hidden_state=                                               \n",
      "                                (None, 4096, 768),                                                \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 hidden_states=None                                               \n",
      "                                , attentions=None,                                                \n",
      "                                global_attentions=N                                               \n",
      "                                one)                                                              \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_longformer_model[1][0]']    \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dropout_50 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 15)           7695        ['dropout_50[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 149,060,879\n",
      "Trainable params: 149,060,879\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "(840, 4096)\n",
      "(840, 4096)\n",
      "(840,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - 83s 649ms/step - loss: 4.0660 - accuracy: 0.7762\n",
      "0.776190459728241\n",
      "Accuracy: 0.7761904761904762\n",
      "Weighted F1: 0.77158880092525\n",
      "Micro F1: 0.7761904761904762\n",
      "Weighted Precision: 0.7754148318077306\n",
      "Micro Precision: 0.7761904761904762\n",
      "Weighted Recall: 0.7761904761904762\n",
      "Micro Recall: 0.7761904761904762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "pred_labels=[]\n",
    "\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "accuracy = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "\n",
    "model_saved= create_model()\n",
    "model_saved.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])\n",
    "model_saved.load_weights('LSM/legallongformer-0-15labels.h5')\n",
    "\n",
    "\n",
    "print(val_inp.shape)\n",
    "print(val_mask.shape)\n",
    "print(val_label.shape)\n",
    "eval_loss, eval_acc=model_saved.evaluate([val_inp,val_mask],val_label,batch_size=8)\n",
    "print(eval_acc)\n",
    "\n",
    "\n",
    "for i in range(0,len(val_inp)):\n",
    "    pred=model_saved.predict([val_inp[i].reshape(1,4096),val_mask[i].reshape(1,4096)],batch_size=1)\n",
    "    pred_label = pred.argmax(axis=1)\n",
    "    pred_labels.append(pred_label)\n",
    "accuracy=accuracy_score(val_label, pred_labels)\n",
    "print(\"Accuracy: \"+str(accuracy))\n",
    "total_accuracy=total_accuracy+accuracy\n",
    "\n",
    "weighted_f1=f1_score(val_label,pred_labels, average='weighted')\n",
    "print(\"Weighted F1: \"+ str(weighted_f1))\n",
    "total_weighted_f1=total_weighted_f1+weighted_f1\n",
    "micro_f1=f1_score(val_label,pred_labels, average='micro')\n",
    "print(\"Micro F1: \"+ str(micro_f1))\n",
    "total_micro_f1=total_micro_f1+micro_f1\n",
    "\n",
    "weighted_precision=precision_score(val_label, pred_labels, average='weighted')\n",
    "print(\"Weighted Precision: \" + str(weighted_precision))\n",
    "total_weighted_precision=total_weighted_precision+weighted_precision\n",
    "micro_precision=precision_score(val_label, pred_labels, average='micro')\n",
    "print(\"Micro Precision: \" + str(micro_precision))\n",
    "total_micro_precision=total_micro_precision+micro_precision\n",
    "\n",
    "weighted_recall=recall_score(val_label, pred_labels, average='weighted')\n",
    "print(\"Weighted Recall: \" + str(weighted_recall))\n",
    "total_weighted_recall=total_weighted_recall+weighted_recall\n",
    "micro_recall=recall_score(val_label, pred_labels, average='micro')\n",
    "print(\"Micro Recall: \" + str(micro_recall))\n",
    "total_micro_recall=total_micro_recall+micro_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'supreme_court', 'site_url': 'http://caselaw.findlaw.com/court/us-supreme-court', 'description': 'Collection of ~8.4k decisions issued by the U.S. Supreme Court between November 1946 and June 2016.'}\n",
      "[-1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "{'-1': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7, '8': 8, '9': 9, '10': 10, '11': 11, '12': 12, '13': 13, '14': 14}\n",
      "8419\n",
      "Average Length 2920.3854377004395\n",
      "Found 8419 texts.\n",
      "Found 15 labels.\n",
      "                                                   text  label\n",
      "0     [ Halliburton Oil Well Cementing Co. v. Walker...      8\n",
      "1     Rehearing Denied Dec. 16, 1946. See . Mr.Claud...      1\n",
      "2     Rehearing Denied Dec. 16, 1946 See . Appeal fr...      8\n",
      "3     Mr. Walter J. Cummings, Jr., of Washington, D....      2\n",
      "4     Mr.A. Devitt Vaneck, of Washington, D.C., for ...      8\n",
      "...                                                 ...    ...\n",
      "8414  Opinion reported: Ante, p. 88. DECREE 1. It is...     11\n",
      "8415  In this dispute between Utah and the United St...     10\n",
      "8416  The United States, to the exclusion of defenda...     10\n",
      "8417  Louisiana's exception to the portion of the Sp...     11\n",
      "8418  To resolve a dispute over the ownership of cer...      9\n",
      "\n",
      "[8419 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at saibo/legal-longformer-base-4096 were not used when initializing TFLongformerModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFLongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFLongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFLongformerModel were initialized from the model checkpoint at saibo/legal-longformer-base-4096.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFLongformerModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 4096)]       0           []                               \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)           [(None, 4096)]       0           []                               \n",
      "                                                                                                  \n",
      " tf_longformer_model (TFLongfor  TFLongformerBaseMod  148659456  ['input_3[0][0]',                \n",
      " merModel)                      elOutputWithPooling               'input_4[0][0]']                \n",
      "                                (last_hidden_state=                                               \n",
      "                                (None, 4096, 768),                                                \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 hidden_states=None                                               \n",
      "                                , attentions=None,                                                \n",
      "                                global_attentions=N                                               \n",
      "                                one)                                                              \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_longformer_model[0][0]']    \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dropout_50 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 15)           7695        ['dropout_50[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 149,060,879\n",
      "Trainable params: 149,060,879\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7577, 4096)\n",
      "(7577, 4096)\n",
      "(7577,)\n",
      "(842, 4096)\n",
      "(842, 4096)\n",
      "(842,)\n",
      "(7576, 4096)\n",
      "(7576, 4096)\n",
      "(7576,)\n",
      "(840, 4096)\n",
      "(840, 4096)\n",
      "(840,)\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "    sc = SupremeCourt()\n",
    "    print(sc.info)\n",
    "    sc.download()\n",
    "\n",
    "    texts = []  # list of text samples\n",
    "    labels_index = {}  # dictionary mapping label name to numeric id\n",
    "    labels = []  # list of label ids\n",
    "\n",
    "    issue_codes = list(sc.issue_area_codes.keys()) # 15 labels\n",
    "    print(issue_codes)\n",
    "    issue_codes.sort()\n",
    "    issue_codes = [str(ic) for ic in issue_codes]\n",
    "\n",
    "    labels_index = dict(zip(issue_codes, np.arange(len(issue_codes))))\n",
    "    print(labels_index)\n",
    "    count=0\n",
    "\n",
    "    for record in sc.records():\n",
    "\n",
    "            count=count+1\n",
    "            if record[1]['issue'] == None: # some cases have None as an issue\n",
    "                labels.append(labels_index['-1'])\n",
    "            else:\n",
    "                labels.append(labels_index[record[1]['issue'][:-4]])\n",
    "\n",
    "            new_sen=record[0].split(\"Footnotes\")[0]\n",
    "            new_new_sen=new_sen.split()\n",
    "            if len(new_new_sen) >= 4096:\n",
    "              new_new_sen=new_new_sen[0:4096]\n",
    "\n",
    "            elif len(new_new_sen) < 4096:\n",
    "              new_new_sen=new_new_sen[0:len(new_new_sen)]\n",
    "\n",
    "            new_new_sen=' '.join(new_new_sen)\n",
    "\n",
    "            texts.append(new_new_sen)\n",
    "\n",
    "    len_list = [len(ele.split()) for ele in texts]\n",
    "\n",
    "    #print(labels)\n",
    "    print(len(labels))\n",
    "\n",
    "    res = 0 if len(len_list) == 0 else (float(sum(len_list)) / len(len_list))\n",
    "    print(\"Average Length %s\" % res) \n",
    "    print('Found %s texts.' % len(texts))\n",
    "    print('Found %s labels.' % len(labels_index))\n",
    "    temp_file = open(\"labels_sc.txt\", \"r\")\n",
    "\n",
    "    data = temp_file.read()\n",
    "    label_list = data.split(\"\\n\")\n",
    "    #print(label_list)\n",
    "    label_list = label_list[0:-1]\n",
    "    #print(label_list)\n",
    "    label_list = [int(i) for i in label_list]\n",
    "    summarized_data = pd.DataFrame(texts,\n",
    "                   columns =['text'])\n",
    "    summarized_data['label'] = label_list\n",
    "    print(summarized_data)\n",
    "    #gc.collect()\n",
    "    #tf.keras.backend.clear_session()\n",
    "    dbert_tokenizer = LongformerTokenizer.from_pretrained('saibo/legal-longformer-base-4096')\n",
    "    dbert_model = TFLongformerModel.from_pretrained('saibo/legal-longformer-base-4096')\n",
    "    max_len=4096\n",
    "    sentences=summarized_data['text']\n",
    "    labels=summarized_data['label']\n",
    "    len(sentences),len(labels)\n",
    "    model_0=create_model()\n",
    "    input_ids=[]\n",
    "    attention_masks=[]\n",
    "\n",
    "    for sent in sentences:\n",
    "      dbert_inps=dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n",
    "      input_ids.append(dbert_inps['input_ids'])\n",
    "      attention_masks.append(dbert_inps['attention_mask'])\n",
    "    input_ids=np.asarray(input_ids)\n",
    "\n",
    "    attention_masks=np.array(attention_masks)\n",
    "    labels=np.array(labels)\n",
    "    train_inp,val_inp,train_label,val_label,train_mask,val_mask=train_test_split(input_ids,labels,attention_masks,test_size=0.1,random_state=42)\n",
    "    print(train_inp.shape)\n",
    "    print(train_mask.shape)\n",
    "    print(train_label.shape)\n",
    "    print(val_inp.shape)\n",
    "    print(val_mask.shape)\n",
    "    print(val_label.shape)\n",
    "    train_inp=train_inp[0:7576]\n",
    "    train_mask=train_mask[0:7576]\n",
    "    train_label=train_label[0:7576]\n",
    "\n",
    "    val_inp=val_inp[0:840]\n",
    "    val_label=val_label[0:840]\n",
    "    val_mask=val_mask[0:840]\n",
    "    print(train_inp.shape)\n",
    "    print(train_mask.shape)\n",
    "    print(train_label.shape)\n",
    "    print(val_inp.shape)\n",
    "    print(val_mask.shape)\n",
    "    print(val_label.shape)\n",
    "    log_dir='dbert_model'\n",
    "\n",
    "    model_save_path='LSM/legallongformer-1-15labels.h5'\n",
    "\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    accuracy = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "    callbacks= [tf.keras.callbacks.ModelCheckpoint(filepath=model_save_path,monitor='val_accuracy',mode='max',save_best_only=True,save_weights_only=True),keras.callbacks.TensorBoard(log_dir=log_dir)]\n",
    "    model_0.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-14 02:23:31.998757: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:766] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_9\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"PrefetchDataset/_8\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_slice_batch_indices_989651\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\024FlatMapDataset:42183\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 8\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT64\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_longformer_model/longformer/pooler/dense/kernel:0', 'tf_longformer_model/longformer/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_longformer_model/longformer/pooler/dense/kernel:0', 'tf_longformer_model/longformer/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_longformer_model/longformer/pooler/dense/kernel:0', 'tf_longformer_model/longformer/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_longformer_model/longformer/pooler/dense/kernel:0', 'tf_longformer_model/longformer/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "947/947 [==============================] - ETA: 0s - loss: 6.7556 - accuracy: 0.6412"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-14 02:40:25.065045: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:766] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_9\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"PrefetchDataset/_8\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_slice_batch_indices_1128791\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\024FlatMapDataset:42208\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 8\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT64\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "947/947 [==============================] - 1075s 984ms/step - loss: 6.7556 - accuracy: 0.6412 - val_loss: 5.7645 - val_accuracy: 0.7369\n",
      "Epoch 2/5\n",
      "947/947 [==============================] - 908s 959ms/step - loss: 5.1765 - accuracy: 0.7810 - val_loss: 4.7889 - val_accuracy: 0.7667\n",
      "Epoch 3/5\n",
      "947/947 [==============================] - 906s 957ms/step - loss: 4.1450 - accuracy: 0.8449 - val_loss: 4.0304 - val_accuracy: 0.7726\n",
      "Epoch 4/5\n",
      "947/947 [==============================] - 905s 956ms/step - loss: 3.2874 - accuracy: 0.8903 - val_loss: 3.4558 - val_accuracy: 0.7631\n",
      "Epoch 5/5\n",
      "947/947 [==============================] - 907s 957ms/step - loss: 2.6130 - accuracy: 0.9187 - val_loss: 3.0057 - val_accuracy: 0.7679\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "tf.keras.backend.clear_session()\n",
    "history=model_0.fit([train_inp,train_mask],train_label,batch_size=8,epochs=5,validation_data=([val_inp,val_mask],val_label),callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)           [(None, 4096)]       0           []                               \n",
      "                                                                                                  \n",
      " input_6 (InputLayer)           [(None, 4096)]       0           []                               \n",
      "                                                                                                  \n",
      " tf_longformer_model (TFLongfor  TFLongformerBaseMod  148659456  ['input_5[0][0]',                \n",
      " merModel)                      elOutputWithPooling               'input_6[0][0]']                \n",
      "                                (last_hidden_state=                                               \n",
      "                                (None, 4096, 768),                                                \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 hidden_states=None                                               \n",
      "                                , attentions=None,                                                \n",
      "                                global_attentions=N                                               \n",
      "                                one)                                                              \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_2 (Sl  (None, 768)         0           ['tf_longformer_model[2][0]']    \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_2[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dropout_51 (Dropout)           (None, 512)          0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 15)           7695        ['dropout_51[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 149,060,879\n",
      "Trainable params: 149,060,879\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Accuracy: 0.7726190476190476\n",
      "Weighted F1: 0.7691662472407268\n",
      "Micro F1: 0.7726190476190476\n",
      "Weighted Precision: 0.7779722832531375\n",
      "Micro Precision: 0.7726190476190476\n",
      "Weighted Recall: 0.7726190476190476\n",
      "Micro Recall: 0.7726190476190476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "pred_labels=[]\n",
    "\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "accuracy = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "\n",
    "model_saved= create_model()\n",
    "model_saved.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])\n",
    "model_saved.load_weights('LSM/legallongformer-1-15labels.h5')\n",
    "\n",
    "for i in range(0,len(val_inp)):\n",
    "    pred=model_saved.predict([val_inp[i].reshape(1,4096),val_mask[i].reshape(1,4096)],batch_size=8)\n",
    "    pred_label = pred.argmax(axis=1)\n",
    "    pred_labels.append(pred_label)\n",
    "accuracy=accuracy_score(val_label, pred_labels)\n",
    "print(\"Accuracy: \"+str(accuracy))\n",
    "total_accuracy=total_accuracy+accuracy\n",
    "\n",
    "weighted_f1=f1_score(val_label,pred_labels, average='weighted')\n",
    "print(\"Weighted F1: \"+ str(weighted_f1))\n",
    "total_weighted_f1=total_weighted_f1+weighted_f1\n",
    "micro_f1=f1_score(val_label,pred_labels, average='micro')\n",
    "print(\"Micro F1: \"+ str(micro_f1))\n",
    "total_micro_f1=total_micro_f1+micro_f1\n",
    "\n",
    "weighted_precision=precision_score(val_label, pred_labels, average='weighted')\n",
    "print(\"Weighted Precision: \" + str(weighted_precision))\n",
    "total_weighted_precision=total_weighted_precision+weighted_precision\n",
    "micro_precision=precision_score(val_label, pred_labels, average='micro')\n",
    "print(\"Micro Precision: \" + str(micro_precision))\n",
    "total_micro_precision=total_micro_precision+micro_precision\n",
    "\n",
    "weighted_recall=recall_score(val_label, pred_labels, average='weighted')\n",
    "print(\"Weighted Recall: \" + str(weighted_recall))\n",
    "total_weighted_recall=total_weighted_recall+weighted_recall\n",
    "micro_recall=recall_score(val_label, pred_labels, average='micro')\n",
    "print(\"Micro Recall: \" + str(micro_recall))\n",
    "total_micro_recall=total_micro_recall+micro_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'supreme_court', 'site_url': 'http://caselaw.findlaw.com/court/us-supreme-court', 'description': 'Collection of ~8.4k decisions issued by the U.S. Supreme Court between November 1946 and June 2016.'}\n",
      "[-1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "{'-1': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7, '8': 8, '9': 9, '10': 10, '11': 11, '12': 12, '13': 13, '14': 14}\n",
      "8419\n",
      "Average Length 2920.3854377004395\n",
      "Found 8419 texts.\n",
      "Found 15 labels.\n",
      "                                                   text  label\n",
      "0     [ Halliburton Oil Well Cementing Co. v. Walker...      8\n",
      "1     Rehearing Denied Dec. 16, 1946. See . Mr.Claud...      1\n",
      "2     Rehearing Denied Dec. 16, 1946 See . Appeal fr...      8\n",
      "3     Mr. Walter J. Cummings, Jr., of Washington, D....      2\n",
      "4     Mr.A. Devitt Vaneck, of Washington, D.C., for ...      8\n",
      "...                                                 ...    ...\n",
      "8414  Opinion reported: Ante, p. 88. DECREE 1. It is...     11\n",
      "8415  In this dispute between Utah and the United St...     10\n",
      "8416  The United States, to the exclusion of defenda...     10\n",
      "8417  Louisiana's exception to the portion of the Sp...     11\n",
      "8418  To resolve a dispute over the ownership of cer...      9\n",
      "\n",
      "[8419 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-14 11:51:26.499913: I tensorflow/stream_executor/cuda/cuda_blas.cc:1774] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "Some layers from the model checkpoint at saibo/legal-longformer-base-4096 were not used when initializing TFLongformerModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFLongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFLongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFLongformerModel were initialized from the model checkpoint at saibo/legal-longformer-base-4096.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFLongformerModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 4096)]       0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 4096)]       0           []                               \n",
      "                                                                                                  \n",
      " tf_longformer_model (TFLongfor  TFLongformerBaseMod  148659456  ['input_1[0][0]',                \n",
      " merModel)                      elOutputWithPooling               'input_2[0][0]']                \n",
      "                                (last_hidden_state=                                               \n",
      "                                (None, 4096, 768),                                                \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 hidden_states=None                                               \n",
      "                                , attentions=None,                                                \n",
      "                                global_attentions=N                                               \n",
      "                                one)                                                              \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_longformer_model[0][0]']    \n",
      " ingOpLambda)                                                                                     \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dropout_49 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 15)           7695        ['dropout_49[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 149,060,879\n",
      "Trainable params: 149,060,879\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7577, 4096)\n",
      "(7577, 4096)\n",
      "(7577,)\n",
      "(842, 4096)\n",
      "(842, 4096)\n",
      "(842,)\n",
      "(7576, 4096)\n",
      "(7576, 4096)\n",
      "(7576,)\n",
      "(840, 4096)\n",
      "(840, 4096)\n",
      "(840,)\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "    sc = SupremeCourt()\n",
    "    print(sc.info)\n",
    "    sc.download()\n",
    "\n",
    "    texts = []  # list of text samples\n",
    "    labels_index = {}  # dictionary mapping label name to numeric id\n",
    "    labels = []  # list of label ids\n",
    "\n",
    "    issue_codes = list(sc.issue_area_codes.keys()) # 15 labels\n",
    "    print(issue_codes)\n",
    "    issue_codes.sort()\n",
    "    issue_codes = [str(ic) for ic in issue_codes]\n",
    "\n",
    "    labels_index = dict(zip(issue_codes, np.arange(len(issue_codes))))\n",
    "    print(labels_index)\n",
    "    count=0\n",
    "\n",
    "    for record in sc.records():\n",
    "\n",
    "            count=count+1\n",
    "            if record[1]['issue'] == None: # some cases have None as an issue\n",
    "                labels.append(labels_index['-1'])\n",
    "            else:\n",
    "                labels.append(labels_index[record[1]['issue'][:-4]])\n",
    "\n",
    "            new_sen=record[0].split(\"Footnotes\")[0]\n",
    "            new_new_sen=new_sen.split()\n",
    "            if len(new_new_sen) >= 4096:\n",
    "              new_new_sen=new_new_sen[0:4096]\n",
    "\n",
    "            elif len(new_new_sen) < 4096:\n",
    "              new_new_sen=new_new_sen[0:len(new_new_sen)]\n",
    "\n",
    "            new_new_sen=' '.join(new_new_sen)\n",
    "\n",
    "            texts.append(new_new_sen)\n",
    "\n",
    "    len_list = [len(ele.split()) for ele in texts]\n",
    "\n",
    "    #print(labels)\n",
    "    print(len(labels))\n",
    "\n",
    "    res = 0 if len(len_list) == 0 else (float(sum(len_list)) / len(len_list))\n",
    "    print(\"Average Length %s\" % res) \n",
    "    print('Found %s texts.' % len(texts))\n",
    "    print('Found %s labels.' % len(labels_index))\n",
    "    temp_file = open(\"labels_sc.txt\", \"r\")\n",
    "\n",
    "    data = temp_file.read()\n",
    "    label_list = data.split(\"\\n\")\n",
    "    #print(label_list)\n",
    "    label_list = label_list[0:-1]\n",
    "    #print(label_list)\n",
    "    label_list = [int(i) for i in label_list]\n",
    "    summarized_data = pd.DataFrame(texts,\n",
    "                   columns =['text'])\n",
    "    summarized_data['label'] = label_list\n",
    "    print(summarized_data)\n",
    "    #gc.collect()\n",
    "    #tf.keras.backend.clear_session()\n",
    "    dbert_tokenizer = LongformerTokenizer.from_pretrained('saibo/legal-longformer-base-4096')\n",
    "    dbert_model = TFLongformerModel.from_pretrained('saibo/legal-longformer-base-4096')\n",
    "    max_len=4096\n",
    "    sentences=summarized_data['text']\n",
    "    labels=summarized_data['label']\n",
    "    len(sentences),len(labels)\n",
    "    model_0=create_model()\n",
    "    input_ids=[]\n",
    "    attention_masks=[]\n",
    "\n",
    "    for sent in sentences:\n",
    "      dbert_inps=dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n",
    "      input_ids.append(dbert_inps['input_ids'])\n",
    "      attention_masks.append(dbert_inps['attention_mask'])\n",
    "    input_ids=np.asarray(input_ids)\n",
    "\n",
    "    attention_masks=np.array(attention_masks)\n",
    "    labels=np.array(labels)\n",
    "    train_inp,val_inp,train_label,val_label,train_mask,val_mask=train_test_split(input_ids,labels,attention_masks,test_size=0.1,random_state=42)\n",
    "    print(train_inp.shape)\n",
    "    print(train_mask.shape)\n",
    "    print(train_label.shape)\n",
    "    print(val_inp.shape)\n",
    "    print(val_mask.shape)\n",
    "    print(val_label.shape)\n",
    "    train_inp=train_inp[0:7576]\n",
    "    train_mask=train_mask[0:7576]\n",
    "    train_label=train_label[0:7576]\n",
    "\n",
    "    val_inp=val_inp[0:840]\n",
    "    val_label=val_label[0:840]\n",
    "    val_mask=val_mask[0:840]\n",
    "    print(train_inp.shape)\n",
    "    print(train_mask.shape)\n",
    "    print(train_label.shape)\n",
    "    print(val_inp.shape)\n",
    "    print(val_mask.shape)\n",
    "    print(val_label.shape)\n",
    "    log_dir='dbert_model'\n",
    "\n",
    "    model_save_path='LSM/legallongformer-2-15labels.h5'\n",
    "\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    accuracy = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "    callbacks= [tf.keras.callbacks.ModelCheckpoint(filepath=model_save_path,monitor='val_accuracy',mode='max',save_best_only=True,save_weights_only=True),keras.callbacks.TensorBoard(log_dir=log_dir)]\n",
    "    model_0.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-14 11:53:49.799720: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:766] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_9\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"PrefetchDataset/_8\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_slice_batch_indices_50122\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\020FlatMapDataset:4\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 8\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT64\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_longformer_model/longformer/pooler/dense/kernel:0', 'tf_longformer_model/longformer/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_longformer_model/longformer/pooler/dense/kernel:0', 'tf_longformer_model/longformer/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:tensorflow:batch_all_reduce: 270 all-reduces with algorithm = nccl, num_packs = 1\n",
      "WARNING:tensorflow:Efficient allreduce is not supported for 3 IndexedSlices\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_longformer_model/longformer/pooler/dense/kernel:0', 'tf_longformer_model/longformer/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_longformer_model/longformer/pooler/dense/kernel:0', 'tf_longformer_model/longformer/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:tensorflow:batch_all_reduce: 270 all-reduces with algorithm = nccl, num_packs = 1\n",
      "WARNING:tensorflow:Efficient allreduce is not supported for 3 IndexedSlices\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
      "947/947 [==============================] - ETA: 0s - loss: 6.7856 - accuracy: 0.6172"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-14 12:10:09.700022: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:766] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_9\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"PrefetchDataset/_8\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_slice_batch_indices_189266\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\021FlatMapDataset:29\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 8\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT64\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "947/947 [==============================] - 1040s 944ms/step - loss: 6.7856 - accuracy: 0.6172 - val_loss: 5.7542 - val_accuracy: 0.7429\n",
      "Epoch 2/5\n",
      "947/947 [==============================] - 869s 918ms/step - loss: 5.2215 - accuracy: 0.7860 - val_loss: 4.8713 - val_accuracy: 0.7524\n",
      "Epoch 3/5\n",
      "947/947 [==============================] - 869s 918ms/step - loss: 4.2033 - accuracy: 0.8445 - val_loss: 4.0300 - val_accuracy: 0.7667\n",
      "Epoch 4/5\n",
      "947/947 [==============================] - 868s 917ms/step - loss: 3.3622 - accuracy: 0.8873 - val_loss: 3.4671 - val_accuracy: 0.7702\n",
      "Epoch 5/5\n",
      "947/947 [==============================] - 869s 918ms/step - loss: 2.6731 - accuracy: 0.9209 - val_loss: 3.0595 - val_accuracy: 0.7702\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "tf.keras.backend.clear_session()\n",
    "history=model_0.fit([train_inp,train_mask],train_label,batch_size=8,epochs=5,validation_data=([val_inp,val_mask],val_label),callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_7 (InputLayer)           [(None, 4096)]       0           []                               \n",
      "                                                                                                  \n",
      " input_8 (InputLayer)           [(None, 4096)]       0           []                               \n",
      "                                                                                                  \n",
      " tf_longformer_model (TFLongfor  TFLongformerBaseMod  148659456  ['input_7[0][0]',                \n",
      " merModel)                      elOutputWithPooling               'input_8[0][0]']                \n",
      "                                (last_hidden_state=                                               \n",
      "                                (None, 4096, 768),                                                \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 hidden_states=None                                               \n",
      "                                , attentions=None,                                                \n",
      "                                global_attentions=N                                               \n",
      "                                one)                                                              \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_3 (Sl  (None, 768)         0           ['tf_longformer_model[3][0]']    \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_3[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dropout_52 (Dropout)           (None, 512)          0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 15)           7695        ['dropout_52[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 149,060,879\n",
      "Trainable params: 149,060,879\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Accuracy: 0.7702380952380953\n",
      "Weighted F1: 0.7736750036958562\n",
      "Micro F1: 0.7702380952380953\n",
      "Weighted Precision: 0.7834334911533238\n",
      "Micro Precision: 0.7702380952380953\n",
      "Weighted Recall: 0.7702380952380953\n",
      "Micro Recall: 0.7702380952380953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "pred_labels=[]\n",
    "\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "accuracy = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "\n",
    "model_saved= create_model()\n",
    "model_saved.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])\n",
    "model_saved.load_weights('LSM/legallongformer-2-15labels.h5')\n",
    "\n",
    "for i in range(0,len(val_inp)):\n",
    "    pred=model_saved.predict([val_inp[i].reshape(1,4096),val_mask[i].reshape(1,4096)],batch_size=8)\n",
    "    pred_label = pred.argmax(axis=1)\n",
    "    pred_labels.append(pred_label)\n",
    "accuracy=accuracy_score(val_label, pred_labels)\n",
    "print(\"Accuracy: \"+str(accuracy))\n",
    "total_accuracy=total_accuracy+accuracy\n",
    "\n",
    "weighted_f1=f1_score(val_label,pred_labels, average='weighted')\n",
    "print(\"Weighted F1: \"+ str(weighted_f1))\n",
    "total_weighted_f1=total_weighted_f1+weighted_f1\n",
    "micro_f1=f1_score(val_label,pred_labels, average='micro')\n",
    "print(\"Micro F1: \"+ str(micro_f1))\n",
    "total_micro_f1=total_micro_f1+micro_f1\n",
    "\n",
    "weighted_precision=precision_score(val_label, pred_labels, average='weighted')\n",
    "print(\"Weighted Precision: \" + str(weighted_precision))\n",
    "total_weighted_precision=total_weighted_precision+weighted_precision\n",
    "micro_precision=precision_score(val_label, pred_labels, average='micro')\n",
    "print(\"Micro Precision: \" + str(micro_precision))\n",
    "total_micro_precision=total_micro_precision+micro_precision\n",
    "\n",
    "weighted_recall=recall_score(val_label, pred_labels, average='weighted')\n",
    "print(\"Weighted Recall: \" + str(weighted_recall))\n",
    "total_weighted_recall=total_weighted_recall+weighted_recall\n",
    "micro_recall=recall_score(val_label, pred_labels, average='micro')\n",
    "print(\"Micro Recall: \" + str(micro_recall))\n",
    "total_micro_recall=total_micro_recall+micro_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'supreme_court', 'site_url': 'http://caselaw.findlaw.com/court/us-supreme-court', 'description': 'Collection of ~8.4k decisions issued by the U.S. Supreme Court between November 1946 and June 2016.'}\n",
      "[-1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "{'-1': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7, '8': 8, '9': 9, '10': 10, '11': 11, '12': 12, '13': 13, '14': 14}\n",
      "8419\n",
      "Average Length 2920.3854377004395\n",
      "Found 8419 texts.\n",
      "Found 15 labels.\n",
      "                                                   text  label\n",
      "0     [ Halliburton Oil Well Cementing Co. v. Walker...      8\n",
      "1     Rehearing Denied Dec. 16, 1946. See . Mr.Claud...      1\n",
      "2     Rehearing Denied Dec. 16, 1946 See . Appeal fr...      8\n",
      "3     Mr. Walter J. Cummings, Jr., of Washington, D....      2\n",
      "4     Mr.A. Devitt Vaneck, of Washington, D.C., for ...      8\n",
      "...                                                 ...    ...\n",
      "8414  Opinion reported: Ante, p. 88. DECREE 1. It is...     11\n",
      "8415  In this dispute between Utah and the United St...     10\n",
      "8416  The United States, to the exclusion of defenda...     10\n",
      "8417  Louisiana's exception to the portion of the Sp...     11\n",
      "8418  To resolve a dispute over the ownership of cer...      9\n",
      "\n",
      "[8419 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at saibo/legal-longformer-base-4096 were not used when initializing TFLongformerModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFLongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFLongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFLongformerModel were initialized from the model checkpoint at saibo/legal-longformer-base-4096.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFLongformerModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 4096)]       0           []                               \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)           [(None, 4096)]       0           []                               \n",
      "                                                                                                  \n",
      " tf_longformer_model (TFLongfor  TFLongformerBaseMod  148659456  ['input_3[0][0]',                \n",
      " merModel)                      elOutputWithPooling               'input_4[0][0]']                \n",
      "                                (last_hidden_state=                                               \n",
      "                                (None, 4096, 768),                                                \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 hidden_states=None                                               \n",
      "                                , attentions=None,                                                \n",
      "                                global_attentions=N                                               \n",
      "                                one)                                                              \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_longformer_model[0][0]']    \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dropout_50 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 15)           7695        ['dropout_50[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 149,060,879\n",
      "Trainable params: 149,060,879\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7577, 4096)\n",
      "(7577, 4096)\n",
      "(7577,)\n",
      "(842, 4096)\n",
      "(842, 4096)\n",
      "(842,)\n",
      "(7576, 4096)\n",
      "(7576, 4096)\n",
      "(7576,)\n",
      "(840, 4096)\n",
      "(840, 4096)\n",
      "(840,)\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "    sc = SupremeCourt()\n",
    "    print(sc.info)\n",
    "    sc.download()\n",
    "\n",
    "    texts = []  # list of text samples\n",
    "    labels_index = {}  # dictionary mapping label name to numeric id\n",
    "    labels = []  # list of label ids\n",
    "\n",
    "    issue_codes = list(sc.issue_area_codes.keys()) # 15 labels\n",
    "    print(issue_codes)\n",
    "    issue_codes.sort()\n",
    "    issue_codes = [str(ic) for ic in issue_codes]\n",
    "\n",
    "    labels_index = dict(zip(issue_codes, np.arange(len(issue_codes))))\n",
    "    print(labels_index)\n",
    "    count=0\n",
    "\n",
    "    for record in sc.records():\n",
    "\n",
    "            count=count+1\n",
    "            if record[1]['issue'] == None: # some cases have None as an issue\n",
    "                labels.append(labels_index['-1'])\n",
    "            else:\n",
    "                labels.append(labels_index[record[1]['issue'][:-4]])\n",
    "\n",
    "            new_sen=record[0].split(\"Footnotes\")[0]\n",
    "            new_new_sen=new_sen.split()\n",
    "            if len(new_new_sen) >= 4096:\n",
    "              new_new_sen=new_new_sen[0:4096]\n",
    "\n",
    "            elif len(new_new_sen) < 4096:\n",
    "              new_new_sen=new_new_sen[0:len(new_new_sen)]\n",
    "\n",
    "            new_new_sen=' '.join(new_new_sen)\n",
    "\n",
    "            texts.append(new_new_sen)\n",
    "\n",
    "    len_list = [len(ele.split()) for ele in texts]\n",
    "\n",
    "    #print(labels)\n",
    "    print(len(labels))\n",
    "\n",
    "    res = 0 if len(len_list) == 0 else (float(sum(len_list)) / len(len_list))\n",
    "    print(\"Average Length %s\" % res) \n",
    "    print('Found %s texts.' % len(texts))\n",
    "    print('Found %s labels.' % len(labels_index))\n",
    "    temp_file = open(\"labels_sc.txt\", \"r\")\n",
    "\n",
    "    data = temp_file.read()\n",
    "    label_list = data.split(\"\\n\")\n",
    "    #print(label_list)\n",
    "    label_list = label_list[0:-1]\n",
    "    #print(label_list)\n",
    "    label_list = [int(i) for i in label_list]\n",
    "    summarized_data = pd.DataFrame(texts,\n",
    "                   columns =['text'])\n",
    "    summarized_data['label'] = label_list\n",
    "    print(summarized_data)\n",
    "    #gc.collect()\n",
    "    #tf.keras.backend.clear_session()\n",
    "    dbert_tokenizer = LongformerTokenizer.from_pretrained('saibo/legal-longformer-base-4096')\n",
    "    dbert_model = TFLongformerModel.from_pretrained('saibo/legal-longformer-base-4096')\n",
    "    max_len=4096\n",
    "    sentences=summarized_data['text']\n",
    "    labels=summarized_data['label']\n",
    "    len(sentences),len(labels)\n",
    "    model_0=create_model()\n",
    "    input_ids=[]\n",
    "    attention_masks=[]\n",
    "\n",
    "    for sent in sentences:\n",
    "      dbert_inps=dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n",
    "      input_ids.append(dbert_inps['input_ids'])\n",
    "      attention_masks.append(dbert_inps['attention_mask'])\n",
    "    input_ids=np.asarray(input_ids)\n",
    "\n",
    "    attention_masks=np.array(attention_masks)\n",
    "    labels=np.array(labels)\n",
    "    train_inp,val_inp,train_label,val_label,train_mask,val_mask=train_test_split(input_ids,labels,attention_masks,test_size=0.1,random_state=42)\n",
    "    print(train_inp.shape)\n",
    "    print(train_mask.shape)\n",
    "    print(train_label.shape)\n",
    "    print(val_inp.shape)\n",
    "    print(val_mask.shape)\n",
    "    print(val_label.shape)\n",
    "    train_inp=train_inp[0:7576]\n",
    "    train_mask=train_mask[0:7576]\n",
    "    train_label=train_label[0:7576]\n",
    "\n",
    "    val_inp=val_inp[0:840]\n",
    "    val_label=val_label[0:840]\n",
    "    val_mask=val_mask[0:840]\n",
    "    print(train_inp.shape)\n",
    "    print(train_mask.shape)\n",
    "    print(train_label.shape)\n",
    "    print(val_inp.shape)\n",
    "    print(val_mask.shape)\n",
    "    print(val_label.shape)\n",
    "    log_dir='dbert_model'\n",
    "\n",
    "    model_save_path='LSM/legallongformer-3-15labels.h5'\n",
    "\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    accuracy = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "    callbacks= [tf.keras.callbacks.ModelCheckpoint(filepath=model_save_path,monitor='val_accuracy',mode='max',save_best_only=True,save_weights_only=True),keras.callbacks.TensorBoard(log_dir=log_dir)]\n",
    "    model_0.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-14 13:14:20.794299: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:766] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_9\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"PrefetchDataset/_8\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_slice_batch_indices_393683\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\024FlatMapDataset:11008\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 8\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT64\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_longformer_model/longformer/pooler/dense/kernel:0', 'tf_longformer_model/longformer/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_longformer_model/longformer/pooler/dense/kernel:0', 'tf_longformer_model/longformer/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_longformer_model/longformer/pooler/dense/kernel:0', 'tf_longformer_model/longformer/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_longformer_model/longformer/pooler/dense/kernel:0', 'tf_longformer_model/longformer/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "947/947 [==============================] - ETA: 0s - loss: 6.7597 - accuracy: 0.6408"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-14 13:30:47.175219: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:766] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_9\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"PrefetchDataset/_8\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_slice_batch_indices_532823\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\024FlatMapDataset:11033\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 8\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT64\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "947/947 [==============================] - 1047s 955ms/step - loss: 6.7597 - accuracy: 0.6408 - val_loss: 5.7338 - val_accuracy: 0.7476\n",
      "Epoch 2/5\n",
      "947/947 [==============================] - 876s 925ms/step - loss: 5.1899 - accuracy: 0.7789 - val_loss: 4.7964 - val_accuracy: 0.7643\n",
      "Epoch 3/5\n",
      "947/947 [==============================] - 876s 925ms/step - loss: 4.1445 - accuracy: 0.8454 - val_loss: 4.0534 - val_accuracy: 0.7738\n",
      "Epoch 4/5\n",
      "947/947 [==============================] - 875s 924ms/step - loss: 3.2942 - accuracy: 0.8904 - val_loss: 3.4981 - val_accuracy: 0.7571\n",
      "Epoch 5/5\n",
      "947/947 [==============================] - 876s 925ms/step - loss: 2.6170 - accuracy: 0.9175 - val_loss: 2.9994 - val_accuracy: 0.7810\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "tf.keras.backend.clear_session()\n",
    "history=model_0.fit([train_inp,train_mask],train_label,batch_size=8,epochs=5,validation_data=([val_inp,val_mask],val_label),callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_9 (InputLayer)           [(None, 4096)]       0           []                               \n",
      "                                                                                                  \n",
      " input_10 (InputLayer)          [(None, 4096)]       0           []                               \n",
      "                                                                                                  \n",
      " tf_longformer_model (TFLongfor  TFLongformerBaseMod  148659456  ['input_9[0][0]',                \n",
      " merModel)                      elOutputWithPooling               'input_10[0][0]']               \n",
      "                                (last_hidden_state=                                               \n",
      "                                (None, 4096, 768),                                                \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 hidden_states=None                                               \n",
      "                                , attentions=None,                                                \n",
      "                                global_attentions=N                                               \n",
      "                                one)                                                              \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_4 (Sl  (None, 768)         0           ['tf_longformer_model[4][0]']    \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_4[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dropout_53 (Dropout)           (None, 512)          0           ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 15)           7695        ['dropout_53[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 149,060,879\n",
      "Trainable params: 149,060,879\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Accuracy: 0.780952380952381\n",
      "Weighted F1: 0.7818874773671741\n",
      "Micro F1: 0.780952380952381\n",
      "Weighted Precision: 0.7919121682815444\n",
      "Micro Precision: 0.780952380952381\n",
      "Weighted Recall: 0.780952380952381\n",
      "Micro Recall: 0.780952380952381\n"
     ]
    }
   ],
   "source": [
    "pred_labels=[]\n",
    "\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "accuracy = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "\n",
    "model_saved= create_model()\n",
    "model_saved.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])\n",
    "model_saved.load_weights('LSM/legallongformer-3-15labels.h5')\n",
    "\n",
    "for i in range(0,len(val_inp)):\n",
    "    pred=model_saved.predict([val_inp[i].reshape(1,4096),val_mask[i].reshape(1,4096)],batch_size=8)\n",
    "    pred_label = pred.argmax(axis=1)\n",
    "    pred_labels.append(pred_label)\n",
    "accuracy=accuracy_score(val_label, pred_labels)\n",
    "print(\"Accuracy: \"+str(accuracy))\n",
    "total_accuracy=total_accuracy+accuracy\n",
    "\n",
    "weighted_f1=f1_score(val_label,pred_labels, average='weighted')\n",
    "print(\"Weighted F1: \"+ str(weighted_f1))\n",
    "total_weighted_f1=total_weighted_f1+weighted_f1\n",
    "micro_f1=f1_score(val_label,pred_labels, average='micro')\n",
    "print(\"Micro F1: \"+ str(micro_f1))\n",
    "total_micro_f1=total_micro_f1+micro_f1\n",
    "\n",
    "weighted_precision=precision_score(val_label, pred_labels, average='weighted')\n",
    "print(\"Weighted Precision: \" + str(weighted_precision))\n",
    "total_weighted_precision=total_weighted_precision+weighted_precision\n",
    "micro_precision=precision_score(val_label, pred_labels, average='micro')\n",
    "print(\"Micro Precision: \" + str(micro_precision))\n",
    "total_micro_precision=total_micro_precision+micro_precision\n",
    "\n",
    "weighted_recall=recall_score(val_label, pred_labels, average='weighted')\n",
    "print(\"Weighted Recall: \" + str(weighted_recall))\n",
    "total_weighted_recall=total_weighted_recall+weighted_recall\n",
    "micro_recall=recall_score(val_label, pred_labels, average='micro')\n",
    "print(\"Micro Recall: \" + str(micro_recall))\n",
    "total_micro_recall=total_micro_recall+micro_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'supreme_court', 'site_url': 'http://caselaw.findlaw.com/court/us-supreme-court', 'description': 'Collection of ~8.4k decisions issued by the U.S. Supreme Court between November 1946 and June 2016.'}\n",
      "[-1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "{'-1': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7, '8': 8, '9': 9, '10': 10, '11': 11, '12': 12, '13': 13, '14': 14}\n",
      "8419\n",
      "Average Length 2920.3854377004395\n",
      "Found 8419 texts.\n",
      "Found 15 labels.\n",
      "                                                   text  label\n",
      "0     [ Halliburton Oil Well Cementing Co. v. Walker...      8\n",
      "1     Rehearing Denied Dec. 16, 1946. See . Mr.Claud...      1\n",
      "2     Rehearing Denied Dec. 16, 1946 See . Appeal fr...      8\n",
      "3     Mr. Walter J. Cummings, Jr., of Washington, D....      2\n",
      "4     Mr.A. Devitt Vaneck, of Washington, D.C., for ...      8\n",
      "...                                                 ...    ...\n",
      "8414  Opinion reported: Ante, p. 88. DECREE 1. It is...     11\n",
      "8415  In this dispute between Utah and the United St...     10\n",
      "8416  The United States, to the exclusion of defenda...     10\n",
      "8417  Louisiana's exception to the portion of the Sp...     11\n",
      "8418  To resolve a dispute over the ownership of cer...      9\n",
      "\n",
      "[8419 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at saibo/legal-longformer-base-4096 were not used when initializing TFLongformerModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFLongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFLongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFLongformerModel were initialized from the model checkpoint at saibo/legal-longformer-base-4096.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFLongformerModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 4096)]       0           []                               \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)           [(None, 4096)]       0           []                               \n",
      "                                                                                                  \n",
      " tf_longformer_model (TFLongfor  TFLongformerBaseMod  148659456  ['input_3[0][0]',                \n",
      " merModel)                      elOutputWithPooling               'input_4[0][0]']                \n",
      "                                (last_hidden_state=                                               \n",
      "                                (None, 4096, 768),                                                \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 hidden_states=None                                               \n",
      "                                , attentions=None,                                                \n",
      "                                global_attentions=N                                               \n",
      "                                one)                                                              \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_longformer_model[0][0]']    \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dropout_50 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 15)           7695        ['dropout_50[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 149,060,879\n",
      "Trainable params: 149,060,879\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7577, 4096)\n",
      "(7577, 4096)\n",
      "(7577,)\n",
      "(842, 4096)\n",
      "(842, 4096)\n",
      "(842,)\n",
      "(7576, 4096)\n",
      "(7576, 4096)\n",
      "(7576,)\n",
      "(840, 4096)\n",
      "(840, 4096)\n",
      "(840,)\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "    sc = SupremeCourt()\n",
    "    print(sc.info)\n",
    "    sc.download()\n",
    "\n",
    "    texts = []  # list of text samples\n",
    "    labels_index = {}  # dictionary mapping label name to numeric id\n",
    "    labels = []  # list of label ids\n",
    "\n",
    "    issue_codes = list(sc.issue_area_codes.keys()) # 15 labels\n",
    "    print(issue_codes)\n",
    "    issue_codes.sort()\n",
    "    issue_codes = [str(ic) for ic in issue_codes]\n",
    "\n",
    "    labels_index = dict(zip(issue_codes, np.arange(len(issue_codes))))\n",
    "    print(labels_index)\n",
    "    count=0\n",
    "\n",
    "    for record in sc.records():\n",
    "\n",
    "            count=count+1\n",
    "            if record[1]['issue'] == None: # some cases have None as an issue\n",
    "                labels.append(labels_index['-1'])\n",
    "            else:\n",
    "                labels.append(labels_index[record[1]['issue'][:-4]])\n",
    "\n",
    "            new_sen=record[0].split(\"Footnotes\")[0]\n",
    "            new_new_sen=new_sen.split()\n",
    "            if len(new_new_sen) >= 4096:\n",
    "              new_new_sen=new_new_sen[0:4096]\n",
    "\n",
    "            elif len(new_new_sen) < 4096:\n",
    "              new_new_sen=new_new_sen[0:len(new_new_sen)]\n",
    "\n",
    "            new_new_sen=' '.join(new_new_sen)\n",
    "\n",
    "            texts.append(new_new_sen)\n",
    "\n",
    "    len_list = [len(ele.split()) for ele in texts]\n",
    "\n",
    "    #print(labels)\n",
    "    print(len(labels))\n",
    "\n",
    "    res = 0 if len(len_list) == 0 else (float(sum(len_list)) / len(len_list))\n",
    "    print(\"Average Length %s\" % res) \n",
    "    print('Found %s texts.' % len(texts))\n",
    "    print('Found %s labels.' % len(labels_index))\n",
    "    temp_file = open(\"labels_sc.txt\", \"r\")\n",
    "\n",
    "    data = temp_file.read()\n",
    "    label_list = data.split(\"\\n\")\n",
    "    #print(label_list)\n",
    "    label_list = label_list[0:-1]\n",
    "    #print(label_list)\n",
    "    label_list = [int(i) for i in label_list]\n",
    "    summarized_data = pd.DataFrame(texts,\n",
    "                   columns =['text'])\n",
    "    summarized_data['label'] = label_list\n",
    "    print(summarized_data)\n",
    "    #gc.collect()\n",
    "    #tf.keras.backend.clear_session()\n",
    "    dbert_tokenizer = LongformerTokenizer.from_pretrained('saibo/legal-longformer-base-4096')\n",
    "    dbert_model = TFLongformerModel.from_pretrained('saibo/legal-longformer-base-4096')\n",
    "    max_len=4096\n",
    "    sentences=summarized_data['text']\n",
    "    labels=summarized_data['label']\n",
    "    len(sentences),len(labels)\n",
    "    model_0=create_model()\n",
    "    input_ids=[]\n",
    "    attention_masks=[]\n",
    "\n",
    "    for sent in sentences:\n",
    "      dbert_inps=dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n",
    "      input_ids.append(dbert_inps['input_ids'])\n",
    "      attention_masks.append(dbert_inps['attention_mask'])\n",
    "    input_ids=np.asarray(input_ids)\n",
    "\n",
    "    attention_masks=np.array(attention_masks)\n",
    "    labels=np.array(labels)\n",
    "    train_inp,val_inp,train_label,val_label,train_mask,val_mask=train_test_split(input_ids,labels,attention_masks,test_size=0.1,random_state=42)\n",
    "    print(train_inp.shape)\n",
    "    print(train_mask.shape)\n",
    "    print(train_label.shape)\n",
    "    print(val_inp.shape)\n",
    "    print(val_mask.shape)\n",
    "    print(val_label.shape)\n",
    "    train_inp=train_inp[0:7576]\n",
    "    train_mask=train_mask[0:7576]\n",
    "    train_label=train_label[0:7576]\n",
    "\n",
    "    val_inp=val_inp[0:840]\n",
    "    val_label=val_label[0:840]\n",
    "    val_mask=val_mask[0:840]\n",
    "    print(train_inp.shape)\n",
    "    print(train_mask.shape)\n",
    "    print(train_label.shape)\n",
    "    print(val_inp.shape)\n",
    "    print(val_mask.shape)\n",
    "    print(val_label.shape)\n",
    "    log_dir='dbert_model'\n",
    "\n",
    "    model_save_path='LSM/legallongformer-4-15labels.h5'\n",
    "\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    accuracy = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "    callbacks= [tf.keras.callbacks.ModelCheckpoint(filepath=model_save_path,monitor='val_accuracy',mode='max',save_best_only=True,save_weights_only=True),keras.callbacks.TensorBoard(log_dir=log_dir)]\n",
    "    model_0.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-14 14:35:33.099568: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:766] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_9\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"PrefetchDataset/_8\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_slice_batch_indices_737240\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\024FlatMapDataset:22012\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 8\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT64\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_longformer_model/longformer/pooler/dense/kernel:0', 'tf_longformer_model/longformer/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_longformer_model/longformer/pooler/dense/kernel:0', 'tf_longformer_model/longformer/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_longformer_model/longformer/pooler/dense/kernel:0', 'tf_longformer_model/longformer/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_longformer_model/longformer/pooler/dense/kernel:0', 'tf_longformer_model/longformer/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "947/947 [==============================] - ETA: 0s - loss: 6.9106 - accuracy: 0.6364"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-14 14:51:52.563123: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:766] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_9\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"PrefetchDataset/_8\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_slice_batch_indices_876380\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\024FlatMapDataset:22037\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 8\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT64\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "947/947 [==============================] - 1039s 947ms/step - loss: 6.9106 - accuracy: 0.6364 - val_loss: 5.9838 - val_accuracy: 0.7369\n",
      "Epoch 2/5\n",
      "947/947 [==============================] - 870s 918ms/step - loss: 5.3999 - accuracy: 0.7839 - val_loss: 5.0074 - val_accuracy: 0.7679\n",
      "Epoch 3/5\n",
      "947/947 [==============================] - 870s 918ms/step - loss: 4.3638 - accuracy: 0.8355 - val_loss: 4.2023 - val_accuracy: 0.7750\n",
      "Epoch 4/5\n",
      "947/947 [==============================] - 869s 918ms/step - loss: 3.4886 - accuracy: 0.8850 - val_loss: 3.5737 - val_accuracy: 0.7786\n",
      "Epoch 5/5\n",
      "947/947 [==============================] - 870s 918ms/step - loss: 2.7679 - accuracy: 0.9204 - val_loss: 3.1829 - val_accuracy: 0.7643\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "tf.keras.backend.clear_session()\n",
    "history=model_0.fit([train_inp,train_mask],train_label,batch_size=8,epochs=5,validation_data=([val_inp,val_mask],val_label),callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_11 (InputLayer)          [(None, 4096)]       0           []                               \n",
      "                                                                                                  \n",
      " input_12 (InputLayer)          [(None, 4096)]       0           []                               \n",
      "                                                                                                  \n",
      " tf_longformer_model (TFLongfor  TFLongformerBaseMod  148659456  ['input_11[0][0]',               \n",
      " merModel)                      elOutputWithPooling               'input_12[0][0]']               \n",
      "                                (last_hidden_state=                                               \n",
      "                                (None, 4096, 768),                                                \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 hidden_states=None                                               \n",
      "                                , attentions=None,                                                \n",
      "                                global_attentions=N                                               \n",
      "                                one)                                                              \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_5 (Sl  (None, 768)         0           ['tf_longformer_model[5][0]']    \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 512)          393728      ['tf.__operators__.getitem_5[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dropout_54 (Dropout)           (None, 512)          0           ['dense_10[0][0]']               \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 15)           7695        ['dropout_54[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 149,060,879\n",
      "Trainable params: 149,060,879\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "pred_labels=[]\n",
    "\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "accuracy = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "\n",
    "model_saved= create_model()\n",
    "model_saved.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])\n",
    "model_saved.load_weights('LSM/legallongformer-4-15labels.h5')\n",
    "\n",
    "for i in range(0,len(val_inp)):\n",
    "    pred=model_saved.predict([val_inp[i].reshape(1,4096),val_mask[i].reshape(1,4096)],batch_size=8)\n",
    "    pred_label = pred.argmax(axis=1)\n",
    "    pred_labels.append(pred_label)\n",
    "accuracy=accuracy_score(val_label, pred_labels)\n",
    "print(\"Accuracy: \"+str(accuracy))\n",
    "total_accuracy=total_accuracy+accuracy\n",
    "\n",
    "weighted_f1=f1_score(val_label,pred_labels, average='weighted')\n",
    "print(\"Weighted F1: \"+ str(weighted_f1))\n",
    "total_weighted_f1=total_weighted_f1+weighted_f1\n",
    "micro_f1=f1_score(val_label,pred_labels, average='micro')\n",
    "print(\"Micro F1: \"+ str(micro_f1))\n",
    "total_micro_f1=total_micro_f1+micro_f1\n",
    "\n",
    "weighted_precision=precision_score(val_label, pred_labels, average='weighted')\n",
    "print(\"Weighted Precision: \" + str(weighted_precision))\n",
    "total_weighted_precision=total_weighted_precision+weighted_precision\n",
    "micro_precision=precision_score(val_label, pred_labels, average='micro')\n",
    "print(\"Micro Precision: \" + str(micro_precision))\n",
    "total_micro_precision=total_micro_precision+micro_precision\n",
    "\n",
    "weighted_recall=recall_score(val_label, pred_labels, average='weighted')\n",
    "print(\"Weighted Recall: \" + str(weighted_recall))\n",
    "total_weighted_recall=total_weighted_recall+weighted_recall\n",
    "micro_recall=recall_score(val_label, pred_labels, average='micro')\n",
    "print(\"Micro Recall: \" + str(micro_recall))\n",
    "total_micro_recall=total_micro_recall+micro_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average Accuracy: \"+str(total_accuracy/5))\n",
    "print(\"Average Weighted F1: \"+str(total_weighted_f1/5))\n",
    "print(\"Average Micro F1: \"+str(total_micro_f1/5))\n",
    "print(\"Average Weighted Precision: \"+str(total_weighted_precision/5))\n",
    "print(\"Average Micro Precision: \"+str(total_micro_precision/5))\n",
    "print(\"Average Weighted Recall: \"+str(total_weighted_recall/5))\n",
    "print(\"Average Micro Recall: \"+str(total_micro_recall/5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "background_execution": "on",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Best-512_0:512_15labels.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "WOL_ENV",
   "language": "python",
   "name": "wol_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
