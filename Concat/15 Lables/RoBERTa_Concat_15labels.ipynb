{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mj3gUkvvEZW2"
   },
   "source": [
    "### <font color='blue'>Import all packages</font> ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cVqcG_Q_FX4T",
    "outputId": "6b2153b2-2ab0-4d55-b72a-466a4d810567"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "#drive.flush_and_unmount()\n",
    "drive.mount('/content/drive')\n",
    "#drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AwA6C92yJyaP",
    "outputId": "7977b2e6-8e43-40c5-e06e-7b01120112b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (4.19.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: requests in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from transformers) (2022.4.24)\n",
      "Requirement already satisfied: numpy>=1.17 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from transformers) (1.21.6)\n",
      "Requirement already satisfied: filelock in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from transformers) (3.7.0)\n",
      "Requirement already satisfied: importlib-metadata in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from transformers) (4.11.4)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: zipp>=0.5 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.8.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from requests->transformers) (2022.5.18.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from requests->transformers) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: sentencepiece in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (0.1.96)\n",
      "Requirement already satisfied: tensorflow==2.7.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (2.7.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorflow==2.7.0) (1.1.0)\n",
      "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorflow==2.7.0) (2.7.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorflow==2.7.0) (1.16.0)\n",
      "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorflow==2.7.0) (2.0)\n",
      "Requirement already satisfied: libclang>=9.0.1 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorflow==2.7.0) (14.0.1)\n",
      "Requirement already satisfied: tensorboard~=2.6 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorflow==2.7.0) (2.9.0)\n",
      "Requirement already satisfied: numpy>=1.14.5 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorflow==2.7.0) (1.21.6)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorflow==2.7.0) (3.3.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorflow==2.7.0) (3.19.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorflow==2.7.0) (1.14.1)\n",
      "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorflow==2.7.0) (0.4.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorflow==2.7.0) (3.7.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorflow==2.7.0) (1.6.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.32.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorflow==2.7.0) (0.37.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorflow==2.7.0) (0.2.0)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorflow==2.7.0) (1.0.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorflow==2.7.0) (1.1.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorflow==2.7.0) (1.46.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorflow==2.7.0) (3.10.0.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorflow==2.7.0) (2.7.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorflow==2.7.0) (0.26.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow==2.7.0) (61.2.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow==2.7.0) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow==2.7.0) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow==2.7.0) (2.1.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow==2.7.0) (2.6.6)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow==2.7.0) (0.4.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow==2.7.0) (2.27.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow==2.7.0) (3.3.7)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0) (4.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0) (5.2.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.7.0) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow==2.7.0) (4.11.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow==2.7.0) (3.8.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0) (0.4.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0) (2022.5.18.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0) (2.0.12)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.7.0) (3.2.0)\n",
      "Requirement already satisfied: stanza in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (1.4.0)\n",
      "Requirement already satisfied: emoji in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from stanza) (1.7.0)\n",
      "Requirement already satisfied: torch>=1.3.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from stanza) (1.11.0)\n",
      "Requirement already satisfied: six in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from stanza) (1.16.0)\n",
      "Requirement already satisfied: protobuf in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from stanza) (3.19.0)\n",
      "Requirement already satisfied: tqdm in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from stanza) (4.64.0)\n",
      "Requirement already satisfied: transformers in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from stanza) (4.19.2)\n",
      "Requirement already satisfied: numpy in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from stanza) (1.21.6)\n",
      "Requirement already satisfied: requests in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from stanza) (2.27.1)\n",
      "Requirement already satisfied: typing-extensions in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from torch>=1.3.0->stanza) (3.10.0.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from requests->stanza) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from requests->stanza) (2022.5.18.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from requests->stanza) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from requests->stanza) (1.26.9)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from transformers->stanza) (0.7.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from transformers->stanza) (2022.4.24)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from transformers->stanza) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from transformers->stanza) (21.3)\n",
      "Requirement already satisfied: filelock in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from transformers->stanza) (3.7.0)\n",
      "Requirement already satisfied: importlib-metadata in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from transformers->stanza) (4.11.4)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from transformers->stanza) (0.12.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from packaging>=20.0->transformers->stanza) (3.0.9)\n",
      "Requirement already satisfied: zipp>=0.5 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from importlib-metadata->transformers->stanza) (3.8.0)\n",
      "Requirement already satisfied: transformers in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (4.19.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from transformers) (2022.4.24)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from transformers) (1.21.6)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: requests in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: importlib-metadata in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from transformers) (4.11.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: filelock in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from transformers) (3.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: zipp>=0.5 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.8.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from requests->transformers) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from requests->transformers) (2022.5.18.1)\n",
      "Requirement already satisfied: tensorflow-addons in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (0.17.0)\n",
      "Requirement already satisfied: packaging in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorflow-addons) (21.3)\n",
      "Requirement already satisfied: typeguard>=2.7 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from tensorflow-addons) (2.13.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from packaging->tensorflow-addons) (3.0.9)\n",
      "Requirement already satisfied: nltk in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (3.7)\n",
      "Requirement already satisfied: tqdm in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from nltk) (4.64.0)\n",
      "Requirement already satisfied: click in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from nltk) (2022.4.24)\n",
      "Requirement already satisfied: importlib-metadata in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from click->nltk) (4.11.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from importlib-metadata->click->nltk) (3.8.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from importlib-metadata->click->nltk) (3.10.0.2)\n",
      "Requirement already satisfied: textacy in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (0.11.0)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from textacy) (1.7.3)\n",
      "Requirement already satisfied: pyphen>=0.10.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from textacy) (0.12.0)\n",
      "Requirement already satisfied: requests>=2.10.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from textacy) (2.27.1)\n",
      "Requirement already satisfied: cachetools>=4.0.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from textacy) (5.2.0)\n",
      "Requirement already satisfied: joblib>=0.13.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from textacy) (1.1.0)\n",
      "Requirement already satisfied: networkx>=2.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from textacy) (2.6.3)\n",
      "Requirement already satisfied: tqdm>=4.19.6 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from textacy) (4.64.0)\n",
      "Requirement already satisfied: cytoolz>=0.10.1 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from textacy) (0.11.2)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from textacy) (1.21.6)\n",
      "Requirement already satisfied: scikit-learn>=0.19.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from textacy) (1.0.2)\n",
      "Requirement already satisfied: spacy>=3.0.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from textacy) (3.3.0)\n",
      "Requirement already satisfied: jellyfish>=0.8.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from textacy) (0.9.0)\n",
      "Requirement already satisfied: toolz>=0.8.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from cytoolz>=0.10.1->textacy) (0.11.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from requests>=2.10.0->textacy) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from requests>=2.10.0->textacy) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from requests>=2.10.0->textacy) (2022.5.18.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from requests>=2.10.0->textacy) (1.26.9)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from scikit-learn>=0.19.0->textacy) (3.1.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (1.8.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (3.0.9)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (3.0.6)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (1.0.2)\n",
      "Requirement already satisfied: jinja2 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (3.1.2)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (0.4.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (2.4.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (0.9.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (3.3.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (0.7.7)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (2.0.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (2.0.7)\n",
      "Requirement already satisfied: setuptools in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (61.2.0)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (0.6.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (1.0.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (21.3)\n",
      "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (3.10.0.2)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (8.0.16)\n",
      "Requirement already satisfied: zipp>=0.5 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from catalogue<2.1.0,>=2.0.6->spacy>=3.0.0->textacy) (3.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from packaging>=20.0->spacy>=3.0.0->textacy) (3.0.9)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from pathy>=0.3.5->spacy>=3.0.0->textacy) (5.2.1)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from typer<0.5.0,>=0.3.0->spacy>=3.0.0->textacy) (8.1.3)\n",
      "Requirement already satisfied: importlib-metadata in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from click<9.0.0,>=7.1.1->typer<0.5.0,>=0.3.0->spacy>=3.0.0->textacy) (4.11.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages (from jinja2->spacy>=3.0.0->textacy) (2.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install sentencepiece\n",
    "!pip install tensorflow==2.7.0\n",
    "!pip install stanza\n",
    "!pip install transformers\n",
    "!pip install tensorflow-addons\n",
    "!pip install nltk\n",
    "!pip install textacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9jV9qKCQ3qpl",
    "outputId": "cf68b4eb-84a2-44cd-913b-8e666212d3e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "DC59AD4KEZW7"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from textacy.datasets.supreme_court import SupremeCourt\n",
    "import numpy as np\n",
    "import re\n",
    "import unicodedata\n",
    "import statistics\n",
    "import nltk\n",
    "#from transformers import pipeline\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense,Dropout, Input, BatchNormalization, Concatenate, Flatten\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from sklearn.metrics import confusion_matrix,f1_score,classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras import regularizers\n",
    "#from transformers import *\n",
    "from transformers import BertTokenizer, TFBertModel, BertConfig,TFDistilBertModel,DistilBertTokenizer,DistilBertConfig\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "import numpy as np\n",
    "import gc\n",
    "import math\n",
    "import json\n",
    "import stanza\n",
    "from tensorflow.keras import *\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import *\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import TFRobertaModel,RobertaTokenizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.initializers import RandomUniform\n",
    "\n",
    "from numpy.random import seed\n",
    "import random as python_random\n",
    "import os\n",
    "import sys\n",
    "\n",
    "np.random.seed(1)\n",
    "python_random.seed(1)\n",
    "tf.random.set_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vccKXcLYEZW-"
   },
   "source": [
    "### <font color='blue'> Preprocessing and cleaning functions </font> ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OzfozpOUBkOX"
   },
   "outputs": [],
   "source": [
    "!cp \"/content/drive/My Drive/labels_sc.txt\" \"./labels_sc.txt\"\n",
    "!cp \"/content/drive/My Drive/labels_sc_279.txt\" \"./labels_sc_279.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cBXPuBPVmfH9",
    "outputId": "f8dd167d-b865-4d14-d4a1-0ef2f5718f30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'supreme_court', 'site_url': 'http://caselaw.findlaw.com/court/us-supreme-court', 'description': 'Collection of ~8.4k decisions issued by the U.S. Supreme Court between November 1946 and June 2016.'}\n",
      "[-1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "{'-1': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7, '8': 8, '9': 9, '10': 10, '11': 11, '12': 12, '13': 13, '14': 14}\n",
      "[8, 1, 8, 2, 8, 8, 8, 9, 7, 8, 1, 2, 1, 1, 8, 4, 8, 8, 12, 3, 3, 7, 3, 12, 1, 8, 8, 8, 8, 8, 8, 8, 1, 9, 5, 9, 9, 9, 11, 8, 8, 8, 4, 8, 8, 8, 8, 1, 3, 9, 3, 7, 1, 2, 9, 9, 7, 8, 8, 1, 10, 7, 8, 8, 9, 8, 7, 9, 9, 12, 7, 2, 8, 1, 11, 11, 1, 7, 7, 12, 1, 9, 8, 10, 12, 7, 8, 2, 8, 9, 9, 1, 8, 9, 1, 7, 12, 10, 10, 10, 8, 3, 7, 9, 8, 9, 1, 8, 8, 2, 7, 2, 9, 9, 11, 8, 8, 12, 12, 7, 8, 12, 4, 9, 3, 1, 12, 1, 1, 8, 8, 3, 8, 8, 8, 8, 9, 1, 8, 8, 10, 1, 8, 2, 8, 8, 7, 3, 8, 2, 4, 4, 9, 8, 10, 12, 12, 1, 1, 9, 1, 1, 1, 8, 2, 2, 8, 1, 1, 2, 2, 1, 2, 8, 1, 9, 9, 8, 8, 4, 2, 9, 9, 8, 3, 4, 3, 1, 8, 8, 2, 1, 9, 7, 8, 8, 1, 12, 3, 8, 2, 4, 2, 9, 12, 1, 4, 1, 8, 8, 8, 2, 2, 8, 9, 8, 8, 8, 10, 9, 8, 7, 9, 1, 1, 9, 4, 2, 4, 2, 2, 1, 7, 8, 11, 11, 3, 9, 2, 4, 8, 9, 1, 8, 1, 1, 4, 9, 1, 1, 8, 8, 2, 1, 8, 4, 2, 8, 9, 8, 8, 2, 8, 8, 8, 7, 1, 1, 1, 2, 1, 1, 8, 7, 8, 8, 12, 2, 12, 12, 8, 10, 12, 8, 3, 3, 12, 10, 1, 8, 12, 1, 8, 8, 2, 8, 4, 7, 8, 7, 10, 8, 10, 9, 8, 12, 12, 1, 8, 8, 3, 8, 8, 8, 8, 8, 1, 9, 8, 11, 1, 1, 1, 9, 8, 1, 9, 2, 3, 11, 8, 9, 9, 9, 2, 1, 8, 8, 9, 7, 1, 4, 9, 4, 8, 8, 4, 8, 12, 9, 4, 8, 2, 10, 10, 10, 8, 9, 9, 8, 8, 12, 7, 1, 8, 8, 8, 4, 1, 1, 1, 1, 1, 1, 8, 1, 9, 8, 9, 9, 4, 8, 12, 9, 8, 8, 2, 8, 8, 8, 6, 9, 8, 3, 7, 8, 8, 4, 12, 8, 8, 9, 12, 12, 9, 8, 2, 9, 2, 3, 1, 12, 8, 10, 9, 9, 9, 10, 10, 3, 8, 12, 1, 4, 2, 1, 10, 8, 2, 8, 4, 8, 9, 1, 9, 9, 10, 10, 1, 4, 9, 2, 4, 9, 1, 1, 3, 10, 3, 3, 8, 7, 3, 8, 9, 9, 12, 4, 8, 12, 2, 2, 4, 1, 9, 9, 4, 1, 4, 2, 8, 12, 2, 3, 10, 10, 9, 8, 9, 9, 1, 12, 8, 8, 8, 12, 4, 1, 8, 8, 1, 9, 8, 8, 2, 1, 8, 9, 8, 3, 3, 3, 1, 8, 8, 9, 1, 10, 9, 9, 9, 9, 5, 9, 9, 8, 8, 8, 8, 8, 8, 8, 8, 11, 12, 8, 8, 1, 8, 9, 11, 2, 2, 2, 2, 3, 1, 2, 2, 8, 2, 4, 9, 1, 2, 9, 8, 2, 8, 9, 9, 3, 10, 9, 9, 2, 8, 9, 8, 12, 12, 1, 3, 8, 8, 8, 2, 7, 7, 7, 7, 3, 9, 1, 9, 8, 9, 9, 1, 1, 1, 2, 9, 9, 9, 11, 1, 8, 8, 9, 1, 9, 8, 8, 8, 1, 1, 8, 7, 1, 1, 8, 8, 9, 4, 4, 8, 2, 2, 8, 8, 8, 8, 8, 8, 11, 8, 2, 9, 4, 9, 3, 9, 9, 1, 3, 9, 3, 1, 12, 8, 9, 12, 1, 8, 4, 2, 1, 4, 8, 3, 3, 8, 2, 8, 9, 7, 8, 8, 8, 5, 8, 3, 9, 8, 8, 13, 12, 1, 1, 2, 8, 4, 1, 9, 9, 12, 8, 9, 12, 9, 1, 9, 9, 9, 9, 3, 2, 9, 9, 4, 8, 12, 2, 4, 9, 3, 1, 9, 7, 8, 9, 9, 8, 4, 8, 8, 7, 9, 10, 3, 8, 8, 8, 1, 1, 1, 1, 8, 8, 4, 1, 10, 1, 5, 7, 7, 1, 8, 9, 3, 7, 2, 7, 7, 2, 4, 8, 12, 7, 4, 2, 9, 9, 12, 6, 10, 8, 2, 4, 12, 9, 9, 3, 8, 8, 1, 2, 10, 9, 9, 8, 4, 12, 2, 1, 8, 8, 8, 12, 10, 10, 9, 3, 8, 8, 9, 2, 8, 10, 1, 1, 1, 1, 2, 1, 1, 1, 1, 9, 8, 12, 9, 4, 8, 8, 9, 1, 9, 3, 9, 8, 8, 1, 7, 7, 10, 1, 8, 8, 1, 9, 8, 10, 3, 1, 7, 1, 8, 8, 12, 8, 8, 1, 8, 7, 1, 7, 7, 8, 2, 1, 8, 8, 2, 10, 8, 8, 8, 8, 8, 10, 1, 8, 8, 12, 8, 3, 3, 2, 2, 2, 10, 8, 8, 8, 2, 9, 1, 8, 9, 3, 2, 8, 10, 8, 6, 1, 1, 8, 4, 1, 9, 10, 8, 1, 7, 1, 2, 8, 1, 1, 1, 12, 1, 9, 12, 8, 12, 12, 12, 8, 8, 12, 4, 8, 8, 8, 8, 9, 9, 1, 3, 3, 3, 3, 1, 12, 12, 9, 10, 8, 8, 1, 9, 2, 2, 13, 9, 8, 9, 2, 1, 9, 1, 8, 8, 8, 4, 8, 1, 1, 1, 12, 12, 7, 2, 2, 2, 8, 3, 8, 9, 2, 10, 7, 8, 9, 2, 1, 2, 12, 12, 8, 8, 9, 2, 2, 9, 11, 1, 8, 1, 10, 9, 2, 1, 4, 7, 7, 7, 7, 7, 12, 8, 8, 8, 1, 1, 10, 1, 12, 1, 8, 2, 1, 1, 12, 8, 7, 9, 12, 8, 9, 3, 9, 8, 8, 8, 8, 3, 11, 2, 2, 9, 8, 8, 10, 8, 2, 7, 3, 1, 4, 7, 8, 8, 1, 8, 3, 7, 12, 8, 10, 9, 9, 8, 8, 2, 8, 9, 9, 1, 2, 8, 8, 9, 8, 3, 8, 1, 8, 10, 9, 8, 9, 9, 12, 4, 4, 8, 9, 9, 8, 2, 10, 1, 2, 8, 9, 1, 9, 9, 9, 7, 12, 12, 8, 1, 1, 1, 1, 8, 3, 1, 1, 8, 1, 8, 8, 7, 8, 8, 8, 8, 3, 2, 2, 10, 10, 10, 7, 8, 1, 2, 12, 7, 9, 8, 7, 8, 12, 2, 8, 9, 2, 6, 6, 7, 9, 8, 1, 8, 9, 8, 1, 12, 1, 2, 8, 7, 7, 7, 8, 2, 2, 8, 1, 2, 2, 9, 9, 1, 8, 8, 4, 3, 3, 1, 6, 3, 3, 12, 3, 8, 9, 1, 4, 3, 1, 8, 3, 9, 2, 8, 2, 8, 8, 8, 1, 1, 1, 9, 9, 8, 1, 9, 8, 1, 1, 3, 10, 8, 1, 1, 3, 9, 1, 4, 4, 1, 8, 9, 9, 2, 0, 0, 1, 8, 3, 1, 8, 8, 9, 8, 8, 1, 1, 8, 9, 8, 8, 8, 7, 9, 8, 8, 8, 10, 9, 8, 1, 2, 6, 1, 9, 9, 8, 12, 12, 12, 8, 8, 2, 8, 1, 2, 2, 2, 1, 9, 8, 2, 12, 2, 8, 12, 8, 9, 8, 8, 9, 7, 1, 1, 1, 1, 1, 8, 8, 1, 8, 8, 1, 1, 3, 2, 8, 8, 9, 10, 10, 2, 2, 1, 9, 2, 9, 9, 4, 12, 12, 12, 10, 7, 3, 3, 4, 2, 2, 9, 2, 8, 4, 2, 4, 1, 10, 9, 7, 8, 7, 1, 1, 3, 3, 1, 1, 3, 3, 3, 1, 1, 1, 1, 8, 2, 3, 1, 1, 2, 8, 8, 12, 8, 8, 8, 8, 11, 9, 1, 8, 9, 2, 8, 8, 8, 3, 9, 1, 9, 2, 7, 2, 8, 2, 8, 10, 8, 1, 10, 1, 1, 9, 9, 8, 8, 1, 8, 8, 8, 12, 8, 8, 8, 1, 8, 8, 8, 1, 9, 1, 1, 8, 1, 8, 9, 8, 2, 12, 9, 9, 0, 1, 8, 8, 1, 8, 12, 8, 8, 10, 8, 8, 8, 7, 8, 1, 8, 7, 3, 10, 1, 8, 9, 1, 8, 8, 8, 10, 1, 10, 3, 9, 1, 8, 9, 2, 8, 3, 3, 9, 9, 7, 9, 1, 1, 9, 2, 1, 1, 1, 7, 1, 1, 8, 8, 1, 1, 8, 1, 8, 3, 12, 9, 3, 3, 8, 8, 8, 8, 3, 1, 3, 3, 1, 11, 0, 8, 8, 7, 8, 12, 1, 8, 9, 8, 9, 8, 8, 3, 8, 8, 1, 1, 1, 9, 2, 2, 2, 8, 7, 12, 8, 8, 9, 10, 10, 7, 8, 1, 9, 8, 7, 3, 1, 3, 8, 2, 2, 3, 9, 8, 4, 4, 8, 9, 2, 1, 1, 7, 8, 9, 9, 7, 8, 7, 7, 8, 2, 2, 8, 4, 9, 7, 10, 0, 9, 8, 3, 7, 8, 1, 1, 8, 9, 9, 2, 2, 10, 1, 9, 10, 10, 10, 8, 3, 2, 12, 9, 9, 10, 12, 9, 12, 12, 9, 1, 2, 4, 12, 12, 7, 8, 9, 7, 7, 7, 3, 9, 8, 9, 1, 12, 8, 9, 4, 1, 3, 12, 12, 12, 12, 8, 8, 2, 1, 1, 2, 1, 1, 1, 12, 12, 8, 12, 2, 2, 12, 3, 3, 12, 8, 2, 8, 8, 12, 2, 1, 10, 3, 2, 8, 7, 1, 8, 1, 3, 7, 8, 9, 8, 3, 1, 1, 7, 8, 8, 9, 8, 2, 9, 2, 2, 9, 8, 1, 8, 8, 1, 3, 3, 1, 1, 10, 1, 2, 8, 1, 1, 1, 1, 9, 1, 4, 1, 7, 7, 7, 7, 2, 2, 8, 8, 12, 1, 9, 1, 7, 3, 3, 1, 8, 8, 10, 8, 9, 2, 9, 1, 3, 8, 8, 3, 12, 2, 8, 12, 2, 9, 1, 3, 3, 3, 3, 2, 8, 7, 9, 8, 3, 3, 1, 7, 8, 3, 1, 1, 12, 8, 9, 1, 2, 3, 8, 1, 1, 3, 3, 9, 1, 1, 1, 12, 1, 7, 3, 3, 1, 8, 8, 8, 1, 2, 4, 8, 1, 10, 2, 5, 3, 3, 12, 10, 9, 9, 12, 9, 0, 2, 8, 8, 9, 9, 9, 8, 1, 3, 1, 1, 4, 8, 1, 10, 8, 7, 2, 8, 2, 8, 4, 7, 8, 1, 9, 1, 9, 8, 2, 8, 2, 7, 9, 2, 2, 9, 1, 8, 12, 1, 8, 1, 4, 1, 9, 9, 1, 10, 12, 4, 8, 1, 7, 3, 9, 2, 12, 7, 8, 8, 2, 1, 12, 9, 8, 1, 2, 2, 8, 10, 2, 1, 7, 7, 7, 12, 3, 3, 8, 3, 8, 8, 3, 9, 8, 9, 1, 1, 8, 7, 9, 3, 3, 8, 1, 0, 9, 9, 9, 1, 8, 9, 9, 10, 1, 8, 7, 8, 8, 8, 9, 9, 8, 9, 10, 4, 9, 3, 7, 12, 1, 9, 9, 8, 9, 1, 9, 3, 1, 8, 4, 12, 10, 9, 8, 7, 10, 8, 12, 12, 3, 10, 8, 12, 2, 1, 2, 3, 9, 8, 8, 7, 1, 1, 2, 2, 1, 2, 2, 7, 1, 3, 9, 9, 3, 8, 8, 8, 8, 8, 7, 8, 8, 10, 8, 1, 8, 2, 8, 2, 2, 2, 2, 2, 1, 7, 12, 10, 1, 2, 8, 1, 4, 7, 8, 1, 12, 8, 7, 9, 2, 2, 2, 8, 1, 8, 1, 1, 0, 2, 2, 6, 1, 8, 2, 1, 8, 1, 1, 2, 2, 3, 2, 2, 10, 10, 9, 1, 7, 7, 8, 8, 1, 12, 10, 12, 3, 8, 8, 8, 3, 8, 3, 10, 2, 2, 2, 1, 2, 2, 1, 0, 1, 8, 9, 1, 1, 1, 2, 2, 2, 2, 2, 1, 1, 1, 1, 12, 9, 9, 1, 10, 11, 12, 9, 1, 1, 8, 8, 1, 2, 12, 8, 3, 7, 2, 10, 9, 7, 8, 1, 2, 2, 7, 9, 12, 2, 2, 1, 8, 9, 2, 3, 2, 2, 2, 9, 9, 8, 9, 2, 3, 8, 8, 9, 7, 3, 10, 8, 8, 9, 1, 1, 2, 8, 1, 10, 8, 8, 9, 7, 1, 7, 12, 8, 1, 7, 2, 1, 3, 9, 2, 8, 8, 1, 2, 3, 9, 3, 8, 9, 7, 7, 1, 10, 2, 8, 8, 8, 1, 8, 2, 8, 8, 1, 2, 8, 10, 2, 1, 0, 8, 3, 8, 10, 10, 12, 3, 8, 9, 3, 10, 8, 8, 8, 1, 8, 8, 2, 2, 2, 2, 2, 2, 1, 1, 10, 1, 10, 8, 2, 1, 2, 2, 8, 3, 3, 2, 2, 1, 8, 1, 3, 1, 9, 2, 2, 1, 3, 2, 2, 3, 2, 2, 3, 2, 1, 1, 2, 1, 2, 7, 8, 2, 1, 12, 12, 3, 1, 9, 9, 7, 8, 7, 2, 7, 9, 2, 2, 2, 8, 9, 7, 9, 2, 8, 9, 9, 2, 9, 1, 1, 9, 8, 3, 3, 8, 2, 7, 8, 11, 2, 8, 9, 1, 8, 3, 1, 2, 1, 2, 2, 2, 2, 8, 3, 9, 2, 10, 9, 9, 2, 7, 7, 7, 1, 3, 8, 8, 1, 1, 8, 7, 1, 8, 8, 9, 3, 3, 3, 2, 4, 8, 12, 8, 1, 12, 9, 2, 8, 2, 12, 2, 1, 3, 8, 12, 12, 11, 8, 10, 9, 2, 9, 4, 10, 1, 12, 12, 9, 3, 8, 9, 12, 9, 9, 2, 2, 2, 3, 5, 1, 1, 1, 7, 7, 8, 2, 2, 8, 2, 1, 7, 3, 2, 9, 8, 9, 1, 1, 9, 8, 9, 3, 3, 2, 9, 4, 9, 8, 8, 1, 8, 10, 2, 0, 3, 8, 8, 8, 9, 12, 9, 8, 2, 2, 8, 10, 9, 8, 2, 2, 4, 1, 9, 9, 8, 8, 10, 3, 1, 1, 2, 8, 7, 8, 2, 1, 1, 8, 12, 2, 9, 4, 8, 3, 3, 3, 8, 2, 12, 12, 8, 9, 12, 8, 2, 12, 7, 9, 2, 2, 1, 3, 1, 3, 11, 9, 8, 1, 10, 2, 12, 3, 8, 4, 1, 2, 8, 8, 3, 9, 3, 8, 7, 1, 1, 3, 1, 8, 2, 9, 8, 12, 1, 1, 1, 9, 9, 1, 8, 8, 8, 2, 2, 12, 3, 1, 1, 1, 9, 9, 3, 2, 1, 1, 9, 9, 1, 9, 9, 1, 9, 3, 9, 1, 8, 1, 9, 8, 3, 8, 8, 9, 9, 8, 2, 8, 1, 2, 2, 10, 1, 2, 1, 1, 9, 1, 9, 3, 7, 7, 2, 10, 3, 3, 1, 1, 9, 1, 2, 1, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 8, 8, 7, 1, 1, 8, 8, 2, 1, 12, 1, 8, 1, 9, 8, 8, 2, 1, 9, 9, 2, 2, 8, 1, 7, 7, 9, 8, 1, 6, 8, 8, 2, 2, 8, 3, 8, 3, 2, 3, 8, 1, 2, 2, 2, 8, 9, 9, 9, 12, 1, 9, 2, 1, 9, 2, 9, 9, 10, 8, 1, 1, 9, 2, 1, 7, 1, 3, 7, 1, 1, 1, 3, 8, 8, 8, 9, 9, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 0, 1, 1, 1, 2, 2, 3, 1, 1, 2, 3, 3, 3, 7, 3, 1, 9, 3, 9, 1, 4, 1, 1, 9, 10, 9, 8, 2, 3, 10, 9, 3, 10, 12, 8, 9, 8, 2, 1, 9, 8, 1, 7, 8, 1, 9, 1, 2, 10, 7, 7, 8, 1, 3, 3, 9, 3, 3, 1, 1, 1, 9, 1, 3, 8, 8, 8, 1, 8, 1, 9, 7, 8, 8, 2, 1, 3, 8, 2, 1, 6, 1, 8, 8, 2, 8, 2, 2, 9, 8, 1, 1, 6, 8, 6, 10, 2, 1, 1, 8, 9, 3, 3, 3, 1, 3, 9, 6, 8, 1, 8, 3, 1, 2, 2, 12, 7, 1, 1, 1, 1, 12, 1, 8, 8, 3, 2, 1, 9, 2, 3, 3, 2, 2, 7, 2, 2, 2, 3, 1, 9, 8, 7, 1, 1, 3, 1, 9, 4, 3, 9, 2, 1, 1, 9, 8, 8, 7, 2, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 10, 1, 1, 8, 2, 8, 1, 8, 1, 3, 1, 2, 1, 1, 3, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 8, 1, 9, 8, 8, 1, 10, 8, 8, 1, 3, 9, 8, 9, 8, 2, 1, 3, 4, 1, 12, 1, 9, 8, 2, 7, 2, 1, 2, 9, 2, 1, 3, 8, 1, 2, 3, 8, 2, 10, 10, 10, 9, 9, 9, 3, 8, 3, 1, 1, 1, 1, 1, 1, 2, 1, 9, 2, 7, 9, 8, 7, 9, 1, 1, 1, 1, 8, 2, 2, 3, 3, 2, 12, 8, 3, 1, 1, 12, 9, 8, 2, 2, 9, 1, 2, 2, 9, 1, 1, 2, 4, 8, 1, 8, 10, 2, 9, 9, 9, 1, 2, 1, 1, 2, 2, 2, 12, 1, 2, 8, 3, 9, 9, 3, 2, 9, 9, 7, 2, 8, 8, 9, 2, 9, 1, 1, 1, 1, 1, 2, 8, 2, 1, 2, 2, 8, 3, 1, 1, 9, 1, 3, 10, 7, 9, 8, 8, 2, 2, 9, 2, 2, 2, 2, 2, 2, 9, 1, 3, 6, 4, 2, 2, 9, 1, 9, 8, 1, 1, 1, 7, 3, 2, 9, 9, 1, 9, 7, 2, 8, 9, 9, 12, 10, 8, 10, 2, 3, 1, 2, 2, 12, 12, 3, 12, 1, 2, 1, 2, 1, 2, 8, 2, 3, 12, 12, 8, 2, 2, 9, 2, 9, 2, 3, 3, 1, 1, 1, 12, 3, 7, 1, 3, 1, 2, 9, 2, 7, 2, 1, 8, 3, 7, 8, 1, 1, 3, 8, 3, 2, 9, 3, 1, 1, 1, 1, 11, 1, 2, 6, 1, 2, 3, 8, 9, 9, 2, 9, 2, 1, 1, 3, 1, 12, 9, 1, 1, 9, 1, 9, 9, 8, 9, 8, 1, 2, 7, 1, 0, 9, 8, 2, 3, 2, 4, 1, 1, 1, 8, 1, 12, 1, 2, 3, 3, 9, 9, 9, 9, 9, 7, 9, 3, 9, 1, 7, 3, 3, 3, 3, 7, 8, 2, 2, 2, 8, 2, 3, 9, 9, 3, 1, 8, 9, 12, 8, 8, 3, 2, 1, 6, 1, 9, 1, 1, 1, 1, 1, 1, 2, 2, 1, 2, 2, 2, 2, 9, 2, 5, 3, 2, 2, 8, 2, 1, 8, 3, 3, 8, 3, 1, 3, 9, 2, 8, 9, 4, 4, 9, 8, 8, 7, 7, 3, 1, 8, 8, 3, 2, 2, 10, 3, 3, 2, 2, 2, 2, 12, 3, 1, 2, 7, 7, 12, 2, 1, 2, 1, 1, 2, 1, 3, 3, 3, 1, 3, 9, 4, 8, 8, 4, 2, 4, 2, 8, 9, 1, 1, 2, 2, 2, 8, 7, 2, 8, 2, 9, 1, 8, 1, 1, 2, 1, 1, 1, 2, 9, 9, 8, 1, 8, 1, 4, 8, 1, 2, 7, 9, 2, 1, 8, 9, 8, 9, 1, 1, 2, 3, 2, 12, 7, 1, 2, 4, 4, 4, 2, 2, 2, 9, 8, 8, 1, 12, 3, 3, 2, 2, 3, 12, 1, 5, 1, 1, 3, 2, 8, 8, 2, 2, 2, 3, 9, 8, 8, 9, 8, 8, 11, 8, 2, 9, 2, 3, 1, 7, 1, 8, 2, 8, 1, 1, 1, 8, 1, 1, 1, 7, 8, 8, 2, 3, 2, 1, 8, 8, 1, 8, 4, 8, 9, 8, 9, 8, 1, 2, 3, 2, 1, 2, 2, 3, 1, 9, 4, 8, 1, 1, 9, 1, 1, 8, 2, 2, 2, 1, 7, 3, 3, 1, 3, 3, 12, 3, 1, 1, 3, 1, 1, 4, 8, 4, 4, 3, 3, 3, 4, 8, 1, 9, 9, 9, 4, 1, 7, 4, 8, 4, 9, 8, 1, 1, 3, 8, 9, 1, 9, 7, 1, 1, 9, 1, 8, 8, 8, 1, 1, 8, 2, 2, 2, 9, 4, 4, 1, 2, 1, 2, 9, 1, 1, 8, 5, 5, 5, 9, 12, 4, 1, 2, 1, 8, 8, 1, 9, 9, 2, 12, 1, 1, 7, 8, 8, 4, 1, 2, 8, 11, 2, 3, 2, 2, 1, 11, 2, 2, 2, 2, 9, 9, 2, 2, 9, 3, 1, 9, 1, 1, 1, 7, 8, 1, 8, 9, 1, 2, 9, 8, 1, 2, 9, 8, 12, 2, 2, 10, 8, 2, 2, 8, 8, 1, 2, 6, 1, 1, 7, 7, 2, 3, 2, 1, 1, 8, 1, 10, 8, 12, 6, 4, 1, 4, 1, 2, 9, 8, 9, 4, 8, 8, 10, 8, 8, 8, 8, 8, 2, 2, 8, 4, 9, 3, 3, 3, 3, 3, 9, 2, 1, 1, 2, 3, 2, 1, 2, 3, 3, 3, 2, 2, 3, 3, 2, 1, 2, 3, 3, 3, 3, 12, 9, 3, 2, 2, 3, 1, 5, 1, 1, 3, 2, 2, 1, 2, 9, 3, 9, 8, 1, 8, 7, 7, 9, 1, 1, 7, 9, 10, 1, 7, 1, 4, 3, 8, 9, 4, 9, 9, 2, 9, 2, 8, 4, 2, 2, 9, 5, 2, 1, 10, 9, 3, 1, 1, 1, 2, 1, 2, 9, 1, 7, 9, 9, 8, 2, 2, 8, 9, 1, 9, 8, 2, 3, 2, 2, 1, 2, 2, 2, 1, 1, 8, 1, 9, 4, 12, 8, 2, 7, 2, 2, 1, 9, 3, 1, 8, 12, 1, 1, 2, 1, 10, 2, 6, 12, 12, 4, 9, 1, 9, 10, 1, 2, 8, 2, 1, 8, 6, 12, 9, 9, 2, 1, 7, 8, 9, 5, 8, 1, 8, 8, 3, 1, 8, 2, 8, 2, 2, 1, 2, 9, 2, 8, 1, 12, 8, 2, 7, 3, 3, 12, 2, 3, 3, 9, 9, 3, 3, 3, 3, 3, 2, 1, 4, 1, 4, 8, 8, 2, 1, 2, 9, 12, 1, 10, 12, 1, 2, 9, 4, 8, 10, 3, 2, 8, 7, 4, 8, 9, 2, 7, 1, 10, 2, 2, 2, 8, 4, 2, 2, 1, 1, 8, 7, 9, 2, 1, 9, 8, 9, 12, 4, 8, 2, 8, 7, 7, 1, 9, 2, 1, 1, 9, 1, 8, 2, 3, 9, 9, 1, 3, 2, 9, 2, 2, 8, 1, 1, 2, 1, 2, 1, 9, 8, 2, 2, 4, 8, 8, 9, 5, 5, 9, 8, 9, 6, 2, 1, 12, 2, 3, 8, 8, 9, 2, 2, 1, 3, 2, 10, 9, 10, 7, 2, 8, 4, 2, 8, 9, 8, 1, 3, 8, 8, 1, 8, 3, 8, 8, 1, 10, 3, 1, 5, 8, 8, 9, 2, 2, 9, 2, 1, 9, 1, 4, 1, 12, 8, 8, 2, 1, 1, 1, 1, 1, 9, 2, 9, 8, 9, 5, 9, 1, 8, 2, 4, 1, 1, 1, 9, 12, 4, 1, 1, 9, 8, 4, 1, 8, 9, 8, 8, 9, 9, 9, 9, 9, 9, 1, 3, 4, 2, 2, 8, 2, 9, 2, 3, 8, 3, 1, 7, 2, 1, 1, 4, 2, 9, 2, 1, 3, 2, 9, 2, 9, 3, 8, 1, 9, 1, 1, 2, 8, 8, 8, 2, 3, 9, 9, 8, 2, 4, 1, 5, 1, 1, 9, 9, 2, 1, 1, 1, 9, 9, 1, 3, 9, 2, 2, 9, 9, 8, 3, 9, 9, 8, 2, 2, 8, 9, 2, 2, 8, 9, 10, 10, 2, 8, 7, 9, 9, 4, 2, 4, 11, 2, 1, 7, 8, 8, 8, 9, 4, 9, 8, 8, 2, 1, 1, 9, 8, 3, 3, 8, 8, 7, 9, 1, 3, 4, 8, 10, 2, 4, 4, 8, 2, 8, 2, 8, 9, 2, 1, 2, 2, 3, 1, 9, 4, 5, 9, 9, 1, 1, 1, 1, 1, 1, 7, 1, 1, 1, 8, 1, 1, 1, 1, 1, 9, 1, 2, 1, 12, 9, 3, 9, 9, 4, 1, 9, 1, 1, 2, 11, 9, 3, 2, 2, 2, 4, 2, 3, 2, 7, 7, 8, 1, 10, 9, 5, 7, 1, 10, 8, 1, 2, 7, 1, 12, 5, 8, 4, 2, 9, 9, 8, 2, 9, 8, 8, 10, 2, 3, 2, 7, 2, 8, 10, 3, 2, 9, 9, 9, 1, 1, 1, 1, 8, 2, 10, 9, 8, 1, 2, 2, 2, 1, 2, 1, 3, 9, 12, 2, 2, 4, 8, 1, 3, 9, 4, 9, 1, 9, 9, 2, 1, 1, 9, 7, 10, 3, 10, 2, 9, 2, 9, 2, 8, 2, 1, 2, 3, 1, 1, 1, 8, 5, 8, 3, 1, 1, 4, 2, 2, 1, 3, 8, 2, 1, 1, 1, 2, 9, 4, 1, 8, 1, 2, 8, 2, 9, 2, 2, 5, 5, 2, 5, 2, 1, 8, 8, 1, 3, 12, 2, 4, 3, 2, 2, 2, 6, 2, 3, 3, 1, 8, 2, 2, 1, 1, 8, 8, 10, 1, 9, 2, 9, 12, 0, 2, 1, 3, 2, 2, 1, 2, 1, 9, 9, 2, 9, 8, 8, 1, 7, 8, 1, 2, 6, 9, 8, 10, 1, 12, 2, 2, 1, 8, 2, 1, 12, 7, 4, 2, 10, 2, 2, 1, 2, 1, 2, 1, 1, 2, 1, 9, 8, 10, 2, 10, 9, 9, 12, 5, 3, 2, 8, 2, 8, 3, 3, 1, 4, 9, 10, 2, 4, 8, 1, 2, 10, 12, 12, 3, 1, 1, 2, 9, 6, 6, 1, 9, 1, 8, 1, 8, 8, 8, 8, 2, 2, 2, 8, 8, 2, 1, 1, 1, 1, 1, 8, 8, 5, 8, 8, 1, 8, 9, 9, 1, 7, 12, 9, 9, 7, 2, 2, 7, 8, 9, 8, 2, 9, 6, 3, 4, 8, 4, 1, 2, 9, 8, 8, 8, 2, 1, 1, 8, 8, 3, 1, 9, 9, 7, 1, 2, 2, 2, 9, 4, 1, 2, 12, 1, 8, 2, 1, 9, 1, 8, 9, 9, 2, 5, 4, 2, 1, 2, 8, 12, 8, 10, 3, 8, 8, 8, 2, 2, 8, 2, 8, 2, 4, 8, 10, 2, 7, 2, 1, 8, 8, 9, 5, 8, 1, 12, 7, 10, 2, 2, 2, 1, 4, 8, 12, 1, 8, 2, 2, 9, 1, 8, 3, 1, 1, 5, 10, 2, 1, 2, 4, 8, 9, 3, 2, 8, 7, 4, 4, 9, 9, 2, 8, 1, 1, 2, 4, 7, 1, 9, 1, 1, 1, 8, 4, 1, 3, 1, 2, 2, 9, 1, 8, 8, 2, 2, 9, 9, 1, 8, 8, 1, 9, 8, 8, 2, 2, 2, 2, 1, 1, 1, 7, 4, 1, 1, 4, 2, 3, 3, 2, 3, 9, 2, 8, 2, 1, 5, 1, 2, 2, 1, 3, 5, 2, 9, 1, 1, 9, 8, 1, 4, 8, 1, 8, 2, 4, 8, 4, 7, 9, 8, 1, 2, 2, 4, 4, 11, 3, 11, 1, 3, 8, 1, 9, 6, 8, 9, 3, 9, 8, 9, 2, 3, 3, 7, 1, 7, 2, 9, 9, 1, 8, 8, 1, 5, 5, 8, 8, 1, 2, 1, 3, 9, 9, 5, 9, 8, 1, 4, 3, 8, 1, 1, 2, 1, 8, 9, 8, 2, 2, 2, 1, 2, 4, 10, 8, 1, 9, 1, 8, 1, 1, 9, 9, 8, 11, 1, 8, 2, 1, 2, 8, 2, 8, 8, 2, 9, 6, 10, 1, 8, 6, 4, 5, 11, 2, 2, 8, 1, 8, 1, 8, 1, 4, 8, 1, 1, 1, 8, 3, 7, 3, 3, 7, 1, 1, 4, 8, 1, 6, 4, 2, 2, 1, 1, 1, 1, 6, 2, 2, 8, 1, 8, 5, 5, 2, 1, 1, 2, 3, 7, 1, 1, 6, 2, 9, 3, 9, 8, 9, 2, 1, 4, 2, 9, 13, 9, 2, 8, 12, 4, 1, 1, 9, 5, 1, 1, 1, 1, 8, 2, 1, 1, 2, 9, 0, 12, 1, 1, 8, 9, 8, 3, 2, 2, 12, 8, 2, 2, 2, 1, 10, 1, 9, 8, 9, 5, 2, 2, 9, 2, 8, 4, 8, 3, 7, 9, 2, 2, 7, 2, 2, 8, 1, 1, 9, 10, 9, 8, 2, 9, 0, 7, 1, 1, 1, 9, 8, 2, 8, 12, 8, 3, 2, 8, 8, 7, 7, 8, 12, 2, 1, 3, 9, 2, 2, 8, 2, 8, 8, 12, 8, 8, 1, 8, 9, 2, 4, 4, 7, 2, 1, 1, 9, 3, 7, 1, 3, 9, 8, 2, 2, 3, 1, 3, 10, 2, 3, 7, 1, 8, 1, 1, 10, 3, 10, 8, 4, 1, 2, 4, 3, 10, 9, 3, 9, 5, 8, 7, 2, 9, 3, 3, 2, 11, 8, 1, 1, 7, 2, 7, 2, 9, 4, 8, 1, 12, 8, 7, 9, 1, 0, 2, 6, 4, 9, 3, 2, 4, 12, 9, 8, 5, 9, 2, 8, 4, 6, 8, 9, 3, 1, 8, 7, 3, 1, 1, 1, 8, 9, 1, 8, 7, 9, 8, 2, 1, 2, 2, 3, 2, 2, 1, 1, 4, 9, 7, 3, 2, 8, 8, 9, 1, 9, 7, 4, 2, 2, 8, 5, 9, 1, 5, 7, 1, 4, 6, 9, 10, 1, 8, 2, 9, 1, 9, 2, 9, 8, 2, 9, 9, 11, 2, 12, 2, 2, 9, 4, 8, 1, 9, 9, 2, 8, 9, 1, 2, 9, 3, 8, 8, 1, 7, 8, 8, 2, 3, 2, 2, 2, 2, 9, 8, 10, 2, 2, 1, 9, 11, 1, 8, 8, 2, 4, 2, 1, 2, 8, 8, 9, 2, 8, 9, 2, 3, 1, 2, 1, 3, 10, 8, 1, 2, 4, 1, 9, 8, 9, 3, 3, 9, 8, 2, 11, 3, 7, 1, 8, 7, 1, 8, 8, 8, 1, 4, 8, 8, 1, 9, 3, 1, 1, 2, 8, 8, 8, 2, 1, 8, 10, 2, 12, 8, 8, 9, 1, 8, 1, 3, 2, 2, 10, 7, 2, 2, 1, 1, 8, 2, 9, 1, 2, 9, 3, 3, 10, 1, 4, 7, 10, 12, 2, 4, 10, 8, 6, 2, 9, 1, 8, 3, 1, 2, 8, 8, 2, 12, 10, 2, 10, 9, 1, 2, 5, 8, 12, 8, 8, 7, 8, 8, 1, 8, 2, 2, 9, 1, 8, 10, 5, 5, 5, 8, 1, 9, 1, 13, 1, 2, 2, 1, 2, 4, 9, 2, 1, 9, 13, 11, 1, 9, 9, 3, 10, 8, 7, 8, 8, 4, 2, 1, 9, 4, 3, 1, 1, 10, 9, 2, 8, 6, 2, 1, 1, 3, 2, 8, 1, 1, 1, 1, 2, 4, 1, 1, 10, 1, 8, 1, 1, 9, 1, 7, 1, 1, 9, 9, 2, 12, 9, 1, 8, 1, 12, 1, 8, 3, 8, 1, 8, 1, 2, 10, 1, 1, 2, 2, 9, 3, 9, 12, 1, 9, 1, 2, 8, 7, 2, 1, 1, 3, 2, 8, 4, 4, 5, 12, 7, 8, 9, 6, 1, 8, 9, 2, 1, 9, 1, 4, 1, 8, 2, 1, 1, 8, 7, 4, 9, 1, 8, 4, 2, 7, 5, 1, 3, 9, 2, 8, 2, 1, 2, 9, 1, 1, 2, 3, 4, 3, 1, 2, 2, 10, 2, 2, 8, 1, 1, 2, 4, 9, 2, 11, 9, 8, 9, 2, 1, 10, 4, 1, 1, 10, 7, 2, 10, 8, 1, 10, 8, 8, 8, 8, 8, 8, 2, 2, 2, 8, 3, 4, 1, 1, 1, 2, 1, 8, 8, 2, 8, 3, 8, 3, 1, 2, 3, 1, 1, 10, 1, 4, 4, 3, 3, 1, 9, 1, 1, 2, 1, 1, 6, 1, 1, 1, 9, 1, 4, 1, 1, 1, 1, 1, 1, 2, 12, 8, 8, 1, 9, 12, 10, 8, 2, 12, 1, 1, 2, 1, 7, 10, 7, 1, 2, 1, 10, 8, 2, 2, 8, 2, 2, 1, 9, 8, 8, 8, 8, 3, 1, 4, 2, 8, 3, 10, 10, 1, 9, 1, 9, 1, 9, 1, 8, 3, 1, 1, 8, 8, 8, 6, 3, 4, 1, 1, 1, 9, 9, 5, 2, 10, 2, 6, 2, 2, 7, 1, 8, 2, 11, 1, 1, 1, 2, 1, 9, 4, 4, 2, 8, 4, 6, 8, 8, 10, 10, 2, 1, 2, 9, 1, 8, 8, 3, 4, 8, 8, 2, 8, 8, 1, 2, 1, 2, 9, 4, 1, 3, 3, 3, 8, 8, 8, 2, 6, 4, 3, 3, 12, 3, 4, 9, 6, 8, 7, 7, 8, 6, 4, 1, 10, 6, 3, 3, 8, 1, 1, 8, 1, 11, 8, 1, 1, 2, 3, 11, 7, 2, 1, 1, 1, 9, 1, 2, 1, 1, 9, 8, 1, 1, 8, 0, 4, 1, 1, 2, 1, 1, 2, 2, 9, 1, 10, 1, 3, 8, 9, 3, 3, 1, 10, 1, 9, 1, 1, 7, 4, 1, 8, 10, 1, 2, 10, 1, 1, 2, 3, 3, 9, 1, 8, 10, 1, 7, 9, 1, 9, 6, 3, 9, 2, 8, 4, 12, 12, 3, 1, 1, 9, 1, 9, 2, 1, 1, 1, 1, 1, 9, 2, 1, 2, 10, 9, 8, 8, 10, 8, 2, 3, 2, 1, 12, 8, 12, 2, 9, 1, 3, 2, 5, 2, 2, 8, 2, 2, 4, 10, 9, 10, 9, 4, 2, 4, 12, 8, 2, 1, 8, 3, 9, 2, 9, 4, 1, 1, 1, 1, 1, 9, 6, 2, 9, 2, 1, 3, 2, 2, 5, 8, 9, 1, 6, 3, 1, 2, 2, 2, 6, 1, 8, 10, 8, 3, 3, 13, 2, 9, 9, 10, 6, 12, 8, 2, 9, 2, 3, 8, 8, 1, 2, 2, 3, 2, 10, 1, 8, 9, 1, 8, 9, 8, 8, 2, 10, 2, 1, 1, 1, 9, 9, 12, 1, 1, 4, 8, 3, 2, 2, 4, 4, 2, 2, 1, 1, 1, 9, 2, 2, 4, 1, 1, 8, 8, 8, 9, 2, 13, 9, 9, 4, 1, 9, 7, 10, 10, 8, 1, 9, 1, 1, 1, 8, 12, 4, 1, 2, 1, 1, 1, 7, 8, 3, 3, 1, 2, 2, 7, 2, 2, 10, 1, 8, 4, 0, 1, 9, 4, 1, 1, 2, 10, 10, 7, 9, 3, 12, 11, 2, 8, 10, 8, 9, 4, 3, 4, 10, 1, 1, 9, 3, 10, 1, 5, 3, 3, 2, 2, 1, 1, 6, 1, 7, 1, 1, 12, 1, 1, 1, 10, 1, 8, 8, 1, 3, 8, 3, 1, 2, 8, 3, 4, 2, 8, 8, 6, 1, 1, 4, 1, 2, 9, 5, 1, 7, 8, 9, 2, 4, 9, 8, 5, 9, 9, 9, 1, 2, 1, 3, 8, 9, 2, 9, 8, 9, 1, 8, 13, 9, 3, 8, 1, 4, 1, 3, 1, 6, 4, 10, 2, 2, 7, 8, 8, 12, 8, 9, 9, 9, 10, 3, 12, 12, 2, 9, 10, 9, 7, 2, 3, 4, 10, 10, 2, 7, 7, 1, 7, 3, 9, 1, 9, 8, 2, 9, 1, 8, 1, 10, 8, 8, 2, 7, 8, 1, 8, 9, 8, 1, 4, 1, 8, 8, 8, 1, 1, 10, 3, 2, 2, 6, 8, 9, 8, 2, 1, 1, 3, 8, 8, 1, 4, 4, 3, 9, 10, 4, 2, 4, 2, 9, 2, 2, 9, 1, 1, 10, 1, 1, 2, 1, 1, 1, 9, 1, 8, 8, 8, 2, 3, 8, 1, 6, 7, 3, 1, 9, 1, 2, 1, 6, 1, 2, 10, 1, 4, 2, 3, 2, 9, 9, 4, 9, 9, 1, 2, 8, 4, 7, 8, 7, 13, 9, 1, 2, 1, 9, 3, 3, 6, 8, 9, 10, 9, 2, 2, 3, 12, 1, 1, 1, 13, 2, 1, 7, 10, 8, 9, 1, 2, 9, 1, 1, 1, 2, 1, 12, 5, 6, 9, 10, 3, 1, 10, 1, 3, 2, 8, 9, 2, 10, 8, 8, 9, 2, 8, 9, 2, 2, 2, 8, 8, 2, 3, 1, 4, 8, 1, 9, 9, 2, 8, 10, 9, 1, 2, 12, 8, 8, 0, 2, 1, 1, 9, 10, 1, 6, 2, 2, 10, 2, 2, 4, 2, 2, 12, 1, 6, 7, 8, 9, 1, 3, 5, 7, 3, 1, 3, 1, 1, 3, 2, 6, 3, 2, 1, 8, 3, 5, 2, 1, 1, 8, 1, 1, 0, 2, 3, 5, 3, 1, 1, 9, 8, 8, 8, 8, 4, 7, 2, 9, 12, 5, 9, 3, 12, 3, 9, 1, 9, 1, 7, 3, 9, 3, 10, 1, 2, 1, 4, 9, 8, 2, 9, 11, 2, 8, 9, 4, 1, 1, 1, 1, 1, 1, 1, 9, 1, 9, 1, 1, 1, 12, 3, 10, 3, 6, 1, 7, 10, 8, 3, 7, 1, 1, 2, 6, 1, 3, 9, 2, 8, 1, 8, 8, 1, 8, 10, 12, 1, 10, 12, 10, 1, 8, 8, 1, 4, 7, 9, 2, 1, 9, 3, 8, 12, 10, 6, 1, 6, 8, 3, 1, 3, 1, 10, 10, 9, 8, 1, 2, 9, 8, 8, 1, 9, 8, 8, 3, 1, 3, 9, 12, 1, 8, 1, 5, 8, 11, 5, 5, 1, 2, 1, 3, 1, 1, 1, 9, 8, 8, 1, 9, 10, 8, 4, 1, 1, 8, 10, 8, 9, 1, 10, 1, 7, 2, 1, 1, 9, 6, 2, 9, 8, 1, 1, 1, 9, 8, 8, 1, 9, 9, 1, 1, 2, 7, 9, 2, 9, 9, 8, 7, 6, 8, 7, 8, 2, 9, 2, 1, 8, 8, 8, 2, 6, 3, 1, 8, 12, 12, 8, 7, 1, 2, 2, 7, 1, 9, 8, 1, 4, 1, 1, 8, 5, 8, 1, 8, 8, 8, 8, 1, 2, 11, 1, 1, 1, 2, 7, 1, 9, 2, 2, 2, 9, 8, 9, 2, 9, 1, 8, 1, 7, 11, 13, 1, 9, 8, 2, 2, 1, 2, 3, 10, 3, 10, 4, 3, 9, 1, 8, 1, 1, 13, 1, 1, 3, 8, 1, 2, 2, 2, 9, 4, 1, 8, 1, 9, 3, 6, 8, 5, 2, 8, 2, 8, 1, 9, 2, 2, 8, 1, 1, 4, 8, 9, 2, 2, 7, 1, 8, 12, 2, 12, 8, 2, 9, 9, 3, 8, 1, 1, 9, 9, 9, 2, 8, 1, 9, 1, 8, 9, 8, 2, 2, 4, 1, 9, 10, 10, 8, 8, 1, 1, 2, 1, 4, 1, 2, 1, 1, 3, 12, 8, 8, 8, 8, 10, 2, 8, 12, 1, 9, 9, 8, 9, 1, 2, 1, 8, 8, 8, 2, 8, 8, 3, 10, 8, 9, 1, 1, 1, 3, 4, 8, 10, 6, 3, 1, 3, 2, 8, 9, 3, 5, 4, 1, 5, 2, 9, 9, 9, 1, 1, 1, 9, 9, 6, 10, 9, 8, 12, 2, 9, 1, 2, 1, 1, 1, 8, 1, 12, 1, 12, 1, 9, 1, 8, 1, 2, 1, 2, 8, 8, 10, 1, 9, 1, 2, 2, 8, 8, 3, 12, 1, 11, 2, 10, 12, 11, 2, 1, 10, 1, 1, 9, 3, 10, 2, 7, 1, 8, 1, 9, 9, 2, 1, 3, 12, 5, 9, 9, 8, 1, 8, 8, 8, 1, 1, 3, 9, 8, 9, 8, 3, 10, 3, 7, 8, 9, 2, 3, 1, 9, 10, 9, 2, 8, 2, 6, 4, 1, 1, 3, 8, 2, 3, 9, 1, 2, 1, 8, 1, 2, 2, 2, 2, 9, 9, 4, 8, 9, 1, 1, 2, 9, 1, 5, 2, 9, 8, 10, 1, 2, 10, 8, 5, 6, 4, 8, 1, 1, 1, 8, 9, 2, 8, 8, 8, 12, 2, 2, 1, 8, 1, 2, 1, 9, 8, 8, 9, 1, 1, 8, 7, 1, 8, 0, 2, 3, 8, 1, 1, 8, 1, 1, 12, 3, 2, 10, 8, 10, 6, 1, 8, 8, 10, 8, 8, 1, 4, 8, 1, 2, 9, 8, 1, 1, 3, 3, 5, 1, 1, 2, 1, 2, 1, 9, 8, 3, 9, 8, 2, 10, 9, 1, 8, 1, 10, 9, 10, 1, 2, 3, 8, 1, 3, 1, 9, 9, 8, 1, 9, 8, 8, 9, 1, 9, 2, 8, 8, 2, 13, 8, 10, 8, 9, 3, 2, 1, 3, 1, 12, 10, 10, 1, 11, 1, 2, 4, 2, 10, 1, 8, 11, 1, 1, 1, 2, 2, 8, 2, 9, 8, 9, 9, 12, 8, 1, 9, 2, 4, 1, 8, 3, 9, 1, 6, 5, 8, 9, 3, 3, 2, 1, 1, 1, 8, 11, 1, 8, 7, 1, 11, 9, 1, 7, 9, 9, 8, 8, 8, 12, 8, 1, 2, 9, 8, 8, 2, 10, 9, 8, 8, 4, 10, 9, 8, 2, 10, 10, 2, 1, 2, 1, 2, 1, 4, 1, 7, 1, 1, 3, 12, 9, 8, 5, 9, 10, 1, 9, 9, 2, 7, 4, 1, 1, 8, 12, 8, 2, 2, 9, 4, 1, 1, 5, 9, 1, 8, 12, 8, 1, 1, 4, 9, 10, 2, 3, 1, 3, 3, 3, 8, 1, 2, 1, 2, 1, 2, 9, 1, 2, 8, 9, 4, 8, 8, 10, 2, 12, 5, 5, 1, 8, 1, 7, 8, 1, 1, 8, 9, 12, 4, 9, 3, 1, 2, 5, 9, 2, 1, 2, 2, 1, 2, 8, 1, 8, 8, 2, 2, 1, 8, 4, 1, 2, 10, 9, 10, 8, 1, 10, 4, 8, 8, 10, 2, 8, 1, 1, 3, 10, 1, 4, 2, 8, 3, 3, 2, 9, 8, 5, 5, 9, 3, 10, 8, 1, 2, 1, 2, 9, 1, 1, 2, 9, 9, 8, 8, 11, 9, 4, 1, 8, 2, 7, 1, 2, 8, 2, 9, 9, 8, 2, 9, 9, 8, 8, 1, 2, 8, 1, 1, 2, 9, 8, 1, 4, 9, 1, 8, 12, 1, 12, 2, 8, 2, 9, 1, 12, 1, 2, 9, 1, 1, 9, 3, 2, 9, 9, 2, 11, 8, 7, 9, 9, 9, 8, 8, 1, 2, 8, 1, 1, 4, 1, 2, 10, 1, 9, 2, 1, 1, 1, 9, 1, 13, 4, 3, 2, 1, 1, 2, 2, 7, 8, 7, 1, 1, 2, 8, 8, 3, 4, 8, 2, 10, 13, 8, 8, 9, 2, 2, 1, 1, 10, 4, 2, 7, 9, 8, 2, 1, 9, 8, 1, 1, 9, 8, 9, 8, 2, 8, 2, 2, 9, 2, 1, 9, 1, 2, 4, 8, 2, 1, 1, 8, 1, 4, 1, 9, 3, 9, 2, 7, 1, 9, 6, 1, 10, 2, 2, 2, 2, 2, 10, 10, 10, 9, 9, 9, 3, 12, 10, 1, 1, 10, 1, 1, 2, 2, 1, 2, 3, 12, 8, 8, 1, 2, 9, 9, 1, 1, 10, 8, 9, 8, 3, 1, 1, 3, 1, 10, 1, 1, 1, 4, 1, 1, 1, 7, 10, 2, 1, 8, 1, 9, 3, 1, 10, 8, 8, 1, 5, 9, 1, 2, 1, 8, 8, 1, 3, 1, 10, 2, 1, 1, 2, 8, 2, 5, 3, 5, 2, 1, 1, 9, 1, 7, 9, 9, 2, 8, 1, 12, 4, 1, 4, 9, 9, 4, 8, 1, 10, 10, 8, 8, 8, 9, 2, 3, 5, 1, 8, 1, 1, 1, 7, 10, 1, 4, 12, 3, 2, 2, 9, 1, 1, 1, 2, 8, 4, 1, 7, 3, 12, 8, 6, 2, 2, 7, 9, 11, 1, 10, 9, 1, 8, 12, 2, 11, 1, 2, 3, 1, 1, 1, 8, 9, 2, 2, 2, 2, 3, 3, 8, 10, 4, 1, 2, 9, 8, 2, 2, 9, 1, 8, 11, 4, 2, 8, 7, 1, 1, 7, 3, 9, 9, 4, 5, 8, 10, 2, 9, 9, 9, 8, 1, 9, 2, 2, 2, 1, 2, 9, 4, 3, 12, 4, 3, 2, 3, 8, 3, 10, 1, 10, 1, 1, 8, 10, 6, 8, 9, 9, 1, 2, 9, 2, 9, 3, 8, 1, 1, 12, 1, 5, 1, 10, 8, 10, 2, 7, 1, 1, 1, 1, 3, 2, 3, 5, 9, 1, 2, 1, 9, 8, 10, 9, 8, 1, 8, 1, 8, 2, 8, 1, 2, 8, 1, 2, 5, 8, 12, 2, 1, 1, 1, 1, 1, 8, 8, 2, 1, 4, 2, 8, 8, 8, 2, 9, 9, 11, 1, 2, 9, 3, 1, 1, 8, 9, 2, 4, 10, 1, 9, 8, 4, 9, 8, 10, 8, 8, 9, 2, 8, 9, 3, 3, 3, 4, 3, 2, 2, 10, 8, 2, 1, 5, 1, 9, 1, 1, 2, 1, 2, 3, 1, 1, 8, 8, 1, 10, 8, 8, 1, 6, 4, 8, 2, 5, 8, 1, 3, 8, 2, 8, 9, 1, 1, 8, 12, 10, 1, 5, 8, 2, 8, 10, 9, 9, 1, 6, 9, 1, 10, 8, 2, 9, 10, 1, 2, 1, 1, 8, 8, 3, 9, 8, 1, 10, 2, 8, 1, 10, 1, 5, 1, 1, 1, 5, 1, 1, 1, 4, 1, 1, 1, 8, 2, 8, 1, 8, 3, 8, 9, 1, 8, 1, 8, 1, 1, 2, 2, 1, 12, 9, 9, 1, 8, 2, 1, 2, 8, 1, 9, 1, 1, 2, 1, 2, 2, 2, 1, 9, 9, 8, 8, 1, 1, 9, 10, 8, 4, 3, 3, 4, 9, 1, 3, 3, 10, 2, 2, 1, 8, 4, 1, 9, 4, 1, 1, 9, 8, 10, 4, 9, 9, 8, 2, 1, 3, 4, 9, 3, 8, 9, 1, 1, 2, 1, 7, 8, 2, 9, 1, 9, 2, 6, 9, 10, 8, 1, 1, 5, 9, 5, 1, 9, 10, 9, 3, 3, 8, 8, 2, 9, 2, 8, 2, 1, 8, 5, 8, 3, 10, 1, 1, 2, 1, 8, 1, 4, 3, 2, 9, 4, 9, 8, 8, 8, 1, 3, 9, 1, 9, 9, 1, 2, 1, 9, 8, 9, 8, 1, 1, 1, 4, 2, 2, 9, 9, 1, 1, 1, 3, 6, 1, 2, 3, 1, 4, 2, 9, 1, 9, 9, 1, 8, 2, 1, 9, 1, 8, 1, 8, 8, 2, 1, 9, 9, 6, 8, 8, 9, 8, 10, 8, 9, 5, 1, 1, 1, 1, 8, 1, 8, 12, 8, 1, 12, 9, 2, 8, 1, 1, 2, 1, 8, 6, 9, 9, 8, 1, 8, 9, 7, 7, 8, 9, 9, 1, 8, 3, 8, 1, 3, 3, 8, 9, 8, 2, 8, 1, 2, 9, 8, 1, 1, 1, 1, 9, 1, 9, 8, 12, 3, 8, 8, 1, 10, 10, 10, 2, 2, 1, 10, 2, 2, 10, 8, 12, 8, 1, 1, 1, 2, 1, 1, 3, 8, 1, 2, 2, 2, 1, 1, 6, 2, 8, 8, 8, 1, 1, 1, 9, 9, 2, 8, 10, 2, 8, 2, 1, 2, 9, 9, 2, 1, 1, 8, 8, 1, 3, 1, 8, 9, 1, 10, 9, 1, 1, 1, 1, 7, 2, 2, 1, 2, 8, 8, 1, 2, 1, 3, 2, 1, 8, 3, 9, 2, 10, 2, 10, 1, 2, 1, 1, 13, 9, 10, 8, 2, 2, 1, 1, 8, 2, 2, 1, 9, 1, 1, 8, 9, 9, 2, 9, 2, 2, 1, 1, 1, 8, 9, 4, 9, 9, 1, 8, 2, 1, 1, 8, 2, 2, 2, 8, 1, 1, 8, 2, 10, 2, 9, 9, 7, 1, 1, 2, 2, 8, 3, 1, 2, 1, 1, 8, 1, 1, 1, 9, 1, 1, 8, 8, 1, 9, 6, 1, 9, 1, 8, 1, 8, 8, 8, 1, 3, 8, 8, 2, 6, 3, 2, 1, 10, 1, 8, 2, 1, 6, 1, 1, 1, 8, 1, 10, 1, 1, 8, 9, 2, 6, 9, 2, 9, 4, 1, 8, 1, 3, 7, 8, 8, 5, 8, 7, 1, 1, 1, 13, 8, 3, 1, 1, 1, 1, 2, 8, 2, 12, 8, 1, 2, 5, 1, 8, 2, 10, 8, 2, 10, 1, 5, 2, 1, 1, 5, 2, 2, 7, 8, 2, 9, 9, 1, 10, 1, 8, 2, 5, 8, 1, 5, 1, 10, 1, 1, 9, 8, 6, 1, 8, 9, 8, 1, 8, 2, 2, 8, 3, 9, 1, 9, 3, 8, 4, 4, 3, 3, 1, 1, 1, 8, 9, 1, 1, 8, 3, 1, 2, 8, 2, 1, 1, 10, 1, 2, 2, 1, 1, 8, 2, 8, 2, 10, 2, 2, 10, 8, 8, 8, 13, 2, 2, 9, 2, 5, 9, 2, 1, 8, 8, 8, 1, 12, 12, 2, 9, 2, 1, 8, 2, 1, 8, 1, 9, 9, 2, 7, 1, 1, 7, 1, 4, 3, 1, 10, 3, 10, 9, 4, 6, 7, 5, 2, 1, 8, 8, 1, 4, 9, 2, 1, 8, 1, 1, 1, 1, 1, 8, 9, 8, 8, 8, 8, 9, 8, 10, 1, 8, 2, 7, 8, 9, 1, 2, 5, 9, 10, 8, 9, 12, 1, 6, 1, 2, 10, 9, 4, 1, 8, 10, 10, 1, 1, 1, 1, 1, 2, 3, 8, 1, 2, 10, 2, 1, 2, 4, 2, 2, 9, 1, 4, 2, 1, 1, 12, 12, 9, 9, 7, 1, 8, 4, 9, 9, 8, 1, 7, 3, 2, 1, 1, 4, 8, 1, 5, 8, 2, 8, 1, 14, 9, 12, 1, 10, 2, 1, 1, 8, 8, 6, 6, 3, 1, 2, 8, 1, 3, 2, 1, 8, 8, 1, 10, 8, 2, 8, 2, 5, 9, 1, 8, 8, 1, 1, 8, 8, 13, 1, 3, 8, 4, 1, 9, 7, 8, 8, 8, 3, 8, 1, 4, 2, 8, 8, 8, 7, 1, 1, 2, 8, 8, 9, 8, 8, 2, 3, 9, 9, 2, 1, 2, 2, 1, 8, 2, 6, 2, 1, 1, 9, 10, 3, 8, 2, 3, 2, 8, 8, 1, 8, 8, 8, 9, 1, 4, 1, 9, 2, 1, 8, 0, 1, 8, 1, 1, 8, 9, 9, 1, 3, 1, 2, 8, 8, 7, 1, 1, 2, 9, 2, 10, 2, 8, 2, 2, 2, 7, 1, 9, 8, 1, 3, 9, 2, 1, 10, 8, 1, 4, 1, 8, 5, 9, 8, 8, 1, 2, 2, 1, 8, 8, 6, 8, 1, 1, 8, 3, 2, 2, 1, 8, 1, 7, 6, 8, 1, 1, 1, 9, 1, 1, 1, 2, 1, 11, 10, 10, 11, 9]\n",
      "8419\n",
      "Average Length of complete text 4273.3882883952965\n",
      "Found 8419 texts in complete\n",
      "Median Length: 3302 \n",
      "Max length of complete text 82521\n",
      "Min length of complete text 0\n",
      "Average Length of First 512 489.944411450291\n",
      "Average Length of Second 512 459.3993348378667\n",
      "Found 8419 texts in First 512.\n",
      "Found 8419 texts in Second 512.\n",
      "Average Length of Third 512 428.21023874569426\n",
      "Average Length of Fourth 512 387.7405867680247\n",
      "Found 8419 texts in Third 512.\n",
      "Found 8419 texts in Fourth 512.\n",
      "Average Length of Fifth 512 342.95403254543294\n",
      "Average Length of Sixth 512 297.7340539256444\n",
      "Found 8419 texts in Fifth 512.\n",
      "Found 8419 texts in Sixth 512.\n",
      "['8', '1', '8', '2', '8', '8', '8', '9', '7', '8', '1', '2', '1', '1', '8', '4', '8', '8', '12', '3', '3', '7', '3', '12', '1', '8', '8', '8', '8', '8', '8', '8', '1', '9', '5', '9', '9', '9', '11', '8', '8', '8', '4', '8', '8', '8', '8', '1', '3', '9', '3', '7', '1', '2', '9', '9', '7', '8', '8', '1', '10', '7', '8', '8', '9', '8', '7', '9', '9', '12', '7', '2', '8', '1', '11', '11', '1', '7', '7', '12', '1', '9', '8', '10', '12', '7', '8', '2', '8', '9', '9', '1', '8', '9', '1', '7', '12', '10', '10', '10', '8', '3', '7', '9', '8', '9', '1', '8', '8', '2', '7', '2', '9', '9', '11', '8', '8', '12', '12', '7', '8', '12', '4', '9', '3', '1', '12', '1', '1', '8', '8', '3', '8', '8', '8', '8', '9', '1', '8', '8', '10', '1', '8', '2', '8', '8', '7', '3', '8', '2', '4', '4', '9', '8', '10', '12', '12', '1', '1', '9', '1', '1', '1', '8', '2', '2', '8', '1', '1', '2', '2', '1', '2', '8', '1', '9', '9', '8', '8', '4', '2', '9', '9', '8', '3', '4', '3', '1', '8', '8', '2', '1', '9', '7', '8', '8', '1', '12', '3', '8', '2', '4', '2', '9', '12', '1', '4', '1', '8', '8', '8', '2', '2', '8', '9', '8', '8', '8', '10', '9', '8', '7', '9', '1', '1', '9', '4', '2', '4', '2', '2', '1', '7', '8', '11', '11', '3', '9', '2', '4', '8', '9', '1', '8', '1', '1', '4', '9', '1', '1', '8', '8', '2', '1', '8', '4', '2', '8', '9', '8', '8', '2', '8', '8', '8', '7', '1', '1', '1', '2', '1', '1', '8', '7', '8', '8', '12', '2', '12', '12', '8', '10', '12', '8', '3', '3', '12', '10', '1', '8', '12', '1', '8', '8', '2', '8', '4', '7', '8', '7', '10', '8', '10', '9', '8', '12', '12', '1', '8', '8', '3', '8', '8', '8', '8', '8', '1', '9', '8', '11', '1', '1', '1', '9', '8', '1', '9', '2', '3', '11', '8', '9', '9', '9', '2', '1', '8', '8', '9', '7', '1', '4', '9', '4', '8', '8', '4', '8', '12', '9', '4', '8', '2', '10', '10', '10', '8', '9', '9', '8', '8', '12', '7', '1', '8', '8', '8', '4', '1', '1', '1', '1', '1', '1', '8', '1', '9', '8', '9', '9', '4', '8', '12', '9', '8', '8', '2', '8', '8', '8', '6', '9', '8', '3', '7', '8', '8', '4', '12', '8', '8', '9', '12', '12', '9', '8', '2', '9', '2', '3', '1', '12', '8', '10', '9', '9', '9', '10', '10', '3', '8', '12', '1', '4', '2', '1', '10', '8', '2', '8', '4', '8', '9', '1', '9', '9', '10', '10', '1', '4', '9', '2', '4', '9', '1', '1', '3', '10', '3', '3', '8', '7', '3', '8', '9', '9', '12', '4', '8', '12', '2', '2', '4', '1', '9', '9', '4', '1', '4', '2', '8', '12', '2', '3', '10', '10', '9', '8', '9', '9', '1', '12', '8', '8', '8', '12', '4', '1', '8', '8', '1', '9', '8', '8', '2', '1', '8', '9', '8', '3', '3', '3', '1', '8', '8', '9', '1', '10', '9', '9', '9', '9', '5', '9', '9', '8', '8', '8', '8', '8', '8', '8', '8', '11', '12', '8', '8', '1', '8', '9', '11', '2', '2', '2', '2', '3', '1', '2', '2', '8', '2', '4', '9', '1', '2', '9', '8', '2', '8', '9', '9', '3', '10', '9', '9', '2', '8', '9', '8', '12', '12', '1', '3', '8', '8', '8', '2', '7', '7', '7', '7', '3', '9', '1', '9', '8', '9', '9', '1', '1', '1', '2', '9', '9', '9', '11', '1', '8', '8', '9', '1', '9', '8', '8', '8', '1', '1', '8', '7', '1', '1', '8', '8', '9', '4', '4', '8', '2', '2', '8', '8', '8', '8', '8', '8', '11', '8', '2', '9', '4', '9', '3', '9', '9', '1', '3', '9', '3', '1', '12', '8', '9', '12', '1', '8', '4', '2', '1', '4', '8', '3', '3', '8', '2', '8', '9', '7', '8', '8', '8', '5', '8', '3', '9', '8', '8', '13', '12', '1', '1', '2', '8', '4', '1', '9', '9', '12', '8', '9', '12', '9', '1', '9', '9', '9', '9', '3', '2', '9', '9', '4', '8', '12', '2', '4', '9', '3', '1', '9', '7', '8', '9', '9', '8', '4', '8', '8', '7', '9', '10', '3', '8', '8', '8', '1', '1', '1', '1', '8', '8', '4', '1', '10', '1', '5', '7', '7', '1', '8', '9', '3', '7', '2', '7', '7', '2', '4', '8', '12', '7', '4', '2', '9', '9', '12', '6', '10', '8', '2', '4', '12', '9', '9', '3', '8', '8', '1', '2', '10', '9', '9', '8', '4', '12', '2', '1', '8', '8', '8', '12', '10', '10', '9', '3', '8', '8', '9', '2', '8', '10', '1', '1', '1', '1', '2', '1', '1', '1', '1', '9', '8', '12', '9', '4', '8', '8', '9', '1', '9', '3', '9', '8', '8', '1', '7', '7', '10', '1', '8', '8', '1', '9', '8', '10', '3', '1', '7', '1', '8', '8', '12', '8', '8', '1', '8', '7', '1', '7', '7', '8', '2', '1', '8', '8', '2', '10', '8', '8', '8', '8', '8', '10', '1', '8', '8', '12', '8', '3', '3', '2', '2', '2', '10', '8', '8', '8', '2', '9', '1', '8', '9', '3', '2', '8', '10', '8', '6', '1', '1', '8', '4', '1', '9', '10', '8', '1', '7', '1', '2', '8', '1', '1', '1', '12', '1', '9', '12', '8', '12', '12', '12', '8', '8', '12', '4', '8', '8', '8', '8', '9', '9', '1', '3', '3', '3', '3', '1', '12', '12', '9', '10', '8', '8', '1', '9', '2', '2', '13', '9', '8', '9', '2', '1', '9', '1', '8', '8', '8', '4', '8', '1', '1', '1', '12', '12', '7', '2', '2', '2', '8', '3', '8', '9', '2', '10', '7', '8', '9', '2', '1', '2', '12', '12', '8', '8', '9', '2', '2', '9', '11', '1', '8', '1', '10', '9', '2', '1', '4', '7', '7', '7', '7', '7', '12', '8', '8', '8', '1', '1', '10', '1', '12', '1', '8', '2', '1', '1', '12', '8', '7', '9', '12', '8', '9', '3', '9', '8', '8', '8', '8', '3', '11', '2', '2', '9', '8', '8', '10', '8', '2', '7', '3', '1', '4', '7', '8', '8', '1', '8', '3', '7', '12', '8', '10', '9', '9', '8', '8', '2', '8', '9', '9', '1', '2', '8', '8', '9', '8', '3', '8', '1', '8', '10', '9', '8', '9', '9', '12', '4', '4', '8', '9', '9', '8', '2', '10', '1', '2', '8', '9', '1', '9', '9', '9', '7', '12', '12', '8', '1', '1', '1', '1', '8', '3', '1', '1', '8', '1', '8', '8', '7', '8', '8', '8', '8', '3', '2', '2', '10', '10', '10', '7', '8', '1', '2', '12', '7', '9', '8', '7', '8', '12', '2', '8', '9', '2', '6', '6', '7', '9', '8', '1', '8', '9', '8', '1', '12', '1', '2', '8', '7', '7', '7', '8', '2', '2', '8', '1', '2', '2', '9', '9', '1', '8', '8', '4', '3', '3', '1', '6', '3', '3', '12', '3', '8', '9', '1', '4', '3', '1', '8', '3', '9', '2', '8', '2', '8', '8', '8', '1', '1', '1', '9', '9', '8', '1', '9', '8', '1', '1', '3', '10', '8', '1', '1', '3', '9', '1', '4', '4', '1', '8', '9', '9', '2', '0', '0', '1', '8', '3', '1', '8', '8', '9', '8', '8', '1', '1', '8', '9', '8', '8', '8', '7', '9', '8', '8', '8', '10', '9', '8', '1', '2', '6', '1', '9', '9', '8', '12', '12', '12', '8', '8', '2', '8', '1', '2', '2', '2', '1', '9', '8', '2', '12', '2', '8', '12', '8', '9', '8', '8', '9', '7', '1', '1', '1', '1', '1', '8', '8', '1', '8', '8', '1', '1', '3', '2', '8', '8', '9', '10', '10', '2', '2', '1', '9', '2', '9', '9', '4', '12', '12', '12', '10', '7', '3', '3', '4', '2', '2', '9', '2', '8', '4', '2', '4', '1', '10', '9', '7', '8', '7', '1', '1', '3', '3', '1', '1', '3', '3', '3', '1', '1', '1', '1', '8', '2', '3', '1', '1', '2', '8', '8', '12', '8', '8', '8', '8', '11', '9', '1', '8', '9', '2', '8', '8', '8', '3', '9', '1', '9', '2', '7', '2', '8', '2', '8', '10', '8', '1', '10', '1', '1', '9', '9', '8', '8', '1', '8', '8', '8', '12', '8', '8', '8', '1', '8', '8', '8', '1', '9', '1', '1', '8', '1', '8', '9', '8', '2', '12', '9', '9', '0', '1', '8', '8', '1', '8', '12', '8', '8', '10', '8', '8', '8', '7', '8', '1', '8', '7', '3', '10', '1', '8', '9', '1', '8', '8', '8', '10', '1', '10', '3', '9', '1', '8', '9', '2', '8', '3', '3', '9', '9', '7', '9', '1', '1', '9', '2', '1', '1', '1', '7', '1', '1', '8', '8', '1', '1', '8', '1', '8', '3', '12', '9', '3', '3', '8', '8', '8', '8', '3', '1', '3', '3', '1', '11', '0', '8', '8', '7', '8', '12', '1', '8', '9', '8', '9', '8', '8', '3', '8', '8', '1', '1', '1', '9', '2', '2', '2', '8', '7', '12', '8', '8', '9', '10', '10', '7', '8', '1', '9', '8', '7', '3', '1', '3', '8', '2', '2', '3', '9', '8', '4', '4', '8', '9', '2', '1', '1', '7', '8', '9', '9', '7', '8', '7', '7', '8', '2', '2', '8', '4', '9', '7', '10', '0', '9', '8', '3', '7', '8', '1', '1', '8', '9', '9', '2', '2', '10', '1', '9', '10', '10', '10', '8', '3', '2', '12', '9', '9', '10', '12', '9', '12', '12', '9', '1', '2', '4', '12', '12', '7', '8', '9', '7', '7', '7', '3', '9', '8', '9', '1', '12', '8', '9', '4', '1', '3', '12', '12', '12', '12', '8', '8', '2', '1', '1', '2', '1', '1', '1', '12', '12', '8', '12', '2', '2', '12', '3', '3', '12', '8', '2', '8', '8', '12', '2', '1', '10', '3', '2', '8', '7', '1', '8', '1', '3', '7', '8', '9', '8', '3', '1', '1', '7', '8', '8', '9', '8', '2', '9', '2', '2', '9', '8', '1', '8', '8', '1', '3', '3', '1', '1', '10', '1', '2', '8', '1', '1', '1', '1', '9', '1', '4', '1', '7', '7', '7', '7', '2', '2', '8', '8', '12', '1', '9', '1', '7', '3', '3', '1', '8', '8', '10', '8', '9', '2', '9', '1', '3', '8', '8', '3', '12', '2', '8', '12', '2', '9', '1', '3', '3', '3', '3', '2', '8', '7', '9', '8', '3', '3', '1', '7', '8', '3', '1', '1', '12', '8', '9', '1', '2', '3', '8', '1', '1', '3', '3', '9', '1', '1', '1', '12', '1', '7', '3', '3', '1', '8', '8', '8', '1', '2', '4', '8', '1', '10', '2', '5', '3', '3', '12', '10', '9', '9', '12', '9', '0', '2', '8', '8', '9', '9', '9', '8', '1', '3', '1', '1', '4', '8', '1', '10', '8', '7', '2', '8', '2', '8', '4', '7', '8', '1', '9', '1', '9', '8', '2', '8', '2', '7', '9', '2', '2', '9', '1', '8', '12', '1', '8', '1', '4', '1', '9', '9', '1', '10', '12', '4', '8', '1', '7', '3', '9', '2', '12', '7', '8', '8', '2', '1', '12', '9', '8', '1', '2', '2', '8', '10', '2', '1', '7', '7', '7', '12', '3', '3', '8', '3', '8', '8', '3', '9', '8', '9', '1', '1', '8', '7', '9', '3', '3', '8', '1', '0', '9', '9', '9', '1', '8', '9', '9', '10', '1', '8', '7', '8', '8', '8', '9', '9', '8', '9', '10', '4', '9', '3', '7', '12', '1', '9', '9', '8', '9', '1', '9', '3', '1', '8', '4', '12', '10', '9', '8', '7', '10', '8', '12', '12', '3', '10', '8', '12', '2', '1', '2', '3', '9', '8', '8', '7', '1', '1', '2', '2', '1', '2', '2', '7', '1', '3', '9', '9', '3', '8', '8', '8', '8', '8', '7', '8', '8', '10', '8', '1', '8', '2', '8', '2', '2', '2', '2', '2', '1', '7', '12', '10', '1', '2', '8', '1', '4', '7', '8', '1', '12', '8', '7', '9', '2', '2', '2', '8', '1', '8', '1', '1', '0', '2', '2', '6', '1', '8', '2', '1', '8', '1', '1', '2', '2', '3', '2', '2', '10', '10', '9', '1', '7', '7', '8', '8', '1', '12', '10', '12', '3', '8', '8', '8', '3', '8', '3', '10', '2', '2', '2', '1', '2', '2', '1', '0', '1', '8', '9', '1', '1', '1', '2', '2', '2', '2', '2', '1', '1', '1', '1', '12', '9', '9', '1', '10', '11', '12', '9', '1', '1', '8', '8', '1', '2', '12', '8', '3', '7', '2', '10', '9', '7', '8', '1', '2', '2', '7', '9', '12', '2', '2', '1', '8', '9', '2', '3', '2', '2', '2', '9', '9', '8', '9', '2', '3', '8', '8', '9', '7', '3', '10', '8', '8', '9', '1', '1', '2', '8', '1', '10', '8', '8', '9', '7', '1', '7', '12', '8', '1', '7', '2', '1', '3', '9', '2', '8', '8', '1', '2', '3', '9', '3', '8', '9', '7', '7', '1', '10', '2', '8', '8', '8', '1', '8', '2', '8', '8', '1', '2', '8', '10', '2', '1', '0', '8', '3', '8', '10', '10', '12', '3', '8', '9', '3', '10', '8', '8', '8', '1', '8', '8', '2', '2', '2', '2', '2', '2', '1', '1', '10', '1', '10', '8', '2', '1', '2', '2', '8', '3', '3', '2', '2', '1', '8', '1', '3', '1', '9', '2', '2', '1', '3', '2', '2', '3', '2', '2', '3', '2', '1', '1', '2', '1', '2', '7', '8', '2', '1', '12', '12', '3', '1', '9', '9', '7', '8', '7', '2', '7', '9', '2', '2', '2', '8', '9', '7', '9', '2', '8', '9', '9', '2', '9', '1', '1', '9', '8', '3', '3', '8', '2', '7', '8', '11', '2', '8', '9', '1', '8', '3', '1', '2', '1', '2', '2', '2', '2', '8', '3', '9', '2', '10', '9', '9', '2', '7', '7', '7', '1', '3', '8', '8', '1', '1', '8', '7', '1', '8', '8', '9', '3', '3', '3', '2', '4', '8', '12', '8', '1', '12', '9', '2', '8', '2', '12', '2', '1', '3', '8', '12', '12', '11', '8', '10', '9', '2', '9', '4', '10', '1', '12', '12', '9', '3', '8', '9', '12', '9', '9', '2', '2', '2', '3', '5', '1', '1', '1', '7', '7', '8', '2', '2', '8', '2', '1', '7', '3', '2', '9', '8', '9', '1', '1', '9', '8', '9', '3', '3', '2', '9', '4', '9', '8', '8', '1', '8', '10', '2', '0', '3', '8', '8', '8', '9', '12', '9', '8', '2', '2', '8', '10', '9', '8', '2', '2', '4', '1', '9', '9', '8', '8', '10', '3', '1', '1', '2', '8', '7', '8', '2', '1', '1', '8', '12', '2', '9', '4', '8', '3', '3', '3', '8', '2', '12', '12', '8', '9', '12', '8', '2', '12', '7', '9', '2', '2', '1', '3', '1', '3', '11', '9', '8', '1', '10', '2', '12', '3', '8', '4', '1', '2', '8', '8', '3', '9', '3', '8', '7', '1', '1', '3', '1', '8', '2', '9', '8', '12', '1', '1', '1', '9', '9', '1', '8', '8', '8', '2', '2', '12', '3', '1', '1', '1', '9', '9', '3', '2', '1', '1', '9', '9', '1', '9', '9', '1', '9', '3', '9', '1', '8', '1', '9', '8', '3', '8', '8', '9', '9', '8', '2', '8', '1', '2', '2', '10', '1', '2', '1', '1', '9', '1', '9', '3', '7', '7', '2', '10', '3', '3', '1', '1', '9', '1', '2', '1', '3', '2', '1', '1', '1', '1', '1', '1', '1', '1', '1', '2', '8', '8', '7', '1', '1', '8', '8', '2', '1', '12', '1', '8', '1', '9', '8', '8', '2', '1', '9', '9', '2', '2', '8', '1', '7', '7', '9', '8', '1', '6', '8', '8', '2', '2', '8', '3', '8', '3', '2', '3', '8', '1', '2', '2', '2', '8', '9', '9', '9', '12', '1', '9', '2', '1', '9', '2', '9', '9', '10', '8', '1', '1', '9', '2', '1', '7', '1', '3', '7', '1', '1', '1', '3', '8', '8', '8', '9', '9', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '1', '0', '1', '1', '1', '2', '2', '3', '1', '1', '2', '3', '3', '3', '7', '3', '1', '9', '3', '9', '1', '4', '1', '1', '9', '10', '9', '8', '2', '3', '10', '9', '3', '10', '12', '8', '9', '8', '2', '1', '9', '8', '1', '7', '8', '1', '9', '1', '2', '10', '7', '7', '8', '1', '3', '3', '9', '3', '3', '1', '1', '1', '9', '1', '3', '8', '8', '8', '1', '8', '1', '9', '7', '8', '8', '2', '1', '3', '8', '2', '1', '6', '1', '8', '8', '2', '8', '2', '2', '9', '8', '1', '1', '6', '8', '6', '10', '2', '1', '1', '8', '9', '3', '3', '3', '1', '3', '9', '6', '8', '1', '8', '3', '1', '2', '2', '12', '7', '1', '1', '1', '1', '12', '1', '8', '8', '3', '2', '1', '9', '2', '3', '3', '2', '2', '7', '2', '2', '2', '3', '1', '9', '8', '7', '1', '1', '3', '1', '9', '4', '3', '9', '2', '1', '1', '9', '8', '8', '7', '2', '1', '3', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '2', '10', '1', '1', '8', '2', '8', '1', '8', '1', '3', '1', '2', '1', '1', '3', '1', '2', '1', '1', '1', '1', '1', '1', '1', '2', '1', '8', '1', '9', '8', '8', '1', '10', '8', '8', '1', '3', '9', '8', '9', '8', '2', '1', '3', '4', '1', '12', '1', '9', '8', '2', '7', '2', '1', '2', '9', '2', '1', '3', '8', '1', '2', '3', '8', '2', '10', '10', '10', '9', '9', '9', '3', '8', '3', '1', '1', '1', '1', '1', '1', '2', '1', '9', '2', '7', '9', '8', '7', '9', '1', '1', '1', '1', '8', '2', '2', '3', '3', '2', '12', '8', '3', '1', '1', '12', '9', '8', '2', '2', '9', '1', '2', '2', '9', '1', '1', '2', '4', '8', '1', '8', '10', '2', '9', '9', '9', '1', '2', '1', '1', '2', '2', '2', '12', '1', '2', '8', '3', '9', '9', '3', '2', '9', '9', '7', '2', '8', '8', '9', '2', '9', '1', '1', '1', '1', '1', '2', '8', '2', '1', '2', '2', '8', '3', '1', '1', '9', '1', '3', '10', '7', '9', '8', '8', '2', '2', '9', '2', '2', '2', '2', '2', '2', '9', '1', '3', '6', '4', '2', '2', '9', '1', '9', '8', '1', '1', '1', '7', '3', '2', '9', '9', '1', '9', '7', '2', '8', '9', '9', '12', '10', '8', '10', '2', '3', '1', '2', '2', '12', '12', '3', '12', '1', '2', '1', '2', '1', '2', '8', '2', '3', '12', '12', '8', '2', '2', '9', '2', '9', '2', '3', '3', '1', '1', '1', '12', '3', '7', '1', '3', '1', '2', '9', '2', '7', '2', '1', '8', '3', '7', '8', '1', '1', '3', '8', '3', '2', '9', '3', '1', '1', '1', '1', '11', '1', '2', '6', '1', '2', '3', '8', '9', '9', '2', '9', '2', '1', '1', '3', '1', '12', '9', '1', '1', '9', '1', '9', '9', '8', '9', '8', '1', '2', '7', '1', '0', '9', '8', '2', '3', '2', '4', '1', '1', '1', '8', '1', '12', '1', '2', '3', '3', '9', '9', '9', '9', '9', '7', '9', '3', '9', '1', '7', '3', '3', '3', '3', '7', '8', '2', '2', '2', '8', '2', '3', '9', '9', '3', '1', '8', '9', '12', '8', '8', '3', '2', '1', '6', '1', '9', '1', '1', '1', '1', '1', '1', '2', '2', '1', '2', '2', '2', '2', '9', '2', '5', '3', '2', '2', '8', '2', '1', '8', '3', '3', '8', '3', '1', '3', '9', '2', '8', '9', '4', '4', '9', '8', '8', '7', '7', '3', '1', '8', '8', '3', '2', '2', '10', '3', '3', '2', '2', '2', '2', '12', '3', '1', '2', '7', '7', '12', '2', '1', '2', '1', '1', '2', '1', '3', '3', '3', '1', '3', '9', '4', '8', '8', '4', '2', '4', '2', '8', '9', '1', '1', '2', '2', '2', '8', '7', '2', '8', '2', '9', '1', '8', '1', '1', '2', '1', '1', '1', '2', '9', '9', '8', '1', '8', '1', '4', '8', '1', '2', '7', '9', '2', '1', '8', '9', '8', '9', '1', '1', '2', '3', '2', '12', '7', '1', '2', '4', '4', '4', '2', '2', '2', '9', '8', '8', '1', '12', '3', '3', '2', '2', '3', '12', '1', '5', '1', '1', '3', '2', '8', '8', '2', '2', '2', '3', '9', '8', '8', '9', '8', '8', '11', '8', '2', '9', '2', '3', '1', '7', '1', '8', '2', '8', '1', '1', '1', '8', '1', '1', '1', '7', '8', '8', '2', '3', '2', '1', '8', '8', '1', '8', '4', '8', '9', '8', '9', '8', '1', '2', '3', '2', '1', '2', '2', '3', '1', '9', '4', '8', '1', '1', '9', '1', '1', '8', '2', '2', '2', '1', '7', '3', '3', '1', '3', '3', '12', '3', '1', '1', '3', '1', '1', '4', '8', '4', '4', '3', '3', '3', '4', '8', '1', '9', '9', '9', '4', '1', '7', '4', '8', '4', '9', '8', '1', '1', '3', '8', '9', '1', '9', '7', '1', '1', '9', '1', '8', '8', '8', '1', '1', '8', '2', '2', '2', '9', '4', '4', '1', '2', '1', '2', '9', '1', '1', '8', '5', '5', '5', '9', '12', '4', '1', '2', '1', '8', '8', '1', '9', '9', '2', '12', '1', '1', '7', '8', '8', '4', '1', '2', '8', '11', '2', '3', '2', '2', '1', '11', '2', '2', '2', '2', '9', '9', '2', '2', '9', '3', '1', '9', '1', '1', '1', '7', '8', '1', '8', '9', '1', '2', '9', '8', '1', '2', '9', '8', '12', '2', '2', '10', '8', '2', '2', '8', '8', '1', '2', '6', '1', '1', '7', '7', '2', '3', '2', '1', '1', '8', '1', '10', '8', '12', '6', '4', '1', '4', '1', '2', '9', '8', '9', '4', '8', '8', '10', '8', '8', '8', '8', '8', '2', '2', '8', '4', '9', '3', '3', '3', '3', '3', '9', '2', '1', '1', '2', '3', '2', '1', '2', '3', '3', '3', '2', '2', '3', '3', '2', '1', '2', '3', '3', '3', '3', '12', '9', '3', '2', '2', '3', '1', '5', '1', '1', '3', '2', '2', '1', '2', '9', '3', '9', '8', '1', '8', '7', '7', '9', '1', '1', '7', '9', '10', '1', '7', '1', '4', '3', '8', '9', '4', '9', '9', '2', '9', '2', '8', '4', '2', '2', '9', '5', '2', '1', '10', '9', '3', '1', '1', '1', '2', '1', '2', '9', '1', '7', '9', '9', '8', '2', '2', '8', '9', '1', '9', '8', '2', '3', '2', '2', '1', '2', '2', '2', '1', '1', '8', '1', '9', '4', '12', '8', '2', '7', '2', '2', '1', '9', '3', '1', '8', '12', '1', '1', '2', '1', '10', '2', '6', '12', '12', '4', '9', '1', '9', '10', '1', '2', '8', '2', '1', '8', '6', '12', '9', '9', '2', '1', '7', '8', '9', '5', '8', '1', '8', '8', '3', '1', '8', '2', '8', '2', '2', '1', '2', '9', '2', '8', '1', '12', '8', '2', '7', '3', '3', '12', '2', '3', '3', '9', '9', '3', '3', '3', '3', '3', '2', '1', '4', '1', '4', '8', '8', '2', '1', '2', '9', '12', '1', '10', '12', '1', '2', '9', '4', '8', '10', '3', '2', '8', '7', '4', '8', '9', '2', '7', '1', '10', '2', '2', '2', '8', '4', '2', '2', '1', '1', '8', '7', '9', '2', '1', '9', '8', '9', '12', '4', '8', '2', '8', '7', '7', '1', '9', '2', '1', '1', '9', '1', '8', '2', '3', '9', '9', '1', '3', '2', '9', '2', '2', '8', '1', '1', '2', '1', '2', '1', '9', '8', '2', '2', '4', '8', '8', '9', '5', '5', '9', '8', '9', '6', '2', '1', '12', '2', '3', '8', '8', '9', '2', '2', '1', '3', '2', '10', '9', '10', '7', '2', '8', '4', '2', '8', '9', '8', '1', '3', '8', '8', '1', '8', '3', '8', '8', '1', '10', '3', '1', '5', '8', '8', '9', '2', '2', '9', '2', '1', '9', '1', '4', '1', '12', '8', '8', '2', '1', '1', '1', '1', '1', '9', '2', '9', '8', '9', '5', '9', '1', '8', '2', '4', '1', '1', '1', '9', '12', '4', '1', '1', '9', '8', '4', '1', '8', '9', '8', '8', '9', '9', '9', '9', '9', '9', '1', '3', '4', '2', '2', '8', '2', '9', '2', '3', '8', '3', '1', '7', '2', '1', '1', '4', '2', '9', '2', '1', '3', '2', '9', '2', '9', '3', '8', '1', '9', '1', '1', '2', '8', '8', '8', '2', '3', '9', '9', '8', '2', '4', '1', '5', '1', '1', '9', '9', '2', '1', '1', '1', '9', '9', '1', '3', '9', '2', '2', '9', '9', '8', '3', '9', '9', '8', '2', '2', '8', '9', '2', '2', '8', '9', '10', '10', '2', '8', '7', '9', '9', '4', '2', '4', '11', '2', '1', '7', '8', '8', '8', '9', '4', '9', '8', '8', '2', '1', '1', '9', '8', '3', '3', '8', '8', '7', '9', '1', '3', '4', '8', '10', '2', '4', '4', '8', '2', '8', '2', '8', '9', '2', '1', '2', '2', '3', '1', '9', '4', '5', '9', '9', '1', '1', '1', '1', '1', '1', '7', '1', '1', '1', '8', '1', '1', '1', '1', '1', '9', '1', '2', '1', '12', '9', '3', '9', '9', '4', '1', '9', '1', '1', '2', '11', '9', '3', '2', '2', '2', '4', '2', '3', '2', '7', '7', '8', '1', '10', '9', '5', '7', '1', '10', '8', '1', '2', '7', '1', '12', '5', '8', '4', '2', '9', '9', '8', '2', '9', '8', '8', '10', '2', '3', '2', '7', '2', '8', '10', '3', '2', '9', '9', '9', '1', '1', '1', '1', '8', '2', '10', '9', '8', '1', '2', '2', '2', '1', '2', '1', '3', '9', '12', '2', '2', '4', '8', '1', '3', '9', '4', '9', '1', '9', '9', '2', '1', '1', '9', '7', '10', '3', '10', '2', '9', '2', '9', '2', '8', '2', '1', '2', '3', '1', '1', '1', '8', '5', '8', '3', '1', '1', '4', '2', '2', '1', '3', '8', '2', '1', '1', '1', '2', '9', '4', '1', '8', '1', '2', '8', '2', '9', '2', '2', '5', '5', '2', '5', '2', '1', '8', '8', '1', '3', '12', '2', '4', '3', '2', '2', '2', '6', '2', '3', '3', '1', '8', '2', '2', '1', '1', '8', '8', '10', '1', '9', '2', '9', '12', '0', '2', '1', '3', '2', '2', '1', '2', '1', '9', '9', '2', '9', '8', '8', '1', '7', '8', '1', '2', '6', '9', '8', '10', '1', '12', '2', '2', '1', '8', '2', '1', '12', '7', '4', '2', '10', '2', '2', '1', '2', '1', '2', '1', '1', '2', '1', '9', '8', '10', '2', '10', '9', '9', '12', '5', '3', '2', '8', '2', '8', '3', '3', '1', '4', '9', '10', '2', '4', '8', '1', '2', '10', '12', '12', '3', '1', '1', '2', '9', '6', '6', '1', '9', '1', '8', '1', '8', '8', '8', '8', '2', '2', '2', '8', '8', '2', '1', '1', '1', '1', '1', '8', '8', '5', '8', '8', '1', '8', '9', '9', '1', '7', '12', '9', '9', '7', '2', '2', '7', '8', '9', '8', '2', '9', '6', '3', '4', '8', '4', '1', '2', '9', '8', '8', '8', '2', '1', '1', '8', '8', '3', '1', '9', '9', '7', '1', '2', '2', '2', '9', '4', '1', '2', '12', '1', '8', '2', '1', '9', '1', '8', '9', '9', '2', '5', '4', '2', '1', '2', '8', '12', '8', '10', '3', '8', '8', '8', '2', '2', '8', '2', '8', '2', '4', '8', '10', '2', '7', '2', '1', '8', '8', '9', '5', '8', '1', '12', '7', '10', '2', '2', '2', '1', '4', '8', '12', '1', '8', '2', '2', '9', '1', '8', '3', '1', '1', '5', '10', '2', '1', '2', '4', '8', '9', '3', '2', '8', '7', '4', '4', '9', '9', '2', '8', '1', '1', '2', '4', '7', '1', '9', '1', '1', '1', '8', '4', '1', '3', '1', '2', '2', '9', '1', '8', '8', '2', '2', '9', '9', '1', '8', '8', '1', '9', '8', '8', '2', '2', '2', '2', '1', '1', '1', '7', '4', '1', '1', '4', '2', '3', '3', '2', '3', '9', '2', '8', '2', '1', '5', '1', '2', '2', '1', '3', '5', '2', '9', '1', '1', '9', '8', '1', '4', '8', '1', '8', '2', '4', '8', '4', '7', '9', '8', '1', '2', '2', '4', '4', '11', '3', '11', '1', '3', '8', '1', '9', '6', '8', '9', '3', '9', '8', '9', '2', '3', '3', '7', '1', '7', '2', '9', '9', '1', '8', '8', '1', '5', '5', '8', '8', '1', '2', '1', '3', '9', '9', '5', '9', '8', '1', '4', '3', '8', '1', '1', '2', '1', '8', '9', '8', '2', '2', '2', '1', '2', '4', '10', '8', '1', '9', '1', '8', '1', '1', '9', '9', '8', '11', '1', '8', '2', '1', '2', '8', '2', '8', '8', '2', '9', '6', '10', '1', '8', '6', '4', '5', '11', '2', '2', '8', '1', '8', '1', '8', '1', '4', '8', '1', '1', '1', '8', '3', '7', '3', '3', '7', '1', '1', '4', '8', '1', '6', '4', '2', '2', '1', '1', '1', '1', '6', '2', '2', '8', '1', '8', '5', '5', '2', '1', '1', '2', '3', '7', '1', '1', '6', '2', '9', '3', '9', '8', '9', '2', '1', '4', '2', '9', '13', '9', '2', '8', '12', '4', '1', '1', '9', '5', '1', '1', '1', '1', '8', '2', '1', '1', '2', '9', '0', '12', '1', '1', '8', '9', '8', '3', '2', '2', '12', '8', '2', '2', '2', '1', '10', '1', '9', '8', '9', '5', '2', '2', '9', '2', '8', '4', '8', '3', '7', '9', '2', '2', '7', '2', '2', '8', '1', '1', '9', '10', '9', '8', '2', '9', '0', '7', '1', '1', '1', '9', '8', '2', '8', '12', '8', '3', '2', '8', '8', '7', '7', '8', '12', '2', '1', '3', '9', '2', '2', '8', '2', '8', '8', '12', '8', '8', '1', '8', '9', '2', '4', '4', '7', '2', '1', '1', '9', '3', '7', '1', '3', '9', '8', '2', '2', '3', '1', '3', '10', '2', '3', '7', '1', '8', '1', '1', '10', '3', '10', '8', '4', '1', '2', '4', '3', '10', '9', '3', '9', '5', '8', '7', '2', '9', '3', '3', '2', '11', '8', '1', '1', '7', '2', '7', '2', '9', '4', '8', '1', '12', '8', '7', '9', '1', '0', '2', '6', '4', '9', '3', '2', '4', '12', '9', '8', '5', '9', '2', '8', '4', '6', '8', '9', '3', '1', '8', '7', '3', '1', '1', '1', '8', '9', '1', '8', '7', '9', '8', '2', '1', '2', '2', '3', '2', '2', '1', '1', '4', '9', '7', '3', '2', '8', '8', '9', '1', '9', '7', '4', '2', '2', '8', '5', '9', '1', '5', '7', '1', '4', '6', '9', '10', '1', '8', '2', '9', '1', '9', '2', '9', '8', '2', '9', '9', '11', '2', '12', '2', '2', '9', '4', '8', '1', '9', '9', '2', '8', '9', '1', '2', '9', '3', '8', '8', '1', '7', '8', '8', '2', '3', '2', '2', '2', '2', '9', '8', '10', '2', '2', '1', '9', '11', '1', '8', '8', '2', '4', '2', '1', '2', '8', '8', '9', '2', '8', '9', '2', '3', '1', '2', '1', '3', '10', '8', '1', '2', '4', '1', '9', '8', '9', '3', '3', '9', '8', '2', '11', '3', '7', '1', '8', '7', '1', '8', '8', '8', '1', '4', '8', '8', '1', '9', '3', '1', '1', '2', '8', '8', '8', '2', '1', '8', '10', '2', '12', '8', '8', '9', '1', '8', '1', '3', '2', '2', '10', '7', '2', '2', '1', '1', '8', '2', '9', '1', '2', '9', '3', '3', '10', '1', '4', '7', '10', '12', '2', '4', '10', '8', '6', '2', '9', '1', '8', '3', '1', '2', '8', '8', '2', '12', '10', '2', '10', '9', '1', '2', '5', '8', '12', '8', '8', '7', '8', '8', '1', '8', '2', '2', '9', '1', '8', '10', '5', '5', '5', '8', '1', '9', '1', '13', '1', '2', '2', '1', '2', '4', '9', '2', '1', '9', '13', '11', '1', '9', '9', '3', '10', '8', '7', '8', '8', '4', '2', '1', '9', '4', '3', '1', '1', '10', '9', '2', '8', '6', '2', '1', '1', '3', '2', '8', '1', '1', '1', '1', '2', '4', '1', '1', '10', '1', '8', '1', '1', '9', '1', '7', '1', '1', '9', '9', '2', '12', '9', '1', '8', '1', '12', '1', '8', '3', '8', '1', '8', '1', '2', '10', '1', '1', '2', '2', '9', '3', '9', '12', '1', '9', '1', '2', '8', '7', '2', '1', '1', '3', '2', '8', '4', '4', '5', '12', '7', '8', '9', '6', '1', '8', '9', '2', '1', '9', '1', '4', '1', '8', '2', '1', '1', '8', '7', '4', '9', '1', '8', '4', '2', '7', '5', '1', '3', '9', '2', '8', '2', '1', '2', '9', '1', '1', '2', '3', '4', '3', '1', '2', '2', '10', '2', '2', '8', '1', '1', '2', '4', '9', '2', '11', '9', '8', '9', '2', '1', '10', '4', '1', '1', '10', '7', '2', '10', '8', '1', '10', '8', '8', '8', '8', '8', '8', '2', '2', '2', '8', '3', '4', '1', '1', '1', '2', '1', '8', '8', '2', '8', '3', '8', '3', '1', '2', '3', '1', '1', '10', '1', '4', '4', '3', '3', '1', '9', '1', '1', '2', '1', '1', '6', '1', '1', '1', '9', '1', '4', '1', '1', '1', '1', '1', '1', '2', '12', '8', '8', '1', '9', '12', '10', '8', '2', '12', '1', '1', '2', '1', '7', '10', '7', '1', '2', '1', '10', '8', '2', '2', '8', '2', '2', '1', '9', '8', '8', '8', '8', '3', '1', '4', '2', '8', '3', '10', '10', '1', '9', '1', '9', '1', '9', '1', '8', '3', '1', '1', '8', '8', '8', '6', '3', '4', '1', '1', '1', '9', '9', '5', '2', '10', '2', '6', '2', '2', '7', '1', '8', '2', '11', '1', '1', '1', '2', '1', '9', '4', '4', '2', '8', '4', '6', '8', '8', '10', '10', '2', '1', '2', '9', '1', '8', '8', '3', '4', '8', '8', '2', '8', '8', '1', '2', '1', '2', '9', '4', '1', '3', '3', '3', '8', '8', '8', '2', '6', '4', '3', '3', '12', '3', '4', '9', '6', '8', '7', '7', '8', '6', '4', '1', '10', '6', '3', '3', '8', '1', '1', '8', '1', '11', '8', '1', '1', '2', '3', '11', '7', '2', '1', '1', '1', '9', '1', '2', '1', '1', '9', '8', '1', '1', '8', '0', '4', '1', '1', '2', '1', '1', '2', '2', '9', '1', '10', '1', '3', '8', '9', '3', '3', '1', '10', '1', '9', '1', '1', '7', '4', '1', '8', '10', '1', '2', '10', '1', '1', '2', '3', '3', '9', '1', '8', '10', '1', '7', '9', '1', '9', '6', '3', '9', '2', '8', '4', '12', '12', '3', '1', '1', '9', '1', '9', '2', '1', '1', '1', '1', '1', '9', '2', '1', '2', '10', '9', '8', '8', '10', '8', '2', '3', '2', '1', '12', '8', '12', '2', '9', '1', '3', '2', '5', '2', '2', '8', '2', '2', '4', '10', '9', '10', '9', '4', '2', '4', '12', '8', '2', '1', '8', '3', '9', '2', '9', '4', '1', '1', '1', '1', '1', '9', '6', '2', '9', '2', '1', '3', '2', '2', '5', '8', '9', '1', '6', '3', '1', '2', '2', '2', '6', '1', '8', '10', '8', '3', '3', '13', '2', '9', '9', '10', '6', '12', '8', '2', '9', '2', '3', '8', '8', '1', '2', '2', '3', '2', '10', '1', '8', '9', '1', '8', '9', '8', '8', '2', '10', '2', '1', '1', '1', '9', '9', '12', '1', '1', '4', '8', '3', '2', '2', '4', '4', '2', '2', '1', '1', '1', '9', '2', '2', '4', '1', '1', '8', '8', '8', '9', '2', '13', '9', '9', '4', '1', '9', '7', '10', '10', '8', '1', '9', '1', '1', '1', '8', '12', '4', '1', '2', '1', '1', '1', '7', '8', '3', '3', '1', '2', '2', '7', '2', '2', '10', '1', '8', '4', '0', '1', '9', '4', '1', '1', '2', '10', '10', '7', '9', '3', '12', '11', '2', '8', '10', '8', '9', '4', '3', '4', '10', '1', '1', '9', '3', '10', '1', '5', '3', '3', '2', '2', '1', '1', '6', '1', '7', '1', '1', '12', '1', '1', '1', '10', '1', '8', '8', '1', '3', '8', '3', '1', '2', '8', '3', '4', '2', '8', '8', '6', '1', '1', '4', '1', '2', '9', '5', '1', '7', '8', '9', '2', '4', '9', '8', '5', '9', '9', '9', '1', '2', '1', '3', '8', '9', '2', '9', '8', '9', '1', '8', '13', '9', '3', '8', '1', '4', '1', '3', '1', '6', '4', '10', '2', '2', '7', '8', '8', '12', '8', '9', '9', '9', '10', '3', '12', '12', '2', '9', '10', '9', '7', '2', '3', '4', '10', '10', '2', '7', '7', '1', '7', '3', '9', '1', '9', '8', '2', '9', '1', '8', '1', '10', '8', '8', '2', '7', '8', '1', '8', '9', '8', '1', '4', '1', '8', '8', '8', '1', '1', '10', '3', '2', '2', '6', '8', '9', '8', '2', '1', '1', '3', '8', '8', '1', '4', '4', '3', '9', '10', '4', '2', '4', '2', '9', '2', '2', '9', '1', '1', '10', '1', '1', '2', '1', '1', '1', '9', '1', '8', '8', '8', '2', '3', '8', '1', '6', '7', '3', '1', '9', '1', '2', '1', '6', '1', '2', '10', '1', '4', '2', '3', '2', '9', '9', '4', '9', '9', '1', '2', '8', '4', '7', '8', '7', '13', '9', '1', '2', '1', '9', '3', '3', '6', '8', '9', '10', '9', '2', '2', '3', '12', '1', '1', '1', '13', '2', '1', '7', '10', '8', '9', '1', '2', '9', '1', '1', '1', '2', '1', '12', '5', '6', '9', '10', '3', '1', '10', '1', '3', '2', '8', '9', '2', '10', '8', '8', '9', '2', '8', '9', '2', '2', '2', '8', '8', '2', '3', '1', '4', '8', '1', '9', '9', '2', '8', '10', '9', '1', '2', '12', '8', '8', '0', '2', '1', '1', '9', '10', '1', '6', '2', '2', '10', '2', '2', '4', '2', '2', '12', '1', '6', '7', '8', '9', '1', '3', '5', '7', '3', '1', '3', '1', '1', '3', '2', '6', '3', '2', '1', '8', '3', '5', '2', '1', '1', '8', '1', '1', '0', '2', '3', '5', '3', '1', '1', '9', '8', '8', '8', '8', '4', '7', '2', '9', '12', '5', '9', '3', '12', '3', '9', '1', '9', '1', '7', '3', '9', '3', '10', '1', '2', '1', '4', '9', '8', '2', '9', '11', '2', '8', '9', '4', '1', '1', '1', '1', '1', '1', '1', '9', '1', '9', '1', '1', '1', '12', '3', '10', '3', '6', '1', '7', '10', '8', '3', '7', '1', '1', '2', '6', '1', '3', '9', '2', '8', '1', '8', '8', '1', '8', '10', '12', '1', '10', '12', '10', '1', '8', '8', '1', '4', '7', '9', '2', '1', '9', '3', '8', '12', '10', '6', '1', '6', '8', '3', '1', '3', '1', '10', '10', '9', '8', '1', '2', '9', '8', '8', '1', '9', '8', '8', '3', '1', '3', '9', '12', '1', '8', '1', '5', '8', '11', '5', '5', '1', '2', '1', '3', '1', '1', '1', '9', '8', '8', '1', '9', '10', '8', '4', '1', '1', '8', '10', '8', '9', '1', '10', '1', '7', '2', '1', '1', '9', '6', '2', '9', '8', '1', '1', '1', '9', '8', '8', '1', '9', '9', '1', '1', '2', '7', '9', '2', '9', '9', '8', '7', '6', '8', '7', '8', '2', '9', '2', '1', '8', '8', '8', '2', '6', '3', '1', '8', '12', '12', '8', '7', '1', '2', '2', '7', '1', '9', '8', '1', '4', '1', '1', '8', '5', '8', '1', '8', '8', '8', '8', '1', '2', '11', '1', '1', '1', '2', '7', '1', '9', '2', '2', '2', '9', '8', '9', '2', '9', '1', '8', '1', '7', '11', '13', '1', '9', '8', '2', '2', '1', '2', '3', '10', '3', '10', '4', '3', '9', '1', '8', '1', '1', '13', '1', '1', '3', '8', '1', '2', '2', '2', '9', '4', '1', '8', '1', '9', '3', '6', '8', '5', '2', '8', '2', '8', '1', '9', '2', '2', '8', '1', '1', '4', '8', '9', '2', '2', '7', '1', '8', '12', '2', '12', '8', '2', '9', '9', '3', '8', '1', '1', '9', '9', '9', '2', '8', '1', '9', '1', '8', '9', '8', '2', '2', '4', '1', '9', '10', '10', '8', '8', '1', '1', '2', '1', '4', '1', '2', '1', '1', '3', '12', '8', '8', '8', '8', '10', '2', '8', '12', '1', '9', '9', '8', '9', '1', '2', '1', '8', '8', '8', '2', '8', '8', '3', '10', '8', '9', '1', '1', '1', '3', '4', '8', '10', '6', '3', '1', '3', '2', '8', '9', '3', '5', '4', '1', '5', '2', '9', '9', '9', '1', '1', '1', '9', '9', '6', '10', '9', '8', '12', '2', '9', '1', '2', '1', '1', '1', '8', '1', '12', '1', '12', '1', '9', '1', '8', '1', '2', '1', '2', '8', '8', '10', '1', '9', '1', '2', '2', '8', '8', '3', '12', '1', '11', '2', '10', '12', '11', '2', '1', '10', '1', '1', '9', '3', '10', '2', '7', '1', '8', '1', '9', '9', '2', '1', '3', '12', '5', '9', '9', '8', '1', '8', '8', '8', '1', '1', '3', '9', '8', '9', '8', '3', '10', '3', '7', '8', '9', '2', '3', '1', '9', '10', '9', '2', '8', '2', '6', '4', '1', '1', '3', '8', '2', '3', '9', '1', '2', '1', '8', '1', '2', '2', '2', '2', '9', '9', '4', '8', '9', '1', '1', '2', '9', '1', '5', '2', '9', '8', '10', '1', '2', '10', '8', '5', '6', '4', '8', '1', '1', '1', '8', '9', '2', '8', '8', '8', '12', '2', '2', '1', '8', '1', '2', '1', '9', '8', '8', '9', '1', '1', '8', '7', '1', '8', '0', '2', '3', '8', '1', '1', '8', '1', '1', '12', '3', '2', '10', '8', '10', '6', '1', '8', '8', '10', '8', '8', '1', '4', '8', '1', '2', '9', '8', '1', '1', '3', '3', '5', '1', '1', '2', '1', '2', '1', '9', '8', '3', '9', '8', '2', '10', '9', '1', '8', '1', '10', '9', '10', '1', '2', '3', '8', '1', '3', '1', '9', '9', '8', '1', '9', '8', '8', '9', '1', '9', '2', '8', '8', '2', '13', '8', '10', '8', '9', '3', '2', '1', '3', '1', '12', '10', '10', '1', '11', '1', '2', '4', '2', '10', '1', '8', '11', '1', '1', '1', '2', '2', '8', '2', '9', '8', '9', '9', '12', '8', '1', '9', '2', '4', '1', '8', '3', '9', '1', '6', '5', '8', '9', '3', '3', '2', '1', '1', '1', '8', '11', '1', '8', '7', '1', '11', '9', '1', '7', '9', '9', '8', '8', '8', '12', '8', '1', '2', '9', '8', '8', '2', '10', '9', '8', '8', '4', '10', '9', '8', '2', '10', '10', '2', '1', '2', '1', '2', '1', '4', '1', '7', '1', '1', '3', '12', '9', '8', '5', '9', '10', '1', '9', '9', '2', '7', '4', '1', '1', '8', '12', '8', '2', '2', '9', '4', '1', '1', '5', '9', '1', '8', '12', '8', '1', '1', '4', '9', '10', '2', '3', '1', '3', '3', '3', '8', '1', '2', '1', '2', '1', '2', '9', '1', '2', '8', '9', '4', '8', '8', '10', '2', '12', '5', '5', '1', '8', '1', '7', '8', '1', '1', '8', '9', '12', '4', '9', '3', '1', '2', '5', '9', '2', '1', '2', '2', '1', '2', '8', '1', '8', '8', '2', '2', '1', '8', '4', '1', '2', '10', '9', '10', '8', '1', '10', '4', '8', '8', '10', '2', '8', '1', '1', '3', '10', '1', '4', '2', '8', '3', '3', '2', '9', '8', '5', '5', '9', '3', '10', '8', '1', '2', '1', '2', '9', '1', '1', '2', '9', '9', '8', '8', '11', '9', '4', '1', '8', '2', '7', '1', '2', '8', '2', '9', '9', '8', '2', '9', '9', '8', '8', '1', '2', '8', '1', '1', '2', '9', '8', '1', '4', '9', '1', '8', '12', '1', '12', '2', '8', '2', '9', '1', '12', '1', '2', '9', '1', '1', '9', '3', '2', '9', '9', '2', '11', '8', '7', '9', '9', '9', '8', '8', '1', '2', '8', '1', '1', '4', '1', '2', '10', '1', '9', '2', '1', '1', '1', '9', '1', '13', '4', '3', '2', '1', '1', '2', '2', '7', '8', '7', '1', '1', '2', '8', '8', '3', '4', '8', '2', '10', '13', '8', '8', '9', '2', '2', '1', '1', '10', '4', '2', '7', '9', '8', '2', '1', '9', '8', '1', '1', '9', '8', '9', '8', '2', '8', '2', '2', '9', '2', '1', '9', '1', '2', '4', '8', '2', '1', '1', '8', '1', '4', '1', '9', '3', '9', '2', '7', '1', '9', '6', '1', '10', '2', '2', '2', '2', '2', '10', '10', '10', '9', '9', '9', '3', '12', '10', '1', '1', '10', '1', '1', '2', '2', '1', '2', '3', '12', '8', '8', '1', '2', '9', '9', '1', '1', '10', '8', '9', '8', '3', '1', '1', '3', '1', '10', '1', '1', '1', '4', '1', '1', '1', '7', '10', '2', '1', '8', '1', '9', '3', '1', '10', '8', '8', '1', '5', '9', '1', '2', '1', '8', '8', '1', '3', '1', '10', '2', '1', '1', '2', '8', '2', '5', '3', '5', '2', '1', '1', '9', '1', '7', '9', '9', '2', '8', '1', '12', '4', '1', '4', '9', '9', '4', '8', '1', '10', '10', '8', '8', '8', '9', '2', '3', '5', '1', '8', '1', '1', '1', '7', '10', '1', '4', '12', '3', '2', '2', '9', '1', '1', '1', '2', '8', '4', '1', '7', '3', '12', '8', '6', '2', '2', '7', '9', '11', '1', '10', '9', '1', '8', '12', '2', '11', '1', '2', '3', '1', '1', '1', '8', '9', '2', '2', '2', '2', '3', '3', '8', '10', '4', '1', '2', '9', '8', '2', '2', '9', '1', '8', '11', '4', '2', '8', '7', '1', '1', '7', '3', '9', '9', '4', '5', '8', '10', '2', '9', '9', '9', '8', '1', '9', '2', '2', '2', '1', '2', '9', '4', '3', '12', '4', '3', '2', '3', '8', '3', '10', '1', '10', '1', '1', '8', '10', '6', '8', '9', '9', '1', '2', '9', '2', '9', '3', '8', '1', '1', '12', '1', '5', '1', '10', '8', '10', '2', '7', '1', '1', '1', '1', '3', '2', '3', '5', '9', '1', '2', '1', '9', '8', '10', '9', '8', '1', '8', '1', '8', '2', '8', '1', '2', '8', '1', '2', '5', '8', '12', '2', '1', '1', '1', '1', '1', '8', '8', '2', '1', '4', '2', '8', '8', '8', '2', '9', '9', '11', '1', '2', '9', '3', '1', '1', '8', '9', '2', '4', '10', '1', '9', '8', '4', '9', '8', '10', '8', '8', '9', '2', '8', '9', '3', '3', '3', '4', '3', '2', '2', '10', '8', '2', '1', '5', '1', '9', '1', '1', '2', '1', '2', '3', '1', '1', '8', '8', '1', '10', '8', '8', '1', '6', '4', '8', '2', '5', '8', '1', '3', '8', '2', '8', '9', '1', '1', '8', '12', '10', '1', '5', '8', '2', '8', '10', '9', '9', '1', '6', '9', '1', '10', '8', '2', '9', '10', '1', '2', '1', '1', '8', '8', '3', '9', '8', '1', '10', '2', '8', '1', '10', '1', '5', '1', '1', '1', '5', '1', '1', '1', '4', '1', '1', '1', '8', '2', '8', '1', '8', '3', '8', '9', '1', '8', '1', '8', '1', '1', '2', '2', '1', '12', '9', '9', '1', '8', '2', '1', '2', '8', '1', '9', '1', '1', '2', '1', '2', '2', '2', '1', '9', '9', '8', '8', '1', '1', '9', '10', '8', '4', '3', '3', '4', '9', '1', '3', '3', '10', '2', '2', '1', '8', '4', '1', '9', '4', '1', '1', '9', '8', '10', '4', '9', '9', '8', '2', '1', '3', '4', '9', '3', '8', '9', '1', '1', '2', '1', '7', '8', '2', '9', '1', '9', '2', '6', '9', '10', '8', '1', '1', '5', '9', '5', '1', '9', '10', '9', '3', '3', '8', '8', '2', '9', '2', '8', '2', '1', '8', '5', '8', '3', '10', '1', '1', '2', '1', '8', '1', '4', '3', '2', '9', '4', '9', '8', '8', '8', '1', '3', '9', '1', '9', '9', '1', '2', '1', '9', '8', '9', '8', '1', '1', '1', '4', '2', '2', '9', '9', '1', '1', '1', '3', '6', '1', '2', '3', '1', '4', '2', '9', '1', '9', '9', '1', '8', '2', '1', '9', '1', '8', '1', '8', '8', '2', '1', '9', '9', '6', '8', '8', '9', '8', '10', '8', '9', '5', '1', '1', '1', '1', '8', '1', '8', '12', '8', '1', '12', '9', '2', '8', '1', '1', '2', '1', '8', '6', '9', '9', '8', '1', '8', '9', '7', '7', '8', '9', '9', '1', '8', '3', '8', '1', '3', '3', '8', '9', '8', '2', '8', '1', '2', '9', '8', '1', '1', '1', '1', '9', '1', '9', '8', '12', '3', '8', '8', '1', '10', '10', '10', '2', '2', '1', '10', '2', '2', '10', '8', '12', '8', '1', '1', '1', '2', '1', '1', '3', '8', '1', '2', '2', '2', '1', '1', '6', '2', '8', '8', '8', '1', '1', '1', '9', '9', '2', '8', '10', '2', '8', '2', '1', '2', '9', '9', '2', '1', '1', '8', '8', '1', '3', '1', '8', '9', '1', '10', '9', '1', '1', '1', '1', '7', '2', '2', '1', '2', '8', '8', '1', '2', '1', '3', '2', '1', '8', '3', '9', '2', '10', '2', '10', '1', '2', '1', '1', '13', '9', '10', '8', '2', '2', '1', '1', '8', '2', '2', '1', '9', '1', '1', '8', '9', '9', '2', '9', '2', '2', '1', '1', '1', '8', '9', '4', '9', '9', '1', '8', '2', '1', '1', '8', '2', '2', '2', '8', '1', '1', '8', '2', '10', '2', '9', '9', '7', '1', '1', '2', '2', '8', '3', '1', '2', '1', '1', '8', '1', '1', '1', '9', '1', '1', '8', '8', '1', '9', '6', '1', '9', '1', '8', '1', '8', '8', '8', '1', '3', '8', '8', '2', '6', '3', '2', '1', '10', '1', '8', '2', '1', '6', '1', '1', '1', '8', '1', '10', '1', '1', '8', '9', '2', '6', '9', '2', '9', '4', '1', '8', '1', '3', '7', '8', '8', '5', '8', '7', '1', '1', '1', '13', '8', '3', '1', '1', '1', '1', '2', '8', '2', '12', '8', '1', '2', '5', '1', '8', '2', '10', '8', '2', '10', '1', '5', '2', '1', '1', '5', '2', '2', '7', '8', '2', '9', '9', '1', '10', '1', '8', '2', '5', '8', '1', '5', '1', '10', '1', '1', '9', '8', '6', '1', '8', '9', '8', '1', '8', '2', '2', '8', '3', '9', '1', '9', '3', '8', '4', '4', '3', '3', '1', '1', '1', '8', '9', '1', '1', '8', '3', '1', '2', '8', '2', '1', '1', '10', '1', '2', '2', '1', '1', '8', '2', '8', '2', '10', '2', '2', '10', '8', '8', '8', '13', '2', '2', '9', '2', '5', '9', '2', '1', '8', '8', '8', '1', '12', '12', '2', '9', '2', '1', '8', '2', '1', '8', '1', '9', '9', '2', '7', '1', '1', '7', '1', '4', '3', '1', '10', '3', '10', '9', '4', '6', '7', '5', '2', '1', '8', '8', '1', '4', '9', '2', '1', '8', '1', '1', '1', '1', '1', '8', '9', '8', '8', '8', '8', '9', '8', '10', '1', '8', '2', '7', '8', '9', '1', '2', '5', '9', '10', '8', '9', '12', '1', '6', '1', '2', '10', '9', '4', '1', '8', '10', '10', '1', '1', '1', '1', '1', '2', '3', '8', '1', '2', '10', '2', '1', '2', '4', '2', '2', '9', '1', '4', '2', '1', '1', '12', '12', '9', '9', '7', '1', '8', '4', '9', '9', '8', '1', '7', '3', '2', '1', '1', '4', '8', '1', '5', '8', '2', '8', '1', '14', '9', '12', '1', '10', '2', '1', '1', '8', '8', '6', '6', '3', '1', '2', '8', '1', '3', '2', '1', '8', '8', '1', '10', '8', '2', '8', '2', '5', '9', '1', '8', '8', '1', '1', '8', '8', '13', '1', '3', '8', '4', '1', '9', '7', '8', '8', '8', '3', '8', '1', '4', '2', '8', '8', '8', '7', '1', '1', '2', '8', '8', '9', '8', '8', '2', '3', '9', '9', '2', '1', '2', '2', '1', '8', '2', '6', '2', '1', '1', '9', '10', '3', '8', '2', '3', '2', '8', '8', '1', '8', '8', '8', '9', '1', '4', '1', '9', '2', '1', '8', '0', '1', '8', '1', '1', '8', '9', '9', '1', '3', '1', '2', '8', '8', '7', '1', '1', '2', '9', '2', '10', '2', '8', '2', '2', '2', '7', '1', '9', '8', '1', '3', '9', '2', '1', '10', '8', '1', '4', '1', '8', '5', '9', '8', '8', '1', '2', '2', '1', '8', '8', '6', '8', '1', '1', '8', '3', '2', '2', '1', '8', '1', '7', '6', '8', '1', '1', '1', '9', '1', '1', '1', '2', '1', '11', '10', '10', '11', '9', '']\n",
      "['8', '1', '8', '2', '8', '8', '8', '9', '7', '8', '1', '2', '1', '1', '8', '4', '8', '8', '12', '3', '3', '7', '3', '12', '1', '8', '8', '8', '8', '8', '8', '8', '1', '9', '5', '9', '9', '9', '11', '8', '8', '8', '4', '8', '8', '8', '8', '1', '3', '9', '3', '7', '1', '2', '9', '9', '7', '8', '8', '1', '10', '7', '8', '8', '9', '8', '7', '9', '9', '12', '7', '2', '8', '1', '11', '11', '1', '7', '7', '12', '1', '9', '8', '10', '12', '7', '8', '2', '8', '9', '9', '1', '8', '9', '1', '7', '12', '10', '10', '10', '8', '3', '7', '9', '8', '9', '1', '8', '8', '2', '7', '2', '9', '9', '11', '8', '8', '12', '12', '7', '8', '12', '4', '9', '3', '1', '12', '1', '1', '8', '8', '3', '8', '8', '8', '8', '9', '1', '8', '8', '10', '1', '8', '2', '8', '8', '7', '3', '8', '2', '4', '4', '9', '8', '10', '12', '12', '1', '1', '9', '1', '1', '1', '8', '2', '2', '8', '1', '1', '2', '2', '1', '2', '8', '1', '9', '9', '8', '8', '4', '2', '9', '9', '8', '3', '4', '3', '1', '8', '8', '2', '1', '9', '7', '8', '8', '1', '12', '3', '8', '2', '4', '2', '9', '12', '1', '4', '1', '8', '8', '8', '2', '2', '8', '9', '8', '8', '8', '10', '9', '8', '7', '9', '1', '1', '9', '4', '2', '4', '2', '2', '1', '7', '8', '11', '11', '3', '9', '2', '4', '8', '9', '1', '8', '1', '1', '4', '9', '1', '1', '8', '8', '2', '1', '8', '4', '2', '8', '9', '8', '8', '2', '8', '8', '8', '7', '1', '1', '1', '2', '1', '1', '8', '7', '8', '8', '12', '2', '12', '12', '8', '10', '12', '8', '3', '3', '12', '10', '1', '8', '12', '1', '8', '8', '2', '8', '4', '7', '8', '7', '10', '8', '10', '9', '8', '12', '12', '1', '8', '8', '3', '8', '8', '8', '8', '8', '1', '9', '8', '11', '1', '1', '1', '9', '8', '1', '9', '2', '3', '11', '8', '9', '9', '9', '2', '1', '8', '8', '9', '7', '1', '4', '9', '4', '8', '8', '4', '8', '12', '9', '4', '8', '2', '10', '10', '10', '8', '9', '9', '8', '8', '12', '7', '1', '8', '8', '8', '4', '1', '1', '1', '1', '1', '1', '8', '1', '9', '8', '9', '9', '4', '8', '12', '9', '8', '8', '2', '8', '8', '8', '6', '9', '8', '3', '7', '8', '8', '4', '12', '8', '8', '9', '12', '12', '9', '8', '2', '9', '2', '3', '1', '12', '8', '10', '9', '9', '9', '10', '10', '3', '8', '12', '1', '4', '2', '1', '10', '8', '2', '8', '4', '8', '9', '1', '9', '9', '10', '10', '1', '4', '9', '2', '4', '9', '1', '1', '3', '10', '3', '3', '8', '7', '3', '8', '9', '9', '12', '4', '8', '12', '2', '2', '4', '1', '9', '9', '4', '1', '4', '2', '8', '12', '2', '3', '10', '10', '9', '8', '9', '9', '1', '12', '8', '8', '8', '12', '4', '1', '8', '8', '1', '9', '8', '8', '2', '1', '8', '9', '8', '3', '3', '3', '1', '8', '8', '9', '1', '10', '9', '9', '9', '9', '5', '9', '9', '8', '8', '8', '8', '8', '8', '8', '8', '11', '12', '8', '8', '1', '8', '9', '11', '2', '2', '2', '2', '3', '1', '2', '2', '8', '2', '4', '9', '1', '2', '9', '8', '2', '8', '9', '9', '3', '10', '9', '9', '2', '8', '9', '8', '12', '12', '1', '3', '8', '8', '8', '2', '7', '7', '7', '7', '3', '9', '1', '9', '8', '9', '9', '1', '1', '1', '2', '9', '9', '9', '11', '1', '8', '8', '9', '1', '9', '8', '8', '8', '1', '1', '8', '7', '1', '1', '8', '8', '9', '4', '4', '8', '2', '2', '8', '8', '8', '8', '8', '8', '11', '8', '2', '9', '4', '9', '3', '9', '9', '1', '3', '9', '3', '1', '12', '8', '9', '12', '1', '8', '4', '2', '1', '4', '8', '3', '3', '8', '2', '8', '9', '7', '8', '8', '8', '5', '8', '3', '9', '8', '8', '13', '12', '1', '1', '2', '8', '4', '1', '9', '9', '12', '8', '9', '12', '9', '1', '9', '9', '9', '9', '3', '2', '9', '9', '4', '8', '12', '2', '4', '9', '3', '1', '9', '7', '8', '9', '9', '8', '4', '8', '8', '7', '9', '10', '3', '8', '8', '8', '1', '1', '1', '1', '8', '8', '4', '1', '10', '1', '5', '7', '7', '1', '8', '9', '3', '7', '2', '7', '7', '2', '4', '8', '12', '7', '4', '2', '9', '9', '12', '6', '10', '8', '2', '4', '12', '9', '9', '3', '8', '8', '1', '2', '10', '9', '9', '8', '4', '12', '2', '1', '8', '8', '8', '12', '10', '10', '9', '3', '8', '8', '9', '2', '8', '10', '1', '1', '1', '1', '2', '1', '1', '1', '1', '9', '8', '12', '9', '4', '8', '8', '9', '1', '9', '3', '9', '8', '8', '1', '7', '7', '10', '1', '8', '8', '1', '9', '8', '10', '3', '1', '7', '1', '8', '8', '12', '8', '8', '1', '8', '7', '1', '7', '7', '8', '2', '1', '8', '8', '2', '10', '8', '8', '8', '8', '8', '10', '1', '8', '8', '12', '8', '3', '3', '2', '2', '2', '10', '8', '8', '8', '2', '9', '1', '8', '9', '3', '2', '8', '10', '8', '6', '1', '1', '8', '4', '1', '9', '10', '8', '1', '7', '1', '2', '8', '1', '1', '1', '12', '1', '9', '12', '8', '12', '12', '12', '8', '8', '12', '4', '8', '8', '8', '8', '9', '9', '1', '3', '3', '3', '3', '1', '12', '12', '9', '10', '8', '8', '1', '9', '2', '2', '13', '9', '8', '9', '2', '1', '9', '1', '8', '8', '8', '4', '8', '1', '1', '1', '12', '12', '7', '2', '2', '2', '8', '3', '8', '9', '2', '10', '7', '8', '9', '2', '1', '2', '12', '12', '8', '8', '9', '2', '2', '9', '11', '1', '8', '1', '10', '9', '2', '1', '4', '7', '7', '7', '7', '7', '12', '8', '8', '8', '1', '1', '10', '1', '12', '1', '8', '2', '1', '1', '12', '8', '7', '9', '12', '8', '9', '3', '9', '8', '8', '8', '8', '3', '11', '2', '2', '9', '8', '8', '10', '8', '2', '7', '3', '1', '4', '7', '8', '8', '1', '8', '3', '7', '12', '8', '10', '9', '9', '8', '8', '2', '8', '9', '9', '1', '2', '8', '8', '9', '8', '3', '8', '1', '8', '10', '9', '8', '9', '9', '12', '4', '4', '8', '9', '9', '8', '2', '10', '1', '2', '8', '9', '1', '9', '9', '9', '7', '12', '12', '8', '1', '1', '1', '1', '8', '3', '1', '1', '8', '1', '8', '8', '7', '8', '8', '8', '8', '3', '2', '2', '10', '10', '10', '7', '8', '1', '2', '12', '7', '9', '8', '7', '8', '12', '2', '8', '9', '2', '6', '6', '7', '9', '8', '1', '8', '9', '8', '1', '12', '1', '2', '8', '7', '7', '7', '8', '2', '2', '8', '1', '2', '2', '9', '9', '1', '8', '8', '4', '3', '3', '1', '6', '3', '3', '12', '3', '8', '9', '1', '4', '3', '1', '8', '3', '9', '2', '8', '2', '8', '8', '8', '1', '1', '1', '9', '9', '8', '1', '9', '8', '1', '1', '3', '10', '8', '1', '1', '3', '9', '1', '4', '4', '1', '8', '9', '9', '2', '0', '0', '1', '8', '3', '1', '8', '8', '9', '8', '8', '1', '1', '8', '9', '8', '8', '8', '7', '9', '8', '8', '8', '10', '9', '8', '1', '2', '6', '1', '9', '9', '8', '12', '12', '12', '8', '8', '2', '8', '1', '2', '2', '2', '1', '9', '8', '2', '12', '2', '8', '12', '8', '9', '8', '8', '9', '7', '1', '1', '1', '1', '1', '8', '8', '1', '8', '8', '1', '1', '3', '2', '8', '8', '9', '10', '10', '2', '2', '1', '9', '2', '9', '9', '4', '12', '12', '12', '10', '7', '3', '3', '4', '2', '2', '9', '2', '8', '4', '2', '4', '1', '10', '9', '7', '8', '7', '1', '1', '3', '3', '1', '1', '3', '3', '3', '1', '1', '1', '1', '8', '2', '3', '1', '1', '2', '8', '8', '12', '8', '8', '8', '8', '11', '9', '1', '8', '9', '2', '8', '8', '8', '3', '9', '1', '9', '2', '7', '2', '8', '2', '8', '10', '8', '1', '10', '1', '1', '9', '9', '8', '8', '1', '8', '8', '8', '12', '8', '8', '8', '1', '8', '8', '8', '1', '9', '1', '1', '8', '1', '8', '9', '8', '2', '12', '9', '9', '0', '1', '8', '8', '1', '8', '12', '8', '8', '10', '8', '8', '8', '7', '8', '1', '8', '7', '3', '10', '1', '8', '9', '1', '8', '8', '8', '10', '1', '10', '3', '9', '1', '8', '9', '2', '8', '3', '3', '9', '9', '7', '9', '1', '1', '9', '2', '1', '1', '1', '7', '1', '1', '8', '8', '1', '1', '8', '1', '8', '3', '12', '9', '3', '3', '8', '8', '8', '8', '3', '1', '3', '3', '1', '11', '0', '8', '8', '7', '8', '12', '1', '8', '9', '8', '9', '8', '8', '3', '8', '8', '1', '1', '1', '9', '2', '2', '2', '8', '7', '12', '8', '8', '9', '10', '10', '7', '8', '1', '9', '8', '7', '3', '1', '3', '8', '2', '2', '3', '9', '8', '4', '4', '8', '9', '2', '1', '1', '7', '8', '9', '9', '7', '8', '7', '7', '8', '2', '2', '8', '4', '9', '7', '10', '0', '9', '8', '3', '7', '8', '1', '1', '8', '9', '9', '2', '2', '10', '1', '9', '10', '10', '10', '8', '3', '2', '12', '9', '9', '10', '12', '9', '12', '12', '9', '1', '2', '4', '12', '12', '7', '8', '9', '7', '7', '7', '3', '9', '8', '9', '1', '12', '8', '9', '4', '1', '3', '12', '12', '12', '12', '8', '8', '2', '1', '1', '2', '1', '1', '1', '12', '12', '8', '12', '2', '2', '12', '3', '3', '12', '8', '2', '8', '8', '12', '2', '1', '10', '3', '2', '8', '7', '1', '8', '1', '3', '7', '8', '9', '8', '3', '1', '1', '7', '8', '8', '9', '8', '2', '9', '2', '2', '9', '8', '1', '8', '8', '1', '3', '3', '1', '1', '10', '1', '2', '8', '1', '1', '1', '1', '9', '1', '4', '1', '7', '7', '7', '7', '2', '2', '8', '8', '12', '1', '9', '1', '7', '3', '3', '1', '8', '8', '10', '8', '9', '2', '9', '1', '3', '8', '8', '3', '12', '2', '8', '12', '2', '9', '1', '3', '3', '3', '3', '2', '8', '7', '9', '8', '3', '3', '1', '7', '8', '3', '1', '1', '12', '8', '9', '1', '2', '3', '8', '1', '1', '3', '3', '9', '1', '1', '1', '12', '1', '7', '3', '3', '1', '8', '8', '8', '1', '2', '4', '8', '1', '10', '2', '5', '3', '3', '12', '10', '9', '9', '12', '9', '0', '2', '8', '8', '9', '9', '9', '8', '1', '3', '1', '1', '4', '8', '1', '10', '8', '7', '2', '8', '2', '8', '4', '7', '8', '1', '9', '1', '9', '8', '2', '8', '2', '7', '9', '2', '2', '9', '1', '8', '12', '1', '8', '1', '4', '1', '9', '9', '1', '10', '12', '4', '8', '1', '7', '3', '9', '2', '12', '7', '8', '8', '2', '1', '12', '9', '8', '1', '2', '2', '8', '10', '2', '1', '7', '7', '7', '12', '3', '3', '8', '3', '8', '8', '3', '9', '8', '9', '1', '1', '8', '7', '9', '3', '3', '8', '1', '0', '9', '9', '9', '1', '8', '9', '9', '10', '1', '8', '7', '8', '8', '8', '9', '9', '8', '9', '10', '4', '9', '3', '7', '12', '1', '9', '9', '8', '9', '1', '9', '3', '1', '8', '4', '12', '10', '9', '8', '7', '10', '8', '12', '12', '3', '10', '8', '12', '2', '1', '2', '3', '9', '8', '8', '7', '1', '1', '2', '2', '1', '2', '2', '7', '1', '3', '9', '9', '3', '8', '8', '8', '8', '8', '7', '8', '8', '10', '8', '1', '8', '2', '8', '2', '2', '2', '2', '2', '1', '7', '12', '10', '1', '2', '8', '1', '4', '7', '8', '1', '12', '8', '7', '9', '2', '2', '2', '8', '1', '8', '1', '1', '0', '2', '2', '6', '1', '8', '2', '1', '8', '1', '1', '2', '2', '3', '2', '2', '10', '10', '9', '1', '7', '7', '8', '8', '1', '12', '10', '12', '3', '8', '8', '8', '3', '8', '3', '10', '2', '2', '2', '1', '2', '2', '1', '0', '1', '8', '9', '1', '1', '1', '2', '2', '2', '2', '2', '1', '1', '1', '1', '12', '9', '9', '1', '10', '11', '12', '9', '1', '1', '8', '8', '1', '2', '12', '8', '3', '7', '2', '10', '9', '7', '8', '1', '2', '2', '7', '9', '12', '2', '2', '1', '8', '9', '2', '3', '2', '2', '2', '9', '9', '8', '9', '2', '3', '8', '8', '9', '7', '3', '10', '8', '8', '9', '1', '1', '2', '8', '1', '10', '8', '8', '9', '7', '1', '7', '12', '8', '1', '7', '2', '1', '3', '9', '2', '8', '8', '1', '2', '3', '9', '3', '8', '9', '7', '7', '1', '10', '2', '8', '8', '8', '1', '8', '2', '8', '8', '1', '2', '8', '10', '2', '1', '0', '8', '3', '8', '10', '10', '12', '3', '8', '9', '3', '10', '8', '8', '8', '1', '8', '8', '2', '2', '2', '2', '2', '2', '1', '1', '10', '1', '10', '8', '2', '1', '2', '2', '8', '3', '3', '2', '2', '1', '8', '1', '3', '1', '9', '2', '2', '1', '3', '2', '2', '3', '2', '2', '3', '2', '1', '1', '2', '1', '2', '7', '8', '2', '1', '12', '12', '3', '1', '9', '9', '7', '8', '7', '2', '7', '9', '2', '2', '2', '8', '9', '7', '9', '2', '8', '9', '9', '2', '9', '1', '1', '9', '8', '3', '3', '8', '2', '7', '8', '11', '2', '8', '9', '1', '8', '3', '1', '2', '1', '2', '2', '2', '2', '8', '3', '9', '2', '10', '9', '9', '2', '7', '7', '7', '1', '3', '8', '8', '1', '1', '8', '7', '1', '8', '8', '9', '3', '3', '3', '2', '4', '8', '12', '8', '1', '12', '9', '2', '8', '2', '12', '2', '1', '3', '8', '12', '12', '11', '8', '10', '9', '2', '9', '4', '10', '1', '12', '12', '9', '3', '8', '9', '12', '9', '9', '2', '2', '2', '3', '5', '1', '1', '1', '7', '7', '8', '2', '2', '8', '2', '1', '7', '3', '2', '9', '8', '9', '1', '1', '9', '8', '9', '3', '3', '2', '9', '4', '9', '8', '8', '1', '8', '10', '2', '0', '3', '8', '8', '8', '9', '12', '9', '8', '2', '2', '8', '10', '9', '8', '2', '2', '4', '1', '9', '9', '8', '8', '10', '3', '1', '1', '2', '8', '7', '8', '2', '1', '1', '8', '12', '2', '9', '4', '8', '3', '3', '3', '8', '2', '12', '12', '8', '9', '12', '8', '2', '12', '7', '9', '2', '2', '1', '3', '1', '3', '11', '9', '8', '1', '10', '2', '12', '3', '8', '4', '1', '2', '8', '8', '3', '9', '3', '8', '7', '1', '1', '3', '1', '8', '2', '9', '8', '12', '1', '1', '1', '9', '9', '1', '8', '8', '8', '2', '2', '12', '3', '1', '1', '1', '9', '9', '3', '2', '1', '1', '9', '9', '1', '9', '9', '1', '9', '3', '9', '1', '8', '1', '9', '8', '3', '8', '8', '9', '9', '8', '2', '8', '1', '2', '2', '10', '1', '2', '1', '1', '9', '1', '9', '3', '7', '7', '2', '10', '3', '3', '1', '1', '9', '1', '2', '1', '3', '2', '1', '1', '1', '1', '1', '1', '1', '1', '1', '2', '8', '8', '7', '1', '1', '8', '8', '2', '1', '12', '1', '8', '1', '9', '8', '8', '2', '1', '9', '9', '2', '2', '8', '1', '7', '7', '9', '8', '1', '6', '8', '8', '2', '2', '8', '3', '8', '3', '2', '3', '8', '1', '2', '2', '2', '8', '9', '9', '9', '12', '1', '9', '2', '1', '9', '2', '9', '9', '10', '8', '1', '1', '9', '2', '1', '7', '1', '3', '7', '1', '1', '1', '3', '8', '8', '8', '9', '9', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '1', '0', '1', '1', '1', '2', '2', '3', '1', '1', '2', '3', '3', '3', '7', '3', '1', '9', '3', '9', '1', '4', '1', '1', '9', '10', '9', '8', '2', '3', '10', '9', '3', '10', '12', '8', '9', '8', '2', '1', '9', '8', '1', '7', '8', '1', '9', '1', '2', '10', '7', '7', '8', '1', '3', '3', '9', '3', '3', '1', '1', '1', '9', '1', '3', '8', '8', '8', '1', '8', '1', '9', '7', '8', '8', '2', '1', '3', '8', '2', '1', '6', '1', '8', '8', '2', '8', '2', '2', '9', '8', '1', '1', '6', '8', '6', '10', '2', '1', '1', '8', '9', '3', '3', '3', '1', '3', '9', '6', '8', '1', '8', '3', '1', '2', '2', '12', '7', '1', '1', '1', '1', '12', '1', '8', '8', '3', '2', '1', '9', '2', '3', '3', '2', '2', '7', '2', '2', '2', '3', '1', '9', '8', '7', '1', '1', '3', '1', '9', '4', '3', '9', '2', '1', '1', '9', '8', '8', '7', '2', '1', '3', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '2', '10', '1', '1', '8', '2', '8', '1', '8', '1', '3', '1', '2', '1', '1', '3', '1', '2', '1', '1', '1', '1', '1', '1', '1', '2', '1', '8', '1', '9', '8', '8', '1', '10', '8', '8', '1', '3', '9', '8', '9', '8', '2', '1', '3', '4', '1', '12', '1', '9', '8', '2', '7', '2', '1', '2', '9', '2', '1', '3', '8', '1', '2', '3', '8', '2', '10', '10', '10', '9', '9', '9', '3', '8', '3', '1', '1', '1', '1', '1', '1', '2', '1', '9', '2', '7', '9', '8', '7', '9', '1', '1', '1', '1', '8', '2', '2', '3', '3', '2', '12', '8', '3', '1', '1', '12', '9', '8', '2', '2', '9', '1', '2', '2', '9', '1', '1', '2', '4', '8', '1', '8', '10', '2', '9', '9', '9', '1', '2', '1', '1', '2', '2', '2', '12', '1', '2', '8', '3', '9', '9', '3', '2', '9', '9', '7', '2', '8', '8', '9', '2', '9', '1', '1', '1', '1', '1', '2', '8', '2', '1', '2', '2', '8', '3', '1', '1', '9', '1', '3', '10', '7', '9', '8', '8', '2', '2', '9', '2', '2', '2', '2', '2', '2', '9', '1', '3', '6', '4', '2', '2', '9', '1', '9', '8', '1', '1', '1', '7', '3', '2', '9', '9', '1', '9', '7', '2', '8', '9', '9', '12', '10', '8', '10', '2', '3', '1', '2', '2', '12', '12', '3', '12', '1', '2', '1', '2', '1', '2', '8', '2', '3', '12', '12', '8', '2', '2', '9', '2', '9', '2', '3', '3', '1', '1', '1', '12', '3', '7', '1', '3', '1', '2', '9', '2', '7', '2', '1', '8', '3', '7', '8', '1', '1', '3', '8', '3', '2', '9', '3', '1', '1', '1', '1', '11', '1', '2', '6', '1', '2', '3', '8', '9', '9', '2', '9', '2', '1', '1', '3', '1', '12', '9', '1', '1', '9', '1', '9', '9', '8', '9', '8', '1', '2', '7', '1', '0', '9', '8', '2', '3', '2', '4', '1', '1', '1', '8', '1', '12', '1', '2', '3', '3', '9', '9', '9', '9', '9', '7', '9', '3', '9', '1', '7', '3', '3', '3', '3', '7', '8', '2', '2', '2', '8', '2', '3', '9', '9', '3', '1', '8', '9', '12', '8', '8', '3', '2', '1', '6', '1', '9', '1', '1', '1', '1', '1', '1', '2', '2', '1', '2', '2', '2', '2', '9', '2', '5', '3', '2', '2', '8', '2', '1', '8', '3', '3', '8', '3', '1', '3', '9', '2', '8', '9', '4', '4', '9', '8', '8', '7', '7', '3', '1', '8', '8', '3', '2', '2', '10', '3', '3', '2', '2', '2', '2', '12', '3', '1', '2', '7', '7', '12', '2', '1', '2', '1', '1', '2', '1', '3', '3', '3', '1', '3', '9', '4', '8', '8', '4', '2', '4', '2', '8', '9', '1', '1', '2', '2', '2', '8', '7', '2', '8', '2', '9', '1', '8', '1', '1', '2', '1', '1', '1', '2', '9', '9', '8', '1', '8', '1', '4', '8', '1', '2', '7', '9', '2', '1', '8', '9', '8', '9', '1', '1', '2', '3', '2', '12', '7', '1', '2', '4', '4', '4', '2', '2', '2', '9', '8', '8', '1', '12', '3', '3', '2', '2', '3', '12', '1', '5', '1', '1', '3', '2', '8', '8', '2', '2', '2', '3', '9', '8', '8', '9', '8', '8', '11', '8', '2', '9', '2', '3', '1', '7', '1', '8', '2', '8', '1', '1', '1', '8', '1', '1', '1', '7', '8', '8', '2', '3', '2', '1', '8', '8', '1', '8', '4', '8', '9', '8', '9', '8', '1', '2', '3', '2', '1', '2', '2', '3', '1', '9', '4', '8', '1', '1', '9', '1', '1', '8', '2', '2', '2', '1', '7', '3', '3', '1', '3', '3', '12', '3', '1', '1', '3', '1', '1', '4', '8', '4', '4', '3', '3', '3', '4', '8', '1', '9', '9', '9', '4', '1', '7', '4', '8', '4', '9', '8', '1', '1', '3', '8', '9', '1', '9', '7', '1', '1', '9', '1', '8', '8', '8', '1', '1', '8', '2', '2', '2', '9', '4', '4', '1', '2', '1', '2', '9', '1', '1', '8', '5', '5', '5', '9', '12', '4', '1', '2', '1', '8', '8', '1', '9', '9', '2', '12', '1', '1', '7', '8', '8', '4', '1', '2', '8', '11', '2', '3', '2', '2', '1', '11', '2', '2', '2', '2', '9', '9', '2', '2', '9', '3', '1', '9', '1', '1', '1', '7', '8', '1', '8', '9', '1', '2', '9', '8', '1', '2', '9', '8', '12', '2', '2', '10', '8', '2', '2', '8', '8', '1', '2', '6', '1', '1', '7', '7', '2', '3', '2', '1', '1', '8', '1', '10', '8', '12', '6', '4', '1', '4', '1', '2', '9', '8', '9', '4', '8', '8', '10', '8', '8', '8', '8', '8', '2', '2', '8', '4', '9', '3', '3', '3', '3', '3', '9', '2', '1', '1', '2', '3', '2', '1', '2', '3', '3', '3', '2', '2', '3', '3', '2', '1', '2', '3', '3', '3', '3', '12', '9', '3', '2', '2', '3', '1', '5', '1', '1', '3', '2', '2', '1', '2', '9', '3', '9', '8', '1', '8', '7', '7', '9', '1', '1', '7', '9', '10', '1', '7', '1', '4', '3', '8', '9', '4', '9', '9', '2', '9', '2', '8', '4', '2', '2', '9', '5', '2', '1', '10', '9', '3', '1', '1', '1', '2', '1', '2', '9', '1', '7', '9', '9', '8', '2', '2', '8', '9', '1', '9', '8', '2', '3', '2', '2', '1', '2', '2', '2', '1', '1', '8', '1', '9', '4', '12', '8', '2', '7', '2', '2', '1', '9', '3', '1', '8', '12', '1', '1', '2', '1', '10', '2', '6', '12', '12', '4', '9', '1', '9', '10', '1', '2', '8', '2', '1', '8', '6', '12', '9', '9', '2', '1', '7', '8', '9', '5', '8', '1', '8', '8', '3', '1', '8', '2', '8', '2', '2', '1', '2', '9', '2', '8', '1', '12', '8', '2', '7', '3', '3', '12', '2', '3', '3', '9', '9', '3', '3', '3', '3', '3', '2', '1', '4', '1', '4', '8', '8', '2', '1', '2', '9', '12', '1', '10', '12', '1', '2', '9', '4', '8', '10', '3', '2', '8', '7', '4', '8', '9', '2', '7', '1', '10', '2', '2', '2', '8', '4', '2', '2', '1', '1', '8', '7', '9', '2', '1', '9', '8', '9', '12', '4', '8', '2', '8', '7', '7', '1', '9', '2', '1', '1', '9', '1', '8', '2', '3', '9', '9', '1', '3', '2', '9', '2', '2', '8', '1', '1', '2', '1', '2', '1', '9', '8', '2', '2', '4', '8', '8', '9', '5', '5', '9', '8', '9', '6', '2', '1', '12', '2', '3', '8', '8', '9', '2', '2', '1', '3', '2', '10', '9', '10', '7', '2', '8', '4', '2', '8', '9', '8', '1', '3', '8', '8', '1', '8', '3', '8', '8', '1', '10', '3', '1', '5', '8', '8', '9', '2', '2', '9', '2', '1', '9', '1', '4', '1', '12', '8', '8', '2', '1', '1', '1', '1', '1', '9', '2', '9', '8', '9', '5', '9', '1', '8', '2', '4', '1', '1', '1', '9', '12', '4', '1', '1', '9', '8', '4', '1', '8', '9', '8', '8', '9', '9', '9', '9', '9', '9', '1', '3', '4', '2', '2', '8', '2', '9', '2', '3', '8', '3', '1', '7', '2', '1', '1', '4', '2', '9', '2', '1', '3', '2', '9', '2', '9', '3', '8', '1', '9', '1', '1', '2', '8', '8', '8', '2', '3', '9', '9', '8', '2', '4', '1', '5', '1', '1', '9', '9', '2', '1', '1', '1', '9', '9', '1', '3', '9', '2', '2', '9', '9', '8', '3', '9', '9', '8', '2', '2', '8', '9', '2', '2', '8', '9', '10', '10', '2', '8', '7', '9', '9', '4', '2', '4', '11', '2', '1', '7', '8', '8', '8', '9', '4', '9', '8', '8', '2', '1', '1', '9', '8', '3', '3', '8', '8', '7', '9', '1', '3', '4', '8', '10', '2', '4', '4', '8', '2', '8', '2', '8', '9', '2', '1', '2', '2', '3', '1', '9', '4', '5', '9', '9', '1', '1', '1', '1', '1', '1', '7', '1', '1', '1', '8', '1', '1', '1', '1', '1', '9', '1', '2', '1', '12', '9', '3', '9', '9', '4', '1', '9', '1', '1', '2', '11', '9', '3', '2', '2', '2', '4', '2', '3', '2', '7', '7', '8', '1', '10', '9', '5', '7', '1', '10', '8', '1', '2', '7', '1', '12', '5', '8', '4', '2', '9', '9', '8', '2', '9', '8', '8', '10', '2', '3', '2', '7', '2', '8', '10', '3', '2', '9', '9', '9', '1', '1', '1', '1', '8', '2', '10', '9', '8', '1', '2', '2', '2', '1', '2', '1', '3', '9', '12', '2', '2', '4', '8', '1', '3', '9', '4', '9', '1', '9', '9', '2', '1', '1', '9', '7', '10', '3', '10', '2', '9', '2', '9', '2', '8', '2', '1', '2', '3', '1', '1', '1', '8', '5', '8', '3', '1', '1', '4', '2', '2', '1', '3', '8', '2', '1', '1', '1', '2', '9', '4', '1', '8', '1', '2', '8', '2', '9', '2', '2', '5', '5', '2', '5', '2', '1', '8', '8', '1', '3', '12', '2', '4', '3', '2', '2', '2', '6', '2', '3', '3', '1', '8', '2', '2', '1', '1', '8', '8', '10', '1', '9', '2', '9', '12', '0', '2', '1', '3', '2', '2', '1', '2', '1', '9', '9', '2', '9', '8', '8', '1', '7', '8', '1', '2', '6', '9', '8', '10', '1', '12', '2', '2', '1', '8', '2', '1', '12', '7', '4', '2', '10', '2', '2', '1', '2', '1', '2', '1', '1', '2', '1', '9', '8', '10', '2', '10', '9', '9', '12', '5', '3', '2', '8', '2', '8', '3', '3', '1', '4', '9', '10', '2', '4', '8', '1', '2', '10', '12', '12', '3', '1', '1', '2', '9', '6', '6', '1', '9', '1', '8', '1', '8', '8', '8', '8', '2', '2', '2', '8', '8', '2', '1', '1', '1', '1', '1', '8', '8', '5', '8', '8', '1', '8', '9', '9', '1', '7', '12', '9', '9', '7', '2', '2', '7', '8', '9', '8', '2', '9', '6', '3', '4', '8', '4', '1', '2', '9', '8', '8', '8', '2', '1', '1', '8', '8', '3', '1', '9', '9', '7', '1', '2', '2', '2', '9', '4', '1', '2', '12', '1', '8', '2', '1', '9', '1', '8', '9', '9', '2', '5', '4', '2', '1', '2', '8', '12', '8', '10', '3', '8', '8', '8', '2', '2', '8', '2', '8', '2', '4', '8', '10', '2', '7', '2', '1', '8', '8', '9', '5', '8', '1', '12', '7', '10', '2', '2', '2', '1', '4', '8', '12', '1', '8', '2', '2', '9', '1', '8', '3', '1', '1', '5', '10', '2', '1', '2', '4', '8', '9', '3', '2', '8', '7', '4', '4', '9', '9', '2', '8', '1', '1', '2', '4', '7', '1', '9', '1', '1', '1', '8', '4', '1', '3', '1', '2', '2', '9', '1', '8', '8', '2', '2', '9', '9', '1', '8', '8', '1', '9', '8', '8', '2', '2', '2', '2', '1', '1', '1', '7', '4', '1', '1', '4', '2', '3', '3', '2', '3', '9', '2', '8', '2', '1', '5', '1', '2', '2', '1', '3', '5', '2', '9', '1', '1', '9', '8', '1', '4', '8', '1', '8', '2', '4', '8', '4', '7', '9', '8', '1', '2', '2', '4', '4', '11', '3', '11', '1', '3', '8', '1', '9', '6', '8', '9', '3', '9', '8', '9', '2', '3', '3', '7', '1', '7', '2', '9', '9', '1', '8', '8', '1', '5', '5', '8', '8', '1', '2', '1', '3', '9', '9', '5', '9', '8', '1', '4', '3', '8', '1', '1', '2', '1', '8', '9', '8', '2', '2', '2', '1', '2', '4', '10', '8', '1', '9', '1', '8', '1', '1', '9', '9', '8', '11', '1', '8', '2', '1', '2', '8', '2', '8', '8', '2', '9', '6', '10', '1', '8', '6', '4', '5', '11', '2', '2', '8', '1', '8', '1', '8', '1', '4', '8', '1', '1', '1', '8', '3', '7', '3', '3', '7', '1', '1', '4', '8', '1', '6', '4', '2', '2', '1', '1', '1', '1', '6', '2', '2', '8', '1', '8', '5', '5', '2', '1', '1', '2', '3', '7', '1', '1', '6', '2', '9', '3', '9', '8', '9', '2', '1', '4', '2', '9', '13', '9', '2', '8', '12', '4', '1', '1', '9', '5', '1', '1', '1', '1', '8', '2', '1', '1', '2', '9', '0', '12', '1', '1', '8', '9', '8', '3', '2', '2', '12', '8', '2', '2', '2', '1', '10', '1', '9', '8', '9', '5', '2', '2', '9', '2', '8', '4', '8', '3', '7', '9', '2', '2', '7', '2', '2', '8', '1', '1', '9', '10', '9', '8', '2', '9', '0', '7', '1', '1', '1', '9', '8', '2', '8', '12', '8', '3', '2', '8', '8', '7', '7', '8', '12', '2', '1', '3', '9', '2', '2', '8', '2', '8', '8', '12', '8', '8', '1', '8', '9', '2', '4', '4', '7', '2', '1', '1', '9', '3', '7', '1', '3', '9', '8', '2', '2', '3', '1', '3', '10', '2', '3', '7', '1', '8', '1', '1', '10', '3', '10', '8', '4', '1', '2', '4', '3', '10', '9', '3', '9', '5', '8', '7', '2', '9', '3', '3', '2', '11', '8', '1', '1', '7', '2', '7', '2', '9', '4', '8', '1', '12', '8', '7', '9', '1', '0', '2', '6', '4', '9', '3', '2', '4', '12', '9', '8', '5', '9', '2', '8', '4', '6', '8', '9', '3', '1', '8', '7', '3', '1', '1', '1', '8', '9', '1', '8', '7', '9', '8', '2', '1', '2', '2', '3', '2', '2', '1', '1', '4', '9', '7', '3', '2', '8', '8', '9', '1', '9', '7', '4', '2', '2', '8', '5', '9', '1', '5', '7', '1', '4', '6', '9', '10', '1', '8', '2', '9', '1', '9', '2', '9', '8', '2', '9', '9', '11', '2', '12', '2', '2', '9', '4', '8', '1', '9', '9', '2', '8', '9', '1', '2', '9', '3', '8', '8', '1', '7', '8', '8', '2', '3', '2', '2', '2', '2', '9', '8', '10', '2', '2', '1', '9', '11', '1', '8', '8', '2', '4', '2', '1', '2', '8', '8', '9', '2', '8', '9', '2', '3', '1', '2', '1', '3', '10', '8', '1', '2', '4', '1', '9', '8', '9', '3', '3', '9', '8', '2', '11', '3', '7', '1', '8', '7', '1', '8', '8', '8', '1', '4', '8', '8', '1', '9', '3', '1', '1', '2', '8', '8', '8', '2', '1', '8', '10', '2', '12', '8', '8', '9', '1', '8', '1', '3', '2', '2', '10', '7', '2', '2', '1', '1', '8', '2', '9', '1', '2', '9', '3', '3', '10', '1', '4', '7', '10', '12', '2', '4', '10', '8', '6', '2', '9', '1', '8', '3', '1', '2', '8', '8', '2', '12', '10', '2', '10', '9', '1', '2', '5', '8', '12', '8', '8', '7', '8', '8', '1', '8', '2', '2', '9', '1', '8', '10', '5', '5', '5', '8', '1', '9', '1', '13', '1', '2', '2', '1', '2', '4', '9', '2', '1', '9', '13', '11', '1', '9', '9', '3', '10', '8', '7', '8', '8', '4', '2', '1', '9', '4', '3', '1', '1', '10', '9', '2', '8', '6', '2', '1', '1', '3', '2', '8', '1', '1', '1', '1', '2', '4', '1', '1', '10', '1', '8', '1', '1', '9', '1', '7', '1', '1', '9', '9', '2', '12', '9', '1', '8', '1', '12', '1', '8', '3', '8', '1', '8', '1', '2', '10', '1', '1', '2', '2', '9', '3', '9', '12', '1', '9', '1', '2', '8', '7', '2', '1', '1', '3', '2', '8', '4', '4', '5', '12', '7', '8', '9', '6', '1', '8', '9', '2', '1', '9', '1', '4', '1', '8', '2', '1', '1', '8', '7', '4', '9', '1', '8', '4', '2', '7', '5', '1', '3', '9', '2', '8', '2', '1', '2', '9', '1', '1', '2', '3', '4', '3', '1', '2', '2', '10', '2', '2', '8', '1', '1', '2', '4', '9', '2', '11', '9', '8', '9', '2', '1', '10', '4', '1', '1', '10', '7', '2', '10', '8', '1', '10', '8', '8', '8', '8', '8', '8', '2', '2', '2', '8', '3', '4', '1', '1', '1', '2', '1', '8', '8', '2', '8', '3', '8', '3', '1', '2', '3', '1', '1', '10', '1', '4', '4', '3', '3', '1', '9', '1', '1', '2', '1', '1', '6', '1', '1', '1', '9', '1', '4', '1', '1', '1', '1', '1', '1', '2', '12', '8', '8', '1', '9', '12', '10', '8', '2', '12', '1', '1', '2', '1', '7', '10', '7', '1', '2', '1', '10', '8', '2', '2', '8', '2', '2', '1', '9', '8', '8', '8', '8', '3', '1', '4', '2', '8', '3', '10', '10', '1', '9', '1', '9', '1', '9', '1', '8', '3', '1', '1', '8', '8', '8', '6', '3', '4', '1', '1', '1', '9', '9', '5', '2', '10', '2', '6', '2', '2', '7', '1', '8', '2', '11', '1', '1', '1', '2', '1', '9', '4', '4', '2', '8', '4', '6', '8', '8', '10', '10', '2', '1', '2', '9', '1', '8', '8', '3', '4', '8', '8', '2', '8', '8', '1', '2', '1', '2', '9', '4', '1', '3', '3', '3', '8', '8', '8', '2', '6', '4', '3', '3', '12', '3', '4', '9', '6', '8', '7', '7', '8', '6', '4', '1', '10', '6', '3', '3', '8', '1', '1', '8', '1', '11', '8', '1', '1', '2', '3', '11', '7', '2', '1', '1', '1', '9', '1', '2', '1', '1', '9', '8', '1', '1', '8', '0', '4', '1', '1', '2', '1', '1', '2', '2', '9', '1', '10', '1', '3', '8', '9', '3', '3', '1', '10', '1', '9', '1', '1', '7', '4', '1', '8', '10', '1', '2', '10', '1', '1', '2', '3', '3', '9', '1', '8', '10', '1', '7', '9', '1', '9', '6', '3', '9', '2', '8', '4', '12', '12', '3', '1', '1', '9', '1', '9', '2', '1', '1', '1', '1', '1', '9', '2', '1', '2', '10', '9', '8', '8', '10', '8', '2', '3', '2', '1', '12', '8', '12', '2', '9', '1', '3', '2', '5', '2', '2', '8', '2', '2', '4', '10', '9', '10', '9', '4', '2', '4', '12', '8', '2', '1', '8', '3', '9', '2', '9', '4', '1', '1', '1', '1', '1', '9', '6', '2', '9', '2', '1', '3', '2', '2', '5', '8', '9', '1', '6', '3', '1', '2', '2', '2', '6', '1', '8', '10', '8', '3', '3', '13', '2', '9', '9', '10', '6', '12', '8', '2', '9', '2', '3', '8', '8', '1', '2', '2', '3', '2', '10', '1', '8', '9', '1', '8', '9', '8', '8', '2', '10', '2', '1', '1', '1', '9', '9', '12', '1', '1', '4', '8', '3', '2', '2', '4', '4', '2', '2', '1', '1', '1', '9', '2', '2', '4', '1', '1', '8', '8', '8', '9', '2', '13', '9', '9', '4', '1', '9', '7', '10', '10', '8', '1', '9', '1', '1', '1', '8', '12', '4', '1', '2', '1', '1', '1', '7', '8', '3', '3', '1', '2', '2', '7', '2', '2', '10', '1', '8', '4', '0', '1', '9', '4', '1', '1', '2', '10', '10', '7', '9', '3', '12', '11', '2', '8', '10', '8', '9', '4', '3', '4', '10', '1', '1', '9', '3', '10', '1', '5', '3', '3', '2', '2', '1', '1', '6', '1', '7', '1', '1', '12', '1', '1', '1', '10', '1', '8', '8', '1', '3', '8', '3', '1', '2', '8', '3', '4', '2', '8', '8', '6', '1', '1', '4', '1', '2', '9', '5', '1', '7', '8', '9', '2', '4', '9', '8', '5', '9', '9', '9', '1', '2', '1', '3', '8', '9', '2', '9', '8', '9', '1', '8', '13', '9', '3', '8', '1', '4', '1', '3', '1', '6', '4', '10', '2', '2', '7', '8', '8', '12', '8', '9', '9', '9', '10', '3', '12', '12', '2', '9', '10', '9', '7', '2', '3', '4', '10', '10', '2', '7', '7', '1', '7', '3', '9', '1', '9', '8', '2', '9', '1', '8', '1', '10', '8', '8', '2', '7', '8', '1', '8', '9', '8', '1', '4', '1', '8', '8', '8', '1', '1', '10', '3', '2', '2', '6', '8', '9', '8', '2', '1', '1', '3', '8', '8', '1', '4', '4', '3', '9', '10', '4', '2', '4', '2', '9', '2', '2', '9', '1', '1', '10', '1', '1', '2', '1', '1', '1', '9', '1', '8', '8', '8', '2', '3', '8', '1', '6', '7', '3', '1', '9', '1', '2', '1', '6', '1', '2', '10', '1', '4', '2', '3', '2', '9', '9', '4', '9', '9', '1', '2', '8', '4', '7', '8', '7', '13', '9', '1', '2', '1', '9', '3', '3', '6', '8', '9', '10', '9', '2', '2', '3', '12', '1', '1', '1', '13', '2', '1', '7', '10', '8', '9', '1', '2', '9', '1', '1', '1', '2', '1', '12', '5', '6', '9', '10', '3', '1', '10', '1', '3', '2', '8', '9', '2', '10', '8', '8', '9', '2', '8', '9', '2', '2', '2', '8', '8', '2', '3', '1', '4', '8', '1', '9', '9', '2', '8', '10', '9', '1', '2', '12', '8', '8', '0', '2', '1', '1', '9', '10', '1', '6', '2', '2', '10', '2', '2', '4', '2', '2', '12', '1', '6', '7', '8', '9', '1', '3', '5', '7', '3', '1', '3', '1', '1', '3', '2', '6', '3', '2', '1', '8', '3', '5', '2', '1', '1', '8', '1', '1', '0', '2', '3', '5', '3', '1', '1', '9', '8', '8', '8', '8', '4', '7', '2', '9', '12', '5', '9', '3', '12', '3', '9', '1', '9', '1', '7', '3', '9', '3', '10', '1', '2', '1', '4', '9', '8', '2', '9', '11', '2', '8', '9', '4', '1', '1', '1', '1', '1', '1', '1', '9', '1', '9', '1', '1', '1', '12', '3', '10', '3', '6', '1', '7', '10', '8', '3', '7', '1', '1', '2', '6', '1', '3', '9', '2', '8', '1', '8', '8', '1', '8', '10', '12', '1', '10', '12', '10', '1', '8', '8', '1', '4', '7', '9', '2', '1', '9', '3', '8', '12', '10', '6', '1', '6', '8', '3', '1', '3', '1', '10', '10', '9', '8', '1', '2', '9', '8', '8', '1', '9', '8', '8', '3', '1', '3', '9', '12', '1', '8', '1', '5', '8', '11', '5', '5', '1', '2', '1', '3', '1', '1', '1', '9', '8', '8', '1', '9', '10', '8', '4', '1', '1', '8', '10', '8', '9', '1', '10', '1', '7', '2', '1', '1', '9', '6', '2', '9', '8', '1', '1', '1', '9', '8', '8', '1', '9', '9', '1', '1', '2', '7', '9', '2', '9', '9', '8', '7', '6', '8', '7', '8', '2', '9', '2', '1', '8', '8', '8', '2', '6', '3', '1', '8', '12', '12', '8', '7', '1', '2', '2', '7', '1', '9', '8', '1', '4', '1', '1', '8', '5', '8', '1', '8', '8', '8', '8', '1', '2', '11', '1', '1', '1', '2', '7', '1', '9', '2', '2', '2', '9', '8', '9', '2', '9', '1', '8', '1', '7', '11', '13', '1', '9', '8', '2', '2', '1', '2', '3', '10', '3', '10', '4', '3', '9', '1', '8', '1', '1', '13', '1', '1', '3', '8', '1', '2', '2', '2', '9', '4', '1', '8', '1', '9', '3', '6', '8', '5', '2', '8', '2', '8', '1', '9', '2', '2', '8', '1', '1', '4', '8', '9', '2', '2', '7', '1', '8', '12', '2', '12', '8', '2', '9', '9', '3', '8', '1', '1', '9', '9', '9', '2', '8', '1', '9', '1', '8', '9', '8', '2', '2', '4', '1', '9', '10', '10', '8', '8', '1', '1', '2', '1', '4', '1', '2', '1', '1', '3', '12', '8', '8', '8', '8', '10', '2', '8', '12', '1', '9', '9', '8', '9', '1', '2', '1', '8', '8', '8', '2', '8', '8', '3', '10', '8', '9', '1', '1', '1', '3', '4', '8', '10', '6', '3', '1', '3', '2', '8', '9', '3', '5', '4', '1', '5', '2', '9', '9', '9', '1', '1', '1', '9', '9', '6', '10', '9', '8', '12', '2', '9', '1', '2', '1', '1', '1', '8', '1', '12', '1', '12', '1', '9', '1', '8', '1', '2', '1', '2', '8', '8', '10', '1', '9', '1', '2', '2', '8', '8', '3', '12', '1', '11', '2', '10', '12', '11', '2', '1', '10', '1', '1', '9', '3', '10', '2', '7', '1', '8', '1', '9', '9', '2', '1', '3', '12', '5', '9', '9', '8', '1', '8', '8', '8', '1', '1', '3', '9', '8', '9', '8', '3', '10', '3', '7', '8', '9', '2', '3', '1', '9', '10', '9', '2', '8', '2', '6', '4', '1', '1', '3', '8', '2', '3', '9', '1', '2', '1', '8', '1', '2', '2', '2', '2', '9', '9', '4', '8', '9', '1', '1', '2', '9', '1', '5', '2', '9', '8', '10', '1', '2', '10', '8', '5', '6', '4', '8', '1', '1', '1', '8', '9', '2', '8', '8', '8', '12', '2', '2', '1', '8', '1', '2', '1', '9', '8', '8', '9', '1', '1', '8', '7', '1', '8', '0', '2', '3', '8', '1', '1', '8', '1', '1', '12', '3', '2', '10', '8', '10', '6', '1', '8', '8', '10', '8', '8', '1', '4', '8', '1', '2', '9', '8', '1', '1', '3', '3', '5', '1', '1', '2', '1', '2', '1', '9', '8', '3', '9', '8', '2', '10', '9', '1', '8', '1', '10', '9', '10', '1', '2', '3', '8', '1', '3', '1', '9', '9', '8', '1', '9', '8', '8', '9', '1', '9', '2', '8', '8', '2', '13', '8', '10', '8', '9', '3', '2', '1', '3', '1', '12', '10', '10', '1', '11', '1', '2', '4', '2', '10', '1', '8', '11', '1', '1', '1', '2', '2', '8', '2', '9', '8', '9', '9', '12', '8', '1', '9', '2', '4', '1', '8', '3', '9', '1', '6', '5', '8', '9', '3', '3', '2', '1', '1', '1', '8', '11', '1', '8', '7', '1', '11', '9', '1', '7', '9', '9', '8', '8', '8', '12', '8', '1', '2', '9', '8', '8', '2', '10', '9', '8', '8', '4', '10', '9', '8', '2', '10', '10', '2', '1', '2', '1', '2', '1', '4', '1', '7', '1', '1', '3', '12', '9', '8', '5', '9', '10', '1', '9', '9', '2', '7', '4', '1', '1', '8', '12', '8', '2', '2', '9', '4', '1', '1', '5', '9', '1', '8', '12', '8', '1', '1', '4', '9', '10', '2', '3', '1', '3', '3', '3', '8', '1', '2', '1', '2', '1', '2', '9', '1', '2', '8', '9', '4', '8', '8', '10', '2', '12', '5', '5', '1', '8', '1', '7', '8', '1', '1', '8', '9', '12', '4', '9', '3', '1', '2', '5', '9', '2', '1', '2', '2', '1', '2', '8', '1', '8', '8', '2', '2', '1', '8', '4', '1', '2', '10', '9', '10', '8', '1', '10', '4', '8', '8', '10', '2', '8', '1', '1', '3', '10', '1', '4', '2', '8', '3', '3', '2', '9', '8', '5', '5', '9', '3', '10', '8', '1', '2', '1', '2', '9', '1', '1', '2', '9', '9', '8', '8', '11', '9', '4', '1', '8', '2', '7', '1', '2', '8', '2', '9', '9', '8', '2', '9', '9', '8', '8', '1', '2', '8', '1', '1', '2', '9', '8', '1', '4', '9', '1', '8', '12', '1', '12', '2', '8', '2', '9', '1', '12', '1', '2', '9', '1', '1', '9', '3', '2', '9', '9', '2', '11', '8', '7', '9', '9', '9', '8', '8', '1', '2', '8', '1', '1', '4', '1', '2', '10', '1', '9', '2', '1', '1', '1', '9', '1', '13', '4', '3', '2', '1', '1', '2', '2', '7', '8', '7', '1', '1', '2', '8', '8', '3', '4', '8', '2', '10', '13', '8', '8', '9', '2', '2', '1', '1', '10', '4', '2', '7', '9', '8', '2', '1', '9', '8', '1', '1', '9', '8', '9', '8', '2', '8', '2', '2', '9', '2', '1', '9', '1', '2', '4', '8', '2', '1', '1', '8', '1', '4', '1', '9', '3', '9', '2', '7', '1', '9', '6', '1', '10', '2', '2', '2', '2', '2', '10', '10', '10', '9', '9', '9', '3', '12', '10', '1', '1', '10', '1', '1', '2', '2', '1', '2', '3', '12', '8', '8', '1', '2', '9', '9', '1', '1', '10', '8', '9', '8', '3', '1', '1', '3', '1', '10', '1', '1', '1', '4', '1', '1', '1', '7', '10', '2', '1', '8', '1', '9', '3', '1', '10', '8', '8', '1', '5', '9', '1', '2', '1', '8', '8', '1', '3', '1', '10', '2', '1', '1', '2', '8', '2', '5', '3', '5', '2', '1', '1', '9', '1', '7', '9', '9', '2', '8', '1', '12', '4', '1', '4', '9', '9', '4', '8', '1', '10', '10', '8', '8', '8', '9', '2', '3', '5', '1', '8', '1', '1', '1', '7', '10', '1', '4', '12', '3', '2', '2', '9', '1', '1', '1', '2', '8', '4', '1', '7', '3', '12', '8', '6', '2', '2', '7', '9', '11', '1', '10', '9', '1', '8', '12', '2', '11', '1', '2', '3', '1', '1', '1', '8', '9', '2', '2', '2', '2', '3', '3', '8', '10', '4', '1', '2', '9', '8', '2', '2', '9', '1', '8', '11', '4', '2', '8', '7', '1', '1', '7', '3', '9', '9', '4', '5', '8', '10', '2', '9', '9', '9', '8', '1', '9', '2', '2', '2', '1', '2', '9', '4', '3', '12', '4', '3', '2', '3', '8', '3', '10', '1', '10', '1', '1', '8', '10', '6', '8', '9', '9', '1', '2', '9', '2', '9', '3', '8', '1', '1', '12', '1', '5', '1', '10', '8', '10', '2', '7', '1', '1', '1', '1', '3', '2', '3', '5', '9', '1', '2', '1', '9', '8', '10', '9', '8', '1', '8', '1', '8', '2', '8', '1', '2', '8', '1', '2', '5', '8', '12', '2', '1', '1', '1', '1', '1', '8', '8', '2', '1', '4', '2', '8', '8', '8', '2', '9', '9', '11', '1', '2', '9', '3', '1', '1', '8', '9', '2', '4', '10', '1', '9', '8', '4', '9', '8', '10', '8', '8', '9', '2', '8', '9', '3', '3', '3', '4', '3', '2', '2', '10', '8', '2', '1', '5', '1', '9', '1', '1', '2', '1', '2', '3', '1', '1', '8', '8', '1', '10', '8', '8', '1', '6', '4', '8', '2', '5', '8', '1', '3', '8', '2', '8', '9', '1', '1', '8', '12', '10', '1', '5', '8', '2', '8', '10', '9', '9', '1', '6', '9', '1', '10', '8', '2', '9', '10', '1', '2', '1', '1', '8', '8', '3', '9', '8', '1', '10', '2', '8', '1', '10', '1', '5', '1', '1', '1', '5', '1', '1', '1', '4', '1', '1', '1', '8', '2', '8', '1', '8', '3', '8', '9', '1', '8', '1', '8', '1', '1', '2', '2', '1', '12', '9', '9', '1', '8', '2', '1', '2', '8', '1', '9', '1', '1', '2', '1', '2', '2', '2', '1', '9', '9', '8', '8', '1', '1', '9', '10', '8', '4', '3', '3', '4', '9', '1', '3', '3', '10', '2', '2', '1', '8', '4', '1', '9', '4', '1', '1', '9', '8', '10', '4', '9', '9', '8', '2', '1', '3', '4', '9', '3', '8', '9', '1', '1', '2', '1', '7', '8', '2', '9', '1', '9', '2', '6', '9', '10', '8', '1', '1', '5', '9', '5', '1', '9', '10', '9', '3', '3', '8', '8', '2', '9', '2', '8', '2', '1', '8', '5', '8', '3', '10', '1', '1', '2', '1', '8', '1', '4', '3', '2', '9', '4', '9', '8', '8', '8', '1', '3', '9', '1', '9', '9', '1', '2', '1', '9', '8', '9', '8', '1', '1', '1', '4', '2', '2', '9', '9', '1', '1', '1', '3', '6', '1', '2', '3', '1', '4', '2', '9', '1', '9', '9', '1', '8', '2', '1', '9', '1', '8', '1', '8', '8', '2', '1', '9', '9', '6', '8', '8', '9', '8', '10', '8', '9', '5', '1', '1', '1', '1', '8', '1', '8', '12', '8', '1', '12', '9', '2', '8', '1', '1', '2', '1', '8', '6', '9', '9', '8', '1', '8', '9', '7', '7', '8', '9', '9', '1', '8', '3', '8', '1', '3', '3', '8', '9', '8', '2', '8', '1', '2', '9', '8', '1', '1', '1', '1', '9', '1', '9', '8', '12', '3', '8', '8', '1', '10', '10', '10', '2', '2', '1', '10', '2', '2', '10', '8', '12', '8', '1', '1', '1', '2', '1', '1', '3', '8', '1', '2', '2', '2', '1', '1', '6', '2', '8', '8', '8', '1', '1', '1', '9', '9', '2', '8', '10', '2', '8', '2', '1', '2', '9', '9', '2', '1', '1', '8', '8', '1', '3', '1', '8', '9', '1', '10', '9', '1', '1', '1', '1', '7', '2', '2', '1', '2', '8', '8', '1', '2', '1', '3', '2', '1', '8', '3', '9', '2', '10', '2', '10', '1', '2', '1', '1', '13', '9', '10', '8', '2', '2', '1', '1', '8', '2', '2', '1', '9', '1', '1', '8', '9', '9', '2', '9', '2', '2', '1', '1', '1', '8', '9', '4', '9', '9', '1', '8', '2', '1', '1', '8', '2', '2', '2', '8', '1', '1', '8', '2', '10', '2', '9', '9', '7', '1', '1', '2', '2', '8', '3', '1', '2', '1', '1', '8', '1', '1', '1', '9', '1', '1', '8', '8', '1', '9', '6', '1', '9', '1', '8', '1', '8', '8', '8', '1', '3', '8', '8', '2', '6', '3', '2', '1', '10', '1', '8', '2', '1', '6', '1', '1', '1', '8', '1', '10', '1', '1', '8', '9', '2', '6', '9', '2', '9', '4', '1', '8', '1', '3', '7', '8', '8', '5', '8', '7', '1', '1', '1', '13', '8', '3', '1', '1', '1', '1', '2', '8', '2', '12', '8', '1', '2', '5', '1', '8', '2', '10', '8', '2', '10', '1', '5', '2', '1', '1', '5', '2', '2', '7', '8', '2', '9', '9', '1', '10', '1', '8', '2', '5', '8', '1', '5', '1', '10', '1', '1', '9', '8', '6', '1', '8', '9', '8', '1', '8', '2', '2', '8', '3', '9', '1', '9', '3', '8', '4', '4', '3', '3', '1', '1', '1', '8', '9', '1', '1', '8', '3', '1', '2', '8', '2', '1', '1', '10', '1', '2', '2', '1', '1', '8', '2', '8', '2', '10', '2', '2', '10', '8', '8', '8', '13', '2', '2', '9', '2', '5', '9', '2', '1', '8', '8', '8', '1', '12', '12', '2', '9', '2', '1', '8', '2', '1', '8', '1', '9', '9', '2', '7', '1', '1', '7', '1', '4', '3', '1', '10', '3', '10', '9', '4', '6', '7', '5', '2', '1', '8', '8', '1', '4', '9', '2', '1', '8', '1', '1', '1', '1', '1', '8', '9', '8', '8', '8', '8', '9', '8', '10', '1', '8', '2', '7', '8', '9', '1', '2', '5', '9', '10', '8', '9', '12', '1', '6', '1', '2', '10', '9', '4', '1', '8', '10', '10', '1', '1', '1', '1', '1', '2', '3', '8', '1', '2', '10', '2', '1', '2', '4', '2', '2', '9', '1', '4', '2', '1', '1', '12', '12', '9', '9', '7', '1', '8', '4', '9', '9', '8', '1', '7', '3', '2', '1', '1', '4', '8', '1', '5', '8', '2', '8', '1', '14', '9', '12', '1', '10', '2', '1', '1', '8', '8', '6', '6', '3', '1', '2', '8', '1', '3', '2', '1', '8', '8', '1', '10', '8', '2', '8', '2', '5', '9', '1', '8', '8', '1', '1', '8', '8', '13', '1', '3', '8', '4', '1', '9', '7', '8', '8', '8', '3', '8', '1', '4', '2', '8', '8', '8', '7', '1', '1', '2', '8', '8', '9', '8', '8', '2', '3', '9', '9', '2', '1', '2', '2', '1', '8', '2', '6', '2', '1', '1', '9', '10', '3', '8', '2', '3', '2', '8', '8', '1', '8', '8', '8', '9', '1', '4', '1', '9', '2', '1', '8', '0', '1', '8', '1', '1', '8', '9', '9', '1', '3', '1', '2', '8', '8', '7', '1', '1', '2', '9', '2', '10', '2', '8', '2', '2', '2', '7', '1', '9', '8', '1', '3', '9', '2', '1', '10', '8', '1', '4', '1', '8', '5', '9', '8', '8', '1', '2', '2', '1', '8', '8', '6', '8', '1', '1', '8', '3', '2', '2', '1', '8', '1', '7', '6', '8', '1', '1', '1', '9', '1', '1', '1', '2', '1', '11', '10', '10', '11', '9']\n",
      "Found 15 labels.\n"
     ]
    }
   ],
   "source": [
    "sc = SupremeCourt()\n",
    "print(sc.info)\n",
    "sc.download()\n",
    "\n",
    "texts_complete=[]\n",
    "texts_0_512 = []  # list of text samples\n",
    "texts_512_1024 = []  # list of text samples\n",
    "texts_1024_1536 = []  # list of text samples\n",
    "texts_1536_2048 = []  # list of text samples\n",
    "texts_2048_2560 = []  # list of text samples\n",
    "texts_2560_3072 = []  # list of text samples\n",
    "labels_index = {}  # dictionary mapping label name to numeric id\n",
    "labels = []  # list of label ids\n",
    "\n",
    "issue_codes = list(sc.issue_area_codes.keys()) # 15 labels\n",
    "print(issue_codes)\n",
    "issue_codes.sort()\n",
    "issue_codes = [str(ic) for ic in issue_codes]\n",
    "\n",
    "labels_index = dict(zip(issue_codes, np.arange(len(issue_codes))))\n",
    "print(labels_index)\n",
    "count=0\n",
    "\n",
    "for record in sc.records():\n",
    "        \n",
    "        #print(record)\n",
    "        count=count+1\n",
    "        #print(count)\n",
    "        if record[1]['issue'] == None: # some cases have None as an issue\n",
    "            labels.append(labels_index['-1'])\n",
    "        else:\n",
    "            labels.append(labels_index[record[1]['issue'][:-4]])\n",
    "        \n",
    "        temp=record[0].split(\"Footnotes\")[0]\n",
    "        sentence=[]\n",
    "        for word in temp.split():\n",
    "            if any(ch.isdigit() for ch in word):\n",
    "                continue\n",
    "            else:\n",
    "                sentence.append(word)\n",
    "        new_sen=' '.join(sentence)\n",
    "        new_new_sen=new_sen.split()\n",
    "        first_512=new_new_sen[0:512]\n",
    "        first_512_1024=new_new_sen[512:1024]\n",
    "        first_512=' '.join(first_512)\n",
    "        first_512_1024=' '.join(first_512_1024)\n",
    "\n",
    "        first_1024_1536=new_new_sen[1024:1536]\n",
    "        first_1536_2048=new_new_sen[1536:2048]\n",
    "        first_1024_1536=' '.join(first_1024_1536)\n",
    "        first_1536_2048=' '.join(first_1536_2048)\n",
    "\n",
    "        first_2048_2560=new_new_sen[2048:2560]\n",
    "        first_2560_3072=new_new_sen[2560:3072]\n",
    "        first_2048_2560=' '.join(first_2048_2560)\n",
    "        first_2560_3072=' '.join(first_2560_3072)\n",
    "        \n",
    "        texts_complete.append(new_sen)\n",
    "        texts_0_512.append(first_512)\n",
    "        texts_512_1024.append(first_512_1024)\n",
    "        texts_1024_1536.append(first_1024_1536) \n",
    "        texts_1536_2048.append(first_1536_2048) \n",
    "        texts_2048_2560.append(first_2048_2560)  \n",
    "        texts_2560_3072.append(first_2560_3072) \n",
    "\n",
    "len_list_complete = [len(ele.split()) for ele in texts_complete]\n",
    "len_list_0_512 = [len(ele.split()) for ele in texts_0_512]\n",
    "len_list_512_1024 = [len(ele.split()) for ele in texts_512_1024]\n",
    "len_list_1024_1536 = [len(ele.split()) for ele in texts_1024_1536]\n",
    "len_list_1536_2048 = [len(ele.split()) for ele in texts_1536_2048]\n",
    "len_list_2048_2560 = [len(ele.split()) for ele in texts_2048_2560]\n",
    "len_list_2560_3072 = [len(ele.split()) for ele in texts_2560_3072]\n",
    "\n",
    "print(labels)\n",
    "print(len(labels))\n",
    "\n",
    "\n",
    "res_complete = 0 if len(len_list_complete) == 0 else (float(sum(len_list_complete)) / len(len_list_complete))\n",
    "print(\"Average Length of complete text %s\" % res_complete) \n",
    "print('Found %s texts in complete' % len(texts_complete))\n",
    "print(\"Median Length: %s \" % statistics.median(len_list_complete))\n",
    "print(\"Max length of complete text %s\" % max(len_list_complete))\n",
    "print(\"Min length of complete text %s\" % min(len_list_complete))\n",
    "\n",
    "\n",
    "res_0_512 = 0 if len(len_list_0_512) == 0 else (float(sum(len_list_0_512)) / len(len_list_0_512))\n",
    "res_512_1024 = 0 if len(len_list_512_1024) == 0 else (float(sum(len_list_512_1024)) / len(len_list_512_1024))\n",
    "print(\"Average Length of First 512 %s\" % res_0_512) \n",
    "print(\"Average Length of Second 512 %s\" % res_512_1024) \n",
    "print('Found %s texts in First 512.' % len(texts_0_512))\n",
    "print('Found %s texts in Second 512.' % len(texts_512_1024))\n",
    "\n",
    "res_1024_1536 = 0 if len(len_list_1024_1536) == 0 else (float(sum(len_list_1024_1536)) / len(len_list_1024_1536))\n",
    "res_1536_2048 = 0 if len(len_list_1536_2048) == 0 else (float(sum(len_list_1536_2048)) / len(len_list_1536_2048))\n",
    "print(\"Average Length of Third 512 %s\" % res_1024_1536) \n",
    "print(\"Average Length of Fourth 512 %s\" % res_1536_2048) \n",
    "print('Found %s texts in Third 512.' % len(texts_1024_1536))\n",
    "print('Found %s texts in Fourth 512.' % len(texts_1536_2048))\n",
    "\n",
    "res_2048_2560 = 0 if len(len_list_2048_2560) == 0 else (float(sum(len_list_2048_2560)) / len(len_list_2048_2560))\n",
    "res_2560_3072 = 0 if len(len_list_2560_3072) == 0 else (float(sum(len_list_2560_3072)) / len(len_list_2560_3072))\n",
    "print(\"Average Length of Fifth 512 %s\" % res_2048_2560) \n",
    "print(\"Average Length of Sixth 512 %s\" % res_2560_3072) \n",
    "print('Found %s texts in Fifth 512.' % len(texts_2048_2560))\n",
    "print('Found %s texts in Sixth 512.' % len(texts_2560_3072))\n",
    "\n",
    "\n",
    "temp_file = open(\"labels_sc.txt\", \"r\")\n",
    "\n",
    "data = temp_file.read()\n",
    "label_list = data.split(\"\\n\")\n",
    "print(label_list)\n",
    "label_list = label_list[0:-1]\n",
    "print(label_list)\n",
    "label_list = [int(i) for i in label_list]\n",
    "print('Found %s labels.' % len(set(label_list)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "twMfDGJtoHag",
    "outputId": "90165dc0-24ba-49b1-f6c0-b27213739fa0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         First_512_text  \\\n",
      "0     [ Halliburton Oil Well Cementing Co. v. Walker...   \n",
      "1     Rehearing Denied Dec. See . Mr.Claude T. Barne...   \n",
      "2     Rehearing Denied Dec. See . Appeal from the Di...   \n",
      "3     Mr. Walter J. Cummings, Jr., of Washington, D....   \n",
      "4     Mr.A. Devitt Vaneck, of Washington, D.C., for ...   \n",
      "...                                                 ...   \n",
      "8414  Opinion reported: Ante, p. DECREE It is ordere...   \n",
      "8415  In this dispute between Utah and the United St...   \n",
      "8416  The United States, to the exclusion of defenda...   \n",
      "8417  Louisiana's exception to the portion of the Sp...   \n",
      "8418  To resolve a dispute over the ownership of cer...   \n",
      "\n",
      "                                        Second_512_text  \\\n",
      "0     only had this sound-echo-time method been long...   \n",
      "1     has no such implied limitation. In common unde...   \n",
      "2     to apply its conclusion to Champlin. The contr...   \n",
      "3     size of the reservation; in Congress by statut...   \n",
      "4     to them.' Consequently, the Government cannot ...   \n",
      "...                                                 ...   \n",
      "8414                                                      \n",
      "8415  June Stat. and conveyed by quitclaim deed to t...   \n",
      "8416  U.S.C. (b), the United States in April asked l...   \n",
      "8417  AAA pursuant to agreement of the parties. That...   \n",
      "8418  Bd. v. Corvallis Sand & Gravel Co., . In the e...   \n",
      "\n",
      "                                         Third_512_text  \\\n",
      "0     each other. But the section length and therefo...   \n",
      "1     v. United States, L.R.A.,N.S. see Athanasaw v....   \n",
      "2     has made no order which changes the appellant'...   \n",
      "3     to withhold from judicial scrutiny has now bee...   \n",
      "4     the contract terms that changes and delays wer...   \n",
      "...                                                 ...   \n",
      "8414                                                      \n",
      "8415                                                      \n",
      "8416  detailed exceptions. The controversy is now be...   \n",
      "8417  line, or equidistant principle, recognized in ...   \n",
      "8418  maintained in any court. This Court could not ...   \n",
      "\n",
      "                                        Fourth_512_text  \\\n",
      "0     novelty.' General Electric Co. v. Wabash Appli...   \n",
      "1     Court properly to shift to Congress the respon...   \n",
      "2     would sell' at the carrier's price. In the Val...   \n",
      "3     not to coerce the surrender of lands without c...   \n",
      "4                                                         \n",
      "...                                                 ...   \n",
      "8414                                                      \n",
      "8415                                                      \n",
      "8416  \"As we pointed out in United States v. Califor...   \n",
      "8417                                                      \n",
      "8418  Court over such suits. The constitutional gran...   \n",
      "\n",
      "                                         Fifth_512_text  \\\n",
      "0     scope of what is meant by the equivalent of an...   \n",
      "1     time during which the silence has endured, can...   \n",
      "2     We would have a very different case than the o...   \n",
      "3     on original Indian title were held to be outsi...   \n",
      "4                                                         \n",
      "...                                                 ...   \n",
      "8414                                                      \n",
      "8415                                                      \n",
      "8416  it, the States' claims of ownership prior to t...   \n",
      "8417                                                      \n",
      "8418                                                      \n",
      "\n",
      "                                         Sixth_512_text  label  \n",
      "0     ent waves which Lehr and Wyatt recorded on the...      8  \n",
      "1     many times by the writers of the Old Testament...      1  \n",
      "2                                                            8  \n",
      "3     sued, recovery may be had for an involuntary, ...      2  \n",
      "4                                                            8  \n",
      "...                                                 ...    ...  \n",
      "8414                                                        11  \n",
      "8415                                                        10  \n",
      "8416  re-examination, they should be reaffirmed in a...     10  \n",
      "8417                                                        11  \n",
      "8418                                                         9  \n",
      "\n",
      "[8419 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "summarized_data = pd.DataFrame(list(zip(texts_0_512, texts_512_1024,texts_1024_1536,texts_1536_2048,texts_2048_2560,texts_2560_3072)),\n",
    "               columns =['First_512_text','Second_512_text','Third_512_text','Fourth_512_text','Fifth_512_text','Sixth_512_text'])\n",
    "summarized_data['label'] = label_list\n",
    "print(summarized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Q1xfWDUupbeW"
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_model():\n",
    "    inps_first_512 = Input(shape = (max_len,), dtype='int64')\n",
    "    inps_second_512 = Input(shape = (max_len,), dtype='int64')\n",
    "    inps_third_512 = Input(shape = (max_len,), dtype='int64')\n",
    "    inps_fourth_512 = Input(shape = (max_len,), dtype='int64')\n",
    "    inps_fifth_512 = Input(shape = (max_len,), dtype='int64')\n",
    "    inps_sixth_512 = Input(shape = (max_len,), dtype='int64')\n",
    "\n",
    "    masks_first_512= Input(shape = (max_len,), dtype='int64')\n",
    "    masks_second_512= Input(shape = (max_len,), dtype='int64')\n",
    "    masks_third_512= Input(shape = (max_len,), dtype='int64')\n",
    "    masks_fourth_512= Input(shape = (max_len,), dtype='int64')\n",
    "    masks_fifth_512= Input(shape = (max_len,), dtype='int64')\n",
    "    masks_sixth_512= Input(shape = (max_len,), dtype='int64')\n",
    "\n",
    "    dbert_layer_first_512 = dbert_model(inps_first_512, attention_mask=masks_first_512)[0][:,0,:]\n",
    "    dbert_layer_second_512 = dbert_model(inps_second_512, attention_mask=masks_second_512)[0][:,0,:]\n",
    "    dbert_layer_third_512 = dbert_model(inps_third_512, attention_mask=masks_third_512)[0][:,0,:]\n",
    "    dbert_layer_fourth_512 = dbert_model(inps_fourth_512, attention_mask=masks_fourth_512)[0][:,0,:]\n",
    "    dbert_layer_fifth_512 = dbert_model(inps_fifth_512, attention_mask=masks_fifth_512)[0][:,0,:]\n",
    "    dbert_layer_sixth_512 = dbert_model(inps_sixth_512, attention_mask=masks_sixth_512)[0][:,0,:]\n",
    "\n",
    "    concat=Concatenate()([dbert_layer_first_512, dbert_layer_second_512,dbert_layer_third_512, dbert_layer_fourth_512,dbert_layer_fifth_512, dbert_layer_sixth_512])\n",
    "    dense_0 = Dense(512,activation='relu',kernel_regularizer=regularizers.l2(0.001))(concat)\n",
    "    dropout_0= Dropout(0.5)(dense_0)\n",
    "    pred = Dense(15, activation='softmax',kernel_regularizer=regularizers.l2(0.01))(dropout_0)\n",
    "    #pred = Dense(279, activation='softmax',kernel_regularizer=regularizers.l2(0.01))(dropout_0)\n",
    "    model = tf.keras.Model(inputs=[inps_first_512,masks_first_512,inps_second_512,masks_second_512,inps_third_512,masks_third_512,inps_fourth_512,masks_fourth_512,inps_fifth_512,masks_fifth_512,inps_sixth_512,masks_sixth_512], outputs=pred)\n",
    " \n",
    "    \n",
    "    print(model.summary())\n",
    "    return model   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kaLiUuC6pq4t"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 878k/878k [00:00<00:00, 23.0MB/s]\n",
      "Downloading: 100%|██████████| 446k/446k [00:00<00:00, 13.6MB/s]\n",
      "Downloading: 100%|██████████| 481/481 [00:00<00:00, 794kB/s]\n",
      "Downloading: 100%|██████████| 627M/627M [00:13<00:00, 47.2MB/s] \n",
      "2022-06-06 11:49:07.655288: I tensorflow/stream_executor/cuda/cuda_blas.cc:1774] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_7 (InputLayer)           [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_8 (InputLayer)           [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_9 (InputLayer)           [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_10 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_5 (InputLayer)           [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_11 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_6 (InputLayer)           [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_12 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_1[0][0]',                \n",
      " el)                            thPoolingAndCrossAt               'input_7[0][0]',                \n",
      "                                tentions(last_hidde               'input_2[0][0]',                \n",
      "                                n_state=(None, 512,               'input_8[0][0]',                \n",
      "                                 768),                            'input_3[0][0]',                \n",
      "                                 pooler_output=(Non               'input_9[0][0]',                \n",
      "                                e, 768),                          'input_4[0][0]',                \n",
      "                                 past_key_values=No               'input_10[0][0]',               \n",
      "                                ne, hidden_states=N               'input_5[0][0]',                \n",
      "                                one, attentions=Non               'input_11[0][0]',               \n",
      "                                e, cross_attentions               'input_6[0][0]',                \n",
      "                                =None)                            'input_12[0][0]']               \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_roberta_model[0][0]']       \n",
      " ingOpLambda)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_roberta_model[1][0]']       \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_2 (Sl  (None, 768)         0           ['tf_roberta_model[2][0]']       \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_3 (Sl  (None, 768)         0           ['tf_roberta_model[3][0]']       \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_4 (Sl  (None, 768)         0           ['tf_roberta_model[4][0]']       \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_5 (Sl  (None, 768)         0           ['tf_roberta_model[5][0]']       \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 4608)         0           ['tf.__operators__.getitem[0][0]'\n",
      "                                                                 , 'tf.__operators__.getitem_1[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'tf.__operators__.getitem_2[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'tf.__operators__.getitem_3[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'tf.__operators__.getitem_4[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'tf.__operators__.getitem_5[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 512)          2359808     ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 15)           7695        ['dropout_37[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 127,013,135\n",
      "Trainable params: 127,013,135\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jun  6 11:51:10 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM...  On   | 00000000:4B:00.0 Off |                    0 |\n",
      "| N/A   26C    P0    67W / 500W |  79806MiB / 81920MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   3958836      C   python                          79803MiB |\n",
      "+-----------------------------------------------------------------------------+\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "948/948 [==============================] - 693s 634ms/step - loss: 2.6106 - accuracy: 0.5629 - val_loss: 2.0748 - val_accuracy: 0.7031\n",
      "Epoch 2/5\n",
      "948/948 [==============================] - 588s 621ms/step - loss: 1.9905 - accuracy: 0.7455 - val_loss: 1.9177 - val_accuracy: 0.7613\n",
      "Epoch 3/5\n",
      "948/948 [==============================] - 588s 620ms/step - loss: 1.7765 - accuracy: 0.8010 - val_loss: 1.9014 - val_accuracy: 0.7530\n",
      "Epoch 4/5\n",
      "948/948 [==============================] - 588s 621ms/step - loss: 1.6314 - accuracy: 0.8414 - val_loss: 1.8517 - val_accuracy: 0.7767\n",
      "Epoch 5/5\n",
      "948/948 [==============================] - 588s 621ms/step - loss: 1.5007 - accuracy: 0.8717 - val_loss: 1.8649 - val_accuracy: 0.7779\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_13 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_19 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_14 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_20 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_15 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_21 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_16 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_22 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_17 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_23 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_18 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_24 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_13[0][0]',               \n",
      " el)                            thPoolingAndCrossAt               'input_19[0][0]',               \n",
      "                                tentions(last_hidde               'input_14[0][0]',               \n",
      "                                n_state=(None, 512,               'input_20[0][0]',               \n",
      "                                 768),                            'input_15[0][0]',               \n",
      "                                 pooler_output=(Non               'input_21[0][0]',               \n",
      "                                e, 768),                          'input_16[0][0]',               \n",
      "                                 past_key_values=No               'input_22[0][0]',               \n",
      "                                ne, hidden_states=N               'input_17[0][0]',               \n",
      "                                one, attentions=Non               'input_23[0][0]',               \n",
      "                                e, cross_attentions               'input_18[0][0]',               \n",
      "                                =None)                            'input_24[0][0]']               \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_6 (Sl  (None, 768)         0           ['tf_roberta_model[6][0]']       \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_7 (Sl  (None, 768)         0           ['tf_roberta_model[7][0]']       \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_8 (Sl  (None, 768)         0           ['tf_roberta_model[8][0]']       \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_9 (Sl  (None, 768)         0           ['tf_roberta_model[9][0]']       \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_10 (S  (None, 768)         0           ['tf_roberta_model[10][0]']      \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_11 (S  (None, 768)         0           ['tf_roberta_model[11][0]']      \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 4608)         0           ['tf.__operators__.getitem_6[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'tf.__operators__.getitem_7[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'tf.__operators__.getitem_8[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'tf.__operators__.getitem_9[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'tf.__operators__.getitem_10[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'tf.__operators__.getitem_11[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 512)          2359808     ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 15)           7695        ['dropout_38[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 127,013,135\n",
      "Trainable params: 127,013,135\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Accuracy: 0.7779097387173397\n",
      "Weighted F1: 0.7806820892531718\n",
      "Micro F1: 0.7779097387173397\n",
      "Weighted Precision: 0.7924851811258938\n",
      "Micro Precision: 0.7779097387173397\n",
      "Weighted Recall: 0.7779097387173397\n",
      "Micro Recall: 0.7779097387173397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_7 (InputLayer)           [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_8 (InputLayer)           [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_9 (InputLayer)           [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_10 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_5 (InputLayer)           [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_11 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_6 (InputLayer)           [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_12 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_1[0][0]',                \n",
      " el)                            thPoolingAndCrossAt               'input_7[0][0]',                \n",
      "                                tentions(last_hidde               'input_2[0][0]',                \n",
      "                                n_state=(None, 512,               'input_8[0][0]',                \n",
      "                                 768),                            'input_3[0][0]',                \n",
      "                                 pooler_output=(Non               'input_9[0][0]',                \n",
      "                                e, 768),                          'input_4[0][0]',                \n",
      "                                 past_key_values=No               'input_10[0][0]',               \n",
      "                                ne, hidden_states=N               'input_5[0][0]',                \n",
      "                                one, attentions=Non               'input_11[0][0]',               \n",
      "                                e, cross_attentions               'input_6[0][0]',                \n",
      "                                =None)                            'input_12[0][0]']               \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_roberta_model[0][0]']       \n",
      " ingOpLambda)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_roberta_model[1][0]']       \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_2 (Sl  (None, 768)         0           ['tf_roberta_model[2][0]']       \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_3 (Sl  (None, 768)         0           ['tf_roberta_model[3][0]']       \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_4 (Sl  (None, 768)         0           ['tf_roberta_model[4][0]']       \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_5 (Sl  (None, 768)         0           ['tf_roberta_model[5][0]']       \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 4608)         0           ['tf.__operators__.getitem[0][0]'\n",
      "                                                                 , 'tf.__operators__.getitem_1[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'tf.__operators__.getitem_2[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'tf.__operators__.getitem_3[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'tf.__operators__.getitem_4[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'tf.__operators__.getitem_5[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 512)          2359808     ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 15)           7695        ['dropout_37[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 127,013,135\n",
      "Trainable params: 127,013,135\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jun  6 12:46:16 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM...  On   | 00000000:4B:00.0 Off |                    0 |\n",
      "| N/A   26C    P0    65W / 500W |  79808MiB / 81920MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   3958836      C   python                          79805MiB |\n",
      "+-----------------------------------------------------------------------------+\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "948/948 [==============================] - 704s 637ms/step - loss: 2.6914 - accuracy: 0.5374 - val_loss: 2.1084 - val_accuracy: 0.6983\n",
      "Epoch 2/5\n",
      "948/948 [==============================] - 589s 621ms/step - loss: 2.0270 - accuracy: 0.7248 - val_loss: 1.9371 - val_accuracy: 0.7435\n",
      "Epoch 3/5\n",
      "948/948 [==============================] - 589s 621ms/step - loss: 1.7773 - accuracy: 0.7928 - val_loss: 1.8833 - val_accuracy: 0.7577\n",
      "Epoch 4/5\n",
      "948/948 [==============================] - 589s 621ms/step - loss: 1.6152 - accuracy: 0.8366 - val_loss: 1.8365 - val_accuracy: 0.7684\n",
      "Epoch 5/5\n",
      "948/948 [==============================] - 589s 621ms/step - loss: 1.4926 - accuracy: 0.8697 - val_loss: 1.8473 - val_accuracy: 0.7791\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_13 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_19 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_14 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_20 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_15 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_21 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_16 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_22 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_17 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_23 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_18 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_24 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_13[0][0]',               \n",
      " el)                            thPoolingAndCrossAt               'input_19[0][0]',               \n",
      "                                tentions(last_hidde               'input_14[0][0]',               \n",
      "                                n_state=(None, 512,               'input_20[0][0]',               \n",
      "                                 768),                            'input_15[0][0]',               \n",
      "                                 pooler_output=(Non               'input_21[0][0]',               \n",
      "                                e, 768),                          'input_16[0][0]',               \n",
      "                                 past_key_values=No               'input_22[0][0]',               \n",
      "                                ne, hidden_states=N               'input_17[0][0]',               \n",
      "                                one, attentions=Non               'input_23[0][0]',               \n",
      "                                e, cross_attentions               'input_18[0][0]',               \n",
      "                                =None)                            'input_24[0][0]']               \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_6 (Sl  (None, 768)         0           ['tf_roberta_model[6][0]']       \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_7 (Sl  (None, 768)         0           ['tf_roberta_model[7][0]']       \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_8 (Sl  (None, 768)         0           ['tf_roberta_model[8][0]']       \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_9 (Sl  (None, 768)         0           ['tf_roberta_model[9][0]']       \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_10 (S  (None, 768)         0           ['tf_roberta_model[10][0]']      \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_11 (S  (None, 768)         0           ['tf_roberta_model[11][0]']      \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 4608)         0           ['tf.__operators__.getitem_6[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'tf.__operators__.getitem_7[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'tf.__operators__.getitem_8[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'tf.__operators__.getitem_9[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'tf.__operators__.getitem_10[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'tf.__operators__.getitem_11[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 512)          2359808     ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 15)           7695        ['dropout_38[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 127,013,135\n",
      "Trainable params: 127,013,135\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Accuracy: 0.7790973871733967\n",
      "Weighted F1: 0.7808423494095597\n",
      "Micro F1: 0.7790973871733967\n",
      "Weighted Precision: 0.7914423273200011\n",
      "Micro Precision: 0.7790973871733967\n",
      "Weighted Recall: 0.7790973871733967\n",
      "Micro Recall: 0.7790973871733967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_7 (InputLayer)           [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_8 (InputLayer)           [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_9 (InputLayer)           [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_10 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_5 (InputLayer)           [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_11 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_6 (InputLayer)           [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_12 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_1[0][0]',                \n",
      " el)                            thPoolingAndCrossAt               'input_7[0][0]',                \n",
      "                                tentions(last_hidde               'input_2[0][0]',                \n",
      "                                n_state=(None, 512,               'input_8[0][0]',                \n",
      "                                 768),                            'input_3[0][0]',                \n",
      "                                 pooler_output=(Non               'input_9[0][0]',                \n",
      "                                e, 768),                          'input_4[0][0]',                \n",
      "                                 past_key_values=No               'input_10[0][0]',               \n",
      "                                ne, hidden_states=N               'input_5[0][0]',                \n",
      "                                one, attentions=Non               'input_11[0][0]',               \n",
      "                                e, cross_attentions               'input_6[0][0]',                \n",
      "                                =None)                            'input_12[0][0]']               \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_roberta_model[0][0]']       \n",
      " ingOpLambda)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_roberta_model[1][0]']       \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_2 (Sl  (None, 768)         0           ['tf_roberta_model[2][0]']       \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_3 (Sl  (None, 768)         0           ['tf_roberta_model[3][0]']       \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_4 (Sl  (None, 768)         0           ['tf_roberta_model[4][0]']       \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_5 (Sl  (None, 768)         0           ['tf_roberta_model[5][0]']       \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 4608)         0           ['tf.__operators__.getitem[0][0]'\n",
      "                                                                 , 'tf.__operators__.getitem_1[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'tf.__operators__.getitem_2[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'tf.__operators__.getitem_3[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'tf.__operators__.getitem_4[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'tf.__operators__.getitem_5[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 512)          2359808     ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 15)           7695        ['dropout_37[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 127,013,135\n",
      "Trainable params: 127,013,135\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jun  6 13:41:43 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM...  On   | 00000000:4B:00.0 Off |                    0 |\n",
      "| N/A   26C    P0    68W / 500W |  79808MiB / 81920MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   3958836      C   python                          79805MiB |\n",
      "+-----------------------------------------------------------------------------+\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "948/948 [==============================] - 704s 637ms/step - loss: 2.6938 - accuracy: 0.5374 - val_loss: 2.0979 - val_accuracy: 0.7114\n",
      "Epoch 2/5\n",
      "948/948 [==============================] - 588s 621ms/step - loss: 2.0135 - accuracy: 0.7346 - val_loss: 1.9217 - val_accuracy: 0.7601\n",
      "Epoch 3/5\n",
      "948/948 [==============================] - 588s 620ms/step - loss: 1.7734 - accuracy: 0.7898 - val_loss: 1.9343 - val_accuracy: 0.7482\n",
      "Epoch 4/5\n",
      "948/948 [==============================] - 588s 620ms/step - loss: 1.6115 - accuracy: 0.8390 - val_loss: 1.8373 - val_accuracy: 0.7530\n",
      "Epoch 5/5\n",
      "948/948 [==============================] - 588s 620ms/step - loss: 1.4943 - accuracy: 0.8691 - val_loss: 1.8513 - val_accuracy: 0.7553\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_13 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_19 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_14 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_20 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_15 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_21 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_16 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_22 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_17 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_23 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_18 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_24 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_13[0][0]',               \n",
      " el)                            thPoolingAndCrossAt               'input_19[0][0]',               \n",
      "                                tentions(last_hidde               'input_14[0][0]',               \n",
      "                                n_state=(None, 512,               'input_20[0][0]',               \n",
      "                                 768),                            'input_15[0][0]',               \n",
      "                                 pooler_output=(Non               'input_21[0][0]',               \n",
      "                                e, 768),                          'input_16[0][0]',               \n",
      "                                 past_key_values=No               'input_22[0][0]',               \n",
      "                                ne, hidden_states=N               'input_17[0][0]',               \n",
      "                                one, attentions=Non               'input_23[0][0]',               \n",
      "                                e, cross_attentions               'input_18[0][0]',               \n",
      "                                =None)                            'input_24[0][0]']               \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_6 (Sl  (None, 768)         0           ['tf_roberta_model[6][0]']       \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_7 (Sl  (None, 768)         0           ['tf_roberta_model[7][0]']       \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_8 (Sl  (None, 768)         0           ['tf_roberta_model[8][0]']       \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_9 (Sl  (None, 768)         0           ['tf_roberta_model[9][0]']       \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_10 (S  (None, 768)         0           ['tf_roberta_model[10][0]']      \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_11 (S  (None, 768)         0           ['tf_roberta_model[11][0]']      \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 4608)         0           ['tf.__operators__.getitem_6[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'tf.__operators__.getitem_7[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'tf.__operators__.getitem_8[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'tf.__operators__.getitem_9[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'tf.__operators__.getitem_10[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'tf.__operators__.getitem_11[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 512)          2359808     ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 15)           7695        ['dropout_38[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 127,013,135\n",
      "Trainable params: 127,013,135\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Accuracy: 0.7600950118764845\n",
      "Weighted F1: 0.7565712645746279\n",
      "Micro F1: 0.7600950118764844\n",
      "Weighted Precision: 0.762400579506918\n",
      "Micro Precision: 0.7600950118764845\n",
      "Weighted Recall: 0.7600950118764845\n",
      "Micro Recall: 0.7600950118764845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_7 (InputLayer)           [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_8 (InputLayer)           [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_9 (InputLayer)           [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_10 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_5 (InputLayer)           [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_11 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_6 (InputLayer)           [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_12 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_1[0][0]',                \n",
      " el)                            thPoolingAndCrossAt               'input_7[0][0]',                \n",
      "                                tentions(last_hidde               'input_2[0][0]',                \n",
      "                                n_state=(None, 512,               'input_8[0][0]',                \n",
      "                                 768),                            'input_3[0][0]',                \n",
      "                                 pooler_output=(Non               'input_9[0][0]',                \n",
      "                                e, 768),                          'input_4[0][0]',                \n",
      "                                 past_key_values=No               'input_10[0][0]',               \n",
      "                                ne, hidden_states=N               'input_5[0][0]',                \n",
      "                                one, attentions=Non               'input_11[0][0]',               \n",
      "                                e, cross_attentions               'input_6[0][0]',                \n",
      "                                =None)                            'input_12[0][0]']               \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_roberta_model[0][0]']       \n",
      " ingOpLambda)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_roberta_model[1][0]']       \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_2 (Sl  (None, 768)         0           ['tf_roberta_model[2][0]']       \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_3 (Sl  (None, 768)         0           ['tf_roberta_model[3][0]']       \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_4 (Sl  (None, 768)         0           ['tf_roberta_model[4][0]']       \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_5 (Sl  (None, 768)         0           ['tf_roberta_model[5][0]']       \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 4608)         0           ['tf.__operators__.getitem[0][0]'\n",
      "                                                                 , 'tf.__operators__.getitem_1[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'tf.__operators__.getitem_2[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'tf.__operators__.getitem_3[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'tf.__operators__.getitem_4[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'tf.__operators__.getitem_5[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 512)          2359808     ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 15)           7695        ['dropout_37[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 127,013,135\n",
      "Trainable params: 127,013,135\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jun  6 14:37:10 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM...  On   | 00000000:4B:00.0 Off |                    0 |\n",
      "| N/A   26C    P0    66W / 500W |  79808MiB / 81920MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   3958836      C   python                          79805MiB |\n",
      "+-----------------------------------------------------------------------------+\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "948/948 [==============================] - 702s 636ms/step - loss: 2.6865 - accuracy: 0.5300 - val_loss: 2.0832 - val_accuracy: 0.7043\n",
      "Epoch 2/5\n",
      "948/948 [==============================] - 588s 621ms/step - loss: 1.9885 - accuracy: 0.7386 - val_loss: 1.8953 - val_accuracy: 0.7589\n",
      "Epoch 3/5\n",
      "948/948 [==============================] - 588s 621ms/step - loss: 1.7591 - accuracy: 0.7975 - val_loss: 1.8564 - val_accuracy: 0.7601\n",
      "Epoch 4/5\n",
      "948/948 [==============================] - 588s 621ms/step - loss: 1.6022 - accuracy: 0.8387 - val_loss: 1.8109 - val_accuracy: 0.7755\n",
      "Epoch 5/5\n",
      "948/948 [==============================] - 586s 619ms/step - loss: 1.4815 - accuracy: 0.8689 - val_loss: 1.8251 - val_accuracy: 0.7803\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_13 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_19 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_14 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_20 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_15 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_21 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_16 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_22 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_17 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_23 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_18 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_24 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_13[0][0]',               \n",
      " el)                            thPoolingAndCrossAt               'input_19[0][0]',               \n",
      "                                tentions(last_hidde               'input_14[0][0]',               \n",
      "                                n_state=(None, 512,               'input_20[0][0]',               \n",
      "                                 768),                            'input_15[0][0]',               \n",
      "                                 pooler_output=(Non               'input_21[0][0]',               \n",
      "                                e, 768),                          'input_16[0][0]',               \n",
      "                                 past_key_values=No               'input_22[0][0]',               \n",
      "                                ne, hidden_states=N               'input_17[0][0]',               \n",
      "                                one, attentions=Non               'input_23[0][0]',               \n",
      "                                e, cross_attentions               'input_18[0][0]',               \n",
      "                                =None)                            'input_24[0][0]']               \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_6 (Sl  (None, 768)         0           ['tf_roberta_model[6][0]']       \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_7 (Sl  (None, 768)         0           ['tf_roberta_model[7][0]']       \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_8 (Sl  (None, 768)         0           ['tf_roberta_model[8][0]']       \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_9 (Sl  (None, 768)         0           ['tf_roberta_model[9][0]']       \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_10 (S  (None, 768)         0           ['tf_roberta_model[10][0]']      \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_11 (S  (None, 768)         0           ['tf_roberta_model[11][0]']      \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 4608)         0           ['tf.__operators__.getitem_6[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'tf.__operators__.getitem_7[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'tf.__operators__.getitem_8[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'tf.__operators__.getitem_9[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'tf.__operators__.getitem_10[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'tf.__operators__.getitem_11[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 512)          2359808     ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 15)           7695        ['dropout_38[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 127,013,135\n",
      "Trainable params: 127,013,135\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Accuracy: 0.7802850356294537\n",
      "Weighted F1: 0.7799414024179464\n",
      "Micro F1: 0.7802850356294538\n",
      "Weighted Precision: 0.7870813824504076\n",
      "Micro Precision: 0.7802850356294537\n",
      "Weighted Recall: 0.7802850356294537\n",
      "Micro Recall: 0.7802850356294537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_7 (InputLayer)           [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_8 (InputLayer)           [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_9 (InputLayer)           [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_10 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_5 (InputLayer)           [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_11 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_6 (InputLayer)           [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_12 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_1[0][0]',                \n",
      " el)                            thPoolingAndCrossAt               'input_7[0][0]',                \n",
      "                                tentions(last_hidde               'input_2[0][0]',                \n",
      "                                n_state=(None, 512,               'input_8[0][0]',                \n",
      "                                 768),                            'input_3[0][0]',                \n",
      "                                 pooler_output=(Non               'input_9[0][0]',                \n",
      "                                e, 768),                          'input_4[0][0]',                \n",
      "                                 past_key_values=No               'input_10[0][0]',               \n",
      "                                ne, hidden_states=N               'input_5[0][0]',                \n",
      "                                one, attentions=Non               'input_11[0][0]',               \n",
      "                                e, cross_attentions               'input_6[0][0]',                \n",
      "                                =None)                            'input_12[0][0]']               \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_roberta_model[0][0]']       \n",
      " ingOpLambda)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_roberta_model[1][0]']       \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_2 (Sl  (None, 768)         0           ['tf_roberta_model[2][0]']       \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_3 (Sl  (None, 768)         0           ['tf_roberta_model[3][0]']       \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_4 (Sl  (None, 768)         0           ['tf_roberta_model[4][0]']       \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_5 (Sl  (None, 768)         0           ['tf_roberta_model[5][0]']       \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 4608)         0           ['tf.__operators__.getitem[0][0]'\n",
      "                                                                 , 'tf.__operators__.getitem_1[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'tf.__operators__.getitem_2[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'tf.__operators__.getitem_3[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'tf.__operators__.getitem_4[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'tf.__operators__.getitem_5[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 512)          2359808     ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 15)           7695        ['dropout_37[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 127,013,135\n",
      "Trainable params: 127,013,135\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jun  6 15:32:31 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM...  On   | 00000000:4B:00.0 Off |                    0 |\n",
      "| N/A   26C    P0    66W / 500W |  79808MiB / 81920MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   3958836      C   python                          79805MiB |\n",
      "+-----------------------------------------------------------------------------+\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "948/948 [==============================] - 701s 636ms/step - loss: 2.7656 - accuracy: 0.5011 - val_loss: 2.1009 - val_accuracy: 0.6912\n",
      "Epoch 2/5\n",
      "948/948 [==============================] - 586s 618ms/step - loss: 2.0127 - accuracy: 0.7298 - val_loss: 1.9622 - val_accuracy: 0.7340\n",
      "Epoch 3/5\n",
      "948/948 [==============================] - 585s 617ms/step - loss: 1.7800 - accuracy: 0.7920 - val_loss: 1.8781 - val_accuracy: 0.7542\n",
      "Epoch 4/5\n",
      "812/948 [========================>.....] - ETA: 1:21 - loss: 1.6210 - accuracy: 0.8411"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "total_accuracy=0\n",
    "total_weighted_f1=0\n",
    "total_micro_f1=0\n",
    "total_weighted_precision=0\n",
    "total_micro_precision=0\n",
    "total_weighted_recall=0\n",
    "total_micro_recall=0\n",
    "\n",
    "for i in range(5):\n",
    "  gc.collect()\n",
    "  tf.keras.backend.clear_session()\n",
    "  dbert_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "  dbert_model = TFRobertaModel.from_pretrained('roberta-base')\n",
    "  max_len=512\n",
    "  first_512_sentences=summarized_data['First_512_text']\n",
    "  second_512_sentences=summarized_data['Second_512_text']\n",
    "  third_512_sentences=summarized_data['Third_512_text']\n",
    "  fourth_512_sentences=summarized_data['Fourth_512_text']\n",
    "  fifth_512_sentences=summarized_data['Fifth_512_text']\n",
    "  sixth_512_sentences=summarized_data['Sixth_512_text']\n",
    "  labels=summarized_data['label']\n",
    "  len(first_512_sentences),len(labels),len(second_512_sentences),len(third_512_sentences),len(fourth_512_sentences),len(fifth_512_sentences),len(sixth_512_sentences)\n",
    "\n",
    "  model_0=create_model()\n",
    "\n",
    "  input_ids_first_512=[]\n",
    "  attention_masks_first_512=[]\n",
    "  for sent in summarized_data['First_512_text']:\n",
    "      dbert_inps=dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n",
    "      input_ids_first_512.append(dbert_inps['input_ids'])\n",
    "      attention_masks_first_512.append(dbert_inps['attention_mask'])\n",
    "  input_ids_first_512=np.asarray(input_ids_first_512)\n",
    "  attention_masks_first_512=np.array(attention_masks_first_512)\n",
    "  labels=np.array(labels)\n",
    "\n",
    "  input_ids_second_512=[]\n",
    "  attention_masks_second_512=[]\n",
    "  for sent in summarized_data['Second_512_text']:\n",
    "      dbert_inps=dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n",
    "      input_ids_second_512.append(dbert_inps['input_ids'])\n",
    "      attention_masks_second_512.append(dbert_inps['attention_mask'])\n",
    "  input_ids_second_512=np.asarray(input_ids_second_512)\n",
    "  attention_masks_second_512=np.array(attention_masks_second_512)\n",
    "\n",
    "  input_ids_third_512=[]\n",
    "  attention_masks_third_512=[]\n",
    "  for sent in summarized_data['Third_512_text']:\n",
    "      dbert_inps=dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n",
    "      input_ids_third_512.append(dbert_inps['input_ids'])\n",
    "      attention_masks_third_512.append(dbert_inps['attention_mask'])\n",
    "  input_ids_third_512=np.asarray(input_ids_third_512)\n",
    "  attention_masks_third_512=np.array(attention_masks_third_512)\n",
    "\n",
    "  input_ids_fourth_512=[]\n",
    "  attention_masks_fourth_512=[]\n",
    "  for sent in summarized_data['Fourth_512_text']:\n",
    "      dbert_inps=dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n",
    "      input_ids_fourth_512.append(dbert_inps['input_ids'])\n",
    "      attention_masks_fourth_512.append(dbert_inps['attention_mask'])\n",
    "  input_ids_fourth_512=np.asarray(input_ids_fourth_512)\n",
    "  attention_masks_fourth_512=np.array(attention_masks_fourth_512)\n",
    "\n",
    "  input_ids_fifth_512=[]\n",
    "  attention_masks_fifth_512=[]\n",
    "  for sent in summarized_data['Fifth_512_text']:\n",
    "      dbert_inps=dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n",
    "      input_ids_fifth_512.append(dbert_inps['input_ids'])\n",
    "      attention_masks_fifth_512.append(dbert_inps['attention_mask'])\n",
    "  input_ids_fifth_512=np.asarray(input_ids_fifth_512)\n",
    "  attention_masks_fifth_512=np.array(attention_masks_fifth_512)\n",
    "\n",
    "  input_ids_sixth_512=[]\n",
    "  attention_masks_sixth_512=[]\n",
    "  for sent in summarized_data['Sixth_512_text']:\n",
    "      dbert_inps=dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n",
    "      input_ids_sixth_512.append(dbert_inps['input_ids'])\n",
    "      attention_masks_sixth_512.append(dbert_inps['attention_mask'])\n",
    "  input_ids_sixth_512=np.asarray(input_ids_sixth_512)\n",
    "  attention_masks_sixth_512=np.array(attention_masks_sixth_512)\n",
    "\n",
    "  train_inp_first_512,val_inp_first_512,train_label,val_label,train_mask_first_512,val_mask_first_512,train_inp_second_512,val_inp_second_512,train_mask_second_512,val_mask_second_512,train_inp_third_512,val_inp_third_512,train_mask_third_512,val_mask_third_512,train_inp_fourth_512,val_inp_fourth_512,train_mask_fourth_512,val_mask_fourth_512,train_inp_fifth_512,val_inp_fifth_512,train_mask_fifth_512,val_mask_fifth_512,train_inp_sixth_512,val_inp_sixth_512,train_mask_sixth_512,val_mask_sixth_512=train_test_split(input_ids_first_512,labels,attention_masks_first_512,input_ids_second_512,attention_masks_second_512,input_ids_third_512,attention_masks_third_512,input_ids_fourth_512,attention_masks_fourth_512,input_ids_fifth_512,attention_masks_fifth_512,input_ids_sixth_512,attention_masks_sixth_512,test_size=0.1,random_state=42)\n",
    "\n",
    "  log_dir='dbert_model'\n",
    "\n",
    "  model_save_path='Concat-512/roberta-concat-512-'+str(i)+'-15labels.h5'\n",
    "\n",
    "  loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "  accuracy = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "\n",
    "  optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
    "  callbacks= [tf.keras.callbacks.ModelCheckpoint(filepath=model_save_path,monitor='val_accuracy',mode='max',save_best_only=True,save_weights_only=True),keras.callbacks.TensorBoard(log_dir=log_dir)]\n",
    "  model_0.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])\n",
    "\n",
    "  gpu_info = !nvidia-smi\n",
    "  gpu_info = '\\n'.join(gpu_info)\n",
    "  if gpu_info.find('failed') >= 0:\n",
    "    print('Not connected to a GPU')\n",
    "  else:\n",
    "    print(gpu_info)\n",
    "  \n",
    "  history=model_0.fit([train_inp_first_512,train_mask_first_512,train_inp_second_512,train_mask_second_512,train_inp_third_512,train_mask_third_512,train_inp_fourth_512,train_mask_fourth_512,train_inp_fifth_512,train_mask_fifth_512,train_inp_sixth_512,train_mask_sixth_512],train_label,batch_size=8,epochs=5,validation_data=([val_inp_first_512,val_mask_first_512,val_inp_second_512,val_mask_second_512,val_inp_third_512,val_mask_third_512,val_inp_fourth_512,val_mask_fourth_512,val_inp_fifth_512,val_mask_fifth_512,val_inp_sixth_512,val_mask_sixth_512],val_label),callbacks=callbacks)\n",
    "  pred_labels=[]\n",
    "\n",
    "  model_saved= create_model()\n",
    "  model_saved.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])\n",
    "  model_saved.load_weights('Concat-512/roberta-concat-512-'+str(i)+'-15labels.h5')\n",
    "\n",
    "  for i in range(0,len(val_inp_first_512)):\n",
    "    pred=model_saved.predict([val_inp_first_512[i].reshape(1,512),val_mask_first_512[i].reshape(1,512),val_inp_second_512[i].reshape(1,512),val_mask_second_512[i].reshape(1,512),val_inp_third_512[i].reshape(1,512),val_mask_third_512[i].reshape(1,512),val_inp_fourth_512[i].reshape(1,512),val_mask_fourth_512[i].reshape(1,512),val_inp_fifth_512[i].reshape(1,512),val_mask_fifth_512[i].reshape(1,512),val_inp_sixth_512[i].reshape(1,512),val_mask_sixth_512[i].reshape(1,512)])\n",
    "    pred_label = pred.argmax(axis=1)\n",
    "    pred_labels.append(pred_label)\n",
    "  accuracy=accuracy_score(val_label, pred_labels)\n",
    "  print(\"Accuracy: \"+str(accuracy))\n",
    "  total_accuracy=total_accuracy+accuracy\n",
    "  \n",
    "  weighted_f1=f1_score(val_label,pred_labels, average='weighted')\n",
    "  print(\"Weighted F1: \"+ str(weighted_f1))\n",
    "  total_weighted_f1=total_weighted_f1+weighted_f1\n",
    "  micro_f1=f1_score(val_label,pred_labels, average='micro')\n",
    "  print(\"Micro F1: \"+ str(micro_f1))\n",
    "  total_micro_f1=total_micro_f1+micro_f1\n",
    "\n",
    "  weighted_precision=precision_score(val_label, pred_labels, average='weighted')\n",
    "  print(\"Weighted Precision: \" + str(weighted_precision))\n",
    "  total_weighted_precision=total_weighted_precision+weighted_precision\n",
    "  micro_precision=precision_score(val_label, pred_labels, average='micro')\n",
    "  print(\"Micro Precision: \" + str(micro_precision))\n",
    "  total_micro_precision=total_micro_precision+micro_precision\n",
    "\n",
    "  weighted_recall=recall_score(val_label, pred_labels, average='weighted')\n",
    "  print(\"Weighted Recall: \" + str(weighted_recall))\n",
    "  total_weighted_recall=total_weighted_recall+weighted_recall\n",
    "  micro_recall=recall_score(val_label, pred_labels, average='micro')\n",
    "  print(\"Micro Recall: \" + str(micro_recall))\n",
    "  total_micro_recall=total_micro_recall+micro_recall\n",
    "\n",
    "\n",
    "print(\"Average Accuracy: \"+str(total_accuracy/5))\n",
    "print(\"Average Weighted F1: \"+str(total_weighted_f1/5))\n",
    "print(\"Average Micro F1: \"+str(total_micro_f1/5))\n",
    "print(\"Average Weighted Precision: \"+str(total_weighted_precision/5))\n",
    "print(\"Average Micro Precision: \"+str(total_micro_precision/5))\n",
    "print(\"Average Weighted Recall: \"+str(total_weighted_recall/5))\n",
    "print(\"Average Micro Recall: \"+str(total_micro_recall/5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_7 (InputLayer)           [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_8 (InputLayer)           [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_9 (InputLayer)           [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_10 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_5 (InputLayer)           [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_11 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_6 (InputLayer)           [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_12 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_1[0][0]',                \n",
      " el)                            thPoolingAndCrossAt               'input_7[0][0]',                \n",
      "                                tentions(last_hidde               'input_2[0][0]',                \n",
      "                                n_state=(None, 512,               'input_8[0][0]',                \n",
      "                                 768),                            'input_3[0][0]',                \n",
      "                                 pooler_output=(Non               'input_9[0][0]',                \n",
      "                                e, 768),                          'input_4[0][0]',                \n",
      "                                 past_key_values=No               'input_10[0][0]',               \n",
      "                                ne, hidden_states=N               'input_5[0][0]',                \n",
      "                                one, attentions=Non               'input_11[0][0]',               \n",
      "                                e, cross_attentions               'input_6[0][0]',                \n",
      "                                =None)                            'input_12[0][0]']               \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_roberta_model[0][0]']       \n",
      " ingOpLambda)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_roberta_model[1][0]']       \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_2 (Sl  (None, 768)         0           ['tf_roberta_model[2][0]']       \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_3 (Sl  (None, 768)         0           ['tf_roberta_model[3][0]']       \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_4 (Sl  (None, 768)         0           ['tf_roberta_model[4][0]']       \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_5 (Sl  (None, 768)         0           ['tf_roberta_model[5][0]']       \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 4608)         0           ['tf.__operators__.getitem[0][0]'\n",
      "                                                                 , 'tf.__operators__.getitem_1[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'tf.__operators__.getitem_2[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'tf.__operators__.getitem_3[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'tf.__operators__.getitem_4[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'tf.__operators__.getitem_5[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 512)          2359808     ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 15)           7695        ['dropout_37[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 127,013,135\n",
      "Trainable params: 127,013,135\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jun  6 16:58:56 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM...  On   | 00000000:4B:00.0 Off |                    0 |\n",
      "| N/A   26C    P0    66W / 500W |  79808MiB / 81920MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   3958836      C   python                          79805MiB |\n",
      "+-----------------------------------------------------------------------------+\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "948/948 [==============================] - 693s 632ms/step - loss: 2.5966 - accuracy: 0.5680 - val_loss: 2.0907 - val_accuracy: 0.7078\n",
      "Epoch 2/5\n",
      "948/948 [==============================] - 585s 618ms/step - loss: 1.9831 - accuracy: 0.7454 - val_loss: 1.9305 - val_accuracy: 0.7625\n",
      "Epoch 3/5\n",
      "948/948 [==============================] - 585s 617ms/step - loss: 1.7672 - accuracy: 0.7970 - val_loss: 1.8876 - val_accuracy: 0.7601\n",
      "Epoch 4/5\n",
      "948/948 [==============================] - 585s 617ms/step - loss: 1.6245 - accuracy: 0.8386 - val_loss: 1.8436 - val_accuracy: 0.7565\n",
      "Epoch 5/5\n",
      "948/948 [==============================] - 585s 618ms/step - loss: 1.4874 - accuracy: 0.8750 - val_loss: 1.8937 - val_accuracy: 0.7672\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_13 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_19 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_14 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_20 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_15 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_21 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_16 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_22 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_17 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_23 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_18 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_24 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_13[0][0]',               \n",
      " el)                            thPoolingAndCrossAt               'input_19[0][0]',               \n",
      "                                tentions(last_hidde               'input_14[0][0]',               \n",
      "                                n_state=(None, 512,               'input_20[0][0]',               \n",
      "                                 768),                            'input_15[0][0]',               \n",
      "                                 pooler_output=(Non               'input_21[0][0]',               \n",
      "                                e, 768),                          'input_16[0][0]',               \n",
      "                                 past_key_values=No               'input_22[0][0]',               \n",
      "                                ne, hidden_states=N               'input_17[0][0]',               \n",
      "                                one, attentions=Non               'input_23[0][0]',               \n",
      "                                e, cross_attentions               'input_18[0][0]',               \n",
      "                                =None)                            'input_24[0][0]']               \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_6 (Sl  (None, 768)         0           ['tf_roberta_model[6][0]']       \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_7 (Sl  (None, 768)         0           ['tf_roberta_model[7][0]']       \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_8 (Sl  (None, 768)         0           ['tf_roberta_model[8][0]']       \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_9 (Sl  (None, 768)         0           ['tf_roberta_model[9][0]']       \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_10 (S  (None, 768)         0           ['tf_roberta_model[10][0]']      \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_11 (S  (None, 768)         0           ['tf_roberta_model[11][0]']      \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 4608)         0           ['tf.__operators__.getitem_6[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'tf.__operators__.getitem_7[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'tf.__operators__.getitem_8[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'tf.__operators__.getitem_9[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'tf.__operators__.getitem_10[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'tf.__operators__.getitem_11[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 512)          2359808     ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 15)           7695        ['dropout_38[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 127,013,135\n",
      "Trainable params: 127,013,135\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Accuracy: 0.7672209026128266\n",
      "Weighted F1: 0.7681501976285979\n",
      "Micro F1: 0.7672209026128266\n",
      "Weighted Precision: 0.7777235319581102\n",
      "Micro Precision: 0.7672209026128266\n",
      "Weighted Recall: 0.7672209026128266\n",
      "Micro Recall: 0.7672209026128266\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "\n",
    "for i in range(1):\n",
    "  gc.collect()\n",
    "  tf.keras.backend.clear_session()\n",
    "  dbert_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "  dbert_model = TFRobertaModel.from_pretrained('roberta-base')\n",
    "  max_len=512\n",
    "  first_512_sentences=summarized_data['First_512_text']\n",
    "  second_512_sentences=summarized_data['Second_512_text']\n",
    "  third_512_sentences=summarized_data['Third_512_text']\n",
    "  fourth_512_sentences=summarized_data['Fourth_512_text']\n",
    "  fifth_512_sentences=summarized_data['Fifth_512_text']\n",
    "  sixth_512_sentences=summarized_data['Sixth_512_text']\n",
    "  labels=summarized_data['label']\n",
    "  len(first_512_sentences),len(labels),len(second_512_sentences),len(third_512_sentences),len(fourth_512_sentences),len(fifth_512_sentences),len(sixth_512_sentences)\n",
    "\n",
    "  model_0=create_model()\n",
    "\n",
    "  input_ids_first_512=[]\n",
    "  attention_masks_first_512=[]\n",
    "  for sent in summarized_data['First_512_text']:\n",
    "      dbert_inps=dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n",
    "      input_ids_first_512.append(dbert_inps['input_ids'])\n",
    "      attention_masks_first_512.append(dbert_inps['attention_mask'])\n",
    "  input_ids_first_512=np.asarray(input_ids_first_512)\n",
    "  attention_masks_first_512=np.array(attention_masks_first_512)\n",
    "  labels=np.array(labels)\n",
    "\n",
    "  input_ids_second_512=[]\n",
    "  attention_masks_second_512=[]\n",
    "  for sent in summarized_data['Second_512_text']:\n",
    "      dbert_inps=dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n",
    "      input_ids_second_512.append(dbert_inps['input_ids'])\n",
    "      attention_masks_second_512.append(dbert_inps['attention_mask'])\n",
    "  input_ids_second_512=np.asarray(input_ids_second_512)\n",
    "  attention_masks_second_512=np.array(attention_masks_second_512)\n",
    "\n",
    "  input_ids_third_512=[]\n",
    "  attention_masks_third_512=[]\n",
    "  for sent in summarized_data['Third_512_text']:\n",
    "      dbert_inps=dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n",
    "      input_ids_third_512.append(dbert_inps['input_ids'])\n",
    "      attention_masks_third_512.append(dbert_inps['attention_mask'])\n",
    "  input_ids_third_512=np.asarray(input_ids_third_512)\n",
    "  attention_masks_third_512=np.array(attention_masks_third_512)\n",
    "\n",
    "  input_ids_fourth_512=[]\n",
    "  attention_masks_fourth_512=[]\n",
    "  for sent in summarized_data['Fourth_512_text']:\n",
    "      dbert_inps=dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n",
    "      input_ids_fourth_512.append(dbert_inps['input_ids'])\n",
    "      attention_masks_fourth_512.append(dbert_inps['attention_mask'])\n",
    "  input_ids_fourth_512=np.asarray(input_ids_fourth_512)\n",
    "  attention_masks_fourth_512=np.array(attention_masks_fourth_512)\n",
    "\n",
    "  input_ids_fifth_512=[]\n",
    "  attention_masks_fifth_512=[]\n",
    "  for sent in summarized_data['Fifth_512_text']:\n",
    "      dbert_inps=dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n",
    "      input_ids_fifth_512.append(dbert_inps['input_ids'])\n",
    "      attention_masks_fifth_512.append(dbert_inps['attention_mask'])\n",
    "  input_ids_fifth_512=np.asarray(input_ids_fifth_512)\n",
    "  attention_masks_fifth_512=np.array(attention_masks_fifth_512)\n",
    "\n",
    "  input_ids_sixth_512=[]\n",
    "  attention_masks_sixth_512=[]\n",
    "  for sent in summarized_data['Sixth_512_text']:\n",
    "      dbert_inps=dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n",
    "      input_ids_sixth_512.append(dbert_inps['input_ids'])\n",
    "      attention_masks_sixth_512.append(dbert_inps['attention_mask'])\n",
    "  input_ids_sixth_512=np.asarray(input_ids_sixth_512)\n",
    "  attention_masks_sixth_512=np.array(attention_masks_sixth_512)\n",
    "\n",
    "  train_inp_first_512,val_inp_first_512,train_label,val_label,train_mask_first_512,val_mask_first_512,train_inp_second_512,val_inp_second_512,train_mask_second_512,val_mask_second_512,train_inp_third_512,val_inp_third_512,train_mask_third_512,val_mask_third_512,train_inp_fourth_512,val_inp_fourth_512,train_mask_fourth_512,val_mask_fourth_512,train_inp_fifth_512,val_inp_fifth_512,train_mask_fifth_512,val_mask_fifth_512,train_inp_sixth_512,val_inp_sixth_512,train_mask_sixth_512,val_mask_sixth_512=train_test_split(input_ids_first_512,labels,attention_masks_first_512,input_ids_second_512,attention_masks_second_512,input_ids_third_512,attention_masks_third_512,input_ids_fourth_512,attention_masks_fourth_512,input_ids_fifth_512,attention_masks_fifth_512,input_ids_sixth_512,attention_masks_sixth_512,test_size=0.1,random_state=42)\n",
    "\n",
    "  log_dir='dbert_model'\n",
    "\n",
    "  model_save_path='Concat-512/roberta-concat-512-4-15labels.h5'\n",
    "\n",
    "  loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "  accuracy = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "\n",
    "  optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
    "  callbacks= [tf.keras.callbacks.ModelCheckpoint(filepath=model_save_path,monitor='val_accuracy',mode='max',save_best_only=True,save_weights_only=True),keras.callbacks.TensorBoard(log_dir=log_dir)]\n",
    "  model_0.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])\n",
    "\n",
    "  gpu_info = !nvidia-smi\n",
    "  gpu_info = '\\n'.join(gpu_info)\n",
    "  if gpu_info.find('failed') >= 0:\n",
    "    print('Not connected to a GPU')\n",
    "  else:\n",
    "    print(gpu_info)\n",
    "  \n",
    "  history=model_0.fit([train_inp_first_512,train_mask_first_512,train_inp_second_512,train_mask_second_512,train_inp_third_512,train_mask_third_512,train_inp_fourth_512,train_mask_fourth_512,train_inp_fifth_512,train_mask_fifth_512,train_inp_sixth_512,train_mask_sixth_512],train_label,batch_size=8,epochs=5,validation_data=([val_inp_first_512,val_mask_first_512,val_inp_second_512,val_mask_second_512,val_inp_third_512,val_mask_third_512,val_inp_fourth_512,val_mask_fourth_512,val_inp_fifth_512,val_mask_fifth_512,val_inp_sixth_512,val_mask_sixth_512],val_label),callbacks=callbacks)\n",
    "  pred_labels=[]\n",
    "\n",
    "  model_saved= create_model()\n",
    "  model_saved.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])\n",
    "  model_saved.load_weights('Concat-512/roberta-concat-512-4-15labels.h5')\n",
    "\n",
    "  for i in range(0,len(val_inp_first_512)):\n",
    "    pred=model_saved.predict([val_inp_first_512[i].reshape(1,512),val_mask_first_512[i].reshape(1,512),val_inp_second_512[i].reshape(1,512),val_mask_second_512[i].reshape(1,512),val_inp_third_512[i].reshape(1,512),val_mask_third_512[i].reshape(1,512),val_inp_fourth_512[i].reshape(1,512),val_mask_fourth_512[i].reshape(1,512),val_inp_fifth_512[i].reshape(1,512),val_mask_fifth_512[i].reshape(1,512),val_inp_sixth_512[i].reshape(1,512),val_mask_sixth_512[i].reshape(1,512)])\n",
    "    pred_label = pred.argmax(axis=1)\n",
    "    pred_labels.append(pred_label)\n",
    "  accuracy=accuracy_score(val_label, pred_labels)\n",
    "  print(\"Accuracy: \"+str(accuracy))\n",
    "  \n",
    "  weighted_f1=f1_score(val_label,pred_labels, average='weighted')\n",
    "  print(\"Weighted F1: \"+ str(weighted_f1))\n",
    "  micro_f1=f1_score(val_label,pred_labels, average='micro')\n",
    "  print(\"Micro F1: \"+ str(micro_f1))\n",
    "  \n",
    "  weighted_precision=precision_score(val_label, pred_labels, average='weighted')\n",
    "  print(\"Weighted Precision: \" + str(weighted_precision))\n",
    "  micro_precision=precision_score(val_label, pred_labels, average='micro')\n",
    "  print(\"Micro Precision: \" + str(micro_precision))\n",
    "  \n",
    "  weighted_recall=recall_score(val_label, pred_labels, average='weighted')\n",
    "  print(\"Weighted Recall: \" + str(weighted_recall))\n",
    "  micro_recall=recall_score(val_label, pred_labels, average='micro')\n",
    "  print(\"Micro Recall: \" + str(micro_recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_37 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_43 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_38 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_44 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_39 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_45 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_40 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_46 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_41 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_47 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_42 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_48 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_37[0][0]',               \n",
      " el)                            thPoolingAndCrossAt               'input_43[0][0]',               \n",
      "                                tentions(last_hidde               'input_38[0][0]',               \n",
      "                                n_state=(None, 512,               'input_44[0][0]',               \n",
      "                                 768),                            'input_39[0][0]',               \n",
      "                                 pooler_output=(Non               'input_45[0][0]',               \n",
      "                                e, 768),                          'input_40[0][0]',               \n",
      "                                 past_key_values=No               'input_46[0][0]',               \n",
      "                                ne, hidden_states=N               'input_41[0][0]',               \n",
      "                                one, attentions=Non               'input_47[0][0]',               \n",
      "                                e, cross_attentions               'input_42[0][0]',               \n",
      "                                =None)                            'input_48[0][0]']               \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_18 (S  (None, 768)         0           ['tf_roberta_model[18][0]']      \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_19 (S  (None, 768)         0           ['tf_roberta_model[19][0]']      \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_20 (S  (None, 768)         0           ['tf_roberta_model[20][0]']      \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_21 (S  (None, 768)         0           ['tf_roberta_model[21][0]']      \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_22 (S  (None, 768)         0           ['tf_roberta_model[22][0]']      \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_23 (S  (None, 768)         0           ['tf_roberta_model[23][0]']      \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 4608)         0           ['tf.__operators__.getitem_18[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'tf.__operators__.getitem_19[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'tf.__operators__.getitem_20[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'tf.__operators__.getitem_21[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'tf.__operators__.getitem_22[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'tf.__operators__.getitem_23[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 512)          2359808     ['concatenate_3[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_40 (Dropout)           (None, 512)          0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 15)           7695        ['dropout_40[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 127,013,135\n",
      "Trainable params: 127,013,135\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Accuracy: 0.7779097387173397\n",
      "Weighted F1: 0.7806820892531718\n",
      "Micro F1: 0.7779097387173397\n",
      "Weighted Precision: 0.7924851811258938\n",
      "Micro Precision: 0.7779097387173397\n",
      "Weighted Recall: 0.7779097387173397\n",
      "Micro Recall: 0.7779097387173397\n",
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_49 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_55 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_50 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_56 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_51 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_57 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_52 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_58 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_53 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_59 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_54 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_60 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_49[0][0]',               \n",
      " el)                            thPoolingAndCrossAt               'input_55[0][0]',               \n",
      "                                tentions(last_hidde               'input_50[0][0]',               \n",
      "                                n_state=(None, 512,               'input_56[0][0]',               \n",
      "                                 768),                            'input_51[0][0]',               \n",
      "                                 pooler_output=(Non               'input_57[0][0]',               \n",
      "                                e, 768),                          'input_52[0][0]',               \n",
      "                                 past_key_values=No               'input_58[0][0]',               \n",
      "                                ne, hidden_states=N               'input_53[0][0]',               \n",
      "                                one, attentions=Non               'input_59[0][0]',               \n",
      "                                e, cross_attentions               'input_54[0][0]',               \n",
      "                                =None)                            'input_60[0][0]']               \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_24 (S  (None, 768)         0           ['tf_roberta_model[24][0]']      \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_25 (S  (None, 768)         0           ['tf_roberta_model[25][0]']      \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_26 (S  (None, 768)         0           ['tf_roberta_model[26][0]']      \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_27 (S  (None, 768)         0           ['tf_roberta_model[27][0]']      \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_28 (S  (None, 768)         0           ['tf_roberta_model[28][0]']      \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_29 (S  (None, 768)         0           ['tf_roberta_model[29][0]']      \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " concatenate_4 (Concatenate)    (None, 4608)         0           ['tf.__operators__.getitem_24[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'tf.__operators__.getitem_25[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'tf.__operators__.getitem_26[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'tf.__operators__.getitem_27[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'tf.__operators__.getitem_28[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'tf.__operators__.getitem_29[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 512)          2359808     ['concatenate_4[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_41 (Dropout)           (None, 512)          0           ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 15)           7695        ['dropout_41[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 127,013,135\n",
      "Trainable params: 127,013,135\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Accuracy: 0.7790973871733967\n",
      "Weighted F1: 0.7808423494095597\n",
      "Micro F1: 0.7790973871733967\n",
      "Weighted Precision: 0.7914423273200011\n",
      "Micro Precision: 0.7790973871733967\n",
      "Weighted Recall: 0.7790973871733967\n",
      "Micro Recall: 0.7790973871733967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_61 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_67 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_62 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_68 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_63 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_69 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_64 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_70 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_65 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_71 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_66 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_72 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_61[0][0]',               \n",
      " el)                            thPoolingAndCrossAt               'input_67[0][0]',               \n",
      "                                tentions(last_hidde               'input_62[0][0]',               \n",
      "                                n_state=(None, 512,               'input_68[0][0]',               \n",
      "                                 768),                            'input_63[0][0]',               \n",
      "                                 pooler_output=(Non               'input_69[0][0]',               \n",
      "                                e, 768),                          'input_64[0][0]',               \n",
      "                                 past_key_values=No               'input_70[0][0]',               \n",
      "                                ne, hidden_states=N               'input_65[0][0]',               \n",
      "                                one, attentions=Non               'input_71[0][0]',               \n",
      "                                e, cross_attentions               'input_66[0][0]',               \n",
      "                                =None)                            'input_72[0][0]']               \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_30 (S  (None, 768)         0           ['tf_roberta_model[30][0]']      \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_31 (S  (None, 768)         0           ['tf_roberta_model[31][0]']      \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_32 (S  (None, 768)         0           ['tf_roberta_model[32][0]']      \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_33 (S  (None, 768)         0           ['tf_roberta_model[33][0]']      \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_34 (S  (None, 768)         0           ['tf_roberta_model[34][0]']      \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_35 (S  (None, 768)         0           ['tf_roberta_model[35][0]']      \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " concatenate_5 (Concatenate)    (None, 4608)         0           ['tf.__operators__.getitem_30[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'tf.__operators__.getitem_31[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'tf.__operators__.getitem_32[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'tf.__operators__.getitem_33[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'tf.__operators__.getitem_34[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'tf.__operators__.getitem_35[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 512)          2359808     ['concatenate_5[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_42 (Dropout)           (None, 512)          0           ['dense_10[0][0]']               \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 15)           7695        ['dropout_42[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 127,013,135\n",
      "Trainable params: 127,013,135\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Accuracy: 0.7600950118764845\n",
      "Weighted F1: 0.7565712645746279\n",
      "Micro F1: 0.7600950118764844\n",
      "Weighted Precision: 0.762400579506918\n",
      "Micro Precision: 0.7600950118764845\n",
      "Weighted Recall: 0.7600950118764845\n",
      "Micro Recall: 0.7600950118764845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_73 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_79 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_74 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_80 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_75 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_81 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_76 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_82 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_77 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_83 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_78 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_84 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_73[0][0]',               \n",
      " el)                            thPoolingAndCrossAt               'input_79[0][0]',               \n",
      "                                tentions(last_hidde               'input_74[0][0]',               \n",
      "                                n_state=(None, 512,               'input_80[0][0]',               \n",
      "                                 768),                            'input_75[0][0]',               \n",
      "                                 pooler_output=(Non               'input_81[0][0]',               \n",
      "                                e, 768),                          'input_76[0][0]',               \n",
      "                                 past_key_values=No               'input_82[0][0]',               \n",
      "                                ne, hidden_states=N               'input_77[0][0]',               \n",
      "                                one, attentions=Non               'input_83[0][0]',               \n",
      "                                e, cross_attentions               'input_78[0][0]',               \n",
      "                                =None)                            'input_84[0][0]']               \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_36 (S  (None, 768)         0           ['tf_roberta_model[36][0]']      \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_37 (S  (None, 768)         0           ['tf_roberta_model[37][0]']      \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_38 (S  (None, 768)         0           ['tf_roberta_model[38][0]']      \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_39 (S  (None, 768)         0           ['tf_roberta_model[39][0]']      \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_40 (S  (None, 768)         0           ['tf_roberta_model[40][0]']      \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_41 (S  (None, 768)         0           ['tf_roberta_model[41][0]']      \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " concatenate_6 (Concatenate)    (None, 4608)         0           ['tf.__operators__.getitem_36[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'tf.__operators__.getitem_37[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'tf.__operators__.getitem_38[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'tf.__operators__.getitem_39[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'tf.__operators__.getitem_40[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'tf.__operators__.getitem_41[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, 512)          2359808     ['concatenate_6[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_43 (Dropout)           (None, 512)          0           ['dense_12[0][0]']               \n",
      "                                                                                                  \n",
      " dense_13 (Dense)               (None, 15)           7695        ['dropout_43[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 127,013,135\n",
      "Trainable params: 127,013,135\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Accuracy: 0.7802850356294537\n",
      "Weighted F1: 0.7799414024179464\n",
      "Micro F1: 0.7802850356294538\n",
      "Weighted Precision: 0.7870813824504076\n",
      "Micro Precision: 0.7802850356294537\n",
      "Weighted Recall: 0.7802850356294537\n",
      "Micro Recall: 0.7802850356294537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_85 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_91 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_86 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_92 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_87 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_93 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_88 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_94 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_89 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_95 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_90 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_96 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_85[0][0]',               \n",
      " el)                            thPoolingAndCrossAt               'input_91[0][0]',               \n",
      "                                tentions(last_hidde               'input_86[0][0]',               \n",
      "                                n_state=(None, 512,               'input_92[0][0]',               \n",
      "                                 768),                            'input_87[0][0]',               \n",
      "                                 pooler_output=(Non               'input_93[0][0]',               \n",
      "                                e, 768),                          'input_88[0][0]',               \n",
      "                                 past_key_values=No               'input_94[0][0]',               \n",
      "                                ne, hidden_states=N               'input_89[0][0]',               \n",
      "                                one, attentions=Non               'input_95[0][0]',               \n",
      "                                e, cross_attentions               'input_90[0][0]',               \n",
      "                                =None)                            'input_96[0][0]']               \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_42 (S  (None, 768)         0           ['tf_roberta_model[42][0]']      \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_43 (S  (None, 768)         0           ['tf_roberta_model[43][0]']      \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_44 (S  (None, 768)         0           ['tf_roberta_model[44][0]']      \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_45 (S  (None, 768)         0           ['tf_roberta_model[45][0]']      \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_46 (S  (None, 768)         0           ['tf_roberta_model[46][0]']      \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_47 (S  (None, 768)         0           ['tf_roberta_model[47][0]']      \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " concatenate_7 (Concatenate)    (None, 4608)         0           ['tf.__operators__.getitem_42[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'tf.__operators__.getitem_43[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'tf.__operators__.getitem_44[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'tf.__operators__.getitem_45[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'tf.__operators__.getitem_46[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'tf.__operators__.getitem_47[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " dense_14 (Dense)               (None, 512)          2359808     ['concatenate_7[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_44 (Dropout)           (None, 512)          0           ['dense_14[0][0]']               \n",
      "                                                                                                  \n",
      " dense_15 (Dense)               (None, 15)           7695        ['dropout_44[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 127,013,135\n",
      "Trainable params: 127,013,135\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Accuracy: 0.7672209026128266\n",
      "Weighted F1: 0.7681501976285979\n",
      "Micro F1: 0.7672209026128266\n",
      "Weighted Precision: 0.7777235319581102\n",
      "Micro Precision: 0.7672209026128266\n",
      "Weighted Recall: 0.7672209026128266\n",
      "Micro Recall: 0.7672209026128266\n",
      "Average Accuracy: 0.7729216152019003\n",
      "Average Weighted F1: 0.7732374606567808\n",
      "Average Micro F1: 0.7729216152019003\n",
      "Average Weighted Precision: 0.782226600472266\n",
      "Average Micro Precision: 0.7729216152019003\n",
      "Average Weighted Recall: 0.7729216152019003\n",
      "Average Micro Recall: 0.7729216152019003\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "total_accuracy=0\n",
    "total_weighted_f1=0\n",
    "total_micro_f1=0\n",
    "total_weighted_precision=0\n",
    "total_micro_precision=0\n",
    "total_weighted_recall=0\n",
    "total_micro_recall=0\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    pred_labels=[]\n",
    "    model_saved= create_model()\n",
    "    model_saved.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])\n",
    "    model_saved.load_weights('Concat-512/roberta-concat-512-'+str(i)+'-15labels.h5')\n",
    "\n",
    "    for i in range(0,len(val_inp_first_512)):\n",
    "        pred=model_saved.predict([val_inp_first_512[i].reshape(1,512),val_mask_first_512[i].reshape(1,512),val_inp_second_512[i].reshape(1,512),val_mask_second_512[i].reshape(1,512),val_inp_third_512[i].reshape(1,512),val_mask_third_512[i].reshape(1,512),val_inp_fourth_512[i].reshape(1,512),val_mask_fourth_512[i].reshape(1,512),val_inp_fifth_512[i].reshape(1,512),val_mask_fifth_512[i].reshape(1,512),val_inp_sixth_512[i].reshape(1,512),val_mask_sixth_512[i].reshape(1,512)])\n",
    "        pred_label = pred.argmax(axis=1)\n",
    "        pred_labels.append(pred_label)\n",
    "    accuracy=accuracy_score(val_label, pred_labels)\n",
    "    print(\"Accuracy: \"+str(accuracy))\n",
    "    total_accuracy=total_accuracy+accuracy\n",
    "\n",
    "    weighted_f1=f1_score(val_label,pred_labels, average='weighted')\n",
    "    print(\"Weighted F1: \"+ str(weighted_f1))\n",
    "    total_weighted_f1=total_weighted_f1+weighted_f1\n",
    "    micro_f1=f1_score(val_label,pred_labels, average='micro')\n",
    "    print(\"Micro F1: \"+ str(micro_f1))\n",
    "    total_micro_f1=total_micro_f1+micro_f1\n",
    "\n",
    "    weighted_precision=precision_score(val_label, pred_labels, average='weighted')\n",
    "    print(\"Weighted Precision: \" + str(weighted_precision))\n",
    "    total_weighted_precision=total_weighted_precision+weighted_precision\n",
    "    micro_precision=precision_score(val_label, pred_labels, average='micro')\n",
    "    print(\"Micro Precision: \" + str(micro_precision))\n",
    "    total_micro_precision=total_micro_precision+micro_precision\n",
    "\n",
    "    weighted_recall=recall_score(val_label, pred_labels, average='weighted')\n",
    "    print(\"Weighted Recall: \" + str(weighted_recall))\n",
    "    total_weighted_recall=total_weighted_recall+weighted_recall\n",
    "    micro_recall=recall_score(val_label, pred_labels, average='micro')\n",
    "    print(\"Micro Recall: \" + str(micro_recall))\n",
    "    total_micro_recall=total_micro_recall+micro_recall\n",
    "    \n",
    "print(\"Average Accuracy: \"+str(total_accuracy/5))\n",
    "print(\"Average Weighted F1: \"+str(total_weighted_f1/5))\n",
    "print(\"Average Micro F1: \"+str(total_micro_f1/5))\n",
    "print(\"Average Weighted Precision: \"+str(total_weighted_precision/5))\n",
    "print(\"Average Micro Precision: \"+str(total_micro_precision/5))\n",
    "print(\"Average Weighted Recall: \"+str(total_weighted_recall/5))\n",
    "print(\"Average Micro Recall: \"+str(total_micro_recall/5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "background_execution": "on",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "BERT_Concat_15labels.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "WOL_ENV",
   "language": "python",
   "name": "wol_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
