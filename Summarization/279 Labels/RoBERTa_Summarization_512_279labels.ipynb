{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCIR7rd5h9fv",
        "outputId": "fd8381ba-c1d4-4f5d-e6c2-045e3940d4e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "#drive.flush_and_unmount()\n",
        "drive.mount('/content/drive')\n",
        "#drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0k8FHrSIiqDo",
        "outputId": "ed3a3b87-22d1-482e-f49b-9ff6adf0df44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 15.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.6.0-py3-none-any.whl (84 kB)\n",
            "\u001b[K     |████████████████████████████████| 84 kB 2.2 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 74.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 86.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.6.0 pyyaml-6.0 tokenizers-0.12.1 transformers-4.19.2\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 14.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.96\n",
            "Collecting tensorflow==2.7.0\n",
            "  Downloading tensorflow-2.7.0-cp37-cp37m-manylinux2010_x86_64.whl (489.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 489.6 MB 19 kB/s \n",
            "\u001b[?25hCollecting gast<0.5.0,>=0.2.1\n",
            "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (1.1.2)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (14.0.1)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (2.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (1.0.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (3.17.3)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (2.8.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (4.2.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (0.37.1)\n",
            "Collecting keras<2.8,>=2.7.0rc0\n",
            "  Downloading keras-2.7.0-py2.py3-none-any.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 78.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (0.2.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (0.25.0)\n",
            "Collecting tensorflow-estimator<2.8,~=2.7.0rc0\n",
            "  Downloading tensorflow_estimator-2.7.0-py2.py3-none-any.whl (463 kB)\n",
            "\u001b[K     |████████████████████████████████| 463 kB 87.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (1.14.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (3.3.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (1.21.6)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (1.6.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (1.1.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (1.46.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow==2.7.0) (1.5.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (0.6.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (1.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (3.3.7)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (1.35.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (57.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.7.0) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow==2.7.0) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow==2.7.0) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.7.0) (3.2.0)\n",
            "Installing collected packages: tensorflow-estimator, keras, gast, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.8.0\n",
            "    Uninstalling tensorflow-estimator-2.8.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.8.0\n",
            "    Uninstalling keras-2.8.0:\n",
            "      Successfully uninstalled keras-2.8.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.3\n",
            "    Uninstalling gast-0.5.3:\n",
            "      Successfully uninstalled gast-0.5.3\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.8.0+zzzcolab20220506162203\n",
            "    Uninstalling tensorflow-2.8.0+zzzcolab20220506162203:\n",
            "      Successfully uninstalled tensorflow-2.8.0+zzzcolab20220506162203\n",
            "Successfully installed gast-0.4.0 keras-2.7.0 tensorflow-2.7.0 tensorflow-estimator-2.7.0\n",
            "Collecting stanza\n",
            "  Downloading stanza-1.4.0-py3-none-any.whl (574 kB)\n",
            "\u001b[K     |████████████████████████████████| 574 kB 14.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from stanza) (4.64.0)\n",
            "Collecting emoji\n",
            "  Downloading emoji-1.7.0.tar.gz (175 kB)\n",
            "\u001b[K     |████████████████████████████████| 175 kB 96.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from stanza) (1.21.6)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from stanza) (3.17.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from stanza) (1.15.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from stanza) (2.23.0)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from stanza) (1.11.0+cu113)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (from stanza) (4.19.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3.0->stanza) (4.2.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->stanza) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->stanza) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->stanza) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->stanza) (3.0.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers->stanza) (0.6.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers->stanza) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers->stanza) (2019.12.20)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers->stanza) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers->stanza) (4.11.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers->stanza) (3.7.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers->stanza) (0.12.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers->stanza) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers->stanza) (3.8.0)\n",
            "Building wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171046 sha256=b4e06cc8ea30b2c1232cae32b6c4398f83565e851395987da968109babd13c78\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/4e/b6/57b01db010d17ef6ea9b40300af725ef3e210cb1acfb7ac8b6\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji, stanza\n",
            "Successfully installed emoji-1.7.0 stanza-1.4.0\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.19.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.17.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 13.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow-addons) (3.0.9)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.17.0\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n",
            "Collecting textacy\n",
            "  Downloading textacy-0.11.0-py3-none-any.whl (200 kB)\n",
            "\u001b[K     |████████████████████████████████| 200 kB 15.5 MB/s \n",
            "\u001b[?25hCollecting pyphen>=0.10.0\n",
            "  Downloading pyphen-0.12.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0 MB 86.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from textacy) (2.6.3)\n",
            "Requirement already satisfied: tqdm>=4.19.6 in /usr/local/lib/python3.7/dist-packages (from textacy) (4.64.0)\n",
            "Requirement already satisfied: joblib>=0.13.0 in /usr/local/lib/python3.7/dist-packages (from textacy) (1.1.0)\n",
            "Collecting jellyfish>=0.8.0\n",
            "  Downloading jellyfish-0.9.0.tar.gz (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 93.2 MB/s \n",
            "\u001b[?25hCollecting spacy>=3.0.0\n",
            "  Downloading spacy-3.3.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.2 MB 83.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cachetools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from textacy) (4.2.4)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from textacy) (2.23.0)\n",
            "Collecting cytoolz>=0.10.1\n",
            "  Downloading cytoolz-0.11.2.tar.gz (481 kB)\n",
            "\u001b[K     |████████████████████████████████| 481 kB 83.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.19.0 in /usr/local/lib/python3.7/dist-packages (from textacy) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from textacy) (1.21.6)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from textacy) (1.4.1)\n",
            "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from cytoolz>=0.10.1->textacy) (0.11.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->textacy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->textacy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->textacy) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->textacy) (2021.10.8)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.0->textacy) (3.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (21.3)\n",
            "Collecting catalogue<2.1.0,>=2.0.6\n",
            "  Downloading catalogue-2.0.7-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (57.4.0)\n",
            "Collecting pathy>=0.3.5\n",
            "  Downloading pathy-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.7 MB/s \n",
            "\u001b[?25hCollecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
            "  Downloading pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 86.8 MB/s \n",
            "\u001b[?25hCollecting srsly<3.0.0,>=2.4.3\n",
            "  Downloading srsly-2.4.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (457 kB)\n",
            "\u001b[K     |████████████████████████████████| 457 kB 86.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (3.0.6)\n",
            "Collecting typing-extensions<4.0.0.0,>=3.7.4\n",
            "  Downloading typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (0.9.1)\n",
            "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
            "  Downloading spacy_loggers-1.0.2-py3-none-any.whl (7.2 kB)\n",
            "Collecting typer<0.5.0,>=0.3.0\n",
            "  Downloading typer-0.4.1-py3-none-any.whl (27 kB)\n",
            "Collecting thinc<8.1.0,>=8.0.14\n",
            "  Downloading thinc-8.0.16-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (660 kB)\n",
            "\u001b[K     |████████████████████████████████| 660 kB 88.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (2.0.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (1.0.7)\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.9\n",
            "  Downloading spacy_legacy-3.0.9-py2.py3-none-any.whl (20 kB)\n",
            "Collecting langcodes<4.0.0,>=3.2.0\n",
            "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 62.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (2.11.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (0.4.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy>=3.0.0->textacy) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy>=3.0.0->textacy) (3.0.9)\n",
            "Collecting smart-open<6.0.0,>=5.0.0\n",
            "  Downloading smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 7.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy>=3.0.0->textacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy>=3.0.0->textacy) (2.0.1)\n",
            "Building wheels for collected packages: cytoolz, jellyfish\n",
            "  Building wheel for cytoolz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cytoolz: filename=cytoolz-0.11.2-cp37-cp37m-linux_x86_64.whl size=1236770 sha256=fb800ed833313c05ecb4871ccb0f448031d53fa09e80a40c7560c6705a39efbd\n",
            "  Stored in directory: /root/.cache/pip/wheels/38/70/71/ca13ea3d36ccd0b3d0ec7d7a4ca67522048d695b556bba4f59\n",
            "  Building wheel for jellyfish (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jellyfish: filename=jellyfish-0.9.0-cp37-cp37m-linux_x86_64.whl size=73967 sha256=f4938ab36772ea25e663ab187fe124a520de84b8bdc8320f8389cd69d6f940b2\n",
            "  Stored in directory: /root/.cache/pip/wheels/fe/99/4e/646ce766df0d070b0ef04db27aa11543e2767fda3075aec31b\n",
            "Successfully built cytoolz jellyfish\n",
            "Installing collected packages: typing-extensions, catalogue, typer, srsly, smart-open, pydantic, thinc, spacy-loggers, spacy-legacy, pathy, langcodes, spacy, pyphen, jellyfish, cytoolz, textacy\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing-extensions 4.2.0\n",
            "    Uninstalling typing-extensions-4.2.0:\n",
            "      Successfully uninstalled typing-extensions-4.2.0\n",
            "  Attempting uninstall: catalogue\n",
            "    Found existing installation: catalogue 1.0.0\n",
            "    Uninstalling catalogue-1.0.0:\n",
            "      Successfully uninstalled catalogue-1.0.0\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 1.0.5\n",
            "    Uninstalling srsly-1.0.5:\n",
            "      Successfully uninstalled srsly-1.0.5\n",
            "  Attempting uninstall: smart-open\n",
            "    Found existing installation: smart-open 6.0.0\n",
            "    Uninstalling smart-open-6.0.0:\n",
            "      Successfully uninstalled smart-open-6.0.0\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "Successfully installed catalogue-2.0.7 cytoolz-0.11.2 jellyfish-0.9.0 langcodes-3.3.0 pathy-0.6.1 pydantic-1.8.2 pyphen-0.12.0 smart-open-5.2.1 spacy-3.3.0 spacy-legacy-3.0.9 spacy-loggers-1.0.2 srsly-2.4.3 textacy-0.11.0 thinc-8.0.16 typer-0.4.1 typing-extensions-3.10.0.2\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "!pip install tensorflow==2.7.0\n",
        "!pip install stanza\n",
        "!pip install transformers\n",
        "!pip install tensorflow-addons\n",
        "!pip install nltk\n",
        "!pip install textacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0rs0NoritMk",
        "outputId": "15255043-9170-4505-8344-1b8af5f08201"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.7.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "wYwcFK5gixXz"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from textacy.datasets.supreme_court import SupremeCourt\n",
        "import numpy as np\n",
        "import re\n",
        "import unicodedata\n",
        "import nltk\n",
        "#from transformers import pipeline\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Dense,Dropout, Input, BatchNormalization\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "from sklearn.metrics import confusion_matrix,f1_score,classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "from sklearn.utils import shuffle\n",
        "from tensorflow.keras import regularizers\n",
        "#from transformers import *\n",
        "from transformers import BertTokenizer, TFBertModel, BertConfig,TFDistilBertModel,DistilBertTokenizer,DistilBertConfig\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, TFAutoModel\n",
        "import numpy as np\n",
        "import gc\n",
        "import math\n",
        "import json\n",
        "import stanza\n",
        "from tensorflow.keras import *\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import *\n",
        "import tensorflow.keras.backend as K\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import classification_report\n",
        "from transformers import TFRobertaModel,RobertaTokenizer\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.initializers import RandomUniform\n",
        "\n",
        "from numpy.random import seed\n",
        "import random as python_random\n",
        "import os\n",
        "import sys\n",
        "\n",
        "np.random.seed(1)\n",
        "python_random.seed(1)\n",
        "tf.random.set_seed(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jrwKSatri3Lh"
      },
      "outputs": [],
      "source": [
        "!cp \"/content/drive/My Drive/Summarization/summarized_usdb.txt\" \"./summarized_usdb.txt\"\n",
        "!cp \"/content/drive/My Drive/labels_sc.txt\" \"./labels_sc.txt\"\n",
        "!cp \"/content/drive/My Drive/labels_sc_279.txt\" \"./labels_sc_279.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZinwFiui-A3",
        "outputId": "6cd55654-fc01-4e0d-aebd-08354f417f47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Length 389.2275804727402\n",
            "['209', '63', '216', '108', '196', '200', '203', '246', '188', '193', '25', '106', '56', '53', '223', '161', '200', '208', '78', '147', '147', '179', '147', '80', '56', '209', '209', '200', '214', '202', '203', '194', '26', '237', '165', '258', '238', '238', '76', '193', '196', '213', '161', '213', '209', '194', '196', '25', '151', '260', '135', '173', '30', '108', '272', '259', '176', '213', '213', '21', '5', '171', '200', '195', '240', '194', '176', '240', '240', '80', '173', '98', '214', '21', '76', '76', '73', '173', '173', '78', '61', '258', '191', '3', '78', '173', '193', '118', '200', '264', '260', '49', '211', '258', '18', '173', '78', '3', '3', '1', '213', '135', '182', '239', '208', '278', '50', '194', '200', '124', '189', '117', '264', '244', '76', '193', '221', '78', '78', '173', '197', '78', '161', '231', '135', '22', '78', '25', '25', '197', '200', '147', '203', '191', '196', '193', '263', '32', '197', '191', '4', '56', '196', '104', '191', '196', '173', '147', '194', '96', '161', '161', '238', '200', '3', '78', '78', '53', '41', '237', '53', '18', '10', '194', '97', '96', '208', '25', '10', '117', '104', '54', '96', '194', '21', '245', '238', '202', '209', '161', '97', '258', '240', '213', '136', '155', '152', '21', '191', '191', '118', '68', '258', '189', '197', '208', '61', '78', '135', '202', '96', '155', '108', '272', '80', '25', '155', '35', '213', '191', '194', '96', '96', '191', '264', '191', '191', '191', '2', '258', '191', '173', '259', '15', '47', '258', '156', '118', '160', '115', '105', '26', '173', '192', '76', '76', '135', '240', '118', '161', '200', '230', '18', '200', '25', '10', '161', '244', '24', '24', '200', '205', '104', '15', '193', '156', '95', '209', '240', '191', '194', '128', '208', '208', '218', '173', '10', '25', '18', '107', '46', '32', '197', '172', '209', '213', '79', '121', '78', '78', '193', '2', '78', '194', '149', '136', '80', '3', '21', '200', '78', '21', '197', '194', '118', '202', '161', '180', '209', '173', '2', '193', '6', '260', '197', '78', '78', '42', '191', '213', '135', '193', '196', '202', '222', '217', '42', '259', '213', '76', '46', '30', '15', '260', '202', '18', '259', '104', '135', '76', '196', '240', '240', '240', '108', '24', '200', '197', '262', '176', '15', '156', '264', '156', '200', '191', '161', '196', '78', '238', '161', '221', '118', '14', '14', '14', '200', '278', '238', '200', '196', '78', '173', '25', '197', '197', '196', '161', '18', '10', '10', '10', '18', '66', '203', '19', '246', '196', '244', '240', '161', '200', '78', '240', '209', '208', '118', '215', '194', '193', '168', '229', '191', '135', '172', '196', '197', '161', '78', '196', '197', '258', '78', '78', '238', '221', '121', '264', '104', '146', '50', '78', '217', '11', '238', '238', '238', '3', '3', '154', '215', '79', '18', '158', '104', '18', '13', '197', '117', '197', '161', '213', '242', '32', '238', '230', '2', '2', '32', '161', '251', '96', '156', '242', '22', '22', '140', '3', '149', '135', '191', '173', '135', '200', '238', '238', '78', '156', '209', '78', '97', '97', '160', '25', '258', '240', '161', '15', '161', '96', '209', '80', '132', '140', '4', '1', '240', '196', '274', '272', '62', '80', '196', '196', '191', '79', '156', '15', '196', '209', '22', '251', '202', '202', '121', '73', '191', '238', '191', '150', '150', '135', '46', '203', '202', '238', '22', '3', '258', '239', '238', '238', '165', '238', '238', '194', '200', '200', '197', '200', '196', '191', '194', '76', '78', '208', '200', '71', '208', '240', '76', '117', '120', '108', '96', '144', '30', '133', '133', '202', '117', '161', '251', '29', '104', '242', '197', '118', '216', '262', '236', '141', '11', '235', '235', '133', '191', '238', '202', '80', '80', '22', '140', '191', '197', '202', '133', '184', '184', '184', '184', '143', '274', '50', '237', '196', '274', '265', '10', '18', '10', '121', '238', '245', '228', '76', '25', '191', '197', '268', '10', '240', '195', '196', '195', '15', '48', '200', '183', '70', '59', '197', '196', '235', '161', '161', '215', '104', '97', '191', '194', '191', '200', '200', '197', '76', '200', '93', '256', '160', '238', '146', '238', '239', '15', '140', '258', '140', '21', '79', '202', '258', '79', '62', '205', '161', '104', '10', '161', '196', '137', '152', '191', '117', '213', '242', '176', '218', '195', '191', '162', '191', '135', '241', '213', '213', '84', '79', '68', '18', '98', '197', '155', '41', '246', '269', '78', '209', '238', '80', '238', '62', '237', '273', '273', '230', '150', '121', '260', '244', '161', '202', '78', '109', '155', '264', '145', '50', '274', '173', '210', '258', '238', '211', '156', '215', '195', '177', '244', '3', '141', '196', '191', '194', '29', '30', '10', '16', '200', '200', '156', '53', '3', '62', '165', '173', '173', '22', '205', '274', '150', '183', '116', '188', '188', '98', '161', '204', '80', '172', '156', '104', '229', '259', '78', '169', '3', '200', '118', '161', '80', '260', '240', '150', '197', '196', '62', '93', '6', '272', '272', '197', '160', '78', '96', '70', '195', '191', '191', '80', '3', '3', '240', '147', '196', '191', '238', '96', '203', '3', '15', '10', '58', '55', '96', '41', '15', '26', '50', '238', '194', '79', '240', '161', '191', '195', '263', '50', '240', '147', '235', '195', '203', '59', '178', '176', '2', '15', '203', '191', '18', '278', '196', '14', '153', '58', '175', '18', '217', '217', '80', '191', '200', '18', '200', '173', '24', '171', '171', '210', '124', '34', '200', '218', '104', '4', '223', '193', '194', '200', '200', '13', '62', '194', '197', '80', '197', '146', '146', '96', '97', '97', '2', '196', '191', '191', '104', '238', '10', '200', '239', '135', '104', '213', '2', '221', '169', '25', '21', '194', '161', '21', '278', '3', '195', '46', '176', '25', '117', '191', '62', '62', '62', '78', '50', '264', '79', '197', '80', '80', '80', '191', '191', '78', '161', '194', '195', '197', '203', '259', '278', '49', '147', '147', '147', '147', '22', '78', '78', '258', '2', '202', '221', '55', '258', '104', '104', '84', '240', '221', '274', '104', '22', '241', '63', '197', '197', '196', '159', '200', '22', '22', '22', '78', '78', '173', '104', '97', '104', '191', '146', '223', '268', '96', '14', '173', '204', '241', '97', '15', '118', '78', '78', '196', '194', '258', '96', '96', '259', '75', '10', '197', '30', '2', '238', '121', '50', '161', '173', '173', '186', '185', '186', '78', '194', '221', '218', '35', '33', '13', '34', '78', '21', '215', '97', '60', '24', '78', '193', '173', '235', '78', '200', '258', '140', '259', '194', '197', '195', '200', '145', '76', '108', '130', '238', '215', '213', '2', '217', '121', '178', '140', '62', '156', '176', '196', '196', '41', '223', '147', '172', '79', '200', '2', '244', '244', '191', '213', '104', '191', '237', '237', '33', '117', '195', '195', '264', '195', '140', '210', '41', '197', '14', '257', '196', '238', '238', '79', '161', '156', '196', '238', '238', '213', '104', '3', '10', '118', '204', '235', '33', '259', '274', '259', '176', '78', '78', '196', '34', '52', '25', '30', '194', '153', '21', '70', '191', '18', '191', '191', '172', '194', '194', '194', '194', '148', '104', '125', '2', '2', '2', '185', '194', '29', '104', '78', '189', '272', '204', '183', '213', '78', '104', '197', '240', '97', '168', '168', '182', '238', '197', '18', '194', '229', '221', '62', '78', '53', '104', '214', '189', '189', '189', '213', '117', '117', '191', '29', '104', '104', '276', '276', '22', '195', '195', '155', '139', '139', '59', '168', '135', '140', '79', '146', '200', '245', '70', '160', '153', '50', '202', '154', '263', '130', '194', '117', '194', '194', '191', '33', '29', '29', '238', '239', '194', '21', '274', '215', '18', '18', '140', '2', '215', '25', '46', '137', '238', '30', '160', '156', '46', '196', '263', '242', '104', '0', '0', '73', '213', '135', '22', '191', '191', '241', '202', '221', '15', '15', '191', '238', '213', '194', '197', '179', '238', '200', '200', '200', '5', '238', '194', '55', '117', '166', '49', '244', '239', '191', '78', '78', '78', '194', '200', '117', '210', '22', '121', '121', '121', '21', '237', '194', '121', '78', '104', '194', '79', '191', '251', '195', '197', '274', '176', '21', '43', '43', '10', '61', '215', '215', '30', '214', '194', '10', '30', '139', '96', '226', '226', '265', '2', '2', '121', '121', '41', '237', '104', '238', '238', '161', '78', '78', '78', '11', '184', '140', '140', '161', '104', '104', '237', '130', '195', '160', '118', '161', '18', '11', '241', '184', '196', '178', '22', '30', '146', '146', '10', '10', '141', '141', '135', '18', '18', '10', '26', '208', '97', '141', '18', '18', '97', '194', '194', '79', '213', '205', '205', '205', '75', '241', '50', '226', '238', '97', '222', '215', '215', '139', '238', '51', '238', '96', '173', '109', '200', '117', '192', '2', '195', '10', '3', '18', '49', '238', '238', '191', '195', '52', '197', '200', '200', '79', '191', '200', '200', '30', '195', '197', '195', '25', '260', '73', '30', '193', '21', '200', '242', '203', '120', '78', '260', '241', '0', '30', '202', '195', '30', '191', '78', '194', '194', '2', '205', '191', '196', '173', '197', '30', '196', '172', '139', '2', '18', '191', '230', '56', '194', '215', '195', '2', '71', '13', '146', '264', '50', '191', '235', '93', '191', '139', '139', '227', '235', '186', '235', '24', '18', '261', '128', '33', '71', '25', '176', '39', '10', '195', '202', '29', '29', '221', '29', '197', '139', '78', '227', '146', '137', '195', '205', '205', '195', '135', '62', '153', '146', '29', '75', '0', '194', '194', '180', '213', '78', '18', '194', '239', '196', '239', '197', '194', '153', '192', '221', '10', '46', '50', '241', '117', '117', '117', '195', '173', '78', '195', '194', '239', '2', '12', '173', '197', '30', '269', '197', '176', '135', '30', '146', '191', '93', '93', '135', '260', '218', '156', '155', '200', '241', '125', '18', '18', '183', '203', '239', '259', '173', '213', '180', '180', '210', '120', '104', '194', '156', '258', '172', '9', '0', '241', '192', '143', '189', '204', '50', '63', '197', '243', '241', '93', '125', '2', '21', '260', '4', '4', '3', '191', '150', '128', '78', '238', '245', '8', '80', '238', '79', '79', '240', '58', '104', '155', '80', '80', '188', '191', '235', '170', '170', '170', '146', '258', '195', '276', '25', '78', '215', '240', '161', '53', '147', '78', '78', '78', '78', '221', '221', '96', '18', '19', '130', '18', '18', '18', '80', '80', '194', '79', '93', '121', '78', '139', '139', '78', '197', '121', '194', '200', '80', '101', '22', '12', '135', '97', '205', '181', '60', '193', '15', '140', '172', '191', '241', '222', '135', '29', '25', '179', '191', '191', '259', '194', '133', '240', '121', '121', '241', '195', '29', '191', '209', '18', '139', '139', '50', '18', '12', '18', '133', '200', '25', '10', '70', '25', '242', '18', '161', '49', '172', '172', '172', '172', '128', '96', '195', '200', '78', '10', '239', '22', '173', '142', '142', '22', '194', '192', '3', '200', '241', '96', '241', '62', '139', '213', '202', '135', '79', '120', '192', '78', '118', '241', '46', '151', '150', '151', '150', '118', '221', '183', '237', '196', '140', '140', '39', '179', '213', '140', '41', '73', '78', '217', '258', '30', '118', '140', '219', '50', '10', '139', '150', '251', '24', '10', '18', '78', '18', '172', '135', '146', '71', '196', '202', '197', '25', '106', '160', '215', '15', '6', '101', '165', '140', '145', '80', '3', '238', '239', '78', '268', '0', '109', '191', '215', '238', '238', '238', '203', '50', '146', '15', '25', '156', '191', '16', '2', '195', '189', '96', '193', '109', '204', '161', '185', '210', '50', '235', '30', '242', '193', '102', '194', '121', '177', '241', '102', '132', '240', '71', '192', '79', '25', '197', '39', '161', '22', '258', '237', '15', '7', '78', '156', '197', '70', '176', '139', '240', '132', '78', '177', '191', '194', '117', '10', '79', '238', '195', '24', '101', '118', '196', '2', '102', '21', '180', '185', '170', '79', '139', '135', '191', '151', '200', '191', '154', '262', '200', '237', '56', '27', '191', '180', '258', '139', '139', '192', '30', '0', '242', '239', '242', '25', '191', '242', '240', '3', '56', '215', '171', '192', '193', '222', '238', '237', '215', '272', '2', '156', '237', '151', '189', '80', '15', '260', '260', '217', '235', '22', '228', '135', '18', '191', '157', '78', '2', '240', '213', '179', '2', '191', '79', '79', '153', '3', '194', '78', '121', '71', '121', '149', '257', '194', '191', '188', '15', '25', '125', '102', '15', '130', '130', '177', '10', '135', '265', '240', '149', '196', '200', '204', '204', '213', '177', '197', '194', '13', '202', '30', '213', '125', '193', '125', '125', '125', '125', '125', '15', '180', '78', '1', '25', '96', '200', '29', '156', '172', '202', '22', '78', '197', '188', '241', '101', '101', '101', '221', '50', '191', '15', '15', '0', '101', '101', '168', '56', '197', '130', '62', '204', '29', '10', '96', '97', '139', '97', '97', '2', '2', '262', '39', '176', '172', '194', '194', '18', '78', '13', '80', '139', '196', '207', '191', '151', '192', '150', '13', '104', '104', '101', '15', '125', '101', '15', '0', '50', '194', '264', '50', '25', '25', '125', '125', '125', '125', '125', '25', '29', '25', '25', '78', '240', '261', '19', '2', '74', '78', '238', '50', '50', '203', '197', '71', '104', '80', '194', '149', '170', '130', '2', '237', '170', '202', '48', '97', '93', '179', '235', '78', '102', '102', '70', '208', '238', '104', '146', '118', '118', '96', '237', '274', '218', '241', '102', '146', '209', '209', '240', '170', '137', '7', '197', '191', '264', '29', '46', '96', '193', '19', '12', '213', '215', '275', '179', '18', '183', '78', '221', '18', '170', '119', '21', '140', '240', '96', '192', '192', '21', '96', '149', '239', '135', '191', '240', '184', '183', '24', '2', '96', '197', '194', '215', '18', '192', '121', '202', '196', '25', '97', '214', '3', '97', '16', '0', '192', '135', '215', '5', '12', '80', '143', '202', '242', '151', '13', '209', '203', '200', '30', '209', '194', '102', '102', '102', '102', '102', '102', '22', '24', '6', '18', '5', '191', '101', '50', '101', '101', '192', '153', '153', '101', '101', '10', '192', '25', '140', '50', '241', '102', '102', '32', '150', '101', '102', '153', '102', '101', '153', '102', '25', '10', '125', '18', '125', '177', '209', '102', '10', '78', '78', '137', '18', '237', '241', '175', '195', '175', '96', '176', '237', '96', '96', '101', '193', '238', '170', '238', '102', '222', '268', '238', '102', '244', '34', '18', '244', '202', '149', '149', '193', '102', '170', '194', '75', '101', '222', '238', '50', '195', '135', '22', '93', '18', '102', '102', '93', '93', '191', '147', '244', '96', '2', '241', '241', '93', '189', '189', '177', '35', '137', '197', '191', '40', '40', '195', '176', '35', '200', '200', '237', '135', '140', '140', '93', '156', '200', '78', '191', '22', '81', '238', '101', '213', '101', '78', '109', '19', '140', '196', '78', '78', '76', '218', '14', '265', '101', '241', '155', '4', '50', '78', '78', '238', '135', '191', '265', '81', '238', '238', '102', '102', '102', '140', '162', '32', '17', '17', '171', '171', '191', '101', '102', '192', '101', '17', '171', '135', '102', '242', '195', '237', '21', '18', '238', '191', '278', '140', '149', '97', '260', '155', '276', '192', '196', '21', '191', '2', '97', '0', '150', '221', '213', '209', '238', '80', '260', '194', '104', '96', '193', '14', '240', '193', '117', '117', '155', '17', '264', '260', '209', '209', '2', '137', '71', '72', '101', '205', '173', '191', '118', '24', '41', '197', '78', '94', '237', '156', '193', '153', '154', '153', '209', '110', '78', '78', '213', '238', '78', '191', '93', '79', '177', '267', '133', '133', '46', '139', '40', '143', '74', '241', '202', '50', '3', '102', '78', '149', '191', '156', '18', '93', '195', '192', '137', '240', '135', '204', '189', '22', '56', '154', '25', '192', '130', '277', '191', '80', '39', '21', '21', '238', '238', '23', '192', '191', '192', '94', '94', '80', '139', '17', '10', '22', '227', '227', '140', '102', '21', '17', '227', '227', '23', '243', '264', '18', '274', '149', '238', '30', '221', '22', '238', '193', '135', '205', '195', '238', '238', '197', '130', '195', '18', '104', '102', '14', '59', '104', '18', '18', '258', '34', '241', '138', '188', '176', '102', '1', '140', '140', '22', '22', '278', '10', '96', '35', '145', '120', '33', '33', '22', '33', '22', '35', '22', '19', '33', '102', '192', '192', '175', '22', '72', '191', '222', '125', '22', '79', '18', '194', '18', '250', '192', '192', '96', '48', '274', '273', '132', '133', '192', '40', '171', '171', '246', '191', '10', '166', '197', '195', '125', '125', '200', '153', '192', '149', '110', '139', '194', '48', '102', '102', '104', '203', '238', '238', '238', '78', '18', '238', '121', '18', '238', '96', '238', '238', '13', '191', '18', '18', '240', '96', '40', '188', '18', '137', '175', '28', '28', '28', '149', '191', '191', '207', '239', '239', '153', '153', '153', '153', '153', '153', '154', '153', '154', '153', '153', '153', '18', '0', '27', '18', '25', '96', '96', '137', '33', '10', '130', '153', '153', '154', '180', '137', '44', '241', '153', '268', '25', '161', '25', '39', '242', '4', '238', '195', '102', '135', '13', '229', '140', '1', '79', '192', '229', '191', '118', '21', '260', '203', '18', '177', '191', '10', '238', '10', '133', '14', '175', '175', '192', '18', '153', '153', '254', '140', '149', '22', '22', '22', '237', '40', '135', '191', '192', '192', '62', '205', '19', '238', '176', '214', '200', '96', '15', '153', '191', '108', '29', '166', '28', '193', '197', '96', '195', '108', '102', '241', '195', '10', '33', '166', '197', '168', '2', '96', '26', '33', '204', '258', '149', '153', '153', '40', '137', '241', '166', '221', '23', '221', '153', '15', '113', '113', '78', '171', '40', '71', '21', '71', '80', '15', '191', '191', '135', '110', '10', '259', '102', '149', '149', '109', '109', '175', '97', '97', '97', '153', '15', '238', '193', '175', '38', '38', '137', '18', '235', '155', '137', '265', '110', '18', '18', '256', '191', '224', '173', '104', '10', '152', '24', '24', '24', '17', '40', '40', '40', '40', '40', '40', '112', '6', '18', '18', '210', '96', '191', '27', '213', '17', '153', '26', '133', '18', '18', '153', '38', '96', '40', '38', '40', '40', '10', '17', '17', '95', '71', '192', '17', '259', '213', '200', '15', '11', '207', '194', '18', '135', '238', '191', '241', '191', '116', '35', '147', '156', '67', '78', '17', '227', '191', '117', '188', '130', '72', '96', '235', '97', '18', '150', '203', '25', '126', '149', '200', '94', '4', '4', '1', '244', '228', '252', '149', '191', '149', '15', '17', '17', '29', '18', '18', '116', '23', '237', '94', '184', '268', '197', '175', '243', '28', '16', '10', '50', '191', '102', '102', '153', '149', '111', '78', '191', '135', '18', '35', '78', '238', '208', '93', '95', '258', '17', '111', '111', '262', '22', '22', '104', '161', '209', '21', '195', '12', '116', '241', '242', '239', '17', '97', '16', '40', '117', '94', '96', '78', '15', '103', '195', '135', '235', '247', '135', '130', '260', '239', '176', '93', '191', '209', '239', '93', '264', '30', '18', '30', '18', '18', '103', '191', '97', '17', '95', '115', '209', '140', '22', '22', '241', '10', '153', '3', '188', '238', '226', '195', '97', '96', '238', '97', '130', '97', '116', '96', '96', '242', '45', '149', '166', '155', '96', '116', '264', '10', '237', '192', '71', '22', '71', '175', '149', '116', '237', '274', '18', '235', '176', '116', '202', '250', '250', '78', '2', '196', '2', '97', '143', '18', '111', '111', '80', '78', '153', '80', '40', '110', '30', '112', '30', '111', '215', '112', '149', '78', '78', '209', '110', '110', '238', '109', '239', '112', '151', '154', '16', '16', '16', '78', '137', '179', '72', '149', '10', '111', '268', '96', '170', '111', '38', '192', '153', '183', '195', '30', '30', '147', '195', '147', '93', '264', '153', '25', '18', '19', '22', '74', '40', '93', '166', '15', '127', '147', '192', '260', '238', '103', '269', '127', '19', '19', '153', '18', '78', '238', '30', '30', '241', '16', '235', '250', '204', '238', '218', '40', '93', '184', '18', '0', '241', '195', '94', '154', '109', '156', '10', '21', '30', '197', '39', '78', '24', '107', '142', '142', '228', '229', '229', '229', '232', '179', '239', '142', '232', '23', '175', '147', '137', '137', '137', '171', '191', '117', '128', '127', '204', '98', '147', '235', '239', '153', '15', '204', '255', '80', '204', '204', '149', '113', '19', '167', '22', '250', '17', '17', '17', '18', '17', '67', '121', '125', '26', '97', '97', '97', '97', '247', '124', '163', '147', '112', '111', '208', '108', '38', '209', '154', '154', '194', '135', '22', '147', '241', '120', '218', '260', '156', '155', '241', '192', '215', '180', '179', '149', '40', '193', '192', '150', '102', '93', '1', '149', '137', '133', '102', '102', '102', '78', '145', '21', '96', '175', '175', '78', '123', '30', '95', '19', '18', '110', '18', '152', '152', '147', '26', '135', '260', '158', '203', '195', '158', '112', '156', '125', '213', '230', '30', '15', '125', '106', '111', '195', '181', '130', '195', '130', '239', '15', '194', '16', '15', '111', '61', '72', '57', '128', '239', '239', '203', '25', '218', '10', '155', '191', '46', '98', '175', '260', '102', '21', '191', '260', '191', '239', '15', '30', '93', '147', '103', '78', '177', '30', '95', '155', '155', '155', '103', '103', '112', '241', '191', '191', '17', '78', '153', '150', '113', '115', '147', '78', '40', '163', '33', '15', '149', '103', '192', '191', '96', '106', '115', '143', '235', '200', '204', '238', '204', '204', '74', '196', '113', '246', '102', '150', '16', '176', '20', '195', '128', '197', '15', '71', '71', '193', '24', '24', '24', '170', '204', '209', '111', '144', '112', '22', '222', '224', '28', '209', '155', '213', '275', '196', '239', '195', '25', '103', '149', '128', '19', '96', '102', '135', '17', '232', '155', '191', '20', '18', '241', '30', '25', '205', '97', '97', '96', '72', '182', '135', '135', '20', '149', '149', '78', '135', '40', '18', '153', '10', '26', '158', '205', '157', '157', '135', '135', '140', '155', '192', '26', '268', '263', '245', '155', '15', '177', '159', '212', '161', '239', '200', '40', '35', '153', '197', '259', '15', '254', '175', '10', '30', '239', '67', '195', '200', '191', '22', '66', '191', '112', '133', '128', '235', '161', '155', '17', '93', '36', '113', '239', '18', '18', '191', '165', '163', '163', '238', '79', '158', '40', '102', '40', '200', '191', '60', '241', '245', '96', '78', '30', '15', '173', '192', '192', '158', '64', '113', '200', '74', '128', '135', '115', '115', '10', '74', '102', '102', '115', '111', '238', '244', '109', '109', '238', '152', '10', '237', '18', '50', '16', '173', '204', '15', '208', '276', '43', '109', '258', '222', '15', '94', '235', '203', '79', '128', '113', '3', '204', '106', '133', '214', '218', '25', '98', '166', '71', '30', '175', '175', '97', '135', '108', '19', '18', '196', '35', '12', '196', '78', '166', '155', '72', '155', '22', '109', '258', '196', '263', '161', '204', '210', '14', '208', '208', '208', '208', '204', '102', '102', '213', '155', '251', '153', '153', '153', '154', '154', '237', '97', '19', '28', '94', '136', '112', '19', '97', '152', '153', '153', '111', '111', '135', '135', '105', '17', '105', '152', '152', '152', '153', '79', '266', '135', '120', '125', '149', '50', '162', '72', '17', '147', '109', '115', '24', '105', '244', '149', '238', '203', '35', '202', '177', '173', '239', '19', '19', '179', '237', '14', '18', '185', '58', '158', '144', '213', '237', '155', '250', '239', '93', '237', '97', '195', '157', '107', '109', '246', '165', '98', '49', '2', '266', '149', '20', '18', '71', '108', '22', '115', '239', '40', '178', '232', '238', '218', '118', '118', '210', '235', '46', '228', '192', '112', '149', '96', '111', '21', '95', '95', '95', '18', '57', '206', '18', '239', '157', '80', '196', '112', '176', '99', '106', '71', '235', '135', '18', '209', '78', '20', '20', '103', '33', '3', '103', '166', '79', '79', '156', '229', '18', '238', '2', '16', '128', '200', '117', '22', '197', '166', '78', '237', '241', '107', '53', '189', '204', '243', '163', '221', '15', '200', '221', '152', '23', '193', '107', '195', '99', '97', '19', '125', '276', '113', '193', '64', '78', '197', '117', '175', '135', '135', '78', '93', '154', '153', '256', '256', '135', '137', '135', '137', '149', '110', '15', '159', '21', '158', '192', '192', '117', '29', '97', '258', '78', '46', '5', '80', '15', '105', '260', '161', '191', '2', '138', '117', '215', '176', '156', '194', '265', '115', '181', '21', '14', '106', '106', '108', '200', '156', '103', '102', '29', '17', '204', '175', '227', '118', '18', '239', '204', '258', '78', '155', '213', '109', '192', '186', '186', '15', '241', '133', '30', '30', '239', '30', '218', '109', '138', '241', '241', '45', '135', '112', '232', '104', '106', '200', '53', '23', '112', '17', '117', '59', '260', '195', '106', '117', '159', '204', '200', '228', '165', '165', '261', '200', '227', '166', '93', '21', '80', '112', '152', '196', '203', '228', '98', '102', '15', '139', '110', '13', '238', '6', '171', '94', '208', '155', '112', '203', '239', '191', '39', '136', '203', '195', '46', '203', '148', '192', '210', '23', '4', '153', '29', '165', '192', '204', '232', '94', '112', '239', '98', '71', '248', '17', '155', '10', '78', '191', '191', '111', '25', '25', '19', '19', '17', '235', '102', '265', '195', '235', '163', '238', '71', '197', '107', '155', '30', '72', '19', '259', '80', '155', '23', '64', '239', '197', '156', '57', '203', '258', '200', '205', '246', '238', '258', '227', '238', '238', '19', '148', '161', '111', '105', '202', '109', '262', '133', '137', '221', '135', '52', '170', '111', '36', '30', '156', '102', '268', '115', '22', '153', '133', '259', '98', '235', '135', '191', '25', '235', '25', '29', '94', '195', '203', '212', '112', '135', '245', '260', '200', '96', '158', '23', '165', '22', '18', '249', '240', '109', '43', '37', '15', '241', '241', '23', '135', '240', '108', '98', '268', '275', '191', '136', '263', '259', '196', '98', '98', '204', '248', '123', '105', '204', '240', '9', '9', '98', '218', '189', '238', '246', '155', '130', '157', '74', '109', '29', '189', '203', '213', '195', '241', '159', '238', '204', '218', '105', '23', '16', '263', '206', '150', '152', '204', '202', '173', '267', '18', '153', '155', '196', '2', '97', '158', '158', '204', '98', '202', '98', '204', '258', '98', '22', '113', '113', '135', '71', '237', '155', '163', '250', '233', '26', '26', '26', '26', '26', '19', '170', '18', '15', '19', '191', '30', '30', '48', '35', '30', '237', '46', '119', '16', '78', '239', '150', '245', '235', '158', '27', '241', '30', '38', '107', '75', '241', '135', '106', '106', '98', '159', '96', '135', '111', '176', '175', '200', '18', '14', '242', '163', '187', '20', '12', '191', '23', '106', '171', '25', '78', '162', '191', '157', '94', '237', '246', '203', '108', '238', '204', '204', '1', '102', '154', '106', '170', '102', '200', '2', '135', '106', '258', '258', '227', '26', '15', '25', '71', '203', '96', '3', '241', '200', '30', '108', '98', '108', '27', '106', '26', '150', '259', '78', '113', '113', '158', '202', '16', '136', '239', '156', '239', '35', '276', '252', '115', '22', '22', '258', '172', '3', '154', '3', '98', '237', '102', '235', '112', '206', '98', '57', '118', '153', '18', '26', '30', '196', '163', '191', '153', '72', '24', '156', '117', '122', '30', '135', '192', '98', '28', '30', '30', '95', '241', '155', '17', '195', '31', '103', '202', '98', '237', '94', '112', '163', '163', '94', '163', '112', '19', '195', '191', '15', '135', '78', '109', '160', '152', '97', '98', '107', '167', '97', '138', '138', '26', '191', '97', '97', '30', '30', '206', '202', '13', '30', '240', '111', '261', '79', '0', '111', '19', '152', '107', '107', '18', '98', '28', '258', '244', '106', '237', '204', '191', '15', '183', '202', '16', '111', '166', '268', '202', '12', '30', '78', '110', '133', '71', '197', '111', '52', '79', '171', '156', '94', '13', '118', '111', '71', '133', '18', '105', '30', '22', '133', '60', '237', '191', '6', '126', '3', '238', '276', '78', '162', '150', '115', '191', '107', '200', '148', '135', '72', '155', '241', '4', '108', '160', '203', '20', '133', '2', '80', '80', '154', '18', '15', '115', '239', '167', '167', '35', '268', '18', '221', '18', '195', '204', '195', '216', '133', '112', '104', '223', '191', '115', '30', '30', '30', '30', '30', '202', '204', '165', '200', '200', '29', '191', '237', '267', '18', '175', '81', '264', '264', '178', '115', '102', '178', '212', '273', '202', '109', '259', '166', '135', '155', '219', '161', '18', '94', '241', '191', '196', '191', '98', '26', '26', '204', '204', '135', '27', '238', '260', '182', '26', '98', '94', '93', '238', '156', '19', '111', '79', '16', '192', '113', '19', '241', '18', '202', '260', '272', '106', '163', '156', '106', '17', '109', '221', '78', '203', '7', '136', '197', '193', '191', '98', '112', '200', '95', '221', '98', '161', '191', '3', '106', '189', '111', '25', '196', '196', '241', '163', '200', '24', '78', '176', '3', '98', '102', '98', '19', '161', '224', '80', '18', '191', '112', '105', '254', '71', '200', '137', '29', '20', '165', '8', '106', '23', '106', '155', '200', '265', '135', '126', '203', '188', '161', '158', '258', '255', '98', '203', '50', '35', '102', '158', '175', '40', '239', '26', '66', '57', '193', '155', '15', '135', '10', '107', '106', '229', '18', '191', '204', '107', '99', '235', '238', '18', '205', '205', '35', '238', '208', '203', '110', '110', '109', '112', '23', '18', '19', '182', '156', '18', '18', '156', '111', '135', '137', '133', '137', '240', '99', '197', '113', '15', '165', '39', '97', '97', '15', '150', '163', '109', '268', '15', '15', '238', '203', '67', '161', '197', '18', '196', '97', '161', '197', '161', '185', '238', '191', '30', '121', '133', '160', '160', '74', '135', '75', '54', '135', '196', '23', '241', '166', '194', '239', '135', '240', '208', '238', '98', '135', '152', '176', '22', '174', '113', '241', '237', '57', '195', '191', '24', '165', '165', '204', '204', '56', '109', '27', '153', '252', '228', '165', '252', '200', '28', '158', '135', '196', '22', '18', '133', '30', '204', '237', '196', '93', '106', '94', '25', '93', '159', '4', '195', '23', '237', '25', '199', '57', '26', '276', '274', '204', '75', '18', '204', '108', '22', '133', '191', '111', '204', '203', '133', '237', '166', '4', '62', '202', '166', '161', '165', '74', '109', '118', '200', '22', '206', '25', '209', '17', '155', '204', '64', '23', '30', '202', '149', '171', '135', '136', '184', '26', '18', '159', '195', '18', '166', '156', '98', '133', '38', '40', '18', '18', '166', '109', '109', '209', '22', '195', '163', '163', '108', '18', '15', '99', '135', '174', '26', '19', '166', '133', '259', '151', '235', '204', '228', '133', '30', '161', '111', '242', '84', '238', '98', '195', '78', '160', '28', '25', '264', '165', '41', '19', '55', '46', '202', '121', '15', '32', '98', '238', '0', '78', '31', '30', '203', '264', '203', '135', '104', '111', '78', '212', '111', '98', '126', '22', '2', '30', '237', '212', '265', '163', '106', '106', '235', '109', '223', '161', '202', '150', '170', '255', '112', '114', '170', '98', '96', '197', '36', '18', '228', '14', '255', '204', '102', '239', '0', '185', '30', '22', '23', '246', '199', '133', '191', '78', '195', '153', '112', '191', '200', '170', '171', '200', '78', '128', '25', '153', '237', '99', '94', '208', '107', '208', '208', '78', '204', '204', '27', '191', '272', '134', '158', '158', '174', '118', '64', '18', '258', '150', '189', '18', '153', '241', '204', '112', '106', '135', '71', '148', '7', '133', '140', '187', '23', '223', '19', '19', '2', '136', '2', '200', '161', '15', '98', '158', '148', '3', '250', '153', '235', '165', '220', '176', '110', '240', '151', '148', '133', '74', '193', '46', '27', '173', '94', '176', '105', '256', '161', '226', '18', '78', '191', '171', '243', '26', '0', '108', '167', '159', '239', '150', '112', '155', '79', '241', '202', '165', '254', '107', '202', '156', '166', '193', '239', '135', '15', '203', '187', '148', '25', '15', '22', '194', '239', '56', '191', '176', '262', '200', '110', '72', '99', '102', '135', '98', '113', '15', '15', '159', '262', '171', '151', '98', '204', '213', '255', '26', '243', '175', '156', '98', '107', '191', '165', '241', '26', '165', '171', '30', '160', '166', '266', '11', '19', '211', '93', '254', '30', '239', '115', '273', '205', '112', '237', '263', '74', '98', '79', '122', '94', '265', '155', '191', '16', '235', '229', '98', '191', '227', '17', '111', '267', '135', '202', '195', '10', '180', '196', '196', '133', '135', '133', '95', '111', '122', '258', '191', '3', '114', '107', '19', '264', '74', '56', '200', '200', '133', '161', '97', '15', '97', '197', '196', '253', '93', '202', '258', '106', '153', '26', '109', '40', '149', '8', '196', '15', '119', '155', '23', '237', '193', '264', '148', '151', '238', '200', '94', '75', '148', '175', '24', '195', '171', '30', '203', '200', '202', '16', '158', '223', '191', '22', '264', '135', '35', '57', '94', '191', '196', '202', '98', '18', '196', '3', '133', '78', '191', '208', '258', '18', '200', '29', '135', '109', '110', '2', '185', '98', '133', '19', '22', '204', '95', '238', '25', '133', '250', '135', '135', '3', '47', '158', '187', '2', '78', '115', '155', '3', '218', '166', '112', '266', '22', '195', '135', '72', '97', '197', '209', '127', '80', '2', '98', '14', '239', '15', '113', '165', '204', '78', '204', '191', '175', '200', '193', '18', '196', '133', '109', '237', '56', '196', '3', '163', '163', '163', '195', '15', '263', '18', '84', '18', '133', '107', '18', '102', '156', '238', '102', '26', '244', '82', '75', '23', '258', '238', '136', '3', '204', '184', '200', '196', '158', '106', '27', '238', '155', '152', '50', '50', '2', '235', '98', '203', '166', '109', '25', '18', '151', '133', '200', '26', '26', '26', '19', '107', '156', '23', '26', '3', '70', '225', '41', '26', '239', '26', '186', '26', '34', '272', '272', '104', '78', '260', '18', '204', '26', '78', '18', '210', '135', '203', '71', '207', '26', '133', '13', '25', '26', '115', '94', '264', '135', '244', '79', '30', '263', '22', '109', '205', '176', '106', '22', '17', '151', '106', '191', '160', '160', '165', '78', '177', '196', '260', '166', '35', '191', '260', '98', '18', '237', '18', '161', '18', '197', '133', '30', '35', '200', '170', '155', '245', '19', '200', '160', '96', '172', '165', '55', '137', '241', '133', '191', '112', '25', '126', '238', '18', '18', '108', '135', '161', '135', '73', '112', '107', '8', '112', '109', '195', '25', '30', '105', '161', '241', '110', '75', '238', '215', '238', '104', '25', '3', '155', '30', '16', '3', '175', '99', '3', '200', '23', '3', '199', '203', '191', '196', '196', '204', '98', '105', '133', '197', '135', '161', '39', '15', '30', '133', '55', '191', '203', '133', '203', '140', '200', '149', '30', '96', '135', '23', '26', '3', '18', '155', '158', '135', '135', '18', '250', '18', '22', '114', '18', '18', '166', '18', '30', '18', '239', '18', '161', '50', '50', '49', '51', '23', '55', '98', '78', '210', '211', '18', '239', '78', '13', '193', '114', '78', '18', '25', '133', '19', '176', '4', '173', '50', '108', '17', '4', '204', '94', '112', '207', '109', '115', '23', '235', '197', '194', '197', '213', '148', '40', '157', '107', '200', '147', '14', '14', '18', '238', '18', '238', '18', '238', '57', '200', '135', '30', '18', '191', '191', '200', '166', '151', '161', '73', '19', '73', '239', '239', '165', '108', '2', '93', '166', '133', '133', '173', '35', '193', '114', '74', '19', '40', '58', '104', '16', '260', '160', '158', '112', '210', '158', '167', '203', '203', '3', '3', '109', '30', '133', '258', '70', '204', '200', '152', '155', '192', '204', '108', '191', '204', '26', '98', '26', '98', '264', '158', '18', '150', '137', '153', '196', '199', '191', '115', '168', '157', '149', '151', '78', '137', '160', '259', '166', '196', '171', '175', '199', '166', '161', '70', '13', '166', '152', '152', '206', '58', '18', '207', '58', '74', '207', '29', '46', '109', '135', '74', '174', '109', '40', '10', '30', '258', '16', '111', '30', '15', '239', '204', '48', '25', '196', '0', '156', '72', '72', '96', '22', '72', '133', '133', '238', '26', '3', '50', '152', '193', '235', '136', '153', '50', '1', '19', '239', '30', '25', '179', '161', '30', '191', '3', '27', '133', '3', '40', '23', '133', '150', '151', '265', '32', '191', '3', '25', '170', '238', '40', '259', '166', '137', '241', '133', '209', '159', '78', '78', '153', '26', '52', '241', '36', '250', '96', '30', '30', '38', '18', '18', '238', '99', '72', '111', '3', '244', '191', '226', '3', '191', '114', '135', '109', '40', '79', '202', '78', '114', '238', '10', '150', '108', '163', '108', '113', '197', '109', '115', '161', '3', '238', '3', '237', '161', '107', '155', '78', '202', '112', '38', '195', '137', '250', '133', '237', '161', '25', '26', '25', '15', '15', '239', '166', '114', '229', '111', '40', '135', '94', '102', '162', '204', '237', '17', '166', '136', '22', '98', '99', '99', '166', '35', '196', '3', '203', '135', '135', '84', '98', '266', '238', '13', '166', '78', '193', '98', '241', '104', '153', '192', '200', '10', '95', '95', '148', '107', '14', '17', '191', '239', '19', '203', '255', '197', '213', '94', '3', '107', '23', '35', '10', '237', '235', '78', '29', '18', '160', '200', '150', '99', '109', '155', '161', '133', '114', '18', '18', '18', '264', '133', '104', '161', '70', '17', '204', '195', '204', '241', '99', '82', '241', '241', '161', '18', '235', '175', '2', '2', '202', '50', '230', '26', '40', '40', '200', '78', '156', '26', '112', '26', '30', '71', '184', '200', '135', '153', '23', '106', '125', '175', '133', '133', '7', '38', '196', '161', '0', '49', '241', '158', '15', '21', '104', '2', '3', '176', '259', '135', '78', '75', '112', '200', '1', '207', '238', '161', '150', '158', '2', '45', '31', '237', '149', '3', '26', '165', '135', '152', '115', '133', '18', '40', '166', '30', '173', '40', '26', '78', '50', '48', '50', '13', '45', '200', '200', '18', '151', '205', '135', '38', '117', '196', '135', '155', '111', '196', '196', '166', '23', '25', '161', '18', '124', '259', '165', '58', '170', '204', '250', '103', '160', '238', '194', '163', '241', '255', '235', '17', '133', '26', '135', '196', '237', '114', '258', '193', '248', '40', '195', '84', '238', '146', '199', '40', '161', '22', '137', '43', '166', '156', '4', '104', '133', '177', '211', '193', '78', '203', '246', '237', '264', '3', '149', '78', '78', '111', '239', '13', '238', '176', '112', '150', '156', '3', '6', '118', '178', '178', '21', '186', '150', '241', '50', '243', '191', '121', '278', '29', '200', '18', '3', '197', '191', '98', '173', '195', '25', '195', '264', '221', '15', '156', '25', '200', '211', '195', '26', '35', '2', '135', '126', '113', '167', '191', '264', '196', '133', '18', '26', '146', '203', '219', '23', '160', '160', '135', '259', '3', '159', '120', '160', '106', '240', '133', '115', '258', '38', '22', '3', '26', '22', '107', '49', '48', '25', '237', '72', '219', '196', '196', '111', '135', '196', '18', '166', '172', '135', '26', '258', '70', '98', '40', '166', '23', '96', '3', '30', '155', '126', '146', '114', '242', '237', '157', '238', '241', '40', '133', '200', '161', '175', '200', '175', '84', '266', '18', '99', '30', '242', '151', '153', '166', '199', '228', '3', '237', '132', '110', '135', '80', '15', '15', '15', '84', '133', '15', '177', '3', '202', '255', '71', '115', '235', '19', '18', '18', '102', '58', '78', '165', '166', '264', '12', '150', '15', '2', '18', '135', '109', '200', '258', '128', '3', '199', '197', '239', '109', '222', '239', '107', '125', '132', '204', '204', '133', '135', '30', '158', '203', '15', '264', '237', '133', '196', '3', '252', '26', '98', '79', '199', '210', '0', '99', '30', '26', '259', '6', '49', '166', '98', '108', '9', '133', '102', '155', '98', '114', '78', '35', '166', '188', '202', '235', '30', '149', '165', '176', '138', '29', '153', '25', '25', '137', '133', '166', '135', '125', '71', '193', '154', '165', '98', '23', '56', '198', '26', '26', '0', '109', '136', '163', '151', '25', '15', '241', '195', '204', '191', '195', '161', '175', '133', '237', '78', '165', '258', '135', '78', '135', '258', '18', '235', '30', '187', '150', '275', '149', '2', '36', '112', '22', '161', '238', '203', '112', '265', '75', '133', '205', '258', '158', '18', '26', '18', '25', '26', '15', '26', '239', '15', '240', '18', '71', '71', '78', '135', '3', '148', '166', '26', '176', '2', '197', '150', '176', '19', '18', '97', '166', '18', '153', '250', '133', '210', '20', '192', '196', '26', '191', '2', '81', '70', '5', '79', '3', '30', '193', '193', '69', '160', '176', '246', '108', '54', '250', '135', '200', '80', '3', '167', '18', '166', '200', '152', '23', '149', '19', '14', '13', '237', '196', '19', '112', '254', '204', '191', '23', '264', '199', '209', '137', '31', '135', '238', '78', '18', '191', '15', '164', '195', '74', '163', '163', '41', '99', '26', '135', '26', '40', '40', '250', '200', '200', '26', '237', '4', '195', '158', '35', '71', '191', '3', '218', '274', '58', '3', '23', '177', '132', '49', '62', '238', '166', '97', '264', '193', '29', '26', '49', '241', '197', '200', '64', '227', '258', '44', '15', '133', '175', '238', '109', '254', '237', '198', '175', '166', '213', '174', '196', '107', '259', '98', '10', '196', '210', '191', '96', '166', '135', '15', '197', '78', '78', '195', '176', '18', '98', '129', '170', '18', '247', '203', '26', '158', '40', '64', '200', '163', '196', '19', '205', '197', '193', '191', '69', '96', '74', '35', '36', '64', '133', '172', '19', '266', '96', '94', '103', '258', '193', '264', '98', '254', '50', '193', '25', '170', '75', '84', '27', '252', '203', '94', '94', '19', '98', '137', '12', '153', '3', '155', '135', '238', '15', '196', '15', '26', '84', '49', '27', '135', '203', '48', '133', '129', '133', '258', '155', '35', '195', '71', '241', '135', '166', '193', '165', '104', '196', '117', '196', '26', '237', '109', '95', '196', '60', '40', '158', '193', '250', '104', '94', '178', '27', '193', '78', '106', '79', '204', '133', '237', '235', '135', '202', '70', '15', '264', '241', '254', '110', '199', '49', '255', '30', '193', '238', '204', '102', '97', '161', '43', '228', '1', '6', '196', '193', '26', '15', '128', '46', '155', '32', '133', '69', '47', '135', '78', '205', '200', '200', '202', '3', '95', '191', '78', '26', '248', '258', '191', '240', '49', '110', '38', '193', '200', '200', '96', '200', '204', '135', '9', '200', '258', '15', '46', '15', '149', '155', '197', '2', '166', '151', '72', '135', '97', '211', '238', '135', '163', '161', '26', '163', '129', '241', '239', '241', '16', '26', '18', '263', '260', '166', '3', '264', '195', '79', '132', '251', '50', '133', '15', '25', '15', '191', '15', '78', '50', '78', '71', '227', '69', '200', '69', '109', '64', '133', '197', '196', '3', '45', '238', '26', '132', '104', '196', '193', '136', '80', '26', '75', '117', '14', '78', '75', '98', '15', '2', '15', '50', '255', '136', '3', '112', '173', '69', '191', '19', '239', '235', '109', '70', '135', '78', '165', '238', '262', '199', '35', '203', '224', '193', '15', '18', '150', '238', '197', '238', '193', '135', '3', '150', '187', '199', '239', '109', '152', '27', '257', '6', '235', '104', '191', '133', '166', '155', '26', '16', '136', '198', '98', '154', '235', '27', '102', '30', '191', '15', '132', '132', '114', '107', '241', '241', '156', '199', '241', '49', '62', '117', '258', '30', '163', '133', '238', '200', '3', '15', '109', '2', '196', '165', '166', '159', '210', '35', '15', '17', '200', '241', '106', '203', '197', '197', '79', '98', '98', '23', '204', '48', '129', '57', '258', '202', '215', '238', '70', '64', '193', '189', '57', '200', '0', '98', '135', '204', '25', '30', '204', '27', '26', '78', '135', '109', '3', '197', '3', '167', '35', '196', '225', '2', '194', '200', '15', '161', '198', '23', '133', '238', '195', '35', '50', '135', '152', '163', '21', '15', '94', '26', '94', '53', '260', '196', '154', '277', '200', '118', '6', '238', '50', '209', '50', '3', '238', '3', '15', '107', '135', '199', '15', '135', '26', '258', '251', '203', '19', '259', '195', '199', '238', '15', '254', '112', '211', '200', '129', '84', '196', '3', '208', '259', '135', '104', '29', '136', '31', '78', '13', '3', '64', '75', '55', '114', '160', '96', '14', '18', '207', '75', '48', '15', '49', '97', '117', '196', '99', '258', '195', '264', '264', '78', '197', '30', '238', '109', '158', '32', '195', '149', '227', '65', '167', '162', '204', '250', '151', '151', '102', '26', '29', '26', '193', '74', '50', '193', '186', '15', '74', '259', '57', '188', '277', '277', '195', '195', '212', '78', '197', '64', '129', '259', '200', '199', '129', '4', '235', '197', '196', '161', '3', '255', '199', '102', '3', '13', '94', '53', '98', '15', '129', '15', '155', '71', '179', '50', '29', '136', '80', '254', '198', '162', '237', '3', '18', '260', '238', '117', '179', '156', '19', '45', '197', '78', '199', '102', '102', '237', '155', '69', '69', '163', '239', '15', '196', '80', '191', '30', '71', '158', '259', '2', '106', '148', '15', '135', '135', '135', '196', '19', '123', '15', '104', '19', '128', '230', '69', '98', '197', '241', '161', '195', '200', '3', '98', '78', '165', '163', '19', '196', '31', '173', '203', '55', '69', '209', '239', '79', '158', '254', '135', '51', '94', '162', '255', '95', '18', '133', '109', '50', '94', '199', '15', '197', '200', '133', '116', '50', '196', '161', '69', '133', '3', '258', '3', '197', '29', '3', '157', '197', '193', '4', '94', '195', '17', '48', '152', '8', '15', '155', '133', '195', '136', '150', '102', '237', '203', '164', '164', '250', '154', '13', '191', '58', '94', '70', '93', '264', '15', '30', '133', '237', '267', '199', '193', '76', '241', '157', '26', '200', '109', '179', '55', '98', '200', '129', '238', '235', '208', '109', '239', '258', '196', '193', '18', '107', '210', '70', '40', '129', '259', '193', '30', '158', '252', '40', '210', '78', '15', '78', '96', '208', '113', '258', '64', '80', '15', '133', '241', '15', '15', '258', '135', '109', '251', '251', '109', '74', '196', '170', '240', '254', '258', '204', '199', '26', '109', '197', '57', '45', '161', '57', '114', '3', '15', '264', '106', '57', '27', '18', '258', '29', '84', '161', '135', '114', '24', '30', '107', '107', '172', '209', '170', '18', '19', '133', '191', '195', '135', '156', '196', '94', '3', '84', '225', '199', '238', '106', '104', '70', '50', '12', '155', '114', '176', '237', '200', '108', '70', '240', '196', '19', '22', '276', '199', '238', '205', '104', '193', '108', '115', '259', '102', '19', '258', '18', '106', '161', '191', '114', '64', '15', '204', '35', '155', '40', '238', '136', '264', '133', '186', '15', '258', '166', '26', '6', '114', '114', '133', '114', '114', '13', '13', '13', '237', '250', '245', '136', '78', '13', '48', '18', '13', '25', '26', '93', '125', '50', '94', '148', '78', '194', '200', '25', '93', '274', '266', '70', '22', '3', '208', '240', '211', '135', '31', '18', '153', '18', '2', '15', '15', '15', '156', '15', '70', '73', '173', '13', '112', '69', '204', '70', '248', '135', '70', '3', '193', '193', '22', '162', '238', '57', '98', '15', '199', '199', '52', '151', '73', '5', '108', '23', '69', '95', '220', '106', '163', '152', '163', '108', '15', '58', '250', '19', '177', '245', '264', '93', '204', '25', '78', '155', '69', '155', '239', '241', '155', '200', '18', '3', '13', '196', '197', '204', '272', '95', '135', '165', '22', '210', '35', '69', '18', '170', '3', '25', '156', '78', '135', '102', '107', '255', '19', '70', '70', '109', '198', '156', '64', '170', '138', '78', '203', '166', '108', '114', '179', '228', '74', '49', '14', '264', '26', '195', '78', '107', '75', '18', '106', '151', '70', '58', '15', '196', '238', '109', '104', '104', '109', '136', '148', '210', '3', '161', '15', '104', '238', '208', '133', '108', '241', '18', '209', '74', '156', '114', '199', '174', '35', '18', '170', '135', '238', '244', '155', '162', '197', '3', '98', '227', '267', '238', '193', '50', '238', '98', '112', '124', '25', '114', '241', '159', '154', '78', '161', '136', '114', '153', '225', '154', '13', '50', '11', '25', '15', '209', '11', '166', '203', '259', '237', '22', '114', '276', '98', '278', '150', '198', '18', '15', '78', '15', '162', '26', '3', '196', '3', '102', '189', '71', '69', '16', '18', '152', '133', '135', '162', '241', '15', '104', '15', '259', '202', '2', '238', '207', '15', '204', '30', '215', '112', '210', '42', '96', '193', '15', '110', '163', '211', '78', '108', '15', '31', '27', '15', '31', '196', '195', '96', '15', '161', '94', '193', '195', '198', '98', '261', '258', '76', '15', '104', '258', '135', '18', '15', '202', '258', '108', '156', '13', '22', '251', '199', '155', '258', '211', '3', '207', '202', '259', '107', '202', '259', '135', '135', '148', '158', '154', '99', '99', '3', '207', '94', '25', '162', '31', '241', '25', '15', '114', '18', '114', '148', '19', '15', '203', '191', '19', '13', '193', '204', '25', '166', '155', '196', '98', '162', '197', '29', '152', '191', '133', '199', '230', '40', '25', '204', '79', '3', '19', '165', '204', '108', '208', '3', '251', '274', '15', '166', '264', '35', '13', '193', '114', '258', '13', '19', '133', '23', '17', '199', '204', '153', '250', '204', '16', '12', '107', '191', '18', '3', '15', '165', '15', '69', '17', '165', '17', '15', '15', '156', '23', '23', '15', '197', '104', '197', '26', '208', '135', '211', '250', '18', '204', '25', '196', '53', '69', '104', '104', '19', '78', '241', '230', '30', '195', '96', '26', '109', '196', '69', '238', '15', '18', '133', '26', '106', '109', '98', '15', '258', '274', '193', '203', '58', '57', '274', '2', '202', '161', '136', '135', '155', '241', '35', '151', '137', '5', '114', '96', '16', '209', '158', '15', '266', '161', '15', '25', '274', '202', '3', '161', '237', '267', '204', '125', '15', '151', '155', '259', '151', '210', '238', '15', '26', '132', '50', '173', '197', '114', '241', '15', '258', '109', '166', '274', '13', '191', '15', '26', '164', '258', '163', '15', '264', '11', '260', '148', '150', '207', '196', '98', '272', '133', '196', '107', '26', '191', '163', '191', '135', '3', '18', '18', '119', '69', '196', '15', '156', '135', '112', '258', '155', '256', '199', '204', '212', '18', '135', '254', '72', '258', '254', '15', '133', '18', '259', '193', '258', '204', '40', '18', '29', '155', '104', '107', '274', '241', '25', '26', '69', '148', '166', '46', '94', '135', '15', '155', '104', '241', '32', '274', '249', '15', '197', '104', '44', '259', '69', '191', '15', '198', '193', '133', '17', '258', '248', '166', '197', '200', '250', '204', '3', '225', '238', '163', '69', '15', '15', '26', '202', '19', '209', '78', '212', '15', '78', '243', '110', '191', '15', '18', '107', '38', '208', '166', '239', '237', '199', '15', '204', '258', '173', '172', '200', '274', '243', '19', '191', '135', '203', '69', '149', '148', '196', '256', '204', '97', '191', '26', '114', '274', '200', '69', '69', '64', '69', '239', '25', '274', '203', '78', '135', '196', '202', '17', '2', '2', '3', '98', '98', '62', '2', '95', '96', '2', '207', '78', '200', '26', '64', '18', '93', '49', '57', '154', '200', '69', '94', '98', '98', '56', '56', '166', '98', '212', '226', '226', '15', '69', '15', '273', '272', '104', '193', '3', '98', '199', '98', '25', '125', '259', '249', '108', '40', '26', '198', '218', '73', '148', '69', '204', '241', '35', '3', '274', '70', '18', '71', '35', '172', '133', '106', '69', '107', '199', '219', '19', '133', '69', '135', '109', '57', '191', '135', '252', '104', '3', '94', '3', '72', '126', '16', '36', '84', '241', '3', '218', '98', '108', '46', '19', '196', '118', '104', '29', '238', '69', '10', '204', '264', '259', '124', '242', '107', '133', '25', '64', '30', '194', '259', '159', '264', '274', '52', '200', '104', '29', '30', '193', '98', '94', '114', '204', '40', '18', '198', '97', '3', '99', '274', '239', '177', '15', '18', '126', '126', '208', '148', '15', '104', '29', '36', '218', '70', '15', '23', '278', '23', '36', '191', '210', '51', '245', '167', '72', '237', '25', '203', '36', '196', '193', '196', '15', '151', '207', '203', '103', '166', '135', '110', '27', '13', '27', '191', '98', '57', '166', '50', '21', '15', '197', '23', '2', '70', '69', '193', '274', '104', '166', '258', '126', '238', '161', '18', '193', '69', '140', '170', '197', '204', '165', '203', '170', '15', '58', '35', '83', '212', '150', '73', '25', '15', '64', '133', '210', '108', '79', '193', '15', '126', '165', '15', '208', '98', '3', '200', '126', '3', '40', '165', '98', '69', '15', '165', '133', '96', '173', '203', '133', '238', '248', '15', '7', '38', '207', '108', '165', '199', '18', '162', '27', '3', '72', '65', '250', '209', '166', '64', '209', '237', '212', '64', '225', '120', '108', '203', '135', '248', '69', '251', '135', '204', '160', '160', '135', '148', '15', '15', '23', '207', '274', '29', '28', '194', '150', '40', '104', '208', '125', '70', '18', '3', '19', '102', '104', '23', '29', '207', '133', '204', '111', '2', '125', '126', '7', '194', '209', '203', '83', '126', '126', '258', '119', '162', '241', '133', '18', '208', '197', '209', '15', '78', '80', '110', '237', '104', '30', '208', '103', '15', '200', '15', '238', '254', '108', '173', '40', '69', '175', '17', '155', '136', '26', '3', '135', '13', '241', '161', '166', '170', '162', '133', '53', '211', '204', '15', '161', '238', '110', '18', '191', '18', '50', '71', '30', '17', '208', '248', '203', '203', '196', '210', '258', '204', '3', '18', '196', '125', '173', '199', '249', '18', '104', '165', '241', '2', '193', '238', '78', '15', '166', '15', '126', '3', '237', '161', '52', '207', '3', '3', '50', '56', '22', '56', '69', '93', '135', '207', '69', '99', '3', '133', '68', '133', '161', '108', '126', '250', '60', '155', '98', '18', '16', '78', '78', '240', '235', '176', '22', '199', '160', '235', '274', '209', '64', '173', '137', '128', '25', '18', '160', '203', '70', '165', '193', '110', '207', '64', '85', '252', '78', '51', '2', '99', '19', '22', '198', '204', '166', '166', '151', '56', '133', '210', '26', '149', '109', '30', '209', '209', '51', '3', '193', '119', '208', '93', '165', '248', '55', '203', '204', '58', '18', '199', '211', '83', '32', '135', '212', '156', '19', '235', '173', '191', '209', '203', '146', '208', '58', '158', '102', '199', '200', '200', '173', '70', '52', '107', '211', '213', '237', '196', '205', '104', '135', '258', '255', '96', '19', '124', '128', '57', '199', '98', '166', '102', '40', '15', '238', '3', '148', '225', '126', '135', '114', '193', '204', '15', '209', '209', '193', '259', '64', '161', '26', '274', '126', '18', '196', '0', '60', '207', '17', '71', '218', '239', '248', '38', '154', '61', '128', '199', '196', '173', '25', '46', '98', '258', '108', '3', '108', '196', '93', '93', '99', '172', '69', '241', '196', '30', '135', '278', '109', '18', '5', '209', '57', '159', '70', '193', '163', '249', '196', '196', '72', '103', '133', '17', '204', '199', '166', '202', '29', '57', '193', '150', '98', '119', '25', '205', '18', '173', '166', '209', '57', '25', '58', '258', '26', '15', '15', '101', '26', '74', '1', '4', '74', '263', '']\n",
            "['209', '63', '216', '108', '196', '200', '203', '246', '188', '193', '25', '106', '56', '53', '223', '161', '200', '208', '78', '147', '147', '179', '147', '80', '56', '209', '209', '200', '214', '202', '203', '194', '26', '237', '165', '258', '238', '238', '76', '193', '196', '213', '161', '213', '209', '194', '196', '25', '151', '260', '135', '173', '30', '108', '272', '259', '176', '213', '213', '21', '5', '171', '200', '195', '240', '194', '176', '240', '240', '80', '173', '98', '214', '21', '76', '76', '73', '173', '173', '78', '61', '258', '191', '3', '78', '173', '193', '118', '200', '264', '260', '49', '211', '258', '18', '173', '78', '3', '3', '1', '213', '135', '182', '239', '208', '278', '50', '194', '200', '124', '189', '117', '264', '244', '76', '193', '221', '78', '78', '173', '197', '78', '161', '231', '135', '22', '78', '25', '25', '197', '200', '147', '203', '191', '196', '193', '263', '32', '197', '191', '4', '56', '196', '104', '191', '196', '173', '147', '194', '96', '161', '161', '238', '200', '3', '78', '78', '53', '41', '237', '53', '18', '10', '194', '97', '96', '208', '25', '10', '117', '104', '54', '96', '194', '21', '245', '238', '202', '209', '161', '97', '258', '240', '213', '136', '155', '152', '21', '191', '191', '118', '68', '258', '189', '197', '208', '61', '78', '135', '202', '96', '155', '108', '272', '80', '25', '155', '35', '213', '191', '194', '96', '96', '191', '264', '191', '191', '191', '2', '258', '191', '173', '259', '15', '47', '258', '156', '118', '160', '115', '105', '26', '173', '192', '76', '76', '135', '240', '118', '161', '200', '230', '18', '200', '25', '10', '161', '244', '24', '24', '200', '205', '104', '15', '193', '156', '95', '209', '240', '191', '194', '128', '208', '208', '218', '173', '10', '25', '18', '107', '46', '32', '197', '172', '209', '213', '79', '121', '78', '78', '193', '2', '78', '194', '149', '136', '80', '3', '21', '200', '78', '21', '197', '194', '118', '202', '161', '180', '209', '173', '2', '193', '6', '260', '197', '78', '78', '42', '191', '213', '135', '193', '196', '202', '222', '217', '42', '259', '213', '76', '46', '30', '15', '260', '202', '18', '259', '104', '135', '76', '196', '240', '240', '240', '108', '24', '200', '197', '262', '176', '15', '156', '264', '156', '200', '191', '161', '196', '78', '238', '161', '221', '118', '14', '14', '14', '200', '278', '238', '200', '196', '78', '173', '25', '197', '197', '196', '161', '18', '10', '10', '10', '18', '66', '203', '19', '246', '196', '244', '240', '161', '200', '78', '240', '209', '208', '118', '215', '194', '193', '168', '229', '191', '135', '172', '196', '197', '161', '78', '196', '197', '258', '78', '78', '238', '221', '121', '264', '104', '146', '50', '78', '217', '11', '238', '238', '238', '3', '3', '154', '215', '79', '18', '158', '104', '18', '13', '197', '117', '197', '161', '213', '242', '32', '238', '230', '2', '2', '32', '161', '251', '96', '156', '242', '22', '22', '140', '3', '149', '135', '191', '173', '135', '200', '238', '238', '78', '156', '209', '78', '97', '97', '160', '25', '258', '240', '161', '15', '161', '96', '209', '80', '132', '140', '4', '1', '240', '196', '274', '272', '62', '80', '196', '196', '191', '79', '156', '15', '196', '209', '22', '251', '202', '202', '121', '73', '191', '238', '191', '150', '150', '135', '46', '203', '202', '238', '22', '3', '258', '239', '238', '238', '165', '238', '238', '194', '200', '200', '197', '200', '196', '191', '194', '76', '78', '208', '200', '71', '208', '240', '76', '117', '120', '108', '96', '144', '30', '133', '133', '202', '117', '161', '251', '29', '104', '242', '197', '118', '216', '262', '236', '141', '11', '235', '235', '133', '191', '238', '202', '80', '80', '22', '140', '191', '197', '202', '133', '184', '184', '184', '184', '143', '274', '50', '237', '196', '274', '265', '10', '18', '10', '121', '238', '245', '228', '76', '25', '191', '197', '268', '10', '240', '195', '196', '195', '15', '48', '200', '183', '70', '59', '197', '196', '235', '161', '161', '215', '104', '97', '191', '194', '191', '200', '200', '197', '76', '200', '93', '256', '160', '238', '146', '238', '239', '15', '140', '258', '140', '21', '79', '202', '258', '79', '62', '205', '161', '104', '10', '161', '196', '137', '152', '191', '117', '213', '242', '176', '218', '195', '191', '162', '191', '135', '241', '213', '213', '84', '79', '68', '18', '98', '197', '155', '41', '246', '269', '78', '209', '238', '80', '238', '62', '237', '273', '273', '230', '150', '121', '260', '244', '161', '202', '78', '109', '155', '264', '145', '50', '274', '173', '210', '258', '238', '211', '156', '215', '195', '177', '244', '3', '141', '196', '191', '194', '29', '30', '10', '16', '200', '200', '156', '53', '3', '62', '165', '173', '173', '22', '205', '274', '150', '183', '116', '188', '188', '98', '161', '204', '80', '172', '156', '104', '229', '259', '78', '169', '3', '200', '118', '161', '80', '260', '240', '150', '197', '196', '62', '93', '6', '272', '272', '197', '160', '78', '96', '70', '195', '191', '191', '80', '3', '3', '240', '147', '196', '191', '238', '96', '203', '3', '15', '10', '58', '55', '96', '41', '15', '26', '50', '238', '194', '79', '240', '161', '191', '195', '263', '50', '240', '147', '235', '195', '203', '59', '178', '176', '2', '15', '203', '191', '18', '278', '196', '14', '153', '58', '175', '18', '217', '217', '80', '191', '200', '18', '200', '173', '24', '171', '171', '210', '124', '34', '200', '218', '104', '4', '223', '193', '194', '200', '200', '13', '62', '194', '197', '80', '197', '146', '146', '96', '97', '97', '2', '196', '191', '191', '104', '238', '10', '200', '239', '135', '104', '213', '2', '221', '169', '25', '21', '194', '161', '21', '278', '3', '195', '46', '176', '25', '117', '191', '62', '62', '62', '78', '50', '264', '79', '197', '80', '80', '80', '191', '191', '78', '161', '194', '195', '197', '203', '259', '278', '49', '147', '147', '147', '147', '22', '78', '78', '258', '2', '202', '221', '55', '258', '104', '104', '84', '240', '221', '274', '104', '22', '241', '63', '197', '197', '196', '159', '200', '22', '22', '22', '78', '78', '173', '104', '97', '104', '191', '146', '223', '268', '96', '14', '173', '204', '241', '97', '15', '118', '78', '78', '196', '194', '258', '96', '96', '259', '75', '10', '197', '30', '2', '238', '121', '50', '161', '173', '173', '186', '185', '186', '78', '194', '221', '218', '35', '33', '13', '34', '78', '21', '215', '97', '60', '24', '78', '193', '173', '235', '78', '200', '258', '140', '259', '194', '197', '195', '200', '145', '76', '108', '130', '238', '215', '213', '2', '217', '121', '178', '140', '62', '156', '176', '196', '196', '41', '223', '147', '172', '79', '200', '2', '244', '244', '191', '213', '104', '191', '237', '237', '33', '117', '195', '195', '264', '195', '140', '210', '41', '197', '14', '257', '196', '238', '238', '79', '161', '156', '196', '238', '238', '213', '104', '3', '10', '118', '204', '235', '33', '259', '274', '259', '176', '78', '78', '196', '34', '52', '25', '30', '194', '153', '21', '70', '191', '18', '191', '191', '172', '194', '194', '194', '194', '148', '104', '125', '2', '2', '2', '185', '194', '29', '104', '78', '189', '272', '204', '183', '213', '78', '104', '197', '240', '97', '168', '168', '182', '238', '197', '18', '194', '229', '221', '62', '78', '53', '104', '214', '189', '189', '189', '213', '117', '117', '191', '29', '104', '104', '276', '276', '22', '195', '195', '155', '139', '139', '59', '168', '135', '140', '79', '146', '200', '245', '70', '160', '153', '50', '202', '154', '263', '130', '194', '117', '194', '194', '191', '33', '29', '29', '238', '239', '194', '21', '274', '215', '18', '18', '140', '2', '215', '25', '46', '137', '238', '30', '160', '156', '46', '196', '263', '242', '104', '0', '0', '73', '213', '135', '22', '191', '191', '241', '202', '221', '15', '15', '191', '238', '213', '194', '197', '179', '238', '200', '200', '200', '5', '238', '194', '55', '117', '166', '49', '244', '239', '191', '78', '78', '78', '194', '200', '117', '210', '22', '121', '121', '121', '21', '237', '194', '121', '78', '104', '194', '79', '191', '251', '195', '197', '274', '176', '21', '43', '43', '10', '61', '215', '215', '30', '214', '194', '10', '30', '139', '96', '226', '226', '265', '2', '2', '121', '121', '41', '237', '104', '238', '238', '161', '78', '78', '78', '11', '184', '140', '140', '161', '104', '104', '237', '130', '195', '160', '118', '161', '18', '11', '241', '184', '196', '178', '22', '30', '146', '146', '10', '10', '141', '141', '135', '18', '18', '10', '26', '208', '97', '141', '18', '18', '97', '194', '194', '79', '213', '205', '205', '205', '75', '241', '50', '226', '238', '97', '222', '215', '215', '139', '238', '51', '238', '96', '173', '109', '200', '117', '192', '2', '195', '10', '3', '18', '49', '238', '238', '191', '195', '52', '197', '200', '200', '79', '191', '200', '200', '30', '195', '197', '195', '25', '260', '73', '30', '193', '21', '200', '242', '203', '120', '78', '260', '241', '0', '30', '202', '195', '30', '191', '78', '194', '194', '2', '205', '191', '196', '173', '197', '30', '196', '172', '139', '2', '18', '191', '230', '56', '194', '215', '195', '2', '71', '13', '146', '264', '50', '191', '235', '93', '191', '139', '139', '227', '235', '186', '235', '24', '18', '261', '128', '33', '71', '25', '176', '39', '10', '195', '202', '29', '29', '221', '29', '197', '139', '78', '227', '146', '137', '195', '205', '205', '195', '135', '62', '153', '146', '29', '75', '0', '194', '194', '180', '213', '78', '18', '194', '239', '196', '239', '197', '194', '153', '192', '221', '10', '46', '50', '241', '117', '117', '117', '195', '173', '78', '195', '194', '239', '2', '12', '173', '197', '30', '269', '197', '176', '135', '30', '146', '191', '93', '93', '135', '260', '218', '156', '155', '200', '241', '125', '18', '18', '183', '203', '239', '259', '173', '213', '180', '180', '210', '120', '104', '194', '156', '258', '172', '9', '0', '241', '192', '143', '189', '204', '50', '63', '197', '243', '241', '93', '125', '2', '21', '260', '4', '4', '3', '191', '150', '128', '78', '238', '245', '8', '80', '238', '79', '79', '240', '58', '104', '155', '80', '80', '188', '191', '235', '170', '170', '170', '146', '258', '195', '276', '25', '78', '215', '240', '161', '53', '147', '78', '78', '78', '78', '221', '221', '96', '18', '19', '130', '18', '18', '18', '80', '80', '194', '79', '93', '121', '78', '139', '139', '78', '197', '121', '194', '200', '80', '101', '22', '12', '135', '97', '205', '181', '60', '193', '15', '140', '172', '191', '241', '222', '135', '29', '25', '179', '191', '191', '259', '194', '133', '240', '121', '121', '241', '195', '29', '191', '209', '18', '139', '139', '50', '18', '12', '18', '133', '200', '25', '10', '70', '25', '242', '18', '161', '49', '172', '172', '172', '172', '128', '96', '195', '200', '78', '10', '239', '22', '173', '142', '142', '22', '194', '192', '3', '200', '241', '96', '241', '62', '139', '213', '202', '135', '79', '120', '192', '78', '118', '241', '46', '151', '150', '151', '150', '118', '221', '183', '237', '196', '140', '140', '39', '179', '213', '140', '41', '73', '78', '217', '258', '30', '118', '140', '219', '50', '10', '139', '150', '251', '24', '10', '18', '78', '18', '172', '135', '146', '71', '196', '202', '197', '25', '106', '160', '215', '15', '6', '101', '165', '140', '145', '80', '3', '238', '239', '78', '268', '0', '109', '191', '215', '238', '238', '238', '203', '50', '146', '15', '25', '156', '191', '16', '2', '195', '189', '96', '193', '109', '204', '161', '185', '210', '50', '235', '30', '242', '193', '102', '194', '121', '177', '241', '102', '132', '240', '71', '192', '79', '25', '197', '39', '161', '22', '258', '237', '15', '7', '78', '156', '197', '70', '176', '139', '240', '132', '78', '177', '191', '194', '117', '10', '79', '238', '195', '24', '101', '118', '196', '2', '102', '21', '180', '185', '170', '79', '139', '135', '191', '151', '200', '191', '154', '262', '200', '237', '56', '27', '191', '180', '258', '139', '139', '192', '30', '0', '242', '239', '242', '25', '191', '242', '240', '3', '56', '215', '171', '192', '193', '222', '238', '237', '215', '272', '2', '156', '237', '151', '189', '80', '15', '260', '260', '217', '235', '22', '228', '135', '18', '191', '157', '78', '2', '240', '213', '179', '2', '191', '79', '79', '153', '3', '194', '78', '121', '71', '121', '149', '257', '194', '191', '188', '15', '25', '125', '102', '15', '130', '130', '177', '10', '135', '265', '240', '149', '196', '200', '204', '204', '213', '177', '197', '194', '13', '202', '30', '213', '125', '193', '125', '125', '125', '125', '125', '15', '180', '78', '1', '25', '96', '200', '29', '156', '172', '202', '22', '78', '197', '188', '241', '101', '101', '101', '221', '50', '191', '15', '15', '0', '101', '101', '168', '56', '197', '130', '62', '204', '29', '10', '96', '97', '139', '97', '97', '2', '2', '262', '39', '176', '172', '194', '194', '18', '78', '13', '80', '139', '196', '207', '191', '151', '192', '150', '13', '104', '104', '101', '15', '125', '101', '15', '0', '50', '194', '264', '50', '25', '25', '125', '125', '125', '125', '125', '25', '29', '25', '25', '78', '240', '261', '19', '2', '74', '78', '238', '50', '50', '203', '197', '71', '104', '80', '194', '149', '170', '130', '2', '237', '170', '202', '48', '97', '93', '179', '235', '78', '102', '102', '70', '208', '238', '104', '146', '118', '118', '96', '237', '274', '218', '241', '102', '146', '209', '209', '240', '170', '137', '7', '197', '191', '264', '29', '46', '96', '193', '19', '12', '213', '215', '275', '179', '18', '183', '78', '221', '18', '170', '119', '21', '140', '240', '96', '192', '192', '21', '96', '149', '239', '135', '191', '240', '184', '183', '24', '2', '96', '197', '194', '215', '18', '192', '121', '202', '196', '25', '97', '214', '3', '97', '16', '0', '192', '135', '215', '5', '12', '80', '143', '202', '242', '151', '13', '209', '203', '200', '30', '209', '194', '102', '102', '102', '102', '102', '102', '22', '24', '6', '18', '5', '191', '101', '50', '101', '101', '192', '153', '153', '101', '101', '10', '192', '25', '140', '50', '241', '102', '102', '32', '150', '101', '102', '153', '102', '101', '153', '102', '25', '10', '125', '18', '125', '177', '209', '102', '10', '78', '78', '137', '18', '237', '241', '175', '195', '175', '96', '176', '237', '96', '96', '101', '193', '238', '170', '238', '102', '222', '268', '238', '102', '244', '34', '18', '244', '202', '149', '149', '193', '102', '170', '194', '75', '101', '222', '238', '50', '195', '135', '22', '93', '18', '102', '102', '93', '93', '191', '147', '244', '96', '2', '241', '241', '93', '189', '189', '177', '35', '137', '197', '191', '40', '40', '195', '176', '35', '200', '200', '237', '135', '140', '140', '93', '156', '200', '78', '191', '22', '81', '238', '101', '213', '101', '78', '109', '19', '140', '196', '78', '78', '76', '218', '14', '265', '101', '241', '155', '4', '50', '78', '78', '238', '135', '191', '265', '81', '238', '238', '102', '102', '102', '140', '162', '32', '17', '17', '171', '171', '191', '101', '102', '192', '101', '17', '171', '135', '102', '242', '195', '237', '21', '18', '238', '191', '278', '140', '149', '97', '260', '155', '276', '192', '196', '21', '191', '2', '97', '0', '150', '221', '213', '209', '238', '80', '260', '194', '104', '96', '193', '14', '240', '193', '117', '117', '155', '17', '264', '260', '209', '209', '2', '137', '71', '72', '101', '205', '173', '191', '118', '24', '41', '197', '78', '94', '237', '156', '193', '153', '154', '153', '209', '110', '78', '78', '213', '238', '78', '191', '93', '79', '177', '267', '133', '133', '46', '139', '40', '143', '74', '241', '202', '50', '3', '102', '78', '149', '191', '156', '18', '93', '195', '192', '137', '240', '135', '204', '189', '22', '56', '154', '25', '192', '130', '277', '191', '80', '39', '21', '21', '238', '238', '23', '192', '191', '192', '94', '94', '80', '139', '17', '10', '22', '227', '227', '140', '102', '21', '17', '227', '227', '23', '243', '264', '18', '274', '149', '238', '30', '221', '22', '238', '193', '135', '205', '195', '238', '238', '197', '130', '195', '18', '104', '102', '14', '59', '104', '18', '18', '258', '34', '241', '138', '188', '176', '102', '1', '140', '140', '22', '22', '278', '10', '96', '35', '145', '120', '33', '33', '22', '33', '22', '35', '22', '19', '33', '102', '192', '192', '175', '22', '72', '191', '222', '125', '22', '79', '18', '194', '18', '250', '192', '192', '96', '48', '274', '273', '132', '133', '192', '40', '171', '171', '246', '191', '10', '166', '197', '195', '125', '125', '200', '153', '192', '149', '110', '139', '194', '48', '102', '102', '104', '203', '238', '238', '238', '78', '18', '238', '121', '18', '238', '96', '238', '238', '13', '191', '18', '18', '240', '96', '40', '188', '18', '137', '175', '28', '28', '28', '149', '191', '191', '207', '239', '239', '153', '153', '153', '153', '153', '153', '154', '153', '154', '153', '153', '153', '18', '0', '27', '18', '25', '96', '96', '137', '33', '10', '130', '153', '153', '154', '180', '137', '44', '241', '153', '268', '25', '161', '25', '39', '242', '4', '238', '195', '102', '135', '13', '229', '140', '1', '79', '192', '229', '191', '118', '21', '260', '203', '18', '177', '191', '10', '238', '10', '133', '14', '175', '175', '192', '18', '153', '153', '254', '140', '149', '22', '22', '22', '237', '40', '135', '191', '192', '192', '62', '205', '19', '238', '176', '214', '200', '96', '15', '153', '191', '108', '29', '166', '28', '193', '197', '96', '195', '108', '102', '241', '195', '10', '33', '166', '197', '168', '2', '96', '26', '33', '204', '258', '149', '153', '153', '40', '137', '241', '166', '221', '23', '221', '153', '15', '113', '113', '78', '171', '40', '71', '21', '71', '80', '15', '191', '191', '135', '110', '10', '259', '102', '149', '149', '109', '109', '175', '97', '97', '97', '153', '15', '238', '193', '175', '38', '38', '137', '18', '235', '155', '137', '265', '110', '18', '18', '256', '191', '224', '173', '104', '10', '152', '24', '24', '24', '17', '40', '40', '40', '40', '40', '40', '112', '6', '18', '18', '210', '96', '191', '27', '213', '17', '153', '26', '133', '18', '18', '153', '38', '96', '40', '38', '40', '40', '10', '17', '17', '95', '71', '192', '17', '259', '213', '200', '15', '11', '207', '194', '18', '135', '238', '191', '241', '191', '116', '35', '147', '156', '67', '78', '17', '227', '191', '117', '188', '130', '72', '96', '235', '97', '18', '150', '203', '25', '126', '149', '200', '94', '4', '4', '1', '244', '228', '252', '149', '191', '149', '15', '17', '17', '29', '18', '18', '116', '23', '237', '94', '184', '268', '197', '175', '243', '28', '16', '10', '50', '191', '102', '102', '153', '149', '111', '78', '191', '135', '18', '35', '78', '238', '208', '93', '95', '258', '17', '111', '111', '262', '22', '22', '104', '161', '209', '21', '195', '12', '116', '241', '242', '239', '17', '97', '16', '40', '117', '94', '96', '78', '15', '103', '195', '135', '235', '247', '135', '130', '260', '239', '176', '93', '191', '209', '239', '93', '264', '30', '18', '30', '18', '18', '103', '191', '97', '17', '95', '115', '209', '140', '22', '22', '241', '10', '153', '3', '188', '238', '226', '195', '97', '96', '238', '97', '130', '97', '116', '96', '96', '242', '45', '149', '166', '155', '96', '116', '264', '10', '237', '192', '71', '22', '71', '175', '149', '116', '237', '274', '18', '235', '176', '116', '202', '250', '250', '78', '2', '196', '2', '97', '143', '18', '111', '111', '80', '78', '153', '80', '40', '110', '30', '112', '30', '111', '215', '112', '149', '78', '78', '209', '110', '110', '238', '109', '239', '112', '151', '154', '16', '16', '16', '78', '137', '179', '72', '149', '10', '111', '268', '96', '170', '111', '38', '192', '153', '183', '195', '30', '30', '147', '195', '147', '93', '264', '153', '25', '18', '19', '22', '74', '40', '93', '166', '15', '127', '147', '192', '260', '238', '103', '269', '127', '19', '19', '153', '18', '78', '238', '30', '30', '241', '16', '235', '250', '204', '238', '218', '40', '93', '184', '18', '0', '241', '195', '94', '154', '109', '156', '10', '21', '30', '197', '39', '78', '24', '107', '142', '142', '228', '229', '229', '229', '232', '179', '239', '142', '232', '23', '175', '147', '137', '137', '137', '171', '191', '117', '128', '127', '204', '98', '147', '235', '239', '153', '15', '204', '255', '80', '204', '204', '149', '113', '19', '167', '22', '250', '17', '17', '17', '18', '17', '67', '121', '125', '26', '97', '97', '97', '97', '247', '124', '163', '147', '112', '111', '208', '108', '38', '209', '154', '154', '194', '135', '22', '147', '241', '120', '218', '260', '156', '155', '241', '192', '215', '180', '179', '149', '40', '193', '192', '150', '102', '93', '1', '149', '137', '133', '102', '102', '102', '78', '145', '21', '96', '175', '175', '78', '123', '30', '95', '19', '18', '110', '18', '152', '152', '147', '26', '135', '260', '158', '203', '195', '158', '112', '156', '125', '213', '230', '30', '15', '125', '106', '111', '195', '181', '130', '195', '130', '239', '15', '194', '16', '15', '111', '61', '72', '57', '128', '239', '239', '203', '25', '218', '10', '155', '191', '46', '98', '175', '260', '102', '21', '191', '260', '191', '239', '15', '30', '93', '147', '103', '78', '177', '30', '95', '155', '155', '155', '103', '103', '112', '241', '191', '191', '17', '78', '153', '150', '113', '115', '147', '78', '40', '163', '33', '15', '149', '103', '192', '191', '96', '106', '115', '143', '235', '200', '204', '238', '204', '204', '74', '196', '113', '246', '102', '150', '16', '176', '20', '195', '128', '197', '15', '71', '71', '193', '24', '24', '24', '170', '204', '209', '111', '144', '112', '22', '222', '224', '28', '209', '155', '213', '275', '196', '239', '195', '25', '103', '149', '128', '19', '96', '102', '135', '17', '232', '155', '191', '20', '18', '241', '30', '25', '205', '97', '97', '96', '72', '182', '135', '135', '20', '149', '149', '78', '135', '40', '18', '153', '10', '26', '158', '205', '157', '157', '135', '135', '140', '155', '192', '26', '268', '263', '245', '155', '15', '177', '159', '212', '161', '239', '200', '40', '35', '153', '197', '259', '15', '254', '175', '10', '30', '239', '67', '195', '200', '191', '22', '66', '191', '112', '133', '128', '235', '161', '155', '17', '93', '36', '113', '239', '18', '18', '191', '165', '163', '163', '238', '79', '158', '40', '102', '40', '200', '191', '60', '241', '245', '96', '78', '30', '15', '173', '192', '192', '158', '64', '113', '200', '74', '128', '135', '115', '115', '10', '74', '102', '102', '115', '111', '238', '244', '109', '109', '238', '152', '10', '237', '18', '50', '16', '173', '204', '15', '208', '276', '43', '109', '258', '222', '15', '94', '235', '203', '79', '128', '113', '3', '204', '106', '133', '214', '218', '25', '98', '166', '71', '30', '175', '175', '97', '135', '108', '19', '18', '196', '35', '12', '196', '78', '166', '155', '72', '155', '22', '109', '258', '196', '263', '161', '204', '210', '14', '208', '208', '208', '208', '204', '102', '102', '213', '155', '251', '153', '153', '153', '154', '154', '237', '97', '19', '28', '94', '136', '112', '19', '97', '152', '153', '153', '111', '111', '135', '135', '105', '17', '105', '152', '152', '152', '153', '79', '266', '135', '120', '125', '149', '50', '162', '72', '17', '147', '109', '115', '24', '105', '244', '149', '238', '203', '35', '202', '177', '173', '239', '19', '19', '179', '237', '14', '18', '185', '58', '158', '144', '213', '237', '155', '250', '239', '93', '237', '97', '195', '157', '107', '109', '246', '165', '98', '49', '2', '266', '149', '20', '18', '71', '108', '22', '115', '239', '40', '178', '232', '238', '218', '118', '118', '210', '235', '46', '228', '192', '112', '149', '96', '111', '21', '95', '95', '95', '18', '57', '206', '18', '239', '157', '80', '196', '112', '176', '99', '106', '71', '235', '135', '18', '209', '78', '20', '20', '103', '33', '3', '103', '166', '79', '79', '156', '229', '18', '238', '2', '16', '128', '200', '117', '22', '197', '166', '78', '237', '241', '107', '53', '189', '204', '243', '163', '221', '15', '200', '221', '152', '23', '193', '107', '195', '99', '97', '19', '125', '276', '113', '193', '64', '78', '197', '117', '175', '135', '135', '78', '93', '154', '153', '256', '256', '135', '137', '135', '137', '149', '110', '15', '159', '21', '158', '192', '192', '117', '29', '97', '258', '78', '46', '5', '80', '15', '105', '260', '161', '191', '2', '138', '117', '215', '176', '156', '194', '265', '115', '181', '21', '14', '106', '106', '108', '200', '156', '103', '102', '29', '17', '204', '175', '227', '118', '18', '239', '204', '258', '78', '155', '213', '109', '192', '186', '186', '15', '241', '133', '30', '30', '239', '30', '218', '109', '138', '241', '241', '45', '135', '112', '232', '104', '106', '200', '53', '23', '112', '17', '117', '59', '260', '195', '106', '117', '159', '204', '200', '228', '165', '165', '261', '200', '227', '166', '93', '21', '80', '112', '152', '196', '203', '228', '98', '102', '15', '139', '110', '13', '238', '6', '171', '94', '208', '155', '112', '203', '239', '191', '39', '136', '203', '195', '46', '203', '148', '192', '210', '23', '4', '153', '29', '165', '192', '204', '232', '94', '112', '239', '98', '71', '248', '17', '155', '10', '78', '191', '191', '111', '25', '25', '19', '19', '17', '235', '102', '265', '195', '235', '163', '238', '71', '197', '107', '155', '30', '72', '19', '259', '80', '155', '23', '64', '239', '197', '156', '57', '203', '258', '200', '205', '246', '238', '258', '227', '238', '238', '19', '148', '161', '111', '105', '202', '109', '262', '133', '137', '221', '135', '52', '170', '111', '36', '30', '156', '102', '268', '115', '22', '153', '133', '259', '98', '235', '135', '191', '25', '235', '25', '29', '94', '195', '203', '212', '112', '135', '245', '260', '200', '96', '158', '23', '165', '22', '18', '249', '240', '109', '43', '37', '15', '241', '241', '23', '135', '240', '108', '98', '268', '275', '191', '136', '263', '259', '196', '98', '98', '204', '248', '123', '105', '204', '240', '9', '9', '98', '218', '189', '238', '246', '155', '130', '157', '74', '109', '29', '189', '203', '213', '195', '241', '159', '238', '204', '218', '105', '23', '16', '263', '206', '150', '152', '204', '202', '173', '267', '18', '153', '155', '196', '2', '97', '158', '158', '204', '98', '202', '98', '204', '258', '98', '22', '113', '113', '135', '71', '237', '155', '163', '250', '233', '26', '26', '26', '26', '26', '19', '170', '18', '15', '19', '191', '30', '30', '48', '35', '30', '237', '46', '119', '16', '78', '239', '150', '245', '235', '158', '27', '241', '30', '38', '107', '75', '241', '135', '106', '106', '98', '159', '96', '135', '111', '176', '175', '200', '18', '14', '242', '163', '187', '20', '12', '191', '23', '106', '171', '25', '78', '162', '191', '157', '94', '237', '246', '203', '108', '238', '204', '204', '1', '102', '154', '106', '170', '102', '200', '2', '135', '106', '258', '258', '227', '26', '15', '25', '71', '203', '96', '3', '241', '200', '30', '108', '98', '108', '27', '106', '26', '150', '259', '78', '113', '113', '158', '202', '16', '136', '239', '156', '239', '35', '276', '252', '115', '22', '22', '258', '172', '3', '154', '3', '98', '237', '102', '235', '112', '206', '98', '57', '118', '153', '18', '26', '30', '196', '163', '191', '153', '72', '24', '156', '117', '122', '30', '135', '192', '98', '28', '30', '30', '95', '241', '155', '17', '195', '31', '103', '202', '98', '237', '94', '112', '163', '163', '94', '163', '112', '19', '195', '191', '15', '135', '78', '109', '160', '152', '97', '98', '107', '167', '97', '138', '138', '26', '191', '97', '97', '30', '30', '206', '202', '13', '30', '240', '111', '261', '79', '0', '111', '19', '152', '107', '107', '18', '98', '28', '258', '244', '106', '237', '204', '191', '15', '183', '202', '16', '111', '166', '268', '202', '12', '30', '78', '110', '133', '71', '197', '111', '52', '79', '171', '156', '94', '13', '118', '111', '71', '133', '18', '105', '30', '22', '133', '60', '237', '191', '6', '126', '3', '238', '276', '78', '162', '150', '115', '191', '107', '200', '148', '135', '72', '155', '241', '4', '108', '160', '203', '20', '133', '2', '80', '80', '154', '18', '15', '115', '239', '167', '167', '35', '268', '18', '221', '18', '195', '204', '195', '216', '133', '112', '104', '223', '191', '115', '30', '30', '30', '30', '30', '202', '204', '165', '200', '200', '29', '191', '237', '267', '18', '175', '81', '264', '264', '178', '115', '102', '178', '212', '273', '202', '109', '259', '166', '135', '155', '219', '161', '18', '94', '241', '191', '196', '191', '98', '26', '26', '204', '204', '135', '27', '238', '260', '182', '26', '98', '94', '93', '238', '156', '19', '111', '79', '16', '192', '113', '19', '241', '18', '202', '260', '272', '106', '163', '156', '106', '17', '109', '221', '78', '203', '7', '136', '197', '193', '191', '98', '112', '200', '95', '221', '98', '161', '191', '3', '106', '189', '111', '25', '196', '196', '241', '163', '200', '24', '78', '176', '3', '98', '102', '98', '19', '161', '224', '80', '18', '191', '112', '105', '254', '71', '200', '137', '29', '20', '165', '8', '106', '23', '106', '155', '200', '265', '135', '126', '203', '188', '161', '158', '258', '255', '98', '203', '50', '35', '102', '158', '175', '40', '239', '26', '66', '57', '193', '155', '15', '135', '10', '107', '106', '229', '18', '191', '204', '107', '99', '235', '238', '18', '205', '205', '35', '238', '208', '203', '110', '110', '109', '112', '23', '18', '19', '182', '156', '18', '18', '156', '111', '135', '137', '133', '137', '240', '99', '197', '113', '15', '165', '39', '97', '97', '15', '150', '163', '109', '268', '15', '15', '238', '203', '67', '161', '197', '18', '196', '97', '161', '197', '161', '185', '238', '191', '30', '121', '133', '160', '160', '74', '135', '75', '54', '135', '196', '23', '241', '166', '194', '239', '135', '240', '208', '238', '98', '135', '152', '176', '22', '174', '113', '241', '237', '57', '195', '191', '24', '165', '165', '204', '204', '56', '109', '27', '153', '252', '228', '165', '252', '200', '28', '158', '135', '196', '22', '18', '133', '30', '204', '237', '196', '93', '106', '94', '25', '93', '159', '4', '195', '23', '237', '25', '199', '57', '26', '276', '274', '204', '75', '18', '204', '108', '22', '133', '191', '111', '204', '203', '133', '237', '166', '4', '62', '202', '166', '161', '165', '74', '109', '118', '200', '22', '206', '25', '209', '17', '155', '204', '64', '23', '30', '202', '149', '171', '135', '136', '184', '26', '18', '159', '195', '18', '166', '156', '98', '133', '38', '40', '18', '18', '166', '109', '109', '209', '22', '195', '163', '163', '108', '18', '15', '99', '135', '174', '26', '19', '166', '133', '259', '151', '235', '204', '228', '133', '30', '161', '111', '242', '84', '238', '98', '195', '78', '160', '28', '25', '264', '165', '41', '19', '55', '46', '202', '121', '15', '32', '98', '238', '0', '78', '31', '30', '203', '264', '203', '135', '104', '111', '78', '212', '111', '98', '126', '22', '2', '30', '237', '212', '265', '163', '106', '106', '235', '109', '223', '161', '202', '150', '170', '255', '112', '114', '170', '98', '96', '197', '36', '18', '228', '14', '255', '204', '102', '239', '0', '185', '30', '22', '23', '246', '199', '133', '191', '78', '195', '153', '112', '191', '200', '170', '171', '200', '78', '128', '25', '153', '237', '99', '94', '208', '107', '208', '208', '78', '204', '204', '27', '191', '272', '134', '158', '158', '174', '118', '64', '18', '258', '150', '189', '18', '153', '241', '204', '112', '106', '135', '71', '148', '7', '133', '140', '187', '23', '223', '19', '19', '2', '136', '2', '200', '161', '15', '98', '158', '148', '3', '250', '153', '235', '165', '220', '176', '110', '240', '151', '148', '133', '74', '193', '46', '27', '173', '94', '176', '105', '256', '161', '226', '18', '78', '191', '171', '243', '26', '0', '108', '167', '159', '239', '150', '112', '155', '79', '241', '202', '165', '254', '107', '202', '156', '166', '193', '239', '135', '15', '203', '187', '148', '25', '15', '22', '194', '239', '56', '191', '176', '262', '200', '110', '72', '99', '102', '135', '98', '113', '15', '15', '159', '262', '171', '151', '98', '204', '213', '255', '26', '243', '175', '156', '98', '107', '191', '165', '241', '26', '165', '171', '30', '160', '166', '266', '11', '19', '211', '93', '254', '30', '239', '115', '273', '205', '112', '237', '263', '74', '98', '79', '122', '94', '265', '155', '191', '16', '235', '229', '98', '191', '227', '17', '111', '267', '135', '202', '195', '10', '180', '196', '196', '133', '135', '133', '95', '111', '122', '258', '191', '3', '114', '107', '19', '264', '74', '56', '200', '200', '133', '161', '97', '15', '97', '197', '196', '253', '93', '202', '258', '106', '153', '26', '109', '40', '149', '8', '196', '15', '119', '155', '23', '237', '193', '264', '148', '151', '238', '200', '94', '75', '148', '175', '24', '195', '171', '30', '203', '200', '202', '16', '158', '223', '191', '22', '264', '135', '35', '57', '94', '191', '196', '202', '98', '18', '196', '3', '133', '78', '191', '208', '258', '18', '200', '29', '135', '109', '110', '2', '185', '98', '133', '19', '22', '204', '95', '238', '25', '133', '250', '135', '135', '3', '47', '158', '187', '2', '78', '115', '155', '3', '218', '166', '112', '266', '22', '195', '135', '72', '97', '197', '209', '127', '80', '2', '98', '14', '239', '15', '113', '165', '204', '78', '204', '191', '175', '200', '193', '18', '196', '133', '109', '237', '56', '196', '3', '163', '163', '163', '195', '15', '263', '18', '84', '18', '133', '107', '18', '102', '156', '238', '102', '26', '244', '82', '75', '23', '258', '238', '136', '3', '204', '184', '200', '196', '158', '106', '27', '238', '155', '152', '50', '50', '2', '235', '98', '203', '166', '109', '25', '18', '151', '133', '200', '26', '26', '26', '19', '107', '156', '23', '26', '3', '70', '225', '41', '26', '239', '26', '186', '26', '34', '272', '272', '104', '78', '260', '18', '204', '26', '78', '18', '210', '135', '203', '71', '207', '26', '133', '13', '25', '26', '115', '94', '264', '135', '244', '79', '30', '263', '22', '109', '205', '176', '106', '22', '17', '151', '106', '191', '160', '160', '165', '78', '177', '196', '260', '166', '35', '191', '260', '98', '18', '237', '18', '161', '18', '197', '133', '30', '35', '200', '170', '155', '245', '19', '200', '160', '96', '172', '165', '55', '137', '241', '133', '191', '112', '25', '126', '238', '18', '18', '108', '135', '161', '135', '73', '112', '107', '8', '112', '109', '195', '25', '30', '105', '161', '241', '110', '75', '238', '215', '238', '104', '25', '3', '155', '30', '16', '3', '175', '99', '3', '200', '23', '3', '199', '203', '191', '196', '196', '204', '98', '105', '133', '197', '135', '161', '39', '15', '30', '133', '55', '191', '203', '133', '203', '140', '200', '149', '30', '96', '135', '23', '26', '3', '18', '155', '158', '135', '135', '18', '250', '18', '22', '114', '18', '18', '166', '18', '30', '18', '239', '18', '161', '50', '50', '49', '51', '23', '55', '98', '78', '210', '211', '18', '239', '78', '13', '193', '114', '78', '18', '25', '133', '19', '176', '4', '173', '50', '108', '17', '4', '204', '94', '112', '207', '109', '115', '23', '235', '197', '194', '197', '213', '148', '40', '157', '107', '200', '147', '14', '14', '18', '238', '18', '238', '18', '238', '57', '200', '135', '30', '18', '191', '191', '200', '166', '151', '161', '73', '19', '73', '239', '239', '165', '108', '2', '93', '166', '133', '133', '173', '35', '193', '114', '74', '19', '40', '58', '104', '16', '260', '160', '158', '112', '210', '158', '167', '203', '203', '3', '3', '109', '30', '133', '258', '70', '204', '200', '152', '155', '192', '204', '108', '191', '204', '26', '98', '26', '98', '264', '158', '18', '150', '137', '153', '196', '199', '191', '115', '168', '157', '149', '151', '78', '137', '160', '259', '166', '196', '171', '175', '199', '166', '161', '70', '13', '166', '152', '152', '206', '58', '18', '207', '58', '74', '207', '29', '46', '109', '135', '74', '174', '109', '40', '10', '30', '258', '16', '111', '30', '15', '239', '204', '48', '25', '196', '0', '156', '72', '72', '96', '22', '72', '133', '133', '238', '26', '3', '50', '152', '193', '235', '136', '153', '50', '1', '19', '239', '30', '25', '179', '161', '30', '191', '3', '27', '133', '3', '40', '23', '133', '150', '151', '265', '32', '191', '3', '25', '170', '238', '40', '259', '166', '137', '241', '133', '209', '159', '78', '78', '153', '26', '52', '241', '36', '250', '96', '30', '30', '38', '18', '18', '238', '99', '72', '111', '3', '244', '191', '226', '3', '191', '114', '135', '109', '40', '79', '202', '78', '114', '238', '10', '150', '108', '163', '108', '113', '197', '109', '115', '161', '3', '238', '3', '237', '161', '107', '155', '78', '202', '112', '38', '195', '137', '250', '133', '237', '161', '25', '26', '25', '15', '15', '239', '166', '114', '229', '111', '40', '135', '94', '102', '162', '204', '237', '17', '166', '136', '22', '98', '99', '99', '166', '35', '196', '3', '203', '135', '135', '84', '98', '266', '238', '13', '166', '78', '193', '98', '241', '104', '153', '192', '200', '10', '95', '95', '148', '107', '14', '17', '191', '239', '19', '203', '255', '197', '213', '94', '3', '107', '23', '35', '10', '237', '235', '78', '29', '18', '160', '200', '150', '99', '109', '155', '161', '133', '114', '18', '18', '18', '264', '133', '104', '161', '70', '17', '204', '195', '204', '241', '99', '82', '241', '241', '161', '18', '235', '175', '2', '2', '202', '50', '230', '26', '40', '40', '200', '78', '156', '26', '112', '26', '30', '71', '184', '200', '135', '153', '23', '106', '125', '175', '133', '133', '7', '38', '196', '161', '0', '49', '241', '158', '15', '21', '104', '2', '3', '176', '259', '135', '78', '75', '112', '200', '1', '207', '238', '161', '150', '158', '2', '45', '31', '237', '149', '3', '26', '165', '135', '152', '115', '133', '18', '40', '166', '30', '173', '40', '26', '78', '50', '48', '50', '13', '45', '200', '200', '18', '151', '205', '135', '38', '117', '196', '135', '155', '111', '196', '196', '166', '23', '25', '161', '18', '124', '259', '165', '58', '170', '204', '250', '103', '160', '238', '194', '163', '241', '255', '235', '17', '133', '26', '135', '196', '237', '114', '258', '193', '248', '40', '195', '84', '238', '146', '199', '40', '161', '22', '137', '43', '166', '156', '4', '104', '133', '177', '211', '193', '78', '203', '246', '237', '264', '3', '149', '78', '78', '111', '239', '13', '238', '176', '112', '150', '156', '3', '6', '118', '178', '178', '21', '186', '150', '241', '50', '243', '191', '121', '278', '29', '200', '18', '3', '197', '191', '98', '173', '195', '25', '195', '264', '221', '15', '156', '25', '200', '211', '195', '26', '35', '2', '135', '126', '113', '167', '191', '264', '196', '133', '18', '26', '146', '203', '219', '23', '160', '160', '135', '259', '3', '159', '120', '160', '106', '240', '133', '115', '258', '38', '22', '3', '26', '22', '107', '49', '48', '25', '237', '72', '219', '196', '196', '111', '135', '196', '18', '166', '172', '135', '26', '258', '70', '98', '40', '166', '23', '96', '3', '30', '155', '126', '146', '114', '242', '237', '157', '238', '241', '40', '133', '200', '161', '175', '200', '175', '84', '266', '18', '99', '30', '242', '151', '153', '166', '199', '228', '3', '237', '132', '110', '135', '80', '15', '15', '15', '84', '133', '15', '177', '3', '202', '255', '71', '115', '235', '19', '18', '18', '102', '58', '78', '165', '166', '264', '12', '150', '15', '2', '18', '135', '109', '200', '258', '128', '3', '199', '197', '239', '109', '222', '239', '107', '125', '132', '204', '204', '133', '135', '30', '158', '203', '15', '264', '237', '133', '196', '3', '252', '26', '98', '79', '199', '210', '0', '99', '30', '26', '259', '6', '49', '166', '98', '108', '9', '133', '102', '155', '98', '114', '78', '35', '166', '188', '202', '235', '30', '149', '165', '176', '138', '29', '153', '25', '25', '137', '133', '166', '135', '125', '71', '193', '154', '165', '98', '23', '56', '198', '26', '26', '0', '109', '136', '163', '151', '25', '15', '241', '195', '204', '191', '195', '161', '175', '133', '237', '78', '165', '258', '135', '78', '135', '258', '18', '235', '30', '187', '150', '275', '149', '2', '36', '112', '22', '161', '238', '203', '112', '265', '75', '133', '205', '258', '158', '18', '26', '18', '25', '26', '15', '26', '239', '15', '240', '18', '71', '71', '78', '135', '3', '148', '166', '26', '176', '2', '197', '150', '176', '19', '18', '97', '166', '18', '153', '250', '133', '210', '20', '192', '196', '26', '191', '2', '81', '70', '5', '79', '3', '30', '193', '193', '69', '160', '176', '246', '108', '54', '250', '135', '200', '80', '3', '167', '18', '166', '200', '152', '23', '149', '19', '14', '13', '237', '196', '19', '112', '254', '204', '191', '23', '264', '199', '209', '137', '31', '135', '238', '78', '18', '191', '15', '164', '195', '74', '163', '163', '41', '99', '26', '135', '26', '40', '40', '250', '200', '200', '26', '237', '4', '195', '158', '35', '71', '191', '3', '218', '274', '58', '3', '23', '177', '132', '49', '62', '238', '166', '97', '264', '193', '29', '26', '49', '241', '197', '200', '64', '227', '258', '44', '15', '133', '175', '238', '109', '254', '237', '198', '175', '166', '213', '174', '196', '107', '259', '98', '10', '196', '210', '191', '96', '166', '135', '15', '197', '78', '78', '195', '176', '18', '98', '129', '170', '18', '247', '203', '26', '158', '40', '64', '200', '163', '196', '19', '205', '197', '193', '191', '69', '96', '74', '35', '36', '64', '133', '172', '19', '266', '96', '94', '103', '258', '193', '264', '98', '254', '50', '193', '25', '170', '75', '84', '27', '252', '203', '94', '94', '19', '98', '137', '12', '153', '3', '155', '135', '238', '15', '196', '15', '26', '84', '49', '27', '135', '203', '48', '133', '129', '133', '258', '155', '35', '195', '71', '241', '135', '166', '193', '165', '104', '196', '117', '196', '26', '237', '109', '95', '196', '60', '40', '158', '193', '250', '104', '94', '178', '27', '193', '78', '106', '79', '204', '133', '237', '235', '135', '202', '70', '15', '264', '241', '254', '110', '199', '49', '255', '30', '193', '238', '204', '102', '97', '161', '43', '228', '1', '6', '196', '193', '26', '15', '128', '46', '155', '32', '133', '69', '47', '135', '78', '205', '200', '200', '202', '3', '95', '191', '78', '26', '248', '258', '191', '240', '49', '110', '38', '193', '200', '200', '96', '200', '204', '135', '9', '200', '258', '15', '46', '15', '149', '155', '197', '2', '166', '151', '72', '135', '97', '211', '238', '135', '163', '161', '26', '163', '129', '241', '239', '241', '16', '26', '18', '263', '260', '166', '3', '264', '195', '79', '132', '251', '50', '133', '15', '25', '15', '191', '15', '78', '50', '78', '71', '227', '69', '200', '69', '109', '64', '133', '197', '196', '3', '45', '238', '26', '132', '104', '196', '193', '136', '80', '26', '75', '117', '14', '78', '75', '98', '15', '2', '15', '50', '255', '136', '3', '112', '173', '69', '191', '19', '239', '235', '109', '70', '135', '78', '165', '238', '262', '199', '35', '203', '224', '193', '15', '18', '150', '238', '197', '238', '193', '135', '3', '150', '187', '199', '239', '109', '152', '27', '257', '6', '235', '104', '191', '133', '166', '155', '26', '16', '136', '198', '98', '154', '235', '27', '102', '30', '191', '15', '132', '132', '114', '107', '241', '241', '156', '199', '241', '49', '62', '117', '258', '30', '163', '133', '238', '200', '3', '15', '109', '2', '196', '165', '166', '159', '210', '35', '15', '17', '200', '241', '106', '203', '197', '197', '79', '98', '98', '23', '204', '48', '129', '57', '258', '202', '215', '238', '70', '64', '193', '189', '57', '200', '0', '98', '135', '204', '25', '30', '204', '27', '26', '78', '135', '109', '3', '197', '3', '167', '35', '196', '225', '2', '194', '200', '15', '161', '198', '23', '133', '238', '195', '35', '50', '135', '152', '163', '21', '15', '94', '26', '94', '53', '260', '196', '154', '277', '200', '118', '6', '238', '50', '209', '50', '3', '238', '3', '15', '107', '135', '199', '15', '135', '26', '258', '251', '203', '19', '259', '195', '199', '238', '15', '254', '112', '211', '200', '129', '84', '196', '3', '208', '259', '135', '104', '29', '136', '31', '78', '13', '3', '64', '75', '55', '114', '160', '96', '14', '18', '207', '75', '48', '15', '49', '97', '117', '196', '99', '258', '195', '264', '264', '78', '197', '30', '238', '109', '158', '32', '195', '149', '227', '65', '167', '162', '204', '250', '151', '151', '102', '26', '29', '26', '193', '74', '50', '193', '186', '15', '74', '259', '57', '188', '277', '277', '195', '195', '212', '78', '197', '64', '129', '259', '200', '199', '129', '4', '235', '197', '196', '161', '3', '255', '199', '102', '3', '13', '94', '53', '98', '15', '129', '15', '155', '71', '179', '50', '29', '136', '80', '254', '198', '162', '237', '3', '18', '260', '238', '117', '179', '156', '19', '45', '197', '78', '199', '102', '102', '237', '155', '69', '69', '163', '239', '15', '196', '80', '191', '30', '71', '158', '259', '2', '106', '148', '15', '135', '135', '135', '196', '19', '123', '15', '104', '19', '128', '230', '69', '98', '197', '241', '161', '195', '200', '3', '98', '78', '165', '163', '19', '196', '31', '173', '203', '55', '69', '209', '239', '79', '158', '254', '135', '51', '94', '162', '255', '95', '18', '133', '109', '50', '94', '199', '15', '197', '200', '133', '116', '50', '196', '161', '69', '133', '3', '258', '3', '197', '29', '3', '157', '197', '193', '4', '94', '195', '17', '48', '152', '8', '15', '155', '133', '195', '136', '150', '102', '237', '203', '164', '164', '250', '154', '13', '191', '58', '94', '70', '93', '264', '15', '30', '133', '237', '267', '199', '193', '76', '241', '157', '26', '200', '109', '179', '55', '98', '200', '129', '238', '235', '208', '109', '239', '258', '196', '193', '18', '107', '210', '70', '40', '129', '259', '193', '30', '158', '252', '40', '210', '78', '15', '78', '96', '208', '113', '258', '64', '80', '15', '133', '241', '15', '15', '258', '135', '109', '251', '251', '109', '74', '196', '170', '240', '254', '258', '204', '199', '26', '109', '197', '57', '45', '161', '57', '114', '3', '15', '264', '106', '57', '27', '18', '258', '29', '84', '161', '135', '114', '24', '30', '107', '107', '172', '209', '170', '18', '19', '133', '191', '195', '135', '156', '196', '94', '3', '84', '225', '199', '238', '106', '104', '70', '50', '12', '155', '114', '176', '237', '200', '108', '70', '240', '196', '19', '22', '276', '199', '238', '205', '104', '193', '108', '115', '259', '102', '19', '258', '18', '106', '161', '191', '114', '64', '15', '204', '35', '155', '40', '238', '136', '264', '133', '186', '15', '258', '166', '26', '6', '114', '114', '133', '114', '114', '13', '13', '13', '237', '250', '245', '136', '78', '13', '48', '18', '13', '25', '26', '93', '125', '50', '94', '148', '78', '194', '200', '25', '93', '274', '266', '70', '22', '3', '208', '240', '211', '135', '31', '18', '153', '18', '2', '15', '15', '15', '156', '15', '70', '73', '173', '13', '112', '69', '204', '70', '248', '135', '70', '3', '193', '193', '22', '162', '238', '57', '98', '15', '199', '199', '52', '151', '73', '5', '108', '23', '69', '95', '220', '106', '163', '152', '163', '108', '15', '58', '250', '19', '177', '245', '264', '93', '204', '25', '78', '155', '69', '155', '239', '241', '155', '200', '18', '3', '13', '196', '197', '204', '272', '95', '135', '165', '22', '210', '35', '69', '18', '170', '3', '25', '156', '78', '135', '102', '107', '255', '19', '70', '70', '109', '198', '156', '64', '170', '138', '78', '203', '166', '108', '114', '179', '228', '74', '49', '14', '264', '26', '195', '78', '107', '75', '18', '106', '151', '70', '58', '15', '196', '238', '109', '104', '104', '109', '136', '148', '210', '3', '161', '15', '104', '238', '208', '133', '108', '241', '18', '209', '74', '156', '114', '199', '174', '35', '18', '170', '135', '238', '244', '155', '162', '197', '3', '98', '227', '267', '238', '193', '50', '238', '98', '112', '124', '25', '114', '241', '159', '154', '78', '161', '136', '114', '153', '225', '154', '13', '50', '11', '25', '15', '209', '11', '166', '203', '259', '237', '22', '114', '276', '98', '278', '150', '198', '18', '15', '78', '15', '162', '26', '3', '196', '3', '102', '189', '71', '69', '16', '18', '152', '133', '135', '162', '241', '15', '104', '15', '259', '202', '2', '238', '207', '15', '204', '30', '215', '112', '210', '42', '96', '193', '15', '110', '163', '211', '78', '108', '15', '31', '27', '15', '31', '196', '195', '96', '15', '161', '94', '193', '195', '198', '98', '261', '258', '76', '15', '104', '258', '135', '18', '15', '202', '258', '108', '156', '13', '22', '251', '199', '155', '258', '211', '3', '207', '202', '259', '107', '202', '259', '135', '135', '148', '158', '154', '99', '99', '3', '207', '94', '25', '162', '31', '241', '25', '15', '114', '18', '114', '148', '19', '15', '203', '191', '19', '13', '193', '204', '25', '166', '155', '196', '98', '162', '197', '29', '152', '191', '133', '199', '230', '40', '25', '204', '79', '3', '19', '165', '204', '108', '208', '3', '251', '274', '15', '166', '264', '35', '13', '193', '114', '258', '13', '19', '133', '23', '17', '199', '204', '153', '250', '204', '16', '12', '107', '191', '18', '3', '15', '165', '15', '69', '17', '165', '17', '15', '15', '156', '23', '23', '15', '197', '104', '197', '26', '208', '135', '211', '250', '18', '204', '25', '196', '53', '69', '104', '104', '19', '78', '241', '230', '30', '195', '96', '26', '109', '196', '69', '238', '15', '18', '133', '26', '106', '109', '98', '15', '258', '274', '193', '203', '58', '57', '274', '2', '202', '161', '136', '135', '155', '241', '35', '151', '137', '5', '114', '96', '16', '209', '158', '15', '266', '161', '15', '25', '274', '202', '3', '161', '237', '267', '204', '125', '15', '151', '155', '259', '151', '210', '238', '15', '26', '132', '50', '173', '197', '114', '241', '15', '258', '109', '166', '274', '13', '191', '15', '26', '164', '258', '163', '15', '264', '11', '260', '148', '150', '207', '196', '98', '272', '133', '196', '107', '26', '191', '163', '191', '135', '3', '18', '18', '119', '69', '196', '15', '156', '135', '112', '258', '155', '256', '199', '204', '212', '18', '135', '254', '72', '258', '254', '15', '133', '18', '259', '193', '258', '204', '40', '18', '29', '155', '104', '107', '274', '241', '25', '26', '69', '148', '166', '46', '94', '135', '15', '155', '104', '241', '32', '274', '249', '15', '197', '104', '44', '259', '69', '191', '15', '198', '193', '133', '17', '258', '248', '166', '197', '200', '250', '204', '3', '225', '238', '163', '69', '15', '15', '26', '202', '19', '209', '78', '212', '15', '78', '243', '110', '191', '15', '18', '107', '38', '208', '166', '239', '237', '199', '15', '204', '258', '173', '172', '200', '274', '243', '19', '191', '135', '203', '69', '149', '148', '196', '256', '204', '97', '191', '26', '114', '274', '200', '69', '69', '64', '69', '239', '25', '274', '203', '78', '135', '196', '202', '17', '2', '2', '3', '98', '98', '62', '2', '95', '96', '2', '207', '78', '200', '26', '64', '18', '93', '49', '57', '154', '200', '69', '94', '98', '98', '56', '56', '166', '98', '212', '226', '226', '15', '69', '15', '273', '272', '104', '193', '3', '98', '199', '98', '25', '125', '259', '249', '108', '40', '26', '198', '218', '73', '148', '69', '204', '241', '35', '3', '274', '70', '18', '71', '35', '172', '133', '106', '69', '107', '199', '219', '19', '133', '69', '135', '109', '57', '191', '135', '252', '104', '3', '94', '3', '72', '126', '16', '36', '84', '241', '3', '218', '98', '108', '46', '19', '196', '118', '104', '29', '238', '69', '10', '204', '264', '259', '124', '242', '107', '133', '25', '64', '30', '194', '259', '159', '264', '274', '52', '200', '104', '29', '30', '193', '98', '94', '114', '204', '40', '18', '198', '97', '3', '99', '274', '239', '177', '15', '18', '126', '126', '208', '148', '15', '104', '29', '36', '218', '70', '15', '23', '278', '23', '36', '191', '210', '51', '245', '167', '72', '237', '25', '203', '36', '196', '193', '196', '15', '151', '207', '203', '103', '166', '135', '110', '27', '13', '27', '191', '98', '57', '166', '50', '21', '15', '197', '23', '2', '70', '69', '193', '274', '104', '166', '258', '126', '238', '161', '18', '193', '69', '140', '170', '197', '204', '165', '203', '170', '15', '58', '35', '83', '212', '150', '73', '25', '15', '64', '133', '210', '108', '79', '193', '15', '126', '165', '15', '208', '98', '3', '200', '126', '3', '40', '165', '98', '69', '15', '165', '133', '96', '173', '203', '133', '238', '248', '15', '7', '38', '207', '108', '165', '199', '18', '162', '27', '3', '72', '65', '250', '209', '166', '64', '209', '237', '212', '64', '225', '120', '108', '203', '135', '248', '69', '251', '135', '204', '160', '160', '135', '148', '15', '15', '23', '207', '274', '29', '28', '194', '150', '40', '104', '208', '125', '70', '18', '3', '19', '102', '104', '23', '29', '207', '133', '204', '111', '2', '125', '126', '7', '194', '209', '203', '83', '126', '126', '258', '119', '162', '241', '133', '18', '208', '197', '209', '15', '78', '80', '110', '237', '104', '30', '208', '103', '15', '200', '15', '238', '254', '108', '173', '40', '69', '175', '17', '155', '136', '26', '3', '135', '13', '241', '161', '166', '170', '162', '133', '53', '211', '204', '15', '161', '238', '110', '18', '191', '18', '50', '71', '30', '17', '208', '248', '203', '203', '196', '210', '258', '204', '3', '18', '196', '125', '173', '199', '249', '18', '104', '165', '241', '2', '193', '238', '78', '15', '166', '15', '126', '3', '237', '161', '52', '207', '3', '3', '50', '56', '22', '56', '69', '93', '135', '207', '69', '99', '3', '133', '68', '133', '161', '108', '126', '250', '60', '155', '98', '18', '16', '78', '78', '240', '235', '176', '22', '199', '160', '235', '274', '209', '64', '173', '137', '128', '25', '18', '160', '203', '70', '165', '193', '110', '207', '64', '85', '252', '78', '51', '2', '99', '19', '22', '198', '204', '166', '166', '151', '56', '133', '210', '26', '149', '109', '30', '209', '209', '51', '3', '193', '119', '208', '93', '165', '248', '55', '203', '204', '58', '18', '199', '211', '83', '32', '135', '212', '156', '19', '235', '173', '191', '209', '203', '146', '208', '58', '158', '102', '199', '200', '200', '173', '70', '52', '107', '211', '213', '237', '196', '205', '104', '135', '258', '255', '96', '19', '124', '128', '57', '199', '98', '166', '102', '40', '15', '238', '3', '148', '225', '126', '135', '114', '193', '204', '15', '209', '209', '193', '259', '64', '161', '26', '274', '126', '18', '196', '0', '60', '207', '17', '71', '218', '239', '248', '38', '154', '61', '128', '199', '196', '173', '25', '46', '98', '258', '108', '3', '108', '196', '93', '93', '99', '172', '69', '241', '196', '30', '135', '278', '109', '18', '5', '209', '57', '159', '70', '193', '163', '249', '196', '196', '72', '103', '133', '17', '204', '199', '166', '202', '29', '57', '193', '150', '98', '119', '25', '205', '18', '173', '166', '209', '57', '25', '58', '258', '26', '15', '15', '101', '26', '74', '1', '4', '74', '263']\n",
            "                                                   text  label\n",
            "0       Halliburton Oil Well Cementing Co. v. Walker...    209\n",
            "1       Petitioners are members of a Mormon sect, kn...     63\n",
            "2       Champlin owns and operates a line of six-inc...    216\n",
            "3       Eleven Indian tribes have sued the U.S. in t...    108\n",
            "4       The Court of Claims rendered a judgment for ...    196\n",
            "...                                                 ...    ...\n",
            "8414   Opinion reported: Ante, p. 88.\\nDECREE\\n1. It...     74\n",
            "8415    In this dispute between Utah and the United ...      1\n",
            "8416    The U.S. held to have sovereign rights over ...      4\n",
            "8417    Louisiana's exception to the portion of the ...     74\n",
            "8418    California seeks to invoke this Court's orig...    263\n",
            "\n",
            "[8419 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "with open('summarized_usdb.txt') as f:\n",
        "    contents = f.read()\n",
        "\n",
        "records=contents.split(\"---\")\n",
        "records=records[1:]\n",
        "\n",
        "summarized_data = pd.DataFrame(records,\n",
        "               columns =['text'])\n",
        "\n",
        "len_list = [len(ele.split()) for ele in records]\n",
        "# len_list = [len(ele.split()) for ele in texts]\n",
        "res = 0 if len(len_list) == 0 else (float(sum(len_list)) / len(len_list))\n",
        "print(\"Average Length %s\" % res) \n",
        "\n",
        "#temp_file = open(\"labels_sc.txt\", \"r\")\n",
        "temp_file = open(\"labels_sc_279.txt\", \"r\")\n",
        "\n",
        "data = temp_file.read()\n",
        "  \n",
        "# replacing end splitting the text \n",
        "# when newline ('\\n') is seen.\n",
        "label_list = data.split(\"\\n\")\n",
        "print(label_list)\n",
        "label_list = label_list[0:-1]\n",
        "print(label_list)\n",
        "label_list = [int(i) for i in label_list]\n",
        "temp_file.close()\n",
        "# label_list = temp_file.readlines()\n",
        "# print(label_list)\n",
        "\n",
        "summarized_data['label'] = label_list\n",
        "\n",
        "print(summarized_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_tlJZIjXjKkM"
      },
      "outputs": [],
      "source": [
        "def create_model():\n",
        "    inps = Input(shape = (max_len,), dtype='int64')\n",
        "    masks= Input(shape = (max_len,), dtype='int64')\n",
        "    dbert_layer = dbert_model(inps, attention_mask=masks)[0][:,0,:]\n",
        "    dense_0 = Dense(512,activation='relu',kernel_regularizer=regularizers.l2(0.01))(dbert_layer)\n",
        "    dropout_0= Dropout(0.5)(dense_0)\n",
        "    pred = Dense(279, activation='softmax',kernel_regularizer=regularizers.l2(0.01))(dropout_0)\n",
        "    model = tf.keras.Model(inputs=[inps,masks], outputs=pred)\n",
        "    print(model.summary())\n",
        "    return model   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "14785647ab53476a8df37c0248409816",
            "d01f7c6c595f4c529c66f2c49085e014",
            "4203249f59264bed9013f8565a41657f",
            "f3d3605e346441cf97d2eb90616c109c",
            "0d8de003d90c475f821e8f1693df2480",
            "8cac040264f14a1a8dcfba223baa7aa8",
            "bc09dec8d74f49258a554640940274b5",
            "3d7da6fab2b24f9bb777e2646032675b",
            "5e03b191d6c14f24b28d15ab5fc96561",
            "b6676bcb452c4dc1900c6e40fdd1b57f",
            "053d5cce62904c4ab0dd01a382c3c01c",
            "bc52edb4d21c4bc0a2413144ac2c84ef",
            "d46cd059830446ca823375343abae7cc",
            "baea01fd506241aea188e7a667d24fa5",
            "3a50534a9cf84ec691ade65fa5a35d9b",
            "0c2e75448e674197b4aad9a999fc8174",
            "1167192bafc24efcac4ed522dc12105b",
            "ade6231b8aac4863a7c01eeb0722aba8",
            "808663cb530142999cbecebe31617339",
            "8b8a8f77e04f4c379e8700aad4ccf277",
            "362baf73afb34985bbcffe7363c41822",
            "a07fedd4ab27443baf63273285b54a7b",
            "4ef1b5942c214f13a32b43dea33f6148",
            "712f7cce9ff4452ea3f68136183b3ee6",
            "31f3ad72a4804e39bacd6e13a14d0421",
            "e362a3c6e0eb43ffb42031aba1487e98",
            "90036ffd63c34956879d5636fa6f8e46",
            "6da3b80269334b57add634149a121175",
            "4be183ef6e44473fb97d44bab0374e7a",
            "f2ad0dca15da4fa39021e8db89521d31",
            "2c141ef253e64545a475b123c27a60f8",
            "bd55623dbdb84549b70edefe93a7858f",
            "26456bf3ad494b8d84c99fd82d72cb23",
            "64946756c43c4ca2a39d0987436e8563",
            "8b1918fc8c0d4545bab4065264d07f88",
            "05628cca64724512a072462933dcb6c5",
            "0220baabd5314c7d82f700572c61abfd",
            "68843cfcb26b44988ae39d4988bd0212",
            "09aaf85e49b6422db28afb386a57beeb",
            "d1bb92a2e68444ac9372e0af6e06e73b",
            "63c0a5d07f8d46b18df6409e022f7ffd",
            "f658aa08b0324b41be2949cb2fec0bca",
            "3aa4e7a6f4a843ecb2b36706916e7c07",
            "ec4ef3b2f983497f83af8d7c59b54b84"
          ]
        },
        "id": "oXvikWU_jMvE",
        "outputId": "5d6b423c-a8ab-4d6a-fcf9-ec085571c85e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "14785647ab53476a8df37c0248409816"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bc52edb4d21c4bc0a2413144ac2c84ef"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/481 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4ef1b5942c214f13a32b43dea33f6148"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/627M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "64946756c43c4ca2a39d0987436e8563"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_1[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_roberta_model[0][0]']       \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 279)          143127      ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun May 22 16:14:00 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P0    39W / 300W |   1683MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "948/948 [==============================] - 338s 342ms/step - loss: 14.0863 - accuracy: 0.0601 - val_loss: 12.3520 - val_accuracy: 0.2363\n",
            "Epoch 2/5\n",
            "948/948 [==============================] - 324s 342ms/step - loss: 11.6994 - accuracy: 0.3038 - val_loss: 10.8027 - val_accuracy: 0.3765\n",
            "Epoch 3/5\n",
            "948/948 [==============================] - 325s 343ms/step - loss: 10.4706 - accuracy: 0.4188 - val_loss: 10.0036 - val_accuracy: 0.4489\n",
            "Epoch 4/5\n",
            "948/948 [==============================] - 325s 343ms/step - loss: 9.6225 - accuracy: 0.5011 - val_loss: 9.4889 - val_accuracy: 0.4893\n",
            "Epoch 5/5\n",
            "948/948 [==============================] - 324s 342ms/step - loss: 8.9609 - accuracy: 0.5642 - val_loss: 9.0330 - val_accuracy: 0.5249\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_3[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_roberta_model[1][0]']       \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 279)          143127      ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.5249406175771971\n",
            "Weighted F1: 0.4669459381369149\n",
            "Micro F1: 0.5249406175771971\n",
            "Weighted Precision: 0.4533716796579258\n",
            "Micro Precision: 0.5249406175771971\n",
            "Weighted Recall: 0.5249406175771971\n",
            "Micro Recall: 0.5249406175771971\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_1[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_roberta_model[0][0]']       \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 279)          143127      ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun May 22 16:42:56 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P0    38W / 300W |  15237MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "948/948 [==============================] - 338s 342ms/step - loss: 14.2077 - accuracy: 0.0483 - val_loss: 12.5190 - val_accuracy: 0.2031\n",
            "Epoch 2/5\n",
            "948/948 [==============================] - 324s 342ms/step - loss: 11.7660 - accuracy: 0.2841 - val_loss: 10.7833 - val_accuracy: 0.3860\n",
            "Epoch 3/5\n",
            "948/948 [==============================] - 324s 342ms/step - loss: 10.4330 - accuracy: 0.4190 - val_loss: 9.9491 - val_accuracy: 0.4561\n",
            "Epoch 4/5\n",
            "948/948 [==============================] - 324s 342ms/step - loss: 9.5955 - accuracy: 0.4921 - val_loss: 9.4030 - val_accuracy: 0.5119\n",
            "Epoch 5/5\n",
            "948/948 [==============================] - 324s 342ms/step - loss: 8.9348 - accuracy: 0.5600 - val_loss: 8.9355 - val_accuracy: 0.5404\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_3[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_roberta_model[1][0]']       \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 279)          143127      ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.5403800475059383\n",
            "Weighted F1: 0.4855655092755515\n",
            "Micro F1: 0.5403800475059383\n",
            "Weighted Precision: 0.4747350112716036\n",
            "Micro Precision: 0.5403800475059383\n",
            "Weighted Recall: 0.5403800475059383\n",
            "Micro Recall: 0.5403800475059383\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_1[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_roberta_model[0][0]']       \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 279)          143127      ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun May 22 17:11:49 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P0    38W / 300W |  15237MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "948/948 [==============================] - 339s 343ms/step - loss: 14.2616 - accuracy: 0.0347 - val_loss: 12.8408 - val_accuracy: 0.1485\n",
            "Epoch 2/5\n",
            "948/948 [==============================] - 324s 342ms/step - loss: 11.8701 - accuracy: 0.2647 - val_loss: 10.7747 - val_accuracy: 0.3800\n",
            "Epoch 3/5\n",
            "948/948 [==============================] - 325s 343ms/step - loss: 10.4596 - accuracy: 0.4141 - val_loss: 9.9241 - val_accuracy: 0.4489\n",
            "Epoch 4/5\n",
            "948/948 [==============================] - 324s 342ms/step - loss: 9.5894 - accuracy: 0.4841 - val_loss: 9.3441 - val_accuracy: 0.4869\n",
            "Epoch 5/5\n",
            "948/948 [==============================] - 324s 342ms/step - loss: 8.9161 - accuracy: 0.5469 - val_loss: 8.9036 - val_accuracy: 0.5190\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_3[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_roberta_model[1][0]']       \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 279)          143127      ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.5190023752969121\n",
            "Weighted F1: 0.4585546627788163\n",
            "Micro F1: 0.5190023752969121\n",
            "Weighted Precision: 0.4453258516599919\n",
            "Micro Precision: 0.5190023752969121\n",
            "Weighted Recall: 0.5190023752969121\n",
            "Micro Recall: 0.5190023752969121\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_1[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_roberta_model[0][0]']       \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 279)          143127      ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun May 22 17:41:14 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P0    37W / 300W |  15237MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "948/948 [==============================] - 339s 342ms/step - loss: 13.9477 - accuracy: 0.0849 - val_loss: 12.1189 - val_accuracy: 0.2755\n",
            "Epoch 2/5\n",
            "948/948 [==============================] - 324s 342ms/step - loss: 11.5239 - accuracy: 0.3243 - val_loss: 10.6567 - val_accuracy: 0.4252\n",
            "Epoch 3/5\n",
            "948/948 [==============================] - 324s 342ms/step - loss: 10.3559 - accuracy: 0.4380 - val_loss: 9.8662 - val_accuracy: 0.4727\n",
            "Epoch 4/5\n",
            "948/948 [==============================] - 324s 342ms/step - loss: 9.5374 - accuracy: 0.5102 - val_loss: 9.3547 - val_accuracy: 0.5202\n",
            "Epoch 5/5\n",
            "948/948 [==============================] - 324s 342ms/step - loss: 8.8951 - accuracy: 0.5720 - val_loss: 8.9076 - val_accuracy: 0.5238\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_3[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_roberta_model[1][0]']       \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 279)          143127      ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.5237529691211401\n",
            "Weighted F1: 0.4706290299931975\n",
            "Micro F1: 0.5237529691211401\n",
            "Weighted Precision: 0.4579382788408916\n",
            "Micro Precision: 0.5237529691211401\n",
            "Weighted Recall: 0.5237529691211401\n",
            "Micro Recall: 0.5237529691211401\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_1[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_roberta_model[0][0]']       \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 279)          143127      ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun May 22 18:10:09 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P0    38W / 300W |  15237MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "948/948 [==============================] - 339s 342ms/step - loss: 14.1896 - accuracy: 0.0509 - val_loss: 12.5204 - val_accuracy: 0.2280\n",
            "Epoch 2/5\n",
            "948/948 [==============================] - 324s 342ms/step - loss: 11.8202 - accuracy: 0.2947 - val_loss: 10.8644 - val_accuracy: 0.3872\n",
            "Epoch 3/5\n",
            "948/948 [==============================] - 324s 342ms/step - loss: 10.5454 - accuracy: 0.4139 - val_loss: 10.0233 - val_accuracy: 0.4608\n",
            "Epoch 4/5\n",
            "948/948 [==============================] - 324s 342ms/step - loss: 9.7020 - accuracy: 0.4878 - val_loss: 9.5641 - val_accuracy: 0.4869\n",
            "Epoch 5/5\n",
            "948/948 [==============================] - 324s 342ms/step - loss: 9.0271 - accuracy: 0.5522 - val_loss: 9.0790 - val_accuracy: 0.5059\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_3[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_roberta_model[1][0]']       \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 279)          143127      ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,182,487\n",
            "Trainable params: 125,182,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Accuracy: 0.505938242280285\n",
            "Weighted F1: 0.45043112697321525\n",
            "Micro F1: 0.505938242280285\n",
            "Weighted Precision: 0.4486629486029227\n",
            "Micro Precision: 0.505938242280285\n",
            "Weighted Recall: 0.505938242280285\n",
            "Micro Recall: 0.505938242280285\n",
            "Average Accuracy: 0.5228028503562946\n",
            "Average Weighted F1: 0.46642525343153907\n",
            "Average Micro F1: 0.5228028503562946\n",
            "Average Weighted Precision: 0.4560067540066671\n",
            "Average Micro Precision: 0.5228028503562946\n",
            "Average Weighted Recall: 0.5228028503562946\n",
            "Average Micro Recall: 0.5228028503562946\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "total_accuracy=0\n",
        "total_weighted_f1=0\n",
        "total_micro_f1=0\n",
        "total_weighted_precision=0\n",
        "total_micro_precision=0\n",
        "total_weighted_recall=0\n",
        "total_micro_recall=0\n",
        "\n",
        "for i in range(5):\n",
        "  gc.collect()\n",
        "  tf.keras.backend.clear_session()\n",
        "  dbert_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "  dbert_model = TFRobertaModel.from_pretrained('roberta-base')\n",
        "  max_len=512\n",
        "  sentences=summarized_data['text']\n",
        "  labels=summarized_data['label']\n",
        "  len(sentences),len(labels)\n",
        "  model_0=create_model()\n",
        "  input_ids=[]\n",
        "  attention_masks=[]\n",
        "\n",
        "  for sent in sentences:\n",
        "    dbert_inps=dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n",
        "    input_ids.append(dbert_inps['input_ids'])\n",
        "    attention_masks.append(dbert_inps['attention_mask'])\n",
        "  input_ids=np.asarray(input_ids)\n",
        "\n",
        "  attention_masks=np.array(attention_masks)\n",
        "  labels=np.array(labels)\n",
        "  train_inp,val_inp,train_label,val_label,train_mask,val_mask=train_test_split(input_ids,labels,attention_masks,test_size=0.1,random_state=42)\n",
        "  log_dir='dbert_model'\n",
        "\n",
        "  model_save_path='./drive/MyDrive/Summarization/roberta-summarization-512-'+str(i)+'-279labels.h5'\n",
        "\n",
        "  loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "  accuracy = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
        "  callbacks= [tf.keras.callbacks.ModelCheckpoint(filepath=model_save_path,monitor='val_accuracy',mode='max',save_best_only=True,save_weights_only=True),keras.callbacks.TensorBoard(log_dir=log_dir)]\n",
        "  model_0.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])\n",
        "  gpu_info = !nvidia-smi\n",
        "  gpu_info = '\\n'.join(gpu_info)\n",
        "  if gpu_info.find('failed') >= 0:\n",
        "    print('Not connected to a GPU')\n",
        "  else:\n",
        "    print(gpu_info)\n",
        "  history=model_0.fit([train_inp,train_mask],train_label,batch_size=8,epochs=5,validation_data=([val_inp,val_mask],val_label),callbacks=callbacks)\n",
        "  pred_labels=[]\n",
        "\n",
        "  model_saved= create_model()\n",
        "  model_saved.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])\n",
        "  model_saved.load_weights('./drive/MyDrive/Summarization/roberta-summarization-512-'+str(i)+'-279labels.h5')\n",
        "\n",
        "  for i in range(0,len(val_inp)):\n",
        "    pred=model_saved.predict([val_inp[i].reshape(1,512),val_mask[i].reshape(1,512)])\n",
        "    pred_label = pred.argmax(axis=1)\n",
        "    pred_labels.append(pred_label)\n",
        "  accuracy=accuracy_score(val_label, pred_labels)\n",
        "  print(\"Accuracy: \"+str(accuracy))\n",
        "  total_accuracy=total_accuracy+accuracy\n",
        "  \n",
        "  weighted_f1=f1_score(val_label,pred_labels, average='weighted')\n",
        "  print(\"Weighted F1: \"+ str(weighted_f1))\n",
        "  total_weighted_f1=total_weighted_f1+weighted_f1\n",
        "  micro_f1=f1_score(val_label,pred_labels, average='micro')\n",
        "  print(\"Micro F1: \"+ str(micro_f1))\n",
        "  total_micro_f1=total_micro_f1+micro_f1\n",
        "\n",
        "  weighted_precision=precision_score(val_label, pred_labels, average='weighted')\n",
        "  print(\"Weighted Precision: \" + str(weighted_precision))\n",
        "  total_weighted_precision=total_weighted_precision+weighted_precision\n",
        "  micro_precision=precision_score(val_label, pred_labels, average='micro')\n",
        "  print(\"Micro Precision: \" + str(micro_precision))\n",
        "  total_micro_precision=total_micro_precision+micro_precision\n",
        "\n",
        "  weighted_recall=recall_score(val_label, pred_labels, average='weighted')\n",
        "  print(\"Weighted Recall: \" + str(weighted_recall))\n",
        "  total_weighted_recall=total_weighted_recall+weighted_recall\n",
        "  micro_recall=recall_score(val_label, pred_labels, average='micro')\n",
        "  print(\"Micro Recall: \" + str(micro_recall))\n",
        "  total_micro_recall=total_micro_recall+micro_recall\n",
        "\n",
        "\n",
        "print(\"Average Accuracy: \"+str(total_accuracy/5))\n",
        "print(\"Average Weighted F1: \"+str(total_weighted_f1/5))\n",
        "print(\"Average Micro F1: \"+str(total_micro_f1/5))\n",
        "print(\"Average Weighted Precision: \"+str(total_weighted_precision/5))\n",
        "print(\"Average Micro Precision: \"+str(total_micro_precision/5))\n",
        "print(\"Average Weighted Recall: \"+str(total_weighted_recall/5))\n",
        "print(\"Average Micro Recall: \"+str(total_micro_recall/5))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "machine_shape": "hm",
      "name": "RoBERTa_Summarization-512_279labels.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "14785647ab53476a8df37c0248409816": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d01f7c6c595f4c529c66f2c49085e014",
              "IPY_MODEL_4203249f59264bed9013f8565a41657f",
              "IPY_MODEL_f3d3605e346441cf97d2eb90616c109c"
            ],
            "layout": "IPY_MODEL_0d8de003d90c475f821e8f1693df2480"
          }
        },
        "d01f7c6c595f4c529c66f2c49085e014": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8cac040264f14a1a8dcfba223baa7aa8",
            "placeholder": "​",
            "style": "IPY_MODEL_bc09dec8d74f49258a554640940274b5",
            "value": "Downloading: 100%"
          }
        },
        "4203249f59264bed9013f8565a41657f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d7da6fab2b24f9bb777e2646032675b",
            "max": 898823,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5e03b191d6c14f24b28d15ab5fc96561",
            "value": 898823
          }
        },
        "f3d3605e346441cf97d2eb90616c109c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b6676bcb452c4dc1900c6e40fdd1b57f",
            "placeholder": "​",
            "style": "IPY_MODEL_053d5cce62904c4ab0dd01a382c3c01c",
            "value": " 878k/878k [00:00&lt;00:00, 725kB/s]"
          }
        },
        "0d8de003d90c475f821e8f1693df2480": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8cac040264f14a1a8dcfba223baa7aa8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc09dec8d74f49258a554640940274b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3d7da6fab2b24f9bb777e2646032675b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e03b191d6c14f24b28d15ab5fc96561": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b6676bcb452c4dc1900c6e40fdd1b57f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "053d5cce62904c4ab0dd01a382c3c01c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bc52edb4d21c4bc0a2413144ac2c84ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d46cd059830446ca823375343abae7cc",
              "IPY_MODEL_baea01fd506241aea188e7a667d24fa5",
              "IPY_MODEL_3a50534a9cf84ec691ade65fa5a35d9b"
            ],
            "layout": "IPY_MODEL_0c2e75448e674197b4aad9a999fc8174"
          }
        },
        "d46cd059830446ca823375343abae7cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1167192bafc24efcac4ed522dc12105b",
            "placeholder": "​",
            "style": "IPY_MODEL_ade6231b8aac4863a7c01eeb0722aba8",
            "value": "Downloading: 100%"
          }
        },
        "baea01fd506241aea188e7a667d24fa5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_808663cb530142999cbecebe31617339",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8b8a8f77e04f4c379e8700aad4ccf277",
            "value": 456318
          }
        },
        "3a50534a9cf84ec691ade65fa5a35d9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_362baf73afb34985bbcffe7363c41822",
            "placeholder": "​",
            "style": "IPY_MODEL_a07fedd4ab27443baf63273285b54a7b",
            "value": " 446k/446k [00:00&lt;00:00, 590kB/s]"
          }
        },
        "0c2e75448e674197b4aad9a999fc8174": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1167192bafc24efcac4ed522dc12105b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ade6231b8aac4863a7c01eeb0722aba8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "808663cb530142999cbecebe31617339": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b8a8f77e04f4c379e8700aad4ccf277": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "362baf73afb34985bbcffe7363c41822": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a07fedd4ab27443baf63273285b54a7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4ef1b5942c214f13a32b43dea33f6148": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_712f7cce9ff4452ea3f68136183b3ee6",
              "IPY_MODEL_31f3ad72a4804e39bacd6e13a14d0421",
              "IPY_MODEL_e362a3c6e0eb43ffb42031aba1487e98"
            ],
            "layout": "IPY_MODEL_90036ffd63c34956879d5636fa6f8e46"
          }
        },
        "712f7cce9ff4452ea3f68136183b3ee6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6da3b80269334b57add634149a121175",
            "placeholder": "​",
            "style": "IPY_MODEL_4be183ef6e44473fb97d44bab0374e7a",
            "value": "Downloading: 100%"
          }
        },
        "31f3ad72a4804e39bacd6e13a14d0421": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f2ad0dca15da4fa39021e8db89521d31",
            "max": 481,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2c141ef253e64545a475b123c27a60f8",
            "value": 481
          }
        },
        "e362a3c6e0eb43ffb42031aba1487e98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bd55623dbdb84549b70edefe93a7858f",
            "placeholder": "​",
            "style": "IPY_MODEL_26456bf3ad494b8d84c99fd82d72cb23",
            "value": " 481/481 [00:00&lt;00:00, 16.1kB/s]"
          }
        },
        "90036ffd63c34956879d5636fa6f8e46": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6da3b80269334b57add634149a121175": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4be183ef6e44473fb97d44bab0374e7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f2ad0dca15da4fa39021e8db89521d31": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c141ef253e64545a475b123c27a60f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bd55623dbdb84549b70edefe93a7858f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26456bf3ad494b8d84c99fd82d72cb23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "64946756c43c4ca2a39d0987436e8563": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8b1918fc8c0d4545bab4065264d07f88",
              "IPY_MODEL_05628cca64724512a072462933dcb6c5",
              "IPY_MODEL_0220baabd5314c7d82f700572c61abfd"
            ],
            "layout": "IPY_MODEL_68843cfcb26b44988ae39d4988bd0212"
          }
        },
        "8b1918fc8c0d4545bab4065264d07f88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_09aaf85e49b6422db28afb386a57beeb",
            "placeholder": "​",
            "style": "IPY_MODEL_d1bb92a2e68444ac9372e0af6e06e73b",
            "value": "Downloading: 100%"
          }
        },
        "05628cca64724512a072462933dcb6c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_63c0a5d07f8d46b18df6409e022f7ffd",
            "max": 657434796,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f658aa08b0324b41be2949cb2fec0bca",
            "value": 657434796
          }
        },
        "0220baabd5314c7d82f700572c61abfd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3aa4e7a6f4a843ecb2b36706916e7c07",
            "placeholder": "​",
            "style": "IPY_MODEL_ec4ef3b2f983497f83af8d7c59b54b84",
            "value": " 627M/627M [00:09&lt;00:00, 73.0MB/s]"
          }
        },
        "68843cfcb26b44988ae39d4988bd0212": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09aaf85e49b6422db28afb386a57beeb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1bb92a2e68444ac9372e0af6e06e73b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "63c0a5d07f8d46b18df6409e022f7ffd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f658aa08b0324b41be2949cb2fec0bca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3aa4e7a6f4a843ecb2b36706916e7c07": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec4ef3b2f983497f83af8d7c59b54b84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}